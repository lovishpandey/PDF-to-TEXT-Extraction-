{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Downloading https://files.pythonhosted.org/packages/93/f3/4fec7dabe8802ebec46141345bf714cd1fc7d93cb74ddde917e4b6d97d88/pdfminer.six-20201018-py3-none-any.whl (5.6MB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six) (2.1.0)\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six) (3.0.4)\n",
      "Requirement already satisfied: cryptography in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six) (2.7)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography->pdfminer.six) (1.12.3)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography->pdfminer.six) (1.0.1)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography->pdfminer.six) (1.12.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography->pdfminer.six) (2.19)\n",
      "Installing collected packages: pdfminer.six\n",
      "Successfully installed pdfminer.six-20201018\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/38/2f07f859344b26f7f21efc90544c64b1f001ecdb491f2246c766d0bfa940/PyMuPDF-1.18.16-cp37-cp37m-win_amd64.whl (5.4MB)\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.18.16\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdf2image in c:\\programdata\\anaconda3\\lib\\site-packages (1.16.0)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from pdf2image) (6.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##converting pdf to images\n",
    "\n",
    "import os, subprocess\n",
    "\n",
    "pdf_dir = r\"C:\\Users\\madarchod\\Desktop\\FreeL\\yele\"\n",
    "os.chdir(pdf_dir)\n",
    "\n",
    "pdftoppm_path = r\"C:\\Program Files (x86)\\poppler-0.68.0\\bin\\pdftoppm.exe\"\n",
    "\n",
    "for pdf_file in os.listdir(pdf_dir):\n",
    "\n",
    "    if pdf_file.endswith(\".pdf\"):\n",
    "\n",
    "        subprocess.Popen('\"%s\" -jpeg %s out' % (pdftoppm_path, pdf_file), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (6.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in c:\\programdata\\anaconda3\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from pytesseract) (6.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tesseract\n",
      "  Downloading https://files.pythonhosted.org/packages/8d/b7/c4fae9af5842f69d9c45bf1195a94aec090628535c102894552a7a7dbe6c/tesseract-0.1.3.tar.gz (45.6MB)\n",
      "Building wheels for collected packages: tesseract\n",
      "  Building wheel for tesseract (setup.py): started\n",
      "  Building wheel for tesseract (setup.py): finished with status 'done'\n",
      "  Created wheel for tesseract: filename=tesseract-0.1.3-cp37-none-any.whl size=45562576 sha256=54073a3793b64fbd68a60428e9645fee40e0a523b2ad226d3cc3e0c9db8a2051\n",
      "  Stored in directory: C:\\Users\\Madarchod\\AppData\\Local\\pip\\Cache\\wheels\\82\\1f\\d9\\24797b123379e4ea9511cf660835468b62dad609634cad2aba\n",
      "Successfully built tesseract\n",
      "Installing collected packages: tesseract\n",
      "Successfully installed tesseract-0.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (6.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in c:\\programdata\\anaconda3\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from pytesseract) (6.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tesseract in c:\\programdata\\anaconda3\\lib\\site-packages (0.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out-001.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148A1C23108>\n",
      "All of Statistics: A Concise Course in\n",
      "Statistical Inference\n",
      "\n",
      "Brief Contents\n",
      "\n",
      "1. INtrOdUCtON.....c..cccreeccnveaveaecavecnseneeraevenveasseneeaeceesevamenseens 11\n",
      "Part I Probability\n",
      "\n",
      "2, Probability...cccsssassssvssesesecevavsvevecvevsvveverevesesveseesevevesesoevees 21\n",
      "3s Randoin Variables scsieessssvsseesveveseveeuneecserveanseweserecswevsveswess 37\n",
      "A, Expectation sccssevecccssswenssercaeesescecesescaveresszaavesseseewssasxesust 69\n",
      "§, Equalities sssscsssvevecncassssacasacesnacssacceuaresexeste ciesereswxessesoaaees 85\n",
      "6. Convergence of Random Variables.............csssecscescecsseeceeeeesen 89\n",
      "\n",
      "Part II Statistical Inference\n",
      "7. Models, Statistical Inference and Learning...............ssecseceeees 105\n",
      "\n",
      "8. Estimating the CDF and Statistical Functionals\n",
      "\n",
      " \n",
      "\n",
      "9, The Bootstrap cccsssisvsssecvscevssevscsevesevsscesweaveesveeeveswresescwews 129\n",
      "10. Parametric Inference............sscessceccecsseeecceccssceceeseeceeceeoes 145\n",
      "11. Hypothesis Testing and p-values..............csececeesececseeeeeceeees 179\n",
      "12, Bayesian Inference sscsssvscssaszasersspesansansewsenravsavesncenenseen sans 205\n",
      "13. Statistical Decision Theory............cscccsecescesceeeeceecesceeseesees 227\n",
      "\n",
      "Part III Statistical Models and Methods\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-002.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CDAC8>\n",
      "14.\n",
      "\n",
      "15.\n",
      "\n",
      "16.\n",
      "\n",
      "17.\n",
      "\n",
      "18.\n",
      "\n",
      "19.\n",
      "\n",
      "20.\n",
      "\n",
      "21.\n",
      "\n",
      "22.\n",
      "\n",
      "23.\n",
      "\n",
      "24,\n",
      "\n",
      "25,\n",
      "\n",
      "Linear Regressions: cicsscsssssesvesswesswscecsvensaysvenvevsoesvevewenese 245\n",
      "Multivariate. Model Sscssisvsecarssssssoesssceazesnesoosesssavenersevess 269\n",
      "Inference about Independence.............ssccseessececesceceseeseeee 279\n",
      "Undirected Graphs and Conditional Independence............... 297\n",
      "Loglinear Models.............csecsecsecseccecseceececeeceecenseeseeceees 309\n",
      "Causal Inference............ccsecsecsscseceecseccecsecescesceesseeeeeeees 327\n",
      "Directed. Graphs .secssessssssecovsssvanessseenecsssevsssesessevesveaeess 343\n",
      "\n",
      "Nonparametric curve Estimation..............sccscsessceeseeceeeees 359\n",
      "\n",
      "Smoothing Using Orthogonal Functions................cseceeseeeees 393\n",
      "ClaSsifi Cat Ol yesesesssrevssssnserexemmrerenmmnesmrueeseeereens 425\n",
      "\n",
      "SCOCHASTIC PROCESSES: savascssaasassserssnsaserserennvenssesanacnaaeaeeres 473\n",
      "Simulation Methods............csccsscsecceccececesccscceseeseeesseeees 505\n",
      "\n",
      "Appendix Fundamental Concepts in Inference\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-003.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8388>\n",
      "Chapter 1\n",
      "\n",
      "Introduction\n",
      "\n",
      "The goal of this book is to provide a broad background in probability and\n",
      "statistics for students in statistics, computer science (especially data min-\n",
      "ing and machine learning), mathematics, and related disciplines. This book\n",
      "covers a much wider range of topics than a typical introductory text on\n",
      "mathematical statistics. It includes modern topics like nonparametric curve\n",
      "estimation, bootstrapping and classification, topics that are usually relegated\n",
      "to follow-up courses. The reader is assumed to know calculus and a little lin-\n",
      "ear algebra. No previous knowledge of probability and statistics is required.\n",
      "The text is suitable for advanced undergraduates and graduate students.\n",
      "\n",
      "Statistics, data mining and machine learning are all concerned with\n",
      "collecting and analyzing data. For some time, statistics research was con-\n",
      "ducted in statistics departments while data mining and machine learning re-\n",
      "search was conducted in computer science departments. Statisticians thought\n",
      "that computer scientists were reinventing the wheel. Computer scientists\n",
      "thought that statistical theory didn’t apply to their problems.\n",
      "\n",
      "Things are changing. Statisticians now recognize that computer scien-\n",
      "tists are making novel contributions while computer scientists now recognize\n",
      "the generality of statistical theory and methodology. Clever data mining al-\n",
      "gorithms are more scalable than statisticians ever though possible. Formal\n",
      "statistical theory is more pervasive than computer scientists had realized. All\n",
      "agree students who deal with the analysis of data should be well grounded\n",
      "in basic probability and mathematical statistics. Using fancy tools like neu-\n",
      "\n",
      "11\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-004.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148A1C23108>\n",
      "12\n",
      "\n",
      "CHAPTER 1. INTRODUCTION\n",
      "\n",
      "ral nets, boosting and support vector machines without understanding basic\n",
      "statistics is like doing brain surgery before knowing how to use a bandaid.\n",
      "But where can stu\n",
      "Nowhere. At least, that was my conclusion when my computer science col-\n",
      "leagues kept asking me:\n",
      "standing of modern statistics quickly?” The typical mathematical statistic\n",
      "course spends too muc\n",
      "methods, two dimensional integrals etc.) at the expense of covering modern\n",
      "\n",
      "ents learn basic probability and statistics quickly?\n",
      "“Where can I send my students to get a good a under-\n",
      "\n",
      "time on tedious and uninspiring topics (counting\n",
      "\n",
      "concepts (bootstrapping, curve estimation, graphical models etc.). So I set\n",
      "out to redesign our un\n",
      "matical statistics. This\n",
      "the main features of this book.\n",
      "\n",
      "1.\n",
      "\n",
      "lergraduate honors course on probability and mathe-\n",
      "book arose from that course. Here is a summary of\n",
      "\n",
      " \n",
      "\n",
      "The book is suital\n",
      "\n",
      "le for honors undergraduates in math, statistics and\n",
      "\n",
      "computer science as well as graduate students in computer science and\n",
      "other quantitative fields.\n",
      "\n",
      ". I cover advanced\n",
      "\n",
      "topics that are traditionally not taught in a first\n",
      "\n",
      "course. For example, nonparametric regression, bootstrapping, den-\n",
      "sity estimation and graphical models.\n",
      "\n",
      ". I have omitted topics in probability that do not play a central role\n",
      "\n",
      "in statistical inference. For example, counting methods are virtually\n",
      "\n",
      "absent.\n",
      "\n",
      ". In general, I try\n",
      "emphasizing concepts.\n",
      "\n",
      "to avoid belaboring tedious calculations in favor of\n",
      "\n",
      ". I cover nonparametric inference before parametric inference. This is the\n",
      "\n",
      "opposite of most statistics books but I believe it is the right way to do it.\n",
      "Parametric models are unrealistic and pedagogically unnatural. (How\n",
      "would we know the everything about the distribution except for one or\n",
      "\n",
      "two parameters?)\n",
      "\n",
      "I introduce statistical functionals and bootstrapping\n",
      "\n",
      " \n",
      "\n",
      "very early and students find this quite natural.\n",
      "\n",
      ". I abandon the usual “First Term = Probability” and “Second Term\n",
      "\n",
      "= Statistics” approach. Some students only take the first half and it\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-005.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CDB48>\n",
      "13\n",
      "\n",
      "would be a crime if they did not see any statistical theory. Furthermore,\n",
      "probability is more engaging when students can see it put to work in\n",
      "the context of statistics.\n",
      "\n",
      "7. The course moves very quickly and covers much material. My col-\n",
      "leagues joke that I cover all of statistics in this course and hence the\n",
      "title. The course is demanding but I have worked hard to make the ma-\n",
      "terial as intuitive as possible so that the material is very understandable\n",
      "despite the fast pace. Anyway, slow courses are boring.\n",
      "\n",
      "8. As Richard Feynman pointed out, rigor and clarity are not synony-\n",
      "mous. I have tried to strike a good balance. To avoid getting bogged\n",
      "down in uninteresting technical details, many results are stated without\n",
      "proof. The bibliographic references at the end of each chapter point\n",
      "the student to appropriate sources.\n",
      "\n",
      "9. On my website are files with R code which students can use for doing all\n",
      "the computing. However, the book is not tied to R and any computing\n",
      "language can be used.\n",
      "\n",
      "The first part of the text is concerned with probability theory, the formal\n",
      "language of uncertainty which is the basis of statistical inference. The basic\n",
      "problem that we study in probability is:\n",
      "\n",
      "Given a data generating process, what are the properties of the out-\n",
      "comes?\n",
      "\n",
      "The second part of the book is about statistical inference and its close cousins,\n",
      "data mining and machine learning. The basic problem of statistical inference\n",
      "is the inverse of probability:\n",
      "\n",
      "Given the outcomes, what can we say about the process that gener-\n",
      "ated the data?\n",
      "\n",
      "These ideas are illustrated in Figure 1.1. Prediction, classification, clus-\n",
      "tering and estimation are all special cases of statistical inference. Data anal-\n",
      "ysis, machine learning and data mining are various names given to the prac-\n",
      "tice of statistical inference, depending on the context. The second part of\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-006.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 CHAPTER 1. INTRODUCTION\n",
      "\n",
      "Probability\n",
      "\n",
      "a\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Observed data\n",
      "\n",
      "Data generating process\n",
      "\n",
      "   \n",
      "\n",
      "i ae\n",
      "\n",
      "Inference and Data Mining\n",
      "\n",
      "Figure 1.1: Probability and inference.\n",
      "\n",
      "the book contains one more chapter on probability that covers stochastic\n",
      "processes including Markov chains.\n",
      "\n",
      "I have drawn heavily on other books in many places. Most chapters\n",
      "contain a section called Bilbliographic Remarks which serves both to ac-\n",
      "knowledge my debt to other authors and to point readers to other useful\n",
      "references. I would especially like to mention the books by DeGroot and\n",
      "Schervish (2002) and Grimmett and Stirzaker (1982) from which I adapted\n",
      "many examples and excercises.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-007.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "Statistics/Data Mining Dictionary\n",
      "\n",
      "Statisticians and computer scientists often use different language for the\n",
      "same thing. Here is a dictionary that the reader may want to return to\n",
      "\n",
      "throughout the course.\n",
      "\n",
      " \n",
      "\n",
      "Statistics Computer Science Meaning\n",
      "estimation learning using data to estimate\n",
      "an unknown quantity\n",
      "classification supervised learning predicting a discrete Y from X € ¥\n",
      "clustering unsupervised learning putting data into groups\n",
      "data training sample (X1,Vi),---; (Xn, Xn)\n",
      "covariates features the X;,’s\n",
      "classifier hypothesis a map from covariates to outcomes\n",
      "hypothesis = subset of a parameter space O\n",
      "confidence interval — interval that contains unknown quantity\n",
      "with a prescribed frequency\n",
      "directed acyclic graph Bayes net multivariate distribution with\n",
      "\n",
      "Bayesian inference\n",
      "\n",
      "frequentist inference\n",
      "\n",
      "large deviation bounds\n",
      "\n",
      "Bayesian inference\n",
      "\n",
      "PAC learning\n",
      "\n",
      "specified conditional\n",
      "\n",
      "independence relations\n",
      "\n",
      "statistical methods for using data\n",
      "\n",
      "to update subjective beliefs\n",
      "\n",
      "statistical methods for producing\n",
      "\n",
      "point estimates and confidence intervals\n",
      "with guarantees on frequency behavior\n",
      "uniform bounds on probability of errors\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-008.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "16 CHAPTER 1. INTRODUCTION\n",
      "Notation\n",
      "Symbol Meaning\n",
      "R real numbers\n",
      "infrea f(x) infimum: the largest number y such that y < f(x) for allae A\n",
      "think of this as the minimum of f\n",
      "suPread (2) supremum: the smallest number y such that y > f(z) for all x € /\n",
      "think of this as the maximum of f\n",
      "n! 1) x (n—2)x+++x3x2x1\n",
      "(i) may\n",
      "T(a) Gamma function I ye te Udy\n",
      "w outcome\n",
      "Q sample space (set of outcomes)\n",
      "A event (subset of 2)\n",
      "I4(w) indicator function; 1 if w € A and 0 otherwise\n",
      "P(A) probability of event A\n",
      "|A number of points in set A\n",
      "Fx cumulative distribution function\n",
      "fx probability density (or mass) function\n",
      "XwF X has distribution F’\n",
      "Xof X has density f\n",
      "x4y X and Y have the same distribution\n",
      "X,.--5 Xn F sample of size n from F’\n",
      "a) standard Normal probability density\n",
      "od standard Normal distribution function\n",
      "Lee upper a quantile of N(0,1) ie. @-1(1— a)\n",
      "\n",
      "E(X) = frdF(z)\n",
      "E(r(X)) = fr(@)dF(x)\n",
      "V(X)\n",
      "\n",
      "Cov( X,Y)\n",
      "X1,.--)Xn\n",
      "n\n",
      "\n",
      "P:\n",
      "\n",
      "s.\n",
      "\n",
      "qm\n",
      "\n",
      "Xn & N(u,02)\n",
      "\n",
      "expected value (mean) of random variable X\n",
      "expected value (mean) of r(X)\n",
      "\n",
      "variance of random variable X\n",
      "\n",
      "covariance between X and Y\n",
      "\n",
      "data\n",
      "\n",
      "sample size\n",
      "\n",
      "convergence in probability\n",
      "\n",
      "convergence in distribution\n",
      "\n",
      "convergence in quadratic mean\n",
      "\n",
      "(X, — p)/on ~ N(0,1)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-009.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1E88>\n",
      "Notation Continued\n",
      "\n",
      " \n",
      "\n",
      "Symbol Meaning\n",
      "\n",
      "g statistical model; a set of distribution functions,\n",
      "density functions or regression functions\n",
      "\n",
      "0 parameter\n",
      "\n",
      "a estimate of parameter\n",
      "\n",
      "T(F) statistical functional (the mean, for example)\n",
      "\n",
      "L£,(0) likelihood function\n",
      "\n",
      "In = 0(an) Zn/Gn 0\n",
      "\n",
      "In =O(Gn) —|an/an| is bounded for large n\n",
      "\n",
      "Xn =op(an)  Xn/an—+0\n",
      "\n",
      "Xn = Op(an)\n",
      "\n",
      "|Xn/an| is bounded in probability for large n\n",
      "\n",
      "17\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-010.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "18 CHAPTER 1. INTRODUCTION\n",
      "\n",
      "Useful Math Facts\n",
      "\n",
      "f= ye Baltat ht.\n",
      "\n",
      "UE, = GE ford<r<i\n",
      "\n",
      "limp soo (1+ 4)” = e*\n",
      "\n",
      "The Gamma function is is defined by (a) = f[~ y**eYdy for a > 0. If\n",
      "\n",
      "a > 1 then [(a) = (a—1)P(a—1). If n is an integer then I(n) = (n—1)!.\n",
      "Some special values are: (1) = 1 and (1/2) = V7.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-011.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "Part I\n",
      "\n",
      "Probability\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-012.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E1D88>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-013.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "Chapter 2\n",
      "Probability\n",
      "\n",
      "2.1 Introduction\n",
      "\n",
      "Probability is the mathematical language for quantifying uncertainty. We\n",
      "can apply probability theory to a diverse set of problems, from coin flipping to\n",
      "the analysis of computer algorithms. The starting point is to specify sample\n",
      "space, the set of possible outcomes.\n",
      "\n",
      "2.2 Sample Spaces and Events\n",
      "\n",
      "The sample space Q, is the set of possible outcomes of an experiment.\n",
      "Points w in Q are called sample outcomes or realizations. Events are\n",
      "subsets of Q.\n",
      "\n",
      "Example 2.1 If we toss a coin twice thenQ = {HH, HT,TH,TT}. The event\n",
      "that the first toss is heads is A= {HH, HT}. @\n",
      "\n",
      "Example 2.2 Let w be the outcome of a measurement of some physical quan-\n",
      "tity, for example, temperature. Then Q = R = (—00, 00). The event that the\n",
      "measurement is larger than 10 but less than or equal to 23 is A = (10, 23].\n",
      "\n",
      "Example 2.3 If we toss a coin forever then the sample space is the infinite set\n",
      "Q= {uw = (W1,W2,W3,---,), Wi E {H,T}}.\n",
      "\n",
      "Let E be the event that the first head appears on the third toss. Then\n",
      "\n",
      "E= { (1,42, 08,.--5) : wi = T,we = T,w3 = H, w; € {H,T} for i> 3}. a\n",
      "\n",
      "21\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-014.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "22 CHAPTER 2. PROBABILITY\n",
      "\n",
      "Given an event A, let Ac = {w € Q; w ¢ A} denote the complement\n",
      "of A. Informally, A° can be read as “not A.” The complement of 2 is the\n",
      "empty set (. The union of events A and B is defined AU B= {wEQ; we\n",
      "Aorw € Borw € both} which can be thought of as “A or B.” If Aj, Ao,..-\n",
      "is a sequence of sets then\n",
      "\n",
      "00\n",
      "UA= {w €2: we A; for at least one i}.\n",
      "i=1\n",
      "\n",
      "The intersection of A and B is Af] B = {w € Q; w € A and w € B} read\n",
      "“A and B.” Sometimes we write A().B as AB. If Aj, Ao,... is a sequence\n",
      "of sets then\n",
      "\n",
      "eS\n",
      "N= (wee: w € A; for all i}.\n",
      "‘et\n",
      "\n",
      "Let A— B= {w: we A,w ¢ B}. If every element of A is also contained in\n",
      "B we write A C B or, equivalently, B D A. If A is a finite set, let |A| denote\n",
      "the number of elements in A. See Table 1 for a summary.\n",
      "\n",
      " \n",
      "\n",
      "Table 1. Sample space and events.\n",
      "2 sample space\n",
      "w outcome\n",
      "A event (subset of 2)\n",
      "|A| number of points in A (if A is finite)\n",
      "AC complement of A (not A)\n",
      "AUB union (A or B)\n",
      "Af)B or AB intersection(A and B)\n",
      "A-B set difference (points in A that are not in B)\n",
      "ACB set inclusion (A is a subset of or equal to B)\n",
      "0 null event (always false)\n",
      "2 true event (always true)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "We say that A;, Ao, ... are disjoint or are mutually exclusive if A;(] Aj =\n",
      "@ whenever i 4 j. For example, A; = [0,1), Ay = [1,2), A3 = [2,3),... are\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-015.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1EC8>\n",
      "2.3. PROBABILITY 23\n",
      "disjoint. A partition of Q is a sequence of disjoint sets A,,Ao,... such that\n",
      "US, Ai = &. Given an event A, define the indicator function of A by\n",
      "\n",
      "1 ifweA\n",
      "\n",
      "ale) = He ea) = { ifwé A.\n",
      "\n",
      "A sequence of sets Aj, Ao,... is monotone increasing if A; C A) C\n",
      "\n",
      "- and we define limp oo An = U2, Ai- A sequence of sets Aj, Ao,... i\n",
      "\n",
      "monotone decreasing if A; D A) > --- and then we define lim,-,., An\n",
      "(2, Ai. In either case, we will write A, — A.\n",
      "\n",
      "Example 2.4 Let Q = R and let A; = [0,1/i) fori =1,2,.... Then U2, Ai =\n",
      "(0,1) and (\\2, Ai = {0}. If instead we define A; = (0,1/i) then UX, Ai =\n",
      "(0,1) and 2, A: = 0.\n",
      "\n",
      "2.3 Probability\n",
      "\n",
      "We want to assign a real number P(A) to every event A, called the prob-\n",
      "ability of A. We also call P a probability distribution or a probability\n",
      "measure. To qualify as a probability, P has to satisfy three axioms:\n",
      "\n",
      " \n",
      "\n",
      "Definition 2.5 A function P that assigns a real number P(A) to each\n",
      "event A is a probability distribution or a probability measure if\n",
      "it satisfies the following three axioms:\n",
      "\n",
      "Axiom 1: P(A) > 0 for every A\n",
      "\n",
      "Axiom 2: P(Q) = 1\n",
      "\n",
      "Axiom 3: [f Aj, Ao,...\n",
      "\n",
      "P (U 4) = 5 *P(A).\n",
      "\n",
      "are disjoint then\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "There are many interpretations of P(A). The two common interpretations\n",
      "are frequencies and degrees of beliefs. In frequency interpretation, P(A) is\n",
      "\n",
      "It is not always pos-\n",
      "sible to assign a\n",
      "probability to every\n",
      "event A if the sam-\n",
      "ple space is large,\n",
      "such as the whole\n",
      "real line. Instead,\n",
      "we assign probabil-\n",
      "ities to a limited\n",
      "class of set called\n",
      "a o-field. See the\n",
      "technical appendix\n",
      "for details.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-016.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 CHAPTER 2. PROBABILITY\n",
      "\n",
      "the long run proportion of times that A is true in repetitions. For example,\n",
      "if we say that the probability of heads is 1/2, when mean that if we flip the\n",
      "coin many times then the proportion of times we get heads tends to 1/2 as\n",
      "the number of tosses increases. An infinitely long, unpredictable sequence of\n",
      "tosses whose limiting proportion tends to a constant is an idealization, much\n",
      "like the idea of a straight line in geometry. The degree-of-belief interpreta-\n",
      "tion is that P(A) measures an observer’s strength of belief that A is true.\n",
      "In either interpretation, we require that Axioms 1 to 3 hold. The difference\n",
      "in interpretation will not matter much until we deal with statistical infer-\n",
      "ence. There, the differing interpretations lead to two schools of inference:\n",
      "the frequentist and the Bayesian schools. We defer discussion until later.\n",
      "One can derive many properties of P from the axioms. Here are a few:\n",
      "\n",
      "PO) = 0\n",
      "ACB = P(A)<P(B)\n",
      "o< P(A) <1\n",
      "P(A°) = 1-P(A)\n",
      "AQ\\B=0 = P(AUB) =P(4) +P). (2.1)\n",
      "\n",
      "A less obvious property is given in the following Lemma.\n",
      "\n",
      "Lemma 2.6 For any events A and B, P(A\\ B) = P(A) + P(B) — P(AB).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "PROOF. Write AUB = (AB‘)U(AB)U(4°B) and note that these\n",
      "events are disjoint. Hence, making repeated use of the fact that P is ad-\n",
      "ditive for disjoint events, we see that\n",
      "\n",
      "P (4U2) P ((4B») Ute) '2) ))\n",
      ")\n",
      ")\n",
      "\n",
      "= P(AB) + sci + ee\n",
      "= P(AB*) + P(AB) + P(A°B) + P(AB) — P(AB)\n",
      "\n",
      "= P((4B) (4B) +P ((4°B) (4B) — P(AB)\n",
      "= P(A)+P(B)—P(AB). &\n",
      "\n",
      "Example 2.7 Two coin tosses. Let H, be the event that heads occurs on toss 1\n",
      "and let H» be the event that heads occurs on toss 2. If all outcomes are equally\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-017.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1E88>\n",
      "2.4. PROBABILITY ON FINITE SAMPLE SPACES 25\n",
      "\n",
      "likely, that is, P({H1, H2}) = P({Hi,T}) = P({Ti, He}) = P({N, B}) =\n",
      "1/4, then P(H, U Hz) = P(Ai) + P(e) — P(A Aa) = + 5-4 = 3/4.\n",
      "\n",
      "Theorem 2.8 (Continuity of Probabilities.) If 4, + A then P(A,) — P(A) as\n",
      "n— oo.\n",
      "\n",
      "PROOF. Suppose that A, is monotone increasing so that Ay C Ay C ---.\n",
      "Let A = limp An = U2, 4i- Define B, = Ai, Bo = {fw EQ: we\n",
      "Az,w ¢€ Ai}, Bs = {w EQ: w € Az,w ¢ Ao,w ¢ Ai},... It can be\n",
      "shown that B,, Bo,... are disjoint, A, = UL, Ai = UL, B; for each n and\n",
      "Ue, Bi = UE; Ai. (See excercise 1.) From Axiom 3,\n",
      "\n",
      "P(A,) =P (U a) =S PB)\n",
      "\n",
      "and hence, using Axiom 3 again,\n",
      "\n",
      "oo\n",
      "\n",
      "Jim P(A,) = Jim So P(B) = )OP(B) =P (U a.) =P(4). &\n",
      "\n",
      "i=1\n",
      "\n",
      "2.4 Probability on Finite Sample Spaces\n",
      "\n",
      "Suppose that the sample space 2 = {w1,...,Wn} is finite. For example,\n",
      "if we toss a die twice, then has 36 elements: 2 = {(i, 7); i,7 € {1,...6}}.\n",
      "If each outcome is equally likely, then P(A) = |A|/36 where |A| denotes the\n",
      "number of elements in A. The probability that the sum of the dice is 11 is\n",
      "2/36 since there are two outcomes that correspond to this event.\n",
      "\n",
      "In general, if Q is finite and if each outcome is equally likely, then\n",
      "\n",
      "_l\n",
      "\n",
      "Pa)= 5.\n",
      "\n",
      "which is called the uniform probability distribution. To compute prob-\n",
      "abilities, we need to count the number of points in an event A. Methods\n",
      "for counting points are called combinatorial methods. We needn’t delve into\n",
      "these in any great detail. We will, however, need a few facts from counting\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-018.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E19C8>\n",
      "26 CHAPTER 2. PROBABILITY\n",
      "\n",
      "theory that will be useful later. Given n objects, the number of ways of\n",
      "ordering these objects is n! = n(n —1)(n — 2)---3+2-1. For convenience, we\n",
      "define 0! = 1. We also define\n",
      "\n",
      "(:) 7 ae (2.2)\n",
      "\n",
      "read “n choose k”, which is the number of distinct ways of choosing k objects\n",
      "from n. For example, if we have a class of 20 people and we want to select a\n",
      "committee of 3 students, then there are\n",
      "\n",
      "!\n",
      "20 _ 20! _ 20 x 19 x 18 1140\n",
      "3 3!17! 3x2x1\n",
      "\n",
      "possible committees. We note the following properties:\n",
      "n n n n\n",
      "(c)- (2-2. (@)= (02):\n",
      "2.5 Independent Events\n",
      "\n",
      "If we flip a fair coin twice, then the probability of two heads is 3 x 3. We\n",
      "multiply the probabilities because we regard the two tosses as independent.\n",
      "The formal definition of independence is as follows.\n",
      "\n",
      " \n",
      "\n",
      "Definition 2.9 Two events A and B are independent if\n",
      "P(AB) = P(A)P(B) (2.3)\n",
      "and we write AIL B. A set of events {A;: i € I} is independent if\n",
      "\n",
      "P (0 4) =[[P(4)\n",
      "\n",
      "ier ies\n",
      "\n",
      "for every finite subset J of I.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Independence can arise in two distinct ways. Sometimes, we explicitly\n",
      "assume that two events are independent. For example, in tossing a coin\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-019.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CDAC8>\n",
      "2.5. INDEPENDENT EVENTS 27\n",
      "\n",
      "twice, we usually assume the tosses are independent which reflects the fact\n",
      "that the coin has no memory of the first toss. In other instances, we derive\n",
      "independence by verifying that P(AB) = P(A)P(B) holds. For example, in\n",
      "tossing a fair die, let A = {2,4,6} and let B = {1,2,3,4}. Then, A) B=\n",
      "{2,4}, P(AB) = 2/6 = P(A)P(B) = (1/2) x (2/3) and so A and B are\n",
      "independent. In this case, we didn’t assume that A and B are independent\n",
      "it just turned out that they were.\n",
      "\n",
      "Suppose that A and B are disjoint events, each with positive probabil-\n",
      "ity. Can they be independent? No. This follows since P(A)P(B) > 0 yet\n",
      "P(AB) = P(#) = 0. Except in this special case, there is no way to judge\n",
      "independence by looking a the sets in a Venn diagram.\n",
      "\n",
      "Example 2.10 Toss a fair coin 10 times. Let A = “at least one Head.” Let T;\n",
      "be the event that tails occurs on the j\" toss. Then\n",
      "\n",
      "P(A) = 1-P(A)\n",
      "= 1-—P(all tails)\n",
      "= 1-P(T:--T)\n",
      "= 1—P(%)P(2)---P(Tio) using independence\n",
      "\n",
      "1 10\n",
      "= 1-(=) ~.999. &\n",
      "(2)\n",
      "\n",
      "Example 2.11 Two people take turns trying to sink a basketball into a net.\n",
      "Person 1 succeeds with probability 1/3 while person 2 succeeds with probability\n",
      "1/4. What is the probability that person 1 succeeds before person 2? Let EB\n",
      "denote the event of interest. Let Aj be the event that the first success is\n",
      "by person 1 and that it occurs on trial number j. Note that A,, Ao,... are\n",
      "disjoint and that E = U7, Aj. Hence,\n",
      "\n",
      "Now, P(A;) = 1/3. Ay occurs if we have the sequence 1 misses, 2 misses, 1\n",
      "succeeds. This has probability P(A2) = (2/3)(3/4)(1/3) = (1/2)(1/3). Fol-\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-020.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n",
      "28 CHAPTER 2. PROBABILITY\n",
      "\n",
      "lowing this logic we see that P(A;) = (1/2))~1(1/3). Hence,\n",
      "Salvi” Tea7Tr\"* 8\n",
      "\n",
      "Ppe)=y-=(=) =2N°(=) =2.\n",
      "@=¥3(3) -32G) =3\n",
      "\n",
      "Here we used that fact that, if 0 <r <1 then ee ri=r*/(1—r). ©\n",
      "\n",
      " \n",
      "\n",
      "Summary of Independence\n",
      "1. Aand B are independent if P(AB) = P(A)P(B).\n",
      "2. Independence is sometimes assumed and sometimes derived.\n",
      "\n",
      "3. Disjoint events with positive probability are not independent.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "2.6 Conditional Probability\n",
      "\n",
      "Assuming that P(B) > 0, we define the conditional probability of A given\n",
      "that B has occurred as follows.\n",
      "\n",
      " \n",
      "\n",
      "Definition 2.12 If P(B) > 0 then the conditional probability of A\n",
      "\n",
      "given B is\n",
      "P(AB)\n",
      "\n",
      "P(AIB) = Som\n",
      "\n",
      " \n",
      "\n",
      ". (2.4)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Think of P(A|B) as the fraction of times A occurs among those in which\n",
      "B occurs. Here are some facts about conditional probabilities. For any fixed\n",
      "B such that P(B) > 0, P(-|B) is a probability i.e. it satisfies the three axioms\n",
      "of probability. In particular, P(A|B) > 0, P(Q|B) = 1 and if Aj, Ao,... are\n",
      "disjoint then P(U, Ai|B) = OX, P(Ai|B). But it is in general not true that\n",
      "P(A|BUC) = P(A|B) + P(A|C). The rules of probability apply to events\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-021.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3C48>\n",
      "2.6. CONDITIONAL PROBABILITY 29\n",
      "\n",
      "on the left of the bar. In general it is not the case that P(A|B) = P(B|A).\n",
      "People get this confused all the time. For example, the probability of spots\n",
      "given you have measles is 1 but the probability that you have measles given\n",
      "that you have spots is not 1. In this case, the difference between P(A|B) and\n",
      "P(B|A) is obvious but there are cases where it is less obvious. This mistake is\n",
      "made often enough in legal cases that it is sometimes called the prosecutor’s\n",
      "fallacy.\n",
      "\n",
      "Example 2.13 A medical test for a disease D has outcomes + and —. The\n",
      "probabilities are:\n",
      "\n",
      " \n",
      "\n",
      "+} .0081 .0900\n",
      "— | .0090 .9010\n",
      "\n",
      "From the definition of conditional probability, P(+|D) = P(+,D)/P(D) =\n",
      ".0081 /(.0081 + .0009) = 9 and P(—|D°) = P(—, D*)/P(D*) = .9010/(.9010 +\n",
      "0900) = .9. Apparently, the test is fairly accurate. Sick people yield a\n",
      "positive 90 percent of the time and healthy people yield a negative about 90\n",
      "percent of the time. Suppose you go for a test and get a positive. What is the\n",
      "probability you have the disease? Most people answer .90. The correct answer\n",
      "is P(D|+) = P(+, D)/P(+) = .0081/(.0081 + .0900) = .08. The lesson here is\n",
      "that you need to compute the answer numerically. Don’t trust your intuition.\n",
      "|\n",
      "\n",
      "If A and B are independent events then\n",
      "\n",
      "P(AB) _ P(A)P(B)\n",
      "P(A|B) = 777) 17) P(A).\n",
      "So another interpretation of independence is that knowing B doesn’t change\n",
      "the probability of A.\n",
      "From the definition of conditional probability we can write P(AB) =\n",
      "P(A|B)P(B) and also P(AB) = P(B|A)P(A). Often, these formulae give us\n",
      "a convenient way to compute P(AB) when A and B are not independent.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-022.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891699B48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 CHAPTER 2. PROBABILITY\n",
      "\n",
      "Example 2.14 Draw two cards from a deck, without replacement. Let A be the\n",
      "event that the first draw is Ace of Clubs and let B be the event that the second\n",
      "draw is Queen of Diamonds. Then P(A, B) = P(A)P(B|A) = (1/52)x (1/51).\n",
      "|\n",
      "\n",
      " \n",
      "\n",
      "Summary of Conditional Probability\n",
      "\n",
      "1. If P(B) > 0 then\n",
      "P(AIB) = =\n",
      "\n",
      "2. P(-|B) satisfies the axioms of probability, for fixed B. In general, P(A|-)\n",
      "does not satisfy the axioms of probability, for fixed A.\n",
      "\n",
      "3. In general, P(A|B) # P(B|A).\n",
      "\n",
      "4. Aand B are independent if and only if P(A|B) = P(B).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "2.7 Bayes’ Theorem\n",
      "\n",
      "Bayes’ theorem is a useful result that is the basis of “expert systems” and\n",
      "“Bayes’ nets.” First, we need a preliminary result.\n",
      "\n",
      "Theorem 2.15 (The Law of Total Probability.) Let A,,...,A, be a partition\n",
      "of Q. Then, for any event B, P(B) = xe, P(BIA)P(A,)-\n",
      "\n",
      "Proor. Define C; = BA; and note that C,,...,C; are disjoint and that\n",
      "B= Ula Cj. Hence,\n",
      "\n",
      " \n",
      "\n",
      "P(B) = S7P(C;) = PBA) = PBIA)P(A))\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-023.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E19C8>\n",
      "2.8. BIBLIOGRAPHIC REMARKS 31\n",
      "\n",
      "since P(BA;) = P(B|A;)P(A;) from the definition of conditional probability.\n",
      "|\n",
      "\n",
      "Theorem 2.16 (Bayes’ Theorem.) Let Ai,..., Ax be a partition of Q such that\n",
      "P(A;) > 0 for each i. If P(B) > 0 then, for eachi=1,...,k,\n",
      "\n",
      " \n",
      "\n",
      "P(B\\A,)P(Ai) z\n",
      "P(Ai|B) = PPA) (2.5)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Remark 2.17 We call P(A,) the prior probability of A and P(A,|B) the\n",
      "posterior probability of A.\n",
      "\n",
      "PrRooF. We apply the definition of conditional probability twice, followed\n",
      "by the law of total probability:\n",
      "\n",
      "PUB) = PAB) _PBIAIP(A) __ PUBLA)P(A)\n",
      "\n",
      "PCB) PB) ‘3S PUBIA)PUG)\n",
      "\n",
      " \n",
      "\n",
      "Example 2.18 I divide my email into three categories: A, = “spam,” Ay =\n",
      "‘low priority” and A; = “high priority.” From previous experience I find that\n",
      "P(A) = .7, P(A2) = .2 and P(A3) = .1. Of course, .7+.2+.1=1. Let B be\n",
      "the event that the email contains the word “free.” From previous experience,\n",
      "P(B|A;) = 9, P(B\\Ag) = 01, P(B|A,) = 01. (Note: .9+.01+ 0141.) I\n",
      "receive an email with the word “free.” What is the probability that it is spam?\n",
      "Bayes’ theorem yields,\n",
      "9x7\n",
      "\n",
      "P(Ai|B) = (ox aFWix 24x >\n",
      "\n",
      " \n",
      "\n",
      "2.8 Bibliographic Remarks\n",
      "\n",
      "The material in this chapter is standard. Details can be found in any\n",
      "number of books. At the introductory level, there is DeGroot and Schervish\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-024.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3C48>\n",
      "32 CHAPTER 2. PROBABILITY\n",
      "\n",
      "(2002), at the intermediate level, Grimmett and Stirzaker (1982) and Karr\n",
      "(1993), and at the advanced level, Billingsley (1979) and Breiman (1968). I\n",
      "adapted many examples and problems from DeGroot and Schervish (2002)\n",
      "and Grimmett and Stirzaker (1982).\n",
      "\n",
      "2.9 Technical Appendix\n",
      "\n",
      "Generally, it is not feasible to assign probabilities to all subsets of a sample\n",
      "space 2. Instead, one restricts attention to a set of events called a c-algebra\n",
      "or a o-field which is a class A that satisfies:\n",
      "\n",
      "(i) DEA,\n",
      "\n",
      "(ii) if 41, Ap,...,€ A then UZ, A; € A and\n",
      "\n",
      "(iii) A € A implies that Ac € A.\n",
      "\n",
      "The sets in A are said to be measurable. We call (2,A) a measurable\n",
      "space. If P is a probability measure defined on A then (Q, A, P) is called a\n",
      "probability space. When 2 is the real line, we take A to be the smallest\n",
      "o-field that contains all the open subsets, which is called the Borel o-field.\n",
      "\n",
      "2.10 Excercises\n",
      "\n",
      "1. Fill in the details of the proof of Theorem 2.8. Also, prove the monotone\n",
      "decreasing case.\n",
      "\n",
      "2. Prove the statements in equation (2.1).\n",
      "\n",
      "3. Let Q be a sample space and let A,, Ao,..., be events. Define B, =\n",
      "US, Ai and C, = (2, Ai.\n",
      "(a) Show that B, D By, D +++ and that C; C By C++.\n",
      "\n",
      "(b) Show that w € ()°2, B, if and only if w belongs to an infinite\n",
      "number of the events Aj, A»,....\n",
      "\n",
      "(c) Show that w € U%, C, if and only if w belongs to all the events\n",
      "Aj, Ag,... except possibly a finite number of those events.\n",
      "\n",
      "4. Let {A; : i € I} be a collection of events where J is an arbitrary index\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-025.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n",
      "2.10.\n",
      "\n",
      "10.\n",
      "\n",
      "EXCERCISES 33\n",
      "\n",
      "set. Show that\n",
      "\n",
      "(Us) - (4s and (q+) - U4\n",
      "\n",
      "ier ier ier ier\n",
      "\n",
      "Hint: First prove this for J = {1,...,n}.\n",
      "\n",
      ". Suppose we toss a fair coin until we get exactly two heads. Describe\n",
      "\n",
      "the sample space S. What is the probability that exactly k tosses are\n",
      "required?\n",
      "\n",
      ". Let 2 = {0,1,..., }. Prove that there does not exist a uniform distri-\n",
      "\n",
      "bution on Q ie. if P(A) = P(B) whenever |A| = |B| then P cannot\n",
      "satisfy the axioms of probability.\n",
      "\n",
      ". Let Aj, Ao,... be events. Show that\n",
      "\n",
      "P (U 4) < STP (An).\n",
      "\n",
      "Hint: Define B, = A, — U2] Ai. Then show that the B, are disjoint\n",
      "\n",
      "i=\n",
      "\n",
      "and that Up. An = Un2a Bn-\n",
      "\n",
      ". Suppose that P(A;) = 1 for each i. Prove that\n",
      "\n",
      "P (A+) a1\n",
      "\n",
      ". For fixed B such that P(B) > 0, show that P(-|B) satisfies the axioms\n",
      "\n",
      "of probability.\n",
      "\n",
      "‘You have probably heard it before. Now you can solve it rigorously.\n",
      "It is called the “Monty Hall Problem.” A prize is placed at random\n",
      "between one of three doors. You pick a door. To be concrete, let’s\n",
      "suppose you always pick door 1. Now Monty Hall chooses one of the\n",
      "other two doors, opens it and shows you that it is empty. He then gives\n",
      "you the opportunity to keep your door or switch to the other unopened\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-026.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E1A08>\n",
      "34\n",
      "\n",
      "11.\n",
      "\n",
      "12.\n",
      "\n",
      "13.\n",
      "\n",
      "14.\n",
      "\n",
      "16.\n",
      "\n",
      "CHAPTER 2. PROBABILITY\n",
      "\n",
      "door. Should you stay or switch? Intuition suggests it doesn’t matter.\n",
      "The correct answer is that you should switch. Prove it. It will help to\n",
      "specify the sample space and the relevant events carefully. Thus write\n",
      "Q = {(wi, we): w; € {1,2,3}} where w; is where the prize is and wy is\n",
      "the door Monty opens.\n",
      "\n",
      "Suppose that A and B are independent events. Show that A° and B¢\n",
      "are independent events.\n",
      "\n",
      "There are three cards. The first is green on both sides, the second is\n",
      "red on both sides and the third is green on one side and red on the\n",
      "other. We choose a card at random and we see one side (also chosen\n",
      "at random). If the side we see is green, what is the probability that\n",
      "the other side is also green? Many people intuitively answer 1/2. Show\n",
      "that the correct answer is 2/3.\n",
      "\n",
      " \n",
      "\n",
      "Suppose that a fair coin is tossed repeatedly until both a head and tail\n",
      "have appeared at least once.\n",
      "\n",
      "(a) Describe the sample space 2.\n",
      "(b) What is the probability that three tosses will be required?\n",
      "Show that if P(A) = 0 or P(A) = 1 then A is independent of every\n",
      "\n",
      "other event. Show that if A is independent of itself then P(A) is either\n",
      "0 or 1.\n",
      "\n",
      ". The probability that a child has blue eyes is 1/4. Assume independence\n",
      "\n",
      "between children. Consider a family with 5 children.\n",
      "\n",
      "(a) If it is known that at least one child has blue eyes, what is the\n",
      "probability that at least three children have blue eyes?\n",
      "\n",
      "(b) If it is known that the youngest child has blue eyes, what is the\n",
      "probability that at least three children have blue eyes?\n",
      "\n",
      " \n",
      "\n",
      "Show that\n",
      "\n",
      " \n",
      "\n",
      "P(ABC) = P(A|BC)P(B|C)P(C).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-027.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "2.10. EXCERCISES 35\n",
      "\n",
      "17.\n",
      "\n",
      "18.\n",
      "\n",
      "19.\n",
      "\n",
      "20.\n",
      "\n",
      "21.\n",
      "\n",
      "Suppose k events form a partition of the sample space 2, i.e. they\n",
      "are disjoint and U_, A; = 2. Assume that P(B) > 0. Prove that if\n",
      "P(A,|B) < P(A;) then P(A;|B) > P(A;) for some i = 2,...,k.\n",
      "\n",
      "Suppose that 30 percent of computer owners use a Macintosh, 50 use\n",
      "Windows and 20 percent use Linux. Suppose that 65 percent of the Mac\n",
      "users have succumbed to a computer virus, 82 percent of the Windows\n",
      "users get the virus and 50 percent of the Linux users get the virus. We\n",
      "select a person at random and learn that her system was infected with\n",
      "the virus. What is the probability that she is a Windows user?\n",
      "\n",
      "A box contains 5 coins and each has a different probability of showing\n",
      "heads. Let pi,...,p5 denote the probability of heads on each coin.\n",
      "Suppose that\n",
      "\n",
      "Pr =0, po = 1/4, py = 1/2, pa = 3/4 and ps5 = 1.\n",
      "\n",
      "Let H denote “heads is obtained” and let C; denote the event that coin\n",
      "7 is selected.\n",
      "\n",
      "(a) Select a coin at random and toss it. Suppose a head is obtained.\n",
      "What is the posterior probability that coin i was selected (i = 1,...,5)?\n",
      "In other words, find P(C;|H) fori =1,...,5.\n",
      "\n",
      "(b) Toss the coin again. What is the probability of another head? In\n",
      "other words find P(H2|H,) where H; = “heads on toss j.”\n",
      "\n",
      "Now suppose that the experiment was carried out as follows. We select\n",
      "a coin at random and toss it until a head is obtained.\n",
      "\n",
      "(c) Find P(C;,|B,) where B, = “first head is obtained on toss 4.”\n",
      "\n",
      "(Computer Experiment.) Suppose a coin has probability p of falling\n",
      "heads. If we flip the coin many times, we would expect the proportion\n",
      "of heads to be near p. We will make this formal later. Take p= .3 and\n",
      "n = 1000 and simulate n coin flips. Plot the proportion of heads as a\n",
      "function of n. Repeat for p= .03.\n",
      "\n",
      "(Computer Experiment.) Suppose we flip a coin n times and let p denote\n",
      "the probability of heads. Let X be the number of heads. We call X\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-028.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1F08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "\n",
      "22.\n",
      "\n",
      "CHAPTER 2. PROBABILITY\n",
      "\n",
      "a binomial random variable which is discussed in the next chapter.\n",
      "Intuition suggests that X will be close to np. To see if this is true,\n",
      "we can repeat this experiment many times and average the X values.\n",
      "Carry out a simulation and compare the average of the X’s to np. Try\n",
      "this for p= .3 and n = 10, 100, 1000.\n",
      "\n",
      "(Computer Experiment.) Here we will get some experience simulating\n",
      "conditional probabilities. Consider tossing a fair die. Let A = {2, 4,6}\n",
      "and B = {1,2,3,4}. Then, P(A) = 1/2, P(B) = 2/3 and P(AB) = 1/3.\n",
      "Since P(AB) = P(A)P(B), the events A and B are independent. Simu-\n",
      "late draws from the sample space and verify that P(AB) = P(A)P(B)\n",
      "where P(A) is the proportion of times A occurred in the simulation\n",
      "and similarly for P(AB) and P(B). Now find two events A and B that\n",
      "are not independent. Compute P(A), P(B) and P(AB). Compare the\n",
      "calculated values to their theoretical values. Report your results and\n",
      "interpret.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-029.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "Chapter 3\n",
      "\n",
      "Random Variables\n",
      "\n",
      "3.1 Introduction\n",
      "\n",
      "Statistics and data mining are concerned with data. How do we link\n",
      "sample spaces and events to data? The link is provided by the concept of a\n",
      "random variable.\n",
      "\n",
      " \n",
      "\n",
      "Definition 3.1 A random variable is a mapping X : Q — R that\n",
      "assigns a real number X(w) to each outcome w.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Technically, a ran-\n",
      "At acertain point in most probability courses, the sample space is rarely dom variable must\n",
      "mentioned and we work directly with random variables. But you should keep be measurable. See\n",
      "\n",
      "in mind that the sample space is really there, lurking in the background. the _technical _ AP\n",
      "pendix for details.\n",
      "\n",
      "Example 3.2 Flip a coin ten times. Let X(w) be the number of heads in the\n",
      "sequence w. For ecample, ifw = HHTHHTHHTT then X(w)=6.\n",
      "\n",
      "Example 3.3 Let Q = {(x,y); a?+y? < 1} be the unit disc. Consider drawing\n",
      "a point “at random” from Q. (We will make this idea more precise later.) A\n",
      "typical outcome is of the form w = (a, y). Some examples of random variables\n",
      "are X(w) = 2, Yw)=y, Zw) =a+y, Ww) = Vo? +y?.\n",
      "\n",
      "Given a random variable X and asubset A of the real line, define X~'(A) =\n",
      "{w €Q: X(w) € A} and let\n",
      "\n",
      "37\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-030.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891699B48>\n",
      "38 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      " \n",
      "\n",
      "P(X71(A)) = P({w €Q; X(w) € A})\n",
      "P(X\"\"(2)) = P({w € 2; X(w) =2}).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "X denotes the random variable and x denotes a possible value of X.\n",
      "\n",
      " \n",
      "\n",
      "Example 3.4 Flip a coin twice and let X be the number of heads. Then,\n",
      "P(X = 0) = P({TT}) = 1/4, P(X = 1) = P({AT,TH}) = 1/2 and P(X =\n",
      "2) = P({HH}) = 1/4. The random variable and its distribution can be\n",
      "summarized as follows:\n",
      "\n",
      " \n",
      "\n",
      "w_ P((w)) |X) 1868 an)\n",
      "g O} 1/4\n",
      "1 1|1p\n",
      "> 2|1/4\n",
      "\n",
      " \n",
      "\n",
      "Try generalizing this to n flips. Ml\n",
      "\n",
      "3.2 Distribution Functions and Probability Func-\n",
      "tions\n",
      "\n",
      "Given a random variable X, we define an important function called the\n",
      "cumulative distribution function (or distribution function) in the following\n",
      "way.\n",
      "\n",
      " \n",
      "\n",
      "Definition 3.5 The cumulative distribution function cbr Fy : R—\n",
      "[0, 1] of a random variable X is defined by\n",
      "\n",
      "Fx (x) = P(X <2).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-031.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1EC8>\n",
      "3.2. DISTRIBUTION FUNCTIONS AND PROBABILITY FUNCTIONS39\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "——\n",
      "—_—_.\n",
      "2 x\n",
      "\n",
      "Figure 3.1: cpr for flipping a coin twice (Example 3.6.)\n",
      "\n",
      "‘You might wonder why we bother to define the CbF . You will see later\n",
      "that the CpF is a useful function: it effectively contains all the information\n",
      "about the random variable.\n",
      "\n",
      "Example 3.6 Flip a fair coin twice and let X be the number of heads. Then\n",
      "P(X =0) = P(X = 2) =1/4 and P(X =1)=1/2. The distribution function\n",
      "}S\n",
      "\n",
      "‘ 0 a <0\n",
      "\n",
      "1/4 0<a<1\n",
      "\n",
      "3/4 1<a<2\n",
      "\n",
      "1 @>2.\n",
      "\n",
      "Fx (az) =\n",
      "\n",
      "The cpr is shown in Figure 3.1. Although this example is simple, study\n",
      "it carefully. CDF ’s can be very confusing. Notice that the function is right\n",
      "continuous, non-decreasing and that it is defined for all x even though the\n",
      "random variable only takes values 0,1 and 2. Do you see why F(1.4) = .75?\n",
      "|\n",
      "\n",
      "The following result, which we do not prove, shows that the CDF com-\n",
      "pletely determines the distribution of a random variable.\n",
      "\n",
      "Theorem 3.7 Let X have cpr F and let Y have cor G. If F(x) = G(a)\n",
      "for alla then P(X € A) =P(Y € A) for all A.\n",
      "\n",
      "Theorem 3.8 A function F mapping the real line to [0,1] is a CDF for some\n",
      "probability measure P if and only if it satisfies the following three conditions:\n",
      "\n",
      "Technically, we only\n",
      "have that P(X €\n",
      "A) = P(Y € A)\n",
      "for every measurable\n",
      "event A.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-032.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1988>\n",
      "A set is countable\n",
      "if it is finite or\n",
      "it can be put in\n",
      "a one-to-one corre-\n",
      "spondence with the\n",
      "integers. The even\n",
      "numbers, the odd\n",
      "numbers and the ra-\n",
      "tionals are count-\n",
      "able; the set of real\n",
      "numbers between 0\n",
      "and 1 is not count-\n",
      "able.\n",
      "\n",
      "40 CHAPTER 3. RANDOM VARIABLES\n",
      "(i) F is non-decreasing i.e. £1 < x2 implies that F(a) < F(a2).\n",
      "(ii) F is normalized: lim,,-.~ F(x) = 0 and lim, F(x) = 1.\n",
      "(iii) F is right-continuous, i.e. F(x) = F(a*) for all x, where\n",
      "8\n",
      "F(a\") = limy2F'(y).\n",
      "Proor. Suppose that F is a CDF .\n",
      "be a real number and let y1, y2,... be a sequence of real numbers such that\n",
      "Yi > yo >--- and lim; y; = 2. Let A; = (—00, y;] and let A = (—0o,z]. Note\n",
      "that A = Me: A; and also note that A; > A, D ---. Because the events are\n",
      "monotone, lim; P(A;) = P((); Ai). Thus,\n",
      "\n",
      "Let us show that (iii) holds. Let x\n",
      "\n",
      "F(x) = 9 =?(N a) = = limP(A)) = lim F(yi) = F(a\").\n",
      "\n",
      "i\n",
      "Showing (i) and (ii) is similar. Proving the other direction namely, that if\n",
      "F satisfies (i), (ii) and (iii) then it is a CbF for some random variable, uses\n",
      "some deep tools in analysis. ll\n",
      "\n",
      " \n",
      "\n",
      "Definition 3.9 X is discrete if it takes countably many values\n",
      "\n",
      "{1, U2,-- }.\n",
      "\n",
      "We define the probability function or probability mass function\n",
      "for X by\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Thus, fx(x) > 0 for all a € R and 30; fx(a;) = 1. The cpr of X is\n",
      "telated to fx by\n",
      "DY ie\n",
      "\n",
      "a<r\n",
      "\n",
      "Fy@)=PX ez)=\n",
      "\n",
      "Sometimes we write fy and Fy simply as f and F.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-033.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1EC8>\n",
      "3.2. DISTRIBUTION FUNCTIONS AND PROBABILITY FUNCTIONS41\n",
      "\n",
      "fx(z)\n",
      "\n",
      "ar\n",
      "\n",
      "io\n",
      "oe\n",
      "°\n",
      "\n",
      " \n",
      "\n",
      "—\n",
      "\n",
      "0 1 2 x\n",
      "Figure 3.2: Probability function for flipping a coin twice (Example 3.6.)\n",
      "\n",
      " \n",
      "\n",
      "Example 3.10 The probability function for Example 3.6 is\n",
      "\n",
      "1/4 «=0\n",
      "fee)=4 aig baa\n",
      "0 2x ¢ {0,1,2}.\n",
      "\n",
      "See Figure 3.2. @\n",
      "\n",
      " \n",
      "\n",
      "Definition 3.11 A random variable X is continuous if there exists a\n",
      "function fx such that fx(a) > 0 for all x, [°. fx(x)dx = 1 and for\n",
      "every a <b,\n",
      "\n",
      "Pa<X <b)= [ seoee. (3.1)\n",
      "\n",
      "The function fx is called the probability density function (PDF ).\n",
      "We have that\n",
      "\n",
      "Fx (2) = [ fx (dt\n",
      "\n",
      "and fx(x) = FX.(a) at all points « at which Fx is differentiable.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Sometimes we shall write f f(a)dz or simply f f to mean f°. f(«)de.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-034.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "42 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "0 1 x\n",
      "Figure 3.3: Cpr for Uniform (0,1).\n",
      "\n",
      "Example 3.12 Suppose that X has PDF\n",
      "\n",
      "1 for0O<a<1\n",
      "0 otherwise.\n",
      "\n",
      "x(a) = {\n",
      "\n",
      "Clearly, fx(x) > 0 and Jf fx(x)dx = 1. A random variable with this density\n",
      "is said to have a Uniform (0,1) distribution. The CpF is given by\n",
      "\n",
      "0 «2<0\n",
      "Fy(t)={ « 0<a<l\n",
      "L ¢S 1.\n",
      "\n",
      "See Figure 3.3. @\n",
      "\n",
      "Example 3.13 Suppose that X has PDF\n",
      "\n",
      "fa)={? forx <0\n",
      "\n",
      "aa otherwise.\n",
      "Since f f(x)dx =1, this is a well-defined por .\n",
      "\n",
      "Warning! Continuous random variables can lead to confusion. First,\n",
      "note that if X is continuous then P(X = a) = 0 for every x! Don’t try to\n",
      "think of f(x) as P(X = x). This only holds for discrete random variables.\n",
      "We get probabilities from a ppF by integrating. A PDF can be bigger than\n",
      "1 (unlike a mass function). For example, if f(x) = 5 for x € [0,1/5] and 0\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-035.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148A1C23048>\n",
      "3.2. DISTRIBUTION FUNCTIONS AND PROBABILITY FUNCTIONS43\n",
      "\n",
      "otherwise, then f(x) > 0 and f f(x)dx = 1 so this is a well-defined por\n",
      "even though f(z) = 5 in some places. In fact, a PDF can be unbounded.\n",
      "For example, if f(z) = (2/3)x~!8 for 0 < « < 1 and f(z) = 0 otherwise,\n",
      "then f f(x)dx = 1 even though f is not bounded.\n",
      "\n",
      "Example 3.14 Let\n",
      "\n",
      "0 fora <0\n",
      "f@)= +— otherwise\n",
      "ay .\n",
      "\n",
      "This is not appr since f f(x)dx = f>° dx/(1+ 2) = JP du/u=log(oo) =\n",
      "co.\n",
      "\n",
      "Lemma 3.15 Let F be the cbF for a random variable X. Then:\n",
      "(i) P(X = 2) = F(a) — F(a-) where F(a~) = limyze F(y),\n",
      "(ii) P(e < X <y) = Fly) - F(a),\n",
      "\n",
      "(iti) P(X > 2) =1- F(a),\n",
      "(iv) If X is continuous then\n",
      "\n",
      "P(a< X <b) =P(a< X <b) =Pa<X <b) =P(a<X <0).\n",
      "\n",
      "It is also useful to define the inverse CDF (or quantile function).\n",
      "\n",
      " \n",
      "\n",
      "Definition 3.16 Let X be a random variable with cor F. The inverse\n",
      "CDF or quantile function is defined by\n",
      "\n",
      "FQ = inf {2 : F(a) < a}\n",
      "\n",
      "for q € [0,1]. If F is strictly increasing and continuous then F-\\(q) is\n",
      "the unique real number x such that F(x) = q.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "If you are unfamil-\n",
      "We call F-1(1/4) the first quartile, F-1(1/2) the median (or second quar- iar with “inf”, just\n",
      "\n",
      "tile) and F-'(3/4) the third quartile. think of it as the\n",
      "minimum.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-036.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3508>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "Two random variables X and Y are equal in distribution - written\n",
      "X £Y — if Fy() = Fy() for all x. This does not mean that X and Y are\n",
      "equal. Rather, it means that all probability statements about X and Y will\n",
      "be the same.\n",
      "\n",
      "3.3. Some Important Discrete Random Vari-\n",
      "ables\n",
      "\n",
      "Warning About Notation! It is traditional to write X ~ F to indicate\n",
      "that X has distribution F. This is unfortunate notation since the symbol ~\n",
      "is also used to denote an approximation. The notation X ~ F is so pervasive\n",
      "that we are stuck with it. Read X ~ F as “X has distribution F” not as X\n",
      "is approximately F.\n",
      "\n",
      "Tue Point Mass DistriBuTION. X has a point mass distribution at\n",
      "a, written X ~ 6a, if P(X = a) = 1 in which case\n",
      "0 uw<a\n",
      "ra={9 756\n",
      "\n",
      "The probability function is f(x) = 1 for « =a and 0 otherwise.\n",
      "\n",
      "THE DISCRETE UNIFORM DISTRIBUTION. Let k > 1 be a given integer.\n",
      "Suppose that X has probability mass function given by\n",
      "\n",
      "fa) = U/k forc=1,...,k\n",
      "\n",
      "0 otherwise.\n",
      "\n",
      "We say that X has a uniform distribution on {1,..., k}.\n",
      "\n",
      "THE BERNOULLI DISTRIBUTION. Let X represent a coin flip. Then\n",
      "P(X = 1) =pand P(X = 0) =1—> for some p € (0, 1]. We say that X has\n",
      "a Bernoulli distribution written X ~ Bernoulli(p). The probability function\n",
      "is f(z) = p*(1—p)'~* for a € {0,1}.\n",
      "\n",
      "THE BINOMIAL DISTRIBUTION. Suppose we have a coin which falls\n",
      "heads with probability p for some 0 < p< 1. Flip the coin n times and let\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-037.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907A7088>\n",
      "3.3. SOME IMPORTANT DISCRETE RANDOM VARIABLES 45\n",
      "\n",
      "X be the number of heads. Assume that the tosses are independent. Let\n",
      "f(x) = P(X =2) be the mass function. It can be shown that\n",
      "\n",
      "fe)= { (p= p)* fora =0,....n\n",
      "\n",
      "0 otherwise.\n",
      "\n",
      "A random variable with the mass function is called a Binomial random\n",
      "variable and we write X ~ Binomial(n,p). If X, ~ Binomial(n,p,) and\n",
      "X_ ~ Binomial(n, pz) then X; + X2 ~ Binomial(n, p, + p2).\n",
      "\n",
      "Warning! Let us take this opportunity to prevent some confusion. X\n",
      "is a random variable; z denotes a particular value of the random variable;\n",
      "n and p are parameters, that is, fixed real numbers. The parameter p is\n",
      "usually unknown and must be estimated from data; that’s what statistical\n",
      "inference is all about. In most statistical models, there are random variables\n",
      "and parameters: don’t confuse them.\n",
      "\n",
      "Tue GEOMETRIC DisTRIBUTION. X has a geometric distribution with\n",
      "parameter p € (0,1), written X ~ Geom(p), if\n",
      "\n",
      "P(X =k)=p—p)s, ke.\n",
      "\n",
      "We have that\n",
      "\n",
      "ES ES Pe\n",
      "DPX =k) = PP (0-9) =T=G2p 7!\n",
      "\n",
      "Think of X as the number of flips needed until the first heads when flipping\n",
      "a coin.\n",
      "\n",
      "THE Polsson DISTRIBUTION. X has a Poisson distribution with pa-\n",
      "rameter A, written X ~ Poisson() if\n",
      "\n",
      "Note that\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-038.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8708>\n",
      "46 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "The Poisson is often used as a model for counts of rare events like radioactive\n",
      "decay and traffic accidents. If X, ~ Poisson(n, A) and X2 ~ Poisson(n, A2)\n",
      "then X, + X2 ~ Poisson(n, A; + A2).\n",
      "\n",
      "Warning! We defined random variables to be mappings from a sample\n",
      "space 2 to R but we did not mention the sample space in any of the distri-\n",
      "butions above. As I mentioned earlier, the sample space often “disappears”\n",
      "but it is really there in the background. Let’s construct a sample space ex-\n",
      "plicitly for a Bernoulli random variable. Let 2 = [0,1] and define P to satisfy\n",
      "P((a, 6]) = b—a for0 <a <b< 1. Fix pe [0,1] and define\n",
      "\n",
      "xw)={\n",
      "\n",
      "Then P(X = 1) = PW < p) = P((0,p]) = p and P(X = 0) =1-p. Thus,\n",
      "X ~ Bernoulli(p). We could do this for all the distributions defined above.\n",
      "In practice, we think of a random variable like a random number but formally\n",
      "it is a mapping defined on some sample space.\n",
      "\n",
      "3.4 Some Important Continuous Random Vari-\n",
      "ables\n",
      "\n",
      "lw<p\n",
      "0 w>p.\n",
      "\n",
      "THE UNIFORM DIsTRIBUTION. X has a Uniform(a, 6) distribution, writ-\n",
      "ten X ~ Uniform(a, 8), if\n",
      "\n",
      "—, fors €[a,)]\n",
      "—) Fa ’\n",
      "f(a) = { 0 otherwise\n",
      "\n",
      "where a < b. The distribution function is\n",
      "\n",
      "0 L<G\n",
      "F@)=4 = wela,b]\n",
      "1 wi.b.\n",
      "\n",
      "NORMAL (GaussIAN). X has a Normal (or Gaussian) distribution with\n",
      "parameters js and a, denoted by X ~ N(y,07), if\n",
      "\n",
      "f(x) = on {-sa(e-1), ceER\n",
      "\n",
      " \n",
      "\n",
      "oV2n\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-039.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "3.4. SOME IMPORTANT CONTINUOUS RANDOM VARIABLES AT\n",
      "\n",
      "where  € R and o > 0. Later we shall see that ys is the “center” (or\n",
      "mean) of the distribution and o is the “spread” (or standard deviation) of\n",
      "the distribution. The Normal plays an important role in probability and\n",
      "statistics. Many phenomena in nature have approximately Normal distribu-\n",
      "tions. Later, we shall see that the distribution of a sum of random variables\n",
      "can be approximated by a Normal distribution (the central limit theorem).\n",
      "\n",
      "We say that X has astandard Normal distribution if = 0 anda = 1.\n",
      "Tradition dictates that a standard Normal random variable is denoted by Z.\n",
      "The ppF and Cpr of a standard Normal are denoted by ¢(z) and ®(z).\n",
      "The ppr is plotted in Figure 3.4. There is no closed-form expression for ®.\n",
      "Here are some useful facts:\n",
      "\n",
      "(i) If X ~ N(p,07) then Z = (X - p)/o ~ N(0,1).\n",
      "(ii) If Z ~ N(0,1) then X = w+ oZ ~ N(p, 07).\n",
      "\n",
      "(iii) If X; ~ N(y;,0?), i= 1,-..,n are independent then\n",
      "wx ~y (Dm De).\n",
      "a1 i=1 i=1\n",
      "\n",
      "It follows from (i) that if X ~ N(y, 0?) then\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Thus we can compute any probabilities we want as long as we can compute\n",
      "the cpr &(z) of a standard Normal. All statistical computing packages will\n",
      "compute ©(z) and @-!(z). All statistics texts, including this one, have a\n",
      "table of values of ®(z).\n",
      "\n",
      "Example 3.17 Suppose that X ~ N(3,5). Find P(X > 1). The solution is\n",
      "\n",
      " \n",
      "\n",
      "1-3\n",
      "V5\n",
      "\n",
      " \n",
      "\n",
      "P(X > 1)=1-P(x <1)=1-P(Z< ) = 1= 8(-0.8048) = 1.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-040.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891699B48>\n",
      "48 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "—2 -l1 0 1 2 =\n",
      "\n",
      "Figure 3.4: Density of a standard Normal.\n",
      "\n",
      "Now find q such that P(X < q) =.2. In other words, find q = ®-'(.2). We\n",
      "solve this by writing\n",
      "\n",
      "2=P(X<q)=P(2< 44) -9 (24),\n",
      "\n",
      "o o\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "From the Normal table, &(—.8416) = .2. Therefore,\n",
      "\n",
      "- 3\n",
      "—s4ig- 20d?\n",
      "oO\n",
      "\n",
      "  \n",
      "\n",
      "and hence q = 3 — 8416/5 = 1.1181.\n",
      "\n",
      "EXPONENTIAL DISTRIBUTION. X has an Exponential distribution with\n",
      "parameter 3, denoted by X ~ Exp(@), if\n",
      "\n",
      "fle) = sei, 2>0\n",
      "\n",
      "where 3 > 0. The exponential distribution is used to model the lifetimes of\n",
      "electronic components and the waiting times between rare events.\n",
      "\n",
      "GAMMA DISTRIBUTION. For a > 0, the Gamma function is defined\n",
      "by ['(a) = fo? y**e¥dy. X has a Gamma distribution with parameters a\n",
      "and 8, denoted by X ~ Gamma(a, §), if\n",
      "\n",
      "1\n",
      "12) = Bara)\n",
      "\n",
      "ge te tl 4 >0\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-041.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8708>\n",
      "3.4. SOME IMPORTANT CONTINUOUS RANDOM VARIABLES 49\n",
      "\n",
      "where a, 3 > 0. The exponential distribution is just a Gamma(1, 9) distribu-\n",
      "\n",
      "tion. If X; ~ Gamma(a,, 8) are independent, then )>\"_, X; ~ Gamma(}>” 0%, B)\n",
      "i=1 % P)-\n",
      "\n",
      "THE BETA DISTRIBUTION. X has an Beta distribution with parameters\n",
      "a>0and 6 > 0, denoted by X ~ Beta(a, 8), if\n",
      "ce (1—2)P!, O<a<l.\n",
      "\n",
      "f(a) = Pahl\n",
      "\n",
      "¢t AND Caucuy DistTrRiBuTION. X has a t distribution with v degrees of\n",
      "freedom — written X ~ t, —if\n",
      "\n",
      "The ¢ distribution is similar to a Normal but it has thicker tails. In fact, the\n",
      "Normal corresponds to a ¢ with vy = oo. The Cauchy distribution is a special\n",
      "case of the ¢ distribution corresponding to v = 1. The density is\n",
      "\n",
      "1\n",
      "f= aw\n",
      "\n",
      "To see that this is indeed a density, let’s do the integral:\n",
      "\n",
      "8 1? de 1 f® dtan\n",
      "[teu TI olte TJ &\n",
      "\n",
      " \n",
      "\n",
      "= Feo -eartooo) = FR (-B)) =\n",
      "\n",
      "THE y? DISTRIBUTION. X has a x? distribution with p degrees of freedom\n",
      "~ written X ~ xp - if\n",
      "\n",
      "1\n",
      "= (p/2)—1e-a/2\n",
      "f@) Tea” ees 0\n",
      "If Z,..., Z are independent standard Normal random variables then )7?_, Z? ~\n",
      "\n",
      "Xp\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-042.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E1888>\n",
      "50 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "3.5 Bivariate Distributions\n",
      "\n",
      "Given a pair of discrete random variables X and Y, define the joint\n",
      "mass function by f(x,y) = P(X =a and Y = y). From now on, we write\n",
      "P(X =a and Y =y)asP(X =2,Y =y). We write f as fx, when we want\n",
      "to be more explicit.\n",
      "\n",
      "Example 3.18 Here is a bivariate distribution for two random variables X\n",
      "and Y each taking values 0 or 1:\n",
      "\n",
      "Y= Y =I\n",
      "X=0/1/9 2/9 [173\n",
      "X=1| 2/9 4/9 | 1/3\n",
      "73 173 [1\n",
      "\n",
      "Thus, P(X =1,Y =1)=f(1,1)=4/9. @\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Definition 3.19 In the continuous case, we call a function f(x,y) a pdf\n",
      "for the random variables (X,Y) if (i) f(x,y) > 0 for all (x,y), (ii)\n",
      "is {ef een = 1 and, for any set AC RXR, P((X,Y) €\n",
      "\n",
      "=f a a, y)dxdy. In the discrete or continuous case we define the\n",
      "ot cpF as Fy y(z,y)=P(X <2,Y <y).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 3.20 Let (X,Y) be uniform on the unit square. Then,\n",
      "\n",
      "1 if0<2<1,0<y<l\n",
      "0 otherwise.\n",
      "\n",
      "s,0)= {\n",
      "\n",
      "FindP(X <1/2,Y < 1/2). The event A= {X <1/2,Y <1/2} corresponds\n",
      "to a subset of the unit square. Integrating f over this subset corresponds, in\n",
      "this case, to computing the area of the set A which is 1/4. So, P(X <\n",
      "1/2,Y <1/2)=1/4.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-043.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5. BIVARIATE DISTRIBUTIONS 51\n",
      "\n",
      "Example 3.21 Let (X,Y) have density\n",
      "\n",
      "ab if0<a<1,0<y<1\n",
      "f(x,y) { y a5 y\n",
      "Then\n",
      "\n",
      "0 otherwise.\n",
      "1 1\n",
      "/ | (x + y)dxdy\n",
      "0 0\n",
      "\n",
      "1 1\n",
      "LUfee|of [fy rele\n",
      "0 0\n",
      "which verifies that this is a PDF. ll\n",
      "\n",
      "eS\n",
      "\n",
      "1\n",
      "[ xv+ ydy= 5+\n",
      "\n",
      "Example 3.22 If the distribution is defined over a non-rectangular region, then\n",
      "the calcuations are a bit more complicated. Here is an nice erample which I\n",
      "borrowed from DeGroot and Schervish (2002). Let (X,Y) have density\n",
      "ca’y ifa?<y<l\n",
      "Feu) = { 0 otherwise.\n",
      "Note first that -1 < «<1. Now let us find the value of c. The trick here\n",
      "is to be careful about the range of integration. We pick one variable, x say,\n",
      "and let it range over its values. Then, for each fixed value of x, we let y vary\n",
      "over its range which is 22 < y <1. It may help if you look at figure 3.5.\n",
      "\n",
      "Thus,\n",
      "1 1\n",
      "| [ senduar =e f | xy dy daz\n",
      "-1Ja?\n",
      ": ' jl-at 4c\n",
      "2 _ 2 _ de\n",
      "cf 2 [[ve]armefs 5 da =\n",
      "\n",
      "Hence, c = 21/4. Now let us compute P(X > Y). This corresponds to the\n",
      "set A = {(z,y);0 < a < 1,2? < y < a}. (You can see this by drawing a\n",
      "diagram.) So,\n",
      "\n",
      "PX SY) = tf ferme 3 [= lf vay] az\n",
      "1 9a? :\n",
      "\n",
      "at\n",
      "-ife ote= 5\n",
      "\n",
      "ll\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-044.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n",
      "52 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "xz\n",
      "\n",
      "Figure 3.5: The light shaded region is 2? < y < 1. The density is positive\n",
      "over this region. The hatched region is the event X > Y intersected with\n",
      "GPS pred,\n",
      "\n",
      "3.6 Marginal Distributions\n",
      "\n",
      " \n",
      "\n",
      "Definition 3.23 If (X,Y) have joint distribution with mass function fxy,\n",
      "then the marginal mass function for X is defined by\n",
      "\n",
      "fx(a) =P(X =2) = SOP(X=2,Y=9)=Sof@y) (3.3)\n",
      "\n",
      "and the marginal mass function for Y is defined by\n",
      "\n",
      "fry) =PW =9) = OPK =2,Y=y) = Posey). 34)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 3.24 Suppose that fx,y is given in the table that follows. The marginal\n",
      "distribution for X corresponds to the row totals and the marginal distribution\n",
      "for Y corresponds to the columns totals.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-045.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891699B48>\n",
      "3.6. MARGINAL DISTRIBUTIONS 53\n",
      "\n",
      "Y=0 Ysi\n",
      "0| 1/10 2/10 | 3/10\n",
      "1| 3/10 4/10 | 7/10\n",
      "4/10 6/10 |4\n",
      "For eample, fx(0) = 3/10 and fx(1) = 7/10.\n",
      "\n",
      " \n",
      "\n",
      "X=\n",
      "X=\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Definition 3.25 For continuous random variables, the marginal densities\n",
      "are\n",
      "\n",
      "ix) =f Hew), amd foy)= [ Hewde. 5)\n",
      "\n",
      "The corresponding marginal distribution functions are denoted by Fx\n",
      "and Fy.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 3.26 Suppose that\n",
      "\n",
      "fxy(a,y) =e @)\n",
      "\n",
      "for x,y >0. Then fx(x) = e~* fo° eYdy = e~*.\n",
      "Example 3.27 Suppose that\n",
      "\n",
      "_faty f0<¢<l1,0<y<l1\n",
      "F(a,9) = { 0 otherwise.\n",
      "\n",
      "Then 1 1 1\n",
      "1\n",
      "fw) = [ @+sae= f vac + f ydy= 5+. a\n",
      "0 0 0\n",
      "\n",
      "Example 3.28 Let (X,Y) have density\n",
      "\n",
      "a 8. - 2\n",
      "_ f a’y ifa?<y<l\n",
      "F(e.u)= { 0 otherwise.\n",
      "Thus,\n",
      "\n",
      "21 21\n",
      "fete) =f fleway= 2? [yay = 2020-2\")\n",
      "for -1<a<1 and fx(x) = 0 otherwise.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-046.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3C48>\n",
      "54 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "3.7 Independent Random Variables\n",
      "\n",
      " \n",
      "\n",
      "Definition 3.29 Two random variables X and Y are independent jf,\n",
      "for every A and B,\n",
      "\n",
      "P(X € A,Y € B) = P(X € A)P(Y € B). (3.6)\n",
      "\n",
      "We write X WY.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "In principle, to check whether X and Y are independent we need to check\n",
      "equation (3.6) for all subsets A and B. Fortunately, we have the following\n",
      "result which we state for continuous random variables though it is true for\n",
      "discrete random variables too.\n",
      "\n",
      "Theorem 3.30 Let X and Y have joint pdf fxy. Then X UY if and only if\n",
      "The statement is fxy(z,y) = fx(z)fy(y) for all values x and y.\n",
      "not rigorous be-\n",
      "cause the density is\n",
      "defined only up to\n",
      "\n",
      "sets of measure 0. Example 3.31 Let X and Y have the following distribution:\n",
      "\n",
      "Ye 0. Y = 1\n",
      "X-0|177 1/7 |i?\n",
      "X=1/1f4 1744 |172\n",
      "\n",
      "172 172 [a\n",
      "\n",
      "Then, fx(0) = fx(1) = 1/2 and fy(0) = fy(1) = 1/2. X and Y are inde-\n",
      "pendent because fx(0)fy(0) = f(0,0), fx fr) = £0), fe) fr(0) =\n",
      "f(1,0), fxQ)fy(@) = f(,1). Suppose instead that X and Y have the fol-\n",
      "lowing distribution:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Y=0 Y=1\n",
      "\n",
      "X=0[1/72 0 12\n",
      "\n",
      "X=1|0 1/72 | 1/2\n",
      "i 172 [4\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-047.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n",
      "3.7. INDEPENDENT RANDOM VARIABLES 55\n",
      "\n",
      "These are not independent because fx(0)fy(1) = (1/2)(1/2) = 1/4 yet\n",
      "f(0,1)=0. @\n",
      "\n",
      "Example 3.32 Suppose that X and Y are independent and both have the same\n",
      "density\n",
      "\n",
      "Qa if0<ar<l\n",
      "f@)= { 0 otherwise.\n",
      "\n",
      "Let us find P(X +Y <1). Using independence, the joint density is\n",
      "_ _ f 4ay if0<a<1, O<y<l\n",
      "F(t.9) = fx@)fv) = { 0 otherwise.\n",
      "Now,\n",
      "\n",
      "P(X+Y<1) = If F(a, y)dydx\n",
      "\n",
      "aty<1\n",
      "1\n",
      "\n",
      "- fell\n",
      "\n",
      "1 2\n",
      "_ (1 - a) _l\n",
      "= afi 5) dx =>. wl\n",
      "\n",
      " \n",
      "\n",
      "The following result is helpful for verifying independence.\n",
      "\n",
      "Theorem 3.33 Suppose that the range of X and Y is a (possibly infinite)\n",
      "rectangle. If f(x,y) = g(x)h(y) for some functions g and h (not necessarily\n",
      "probability density functions) then X and Y are independent.\n",
      "\n",
      "Example 3.34 Let X and Y have density\n",
      "\n",
      "Qe“) if a >0 and y>0\n",
      "f(#,y) = { 0 otherwise.\n",
      "\n",
      "The range of X andY is the rectangle (0,00) x(0, 00). We can write f (x,y) =\n",
      "g(x)h(y) where g(x) = 2e-* and h(y) =e\"¥. Thus, XUY. @\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-048.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1988>\n",
      "56 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "3.8 Conditional Distributions\n",
      "\n",
      "If X and Y are discrete, then we can compute the conditional distribution\n",
      "of X given that we have observed Y = y. Specifically, P(X = a|Y = y) =\n",
      "P(X = 2,Y = y)/P(Y = y). This leads us to define the conditional mass\n",
      "function as follows.\n",
      "\n",
      " \n",
      "\n",
      "Definition 3.35 The conditional probability mass function is\n",
      "\n",
      "fxyv(aly) = P(X =2|¥ = y) = — y) _ farlea)\n",
      "\n",
      " \n",
      "\n",
      "if fy(y) > 0.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "We are treading in For continuous distributions we use the same definitions. The interpre-\n",
      "deep water here. tation differs: in the discrete case, fx\\y(z|y) is P(X = 2|Y = y) but in the\n",
      "When we compute continuous case, we must integrate to get a probability.\n",
      "\n",
      "P(X € A|Y = y)\n",
      "in the continuous\n",
      "case we are condi-\n",
      "tioning on the event\n",
      "{Y = y} which has\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Definition 3.36 For continuous random variables, the conditional prob\n",
      "ability density function is\n",
      "\n",
      "probability 0. We £53\n",
      "avoid this problem Ixy@ly) = fee\n",
      "by defining things\n",
      "\n",
      "in terms of the assuming that fy(y) > 0. Then,\n",
      "\n",
      "PDF . The fact\n",
      "\n",
      "that this leads to a P(XeA\n",
      "\n",
      "well-defined theory\n",
      "\n",
      "is proved in more\n",
      "\n",
      "advanced courses.\n",
      "\n",
      "We simply take it\n",
      "\n",
      "as a definition. Example 3.37 Let X and Y have a uniform distribution on the unit square.\n",
      "Verify that fxjy (aly) = 1 for0 <a <1 and 0 otherwise. Thus, given Y = y,\n",
      "X is Uniform (0,1). We can write this as X|Y = y ~ Unif(0,1).\n",
      "\n",
      " \n",
      "\n",
      "Y=y)= [ few(olniae.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "From the definition of the conditional density, we see that fxy(x,y) =\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-049.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E92C8>\n",
      "3.8. CONDITIONAL DISTRIBUTIONS 57\n",
      "\n",
      "fxiy (aly) fy) = fyix(y|x)fx(). This can sometimes be useful as in the\n",
      "next example.\n",
      "\n",
      "Example 3.38 Let\n",
      "\n",
      "_faty if0<2<1,0<y<1\n",
      "f(a,y) = { 0 otherwise.\n",
      "\n",
      "Let us find P(X < 1/4[Y = 1/3). In example 3.27 we saw that fy(y) =\n",
      "y + (1/2). Hence,\n",
      "\n",
      "So,\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 3.39 Suppose that X ~ Unif(0,1). After obtaining a value of X we\n",
      "generate Y|X =a ~ Uniform(a,1). What is the marginal distribution of Y ?\n",
      "First note that,\n",
      "\n",
      "1 if0<a¢<1\n",
      "fx(z) = { 0 otherwise\n",
      "and 1 if\n",
      "_ fre if0<r<y<l\n",
      "frix(ylz) = { 0 otherwise.\n",
      "So,\n",
      "\n",
      "a if0<2r<y<1\n",
      "= . = Ts\n",
      "fx(@.y) = frxtule) fx) { 0 ° otherwise.\n",
      "\n",
      "The marginal for Y is\n",
      "\n",
      "-\n",
      "fw) = [' tevewar= f= - [=~ 1080-0)\n",
      "\n",
      "1-2\n",
      "\n",
      " \n",
      "\n",
      "forO<y<1.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-050.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1EC8>\n",
      "58 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "Example 3.40 Consider the density in Example 3.28. Let’s find fy\\x(y|\n",
      "When X = 2, y must satisfy x? < y < 1. Earlier, we saw that fx(z)\n",
      "(21/8)a°(1—a*). Hence, for x? <y <1,\n",
      "\n",
      "f(y) Bay 2y\n",
      "\n",
      "fyx(y\\a) = Fx(@) = ELT 7) “Tog\n",
      "\n",
      " \n",
      "\n",
      "Now let us compute P(Y > 3/4|X = 1/2). This can be computed by first\n",
      "noting that fy|x(y|1/2) = 32y/15. Thus,\n",
      "\n",
      ". 1 1 32y 7\n",
      "P(Y > 3/4|X =1/2)= | f(yl1/2)dy = v= ®\n",
      "3/4 3/4 19 a\n",
      "\n",
      "3.9 Multivariate Distributions and IID Samples\n",
      "\n",
      "Let X = (X1,...,X,) where X,,...,X,, are random variables. We call\n",
      "X a random vector. Let f(21,..-,%n) denote the pdf. It is possible to define\n",
      "their marginals, conditionals etc. much the same way as in the bivariate case.\n",
      "We say that X),...,X,, are independent if, for every Ai,...,An,\n",
      "\n",
      "n\n",
      "P(X, € Ai,..-,Xn © An) = | [ P(X € Ai)- (3.7)\n",
      "ot\n",
      "n\n",
      "It suffices to check that f(@1,..., 2%) = Tr fr,(ae). If X1,...,Xp are inde-\n",
      "pendent and each has the same marginal distribution with density f, we say\n",
      "that X,,...,X,, are 1p (independent and identically distributed). We shall\n",
      "write this as X,,...X, ~ f or, in terms of the cpr , X,,...X, ~ F. This\n",
      "means that X,,...,X, are independent draws from the same distribution.\n",
      "We also call X,,...,X, a random sample from F’.\n",
      "\n",
      "Much of statistical theory and practice begins with [Ip observations and\n",
      "\n",
      "we shall study this case in detail when we discuss statistics.\n",
      "\n",
      "3.10 Two Important Multivariate Distribu-\n",
      "tions\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-051.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3C48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10. TWO IMPORTANT MULTIVARIATE DISTRIBUTIONS 59\n",
      "\n",
      "MULTINOMIAL. The multivariate version of a Binomial is called a Multi-\n",
      "nomial. Consider drawing a ball from an urn which has balls with k different\n",
      "colors labeled color 1, color 2, ..., color k. Let p = (pi,...,px) where\n",
      "pj = 0 and Dye Pi = 1 and suppose that p; is the probability of drawing\n",
      "a ball of color j. Draw n times (independent draws with replacement) and\n",
      "let X = (Xj,--., Xx) where X; is the number of times that color j appears.\n",
      "Hence, n = Ya Xt: We say that X has a Multinomial (n,p) distribution\n",
      "written X ~ Multinomial(n,p). The probability function is\n",
      "\n",
      "fe)=(,.\" ,, )et oat (38)\n",
      "\n",
      "n _ n!\n",
      "@--.Up ay! -- ay!\n",
      "\n",
      "Lemma 3.41 Suppose that X ~ Multinomial(n,p) where X = (X1,...,X«)\n",
      "and p= (pi,---, Px). The marginal distribution of X; is Binomial (n,p;).\n",
      "\n",
      "where\n",
      "\n",
      "MULTIVARIATE NORMAL. The univariate Normal had two parameters,\n",
      "and o. In the multivariate version, js is a vector and a is replaced by a\n",
      "matrix 4. To begin, let\n",
      "\n",
      "“4\n",
      "Z=| 3\n",
      "Zk\n",
      "where Z,,...,Z, ~ N(0,1) are independent. The density of Z is If a and 6 are vec-\n",
      "\n",
      "‘ tors then a7b =\n",
      "\n",
      "1 1S 1 1 fs agbi.\n",
      "1e)= TL) = pene {223} = pepo {ae}\n",
      "We say that Z has a standard multivariate Normal distribution written Z ~\n",
      "N(0,I) where it is understood that 0 represents a vector of k zeroes and I\n",
      "is the k x k identity matrix.\n",
      "\n",
      "More generally, a vector X has a multivariate Normal distribution, de-\n",
      "noted by X ~ N(, 5), if it has density ZT js the inverse of\n",
      "\n",
      "the matrix D.\n",
      "\n",
      "f(a} 4%) = aya? {-5 — po (a — w} (3.9)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-052.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n",
      "A matrix © is pos-\n",
      "itive definite if, for\n",
      "all non-zero vectors\n",
      "x, 27xX > 0.\n",
      "\n",
      "60 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "where det(-) denotes the determinant of a matrix, pz is a vector of length k\n",
      "and » isa k x k symmetric, positive definite matrix. Setting 4 = 0 and\n",
      "X= I gives back the standard Normal.\n",
      "\n",
      "Since & is symmetric and positive definite, it can be shown that there\n",
      "exists a matrix D'/? — called the square root of © — with the following\n",
      "properties: (i) D1 is symmetric, (ii) © = S'/?E!/? and (iii) D?N-V/? =\n",
      "E-M2y 1/2 — J where DW? = (D1/)-1,\n",
      "\n",
      "Theorem 3.42 If Z ~ N(O,J) and X = w+D\"?Z then X ~ N(u,).\n",
      "Conversely, if X ~ N(p,S), then S-/?(X — pw) ~ N(0,1).\n",
      "\n",
      "Suppose we partition a random Normal vector X as X = (Xq,Xs) We\n",
      "can similarly partition ps = (a, Wy) and\n",
      "\n",
      "Yaa\n",
      "ye aa Mab)\n",
      "( Xa Lop )\n",
      "Theorem 3.43 Let X ~ N(u,¥). Then:\n",
      "\n",
      "(1) The marginal distribition of Xq is Xq ~~ N (fa, Naa)-\n",
      "(2) The conditional distribition of X, given X, = tq is\n",
      "\n",
      "Xp|Xa = ta ~ N ( pe + StaZaa (a — Ha): Sov — VsaUaa Lab ) -\n",
      "\n",
      "(3) If a is a vector then aX ~ N(a™p, a7 Sa).\n",
      "(4) V = (X— wPEX — H) ~ XG.\n",
      "\n",
      "3.11 Transformations of Random Variables\n",
      "\n",
      "Suppose that X is a random variable with ppF fx and cpr Fy. Let\n",
      "Y =r(X) be a function of X, for example, Y = X? or Y = e*. We call\n",
      "Y =r(X) a transformation of X. How do we compute the ppF and CDF of\n",
      "Y? In the discrete case, the answer is easy. The mass function of Y is given\n",
      "by\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-053.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "3.11. TRANSFORMATIONS OF RANDOM VARIABLES 61\n",
      "\n",
      " \n",
      "\n",
      "fry) = P(Y = y) = P(r(X) = 9) = P({a5 r(@) = y}) = P(X €r\"(y));\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 3.44 Suppose that P(X = —1) = P(X =1) =1/4 and P(X = 0) =\n",
      "1/2. Let Y = X*. Then, P(Y = 0) = P(X = 0) = 1/2 and P(Y = 1) =\n",
      "P(X =1)+ P(X = -1) = 1/2. Summarizing:\n",
      "x é\n",
      "ae y_ fy)\n",
      "0 172\n",
      "0 1/2 1 1/2\n",
      "1 If\n",
      "Y takes fewer values than X because the transformation is not one-to-one.\n",
      "\n",
      "The continuous case is harder. There are three steps for finding fy:\n",
      "\n",
      " \n",
      "\n",
      "Three steps for transformations\n",
      "1. For each y, find the set Ay = {x: r(x) < y}.\n",
      "2. Find the cbF\n",
      "PY <y)=P(r(X) <y)\n",
      "P({x; r(x) < y}) =| fx (x)dx (3.10)\n",
      "\n",
      "Ay\n",
      "\n",
      "Fy(y)\n",
      "\n",
      "3. The por is fy(y) = Fy-(y).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 3.45 Let fx(x) = e* for x > 0. Then Fx(x) = Io fx(s)ds =\n",
      "l—e*. Let Y =r(X)=logX. Then Ay = {x: x < e%} and\n",
      "\n",
      "Fy(y) =P(Y < y) = P(logX < y) =P(X < e”) = Fx(e’) =1- e.\n",
      "\n",
      "Therefore, fy(y) = ee\" foryeR. wf\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-054.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "62 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "Example 3.46 Let X ~ Unif(-1,3). Find the pdf of Y = X?. The density of\n",
      "X is\n",
      "\n",
      "_f 1/4 if -l<a<3\n",
      "f(a) = { 0 otherwise.\n",
      "\n",
      "Y can only take values in (0,9). Consider two case: (i) 0 < y < 1 and\n",
      "(ti) VS y <9. For case (i), Ay = [—/U./9] and Fy(y) = fy, fx(@)dx =\n",
      "\n",
      "(1/2),/¥. For case (ii), Ay = [-1, /y] and Fy(y) = Sa, fx(a)dx = (1/4)(\\/7+\n",
      "1). Differentiating F we get\n",
      "\n",
      "if0<y<1\n",
      "fyry= ifl<y<9\n",
      "\n",
      "8\n",
      "0 otherwise. MI\n",
      "\n",
      "is\n",
      "\n",
      "a\n",
      "\n",
      "When r is strictly monotone increasing or strictly monotone decreasing\n",
      "then r has an inverse s = r~' and in this case one can show that\n",
      "\n",
      "f(y) = fx(s))\n",
      "\n",
      " \n",
      "\n",
      "(3.11)\n",
      "\n",
      " \n",
      "\n",
      "ao ;\n",
      "dy\n",
      "\n",
      "3.12 Transformations of Several Random Vari-\n",
      "ables\n",
      "\n",
      "In some cases we are interested in transformation of several random vari-\n",
      "ables. For example, if X and Y are given random variables, we might want\n",
      "to know the distribution of X/Y, X + Y, max{X,Y} or min{X,Y}. Let\n",
      "Z = 1r(X,Y) be the function of interest. The steps for finding fz are the\n",
      "same as before:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-055.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1EC8>\n",
      "3.12. TRANSFORMATIONS OF SEVERAL RANDOM VARIABLES 63\n",
      "\n",
      " \n",
      "\n",
      "1. For each z, find the set A, = {(2,y) + r(z,y) <2}.\n",
      "2. Find the cpF\n",
      "F(z) = P(Z <2) =P(r(X,Y) <2)\n",
      "P(e): raw) <2) = ff txv(ew)aedy.\n",
      "\n",
      "Az\n",
      "\n",
      "3. Then fz(z) = F(z).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 3.47 Let X,,X2 ~ Unif(0,1) be independent. Find the density of\n",
      "Y = X,+ X2. The joint density of (X1, X2) is\n",
      "\n",
      "f(#1,%2) = {\n",
      "\n",
      "Let r(#1,%2) = 41 +22. Now,\n",
      "\n",
      "Fy) = PY <y) =P(r(%, Xe) <0)\n",
      "= P({(ai,92) + r(01,02) < y}) = | I flr, 22)derdry.\n",
      "\n",
      "Now comes the hard part: finding Ay. First suppose that 0 < y <1. Then\n",
      "Ay is the triangle with vertices (0,0), (y,0) and (0,y). See Figure 3.6. In\n",
      "this case, Sta f (a1, £2)dxiday is the area of this triangle which y?/2. If\n",
      "1<y <2 then A, is everything in the unit square except the triangle with\n",
      "vertices (1,y — 1), (1,1), (y—1,1). This set has area 1 — y?/2. Therefore,\n",
      "\n",
      "1 0<a<1,0<a%,<1\n",
      "0 otherwise.\n",
      "\n",
      "0 y<0\n",
      "jz\n",
      "Fy(y) = . P : : * :\n",
      "-E isy<2\n",
      "1 y > 2.\n",
      "By differentiation, the PDF is\n",
      "y O<sy<l\n",
      "fry)=4 l-y lsys2\n",
      "\n",
      "0 otherwise. Mi\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-056.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "64 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      " \n",
      "\n",
      "ae\n",
      "r i (y )\n",
      "(0,y) (l,y-1)\n",
      "0 0\n",
      "0 (y,0) 4 0 1\n",
      "This is the case 0 <y <1. This is the case 1 < y <2.\n",
      "\n",
      "Figure 3.6: The set A, for example 3.47.\n",
      "\n",
      "3.13 Technical Appendix\n",
      "\n",
      "Recall that a probability measure P is defined on a o-field A of a sam-\n",
      "ple space 2. A random variable X is a measurable map X :2 > R.\n",
      "Measurable means that, for every x, {w: X(w) <a} eA.\n",
      "\n",
      "3.14 Excercises\n",
      "\n",
      "1. Show that\n",
      "\n",
      "and\n",
      "F (a2) — F(a1) = P(X < a2) —P(X < 21).\n",
      "2. Let X be such that P(X = 2) = P(X = 3) = 1/10 and P(X =\n",
      "5) = 8/10. Plot the cor F. Use F to find P(2 < X < 4.8) and\n",
      "P2< X < 4.8).\n",
      "\n",
      "3. Prove Lemma 3.15.\n",
      "\n",
      "4, Let X have probability density function\n",
      "\n",
      "1/4 0<a<l\n",
      "fx(z) = 4 3/8 3<a<5\n",
      "(0 otherwise.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-057.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148A1C23048>\n",
      "3.14.\n",
      "\n",
      "10.\n",
      "\n",
      "11.\n",
      "\n",
      "12.\n",
      "\n",
      "EXCERCISES 65\n",
      "\n",
      "(a) Find the cumulative distribution function of X.\n",
      "\n",
      "(b) Let Y = 1/X. Find the probability density function fy(y) for Y.\n",
      "Hint: Consider three cases: } <y <$,}<y<landy>1.\n",
      "\n",
      ". Let X and Y be discrete random variables. Show that X and Y are\n",
      "\n",
      "independent if and only if fxy(#,y) = fx(x)fy(y) for all x and y.\n",
      "\n",
      ". Let X have distribution F' and density function f and let A be a subset\n",
      "\n",
      "of the real line. Let I4(x) be the indicator function for A:\n",
      "\n",
      "1aeA\n",
      "\n",
      "m= { 0 tg 4\n",
      "\n",
      "Let Y = J4(X). Find an expression for the cumulative distribution of\n",
      "Y. (Hint: first find the probability mass function for Y.)\n",
      "\n",
      ". Let X and Y be independent and suppose that each has a Uniform(0, 1)\n",
      "\n",
      "distribution. Let Z = min{X, Y}. Find the density fz(z) for Z. Hint:\n",
      "It might be easier to first find P(Z > z).\n",
      "\n",
      ". Let X have cdf F’. Find the cdf of Xt = max{0, X}.\n",
      "\n",
      ". Let X ~ Exp(f). Find F(x) and F-1(q).\n",
      "\n",
      "Let X and Y be independent. Show that g(X) is independent of h(Y)\n",
      "where g and h are functions.\n",
      "\n",
      "Suppose we toss a coin once and let p be the probability of heads. Let\n",
      "X denote the number of heads and let Y denote the number of tails.\n",
      "(a) Prove that X and Y are dependent.\n",
      "\n",
      "(b) Let N ~ Poisson(\\) and suppose we toss a coin N times. Let X\n",
      "and Y be the number of heads and tails. Show that X and Y are\n",
      "independent.\n",
      "\n",
      "Prove Theorem 3.33.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-058.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "\n",
      "13.\n",
      "\n",
      "14,\n",
      "\n",
      "16.\n",
      "\n",
      "I.\n",
      "\n",
      "18.\n",
      "\n",
      "CHAPTER 3. RANDOM VARIABLES\n",
      "Let X ~ N(0,1) and let Y = e*.\n",
      "(a) Find the pdf for Y. Plot it.\n",
      "\n",
      "(b) (Computer Experiment.) Generate a vector « = (21,-.--, 210,000)\n",
      "consisting of 10,000 random standard Normals. Let y = (y1,---,¥10,000)\n",
      "where y; = e*'. Draw a histogram of y and compare it to the PDF you\n",
      "found in part (a).\n",
      "\n",
      "Let (X,Y) be uniformly distributed on the unit dise {(x,y): a?+y? <\n",
      "1}. Let R= /X?+Y?. Find the cdf and pdf of R.\n",
      "\n",
      ". (A universal random number generator.) Let X have a continuous,\n",
      "\n",
      "strictly increasing cor F’. Let Y = F(X). Find the density of Y. This\n",
      "is called the probability integral transform. Now let U ~ Uniform(0, 1)\n",
      "and let X = F-1(U). Show that X ~ F. Now write a program that\n",
      "takes Uniform (0,1) random variables and generates random variables\n",
      "from an Exponential (3) distribution.\n",
      "\n",
      "Let X ~ Poisson(\\) and Y ~ Poisson(y) and assume that X and Y are\n",
      "independent. Show that the distribution of X given that X +Y =n\n",
      "is Binomial(n, 7) where 7 = A/(A + p).\n",
      "\n",
      "Hint 1: You may use the following fact: If X ~ Poisson(A) and Y ~\n",
      "Poisson(y), and X and Y are independent, then X + Y ~ Poisson(y+\n",
      "d).\n",
      "\n",
      "Hint 2: Note that {X =a, X+Y=n}={X =2, Y=n-a}.\n",
      "\n",
      "Let\n",
      "_ f caty’) 0<a<land0<y<1\n",
      "fav @9) = { 0 otherwise.\n",
      "Find P(X <4|¥Y =$).\n",
      "Let X ~ N(3,16). Solve the following using the Normal table and\n",
      "using a computer package.\n",
      "\n",
      "(a) Find P(X <7).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-059.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "3.14. EXCERCISES 67\n",
      "\n",
      "(b) Find P(X > —2).\n",
      "(c) Find g such that P(X > x) = .05.\n",
      "(d) Find P(O< X <4).\n",
      "(e) Find g such that P(|X| > |z|) = .05.\n",
      "19. Prove formula (3.11).\n",
      "20. Let X,Y ~ Unif(0,1) be independent. Find the ppr for X — Y and\n",
      "X/Y.\n",
      "21. Let X1,...,X, ~ Exp(8) be tp . Let Y = max{X,,...,X,}. Find\n",
      "the ppr of Y. Hint: Y < y if and only if X; < y fori =1,...,n.\n",
      "22. Let X and Y be random variables. Suppose that E(Y|X) = X. Show\n",
      "that Cov(X,Y) = V(X).\n",
      "\n",
      "23. Let X ~ Uniform(0,1). Let 0<a<b<1. Let\n",
      "\n",
      "1 0<a<b\n",
      "Y= { 0 otherwise\n",
      "\n",
      "and let\n",
      "\n",
      "Zea la<a<l\n",
      "~ [0 otherwise\n",
      "\n",
      "(a) Are Y and Z independent? Why/Why not?\n",
      "\n",
      "(b) (10 points) Find E(Y|Z). Hint: What values z can Z take? Now\n",
      "find E(Y|Z = z).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-060.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-061.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907BAA88>\n",
      "Chapter 4\n",
      "\n",
      "Expectation\n",
      "\n",
      "4.1 Expectation of a Random Variable\n",
      "\n",
      "The expectation (or mean) of a random variable X is the average value\n",
      "of X. The formal definition is as follows.\n",
      "\n",
      " \n",
      "\n",
      "Definition 4.1 The expected value, or mean, or first moment, of\n",
      "X is defined to be\n",
      "\n",
      "_ _ tf (x) if X is discrete\n",
      "K(X) = [earc) ~ { {af(c)dx if X is continuous (4)\n",
      "\n",
      "assuming that the sum (or integral) is well-defined. We use the follow-\n",
      "ing notation to denote the expected value of X:\n",
      "\n",
      "E(X) = EX = [« dF(é)= w= te: (4.2)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The expectation is a one-number summary of the distribution. Think of\n",
      "E(X) as the average value you would obtain if you computed the numerical\n",
      "average n=! 7\", X; of a large number of 1D draws X,,...,X,. The fact\n",
      "that E(X) + n-! 72, X; is actually more than a heuristic: it is a theorem\n",
      "called the law of large numbers that we will discuss later. The notation\n",
      "\n",
      "69\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-062.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "70 CHAPTER 4. EXPECTATION\n",
      "\n",
      "[ cdF(x) deserves some comment. We use it merely as a convenient unifying\n",
      "notation so we don’t have to write >, f(x) for discrete random variables\n",
      "and f xf(z)dz for continuous random variables but you should be aware that\n",
      "J cdF(z) has a precise meaning that is discussed in real anlysis courses.\n",
      "\n",
      "To ensure that E(X) is well defined, we say that E(X) exists if [, |x|dFx(x) <\n",
      "co. Otherwise we say that the expectation does not exist.\n",
      "\n",
      "Example 4.2 Let X ~ Bernoulli(p). Then E(X) = ye af(z) = (0x -\n",
      "p))+(.x p)=p.\n",
      "\n",
      "Example 4.3 Flip a fair coin two times. Let X be the number of heads. Then,\n",
      "E(X) = fadFx(e) = 0, efx(2) = (0 x f(0)) + Ax £G)) + 2x £2) =\n",
      "(0 x (1/4)) + (1 x (1/2)) + (2x (1/4)) =1.\n",
      "\n",
      "Example 4.4 Let X ~ Unif(—1,3). Then, E(X) = f adF x(x) = f xfx(x)dt =\n",
      "LPiadv=1. a\n",
      "\n",
      "Example 4.5 Recall that a random variable has a Cauchy distribution if it has\n",
      "density fx(x) = {1(1 + .27)}-1. Using integration by parts, (set u = x and\n",
      "v= tans),\n",
      "\n",
      "/ ja|dF (x) = =[ A [z tao] f tan! x da = 00\n",
      "0\n",
      "\n",
      " \n",
      "\n",
      "T 1+ a?\n",
      "\n",
      "so the mean does not exist. If you simulate a Cauchy distribution many\n",
      "times and take the average, you will see that the average never settles down.\n",
      "This is because the Cauchy has thick tails and hence extreme observations\n",
      "are common.\n",
      "\n",
      "From now on, whenever we discuss expectations, we implicitly assume\n",
      "that they exist.\n",
      "\n",
      "Let Y = r(X). How do we compute E(Y)? One way is to find fy(y) and\n",
      "then compute E(Y) = f yfy(y)dy. But there is an easier way.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-063.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n",
      "4.1. EXPECTATION OF A RANDOM VARIABLE 71\n",
      "\n",
      " \n",
      "\n",
      "Theorem 4.6 (The rule of the lazy statistician.) Let Y = r(X). Then\n",
      "\n",
      "EY) =B((x)) = f rare. (4.3)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "This result makes intuitive sense. Think of playing a game where we\n",
      "draw X at random and then I pay you Y = r(X). Your average income is\n",
      "r(x) times the chance that X = x, summed (or integrated) over all values\n",
      "of x. Here is a special case. Let A be an event and let r(x) = I4(x) where\n",
      "I4(z) =1ifae € A and I4(x) = 0 if ¢ A. Then\n",
      "\n",
      "E(14(X)) = [ taw@ix(aae = | fula)de = P(X € A).\n",
      "A\n",
      "In other words, probability is a special case of expectation.\n",
      "\n",
      "Example 4.7 Let X ~ Unif(0,1). Let Y =r(X) =e*. Then,\n",
      "\n",
      "1 1\n",
      "E(Y)= | e*f(a)dr = | @dc =e—1.\n",
      "\n",
      "Alternatively, you could find fy(y) which turns out to be fy(y) = 1/y for\n",
      "l<y<e. Then, EY) = ffyfy)dy=e—1. w\n",
      "\n",
      "Example 4.8 Take a stick of unit length and break it at random. Let Y be the\n",
      "length of the longer piece. What is the mean of Y? If X is the break point\n",
      "then X ~ Unif(0,1) and Y = r(X) = max{X,1— X}. Thus, r(z) =1—a\n",
      "when 0 <a <1/2 andr(x) =a when1/2< 2 <1. Hence,\n",
      "\n",
      "a0) = frware= [\"0-aars f eae = - 1\n",
      "\n",
      "Functions of several variables are handled in a similar way. If Z = r(X,Y)\n",
      "then\n",
      "\n",
      "BZ) =B(r(x,¥)) = ff re,wate,9)- (44)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-064.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1988>\n",
      "72 CHAPTER 4. EXPECTATION\n",
      "\n",
      "Example 4.9 Let (X,Y) have a jointly uniform distribution on the unit square.\n",
      "Let Z=r(X,Y) =X?+Y?. Then,\n",
      "\n",
      "BZ)= ff rowareaa= fo [dra = [oars [vr av= : Lt\n",
      "\n",
      "The k'* moment of X is defined to be E(X*) assuming that E(|X|*) <\n",
      "oo. We shall rarely make much use of moments beyond k = 2.\n",
      "\n",
      "4.2 Properties of Expectations\n",
      "\n",
      "Theorem 4.10 Jf X1,...,X, are random variables and ai,...,Gn are con-\n",
      "stants, then\n",
      "\n",
      "E (= oa =o aE(%). (4.5)\n",
      "\n",
      "Example 4.11 Let X ~ Binomial(n,p). What is the mean of X? We could\n",
      "try to appeal to the definition:\n",
      "\n",
      "Bx) = f edFee) =D 2fs(a) = 02(\"\\ra—p\"\n",
      "\n",
      "a «=0\n",
      "\n",
      "but this is not an easy sum to evaluate. Instead, note that X = SY, X;\n",
      "where X; = 1 if the i* toss is heads and X; = 0 otherwise. Then E(X;) =\n",
      "\n",
      "(p x 1) + ((L—p) x 0) = p and E(X) = E(X), Xi) = DEX) = np.\n",
      "\n",
      "Theorem 4.12 Let X,,...,X, be independent random variables. Then,\n",
      "\n",
      "E (11 x) =| [EX). (4.6)\n",
      "\n",
      "Notice that the summation rule does not require independence but the\n",
      "multiplication rule does.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-065.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3C48>\n",
      "can’t use E(X —\n",
      "as a measure of\n",
      "sad since E(X —\n",
      "= E(X)-p=\n",
      "-p = 0. We\n",
      "and sometimes\n",
      "use E|X — pu| as\n",
      "leasure of spread\n",
      "more often we\n",
      "the variance.\n",
      "\n",
      "4.3. VARIANCE AND COVARIANCE 73\n",
      "\n",
      "4.3. Variance and Covariance\n",
      "\n",
      "The variance measures the “spread” of a distribution.\n",
      "\n",
      " \n",
      "\n",
      "Definition 4.13 Let X be a random variable with mean yp. The variance\n",
      "of X — denoted by 0? or 0% or V(X) or V(X) or VX — is defined by\n",
      "\n",
      "=B(X — p= f (@- are) (47)\n",
      "\n",
      "assuming this expectation exists. The standard deviation is sd(X) =\n",
      "V(X) and is also denoted by o and ox.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 4.14 Assuming the variance is well defined, it has the following\n",
      "properties:\n",
      "\n",
      "1. V(X) = E(X?) =p’.\n",
      "2. Ifa and b are constants then V(aX + b) = a? V(X).\n",
      "\n",
      "3. If Xy,...,Xq are independent and a1,..., Gn are constants, then\n",
      "\n",
      "Vv (= oat = yraV(X). (4.8)\n",
      "\n",
      "Example 4.15 Let X ~ Binomial(n,p). We write X = >, X; where X; = 1\n",
      "if toss i is heads and X; = 0 otherwise. Then X = S°, X; and the random\n",
      "variables are independent. Also, P(X; = 1) = p and P(X; = 0) = 1—p.\n",
      "Recall that\n",
      "\n",
      "E(X;) = [px 1] +[(1—p) x ]=p\n",
      "Now,\n",
      "\n",
      "E(X?) = [px ?]+[G —p) xO] =p.\n",
      "Therefore, V(X;) = E(X?) — p = p—p = p(l—p). Finally, V(X) =\n",
      "V(O; Xi) = 0; VOG) = Op. — p) = np. — p). Notice that V(X) = 0 if\n",
      "\n",
      "p=1orp=0. Make sure you see why this makes intuitive sense.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-066.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E1A08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 CHAPTER 4. EXPECTATION\n",
      "\n",
      "If X,,...,X, are random variables then we define the sample mean to\n",
      "be\n",
      "an bie\n",
      "X= 7 OX (49)\n",
      "t=1\n",
      "\n",
      "and the sample variance to be\n",
      "\n",
      "Sis Se i-®). (4.10)\n",
      "a=1\n",
      "\n",
      " \n",
      "\n",
      "Theorem 4.16 Let X1,...,X, be UD and let p= E(X;), 0? = V(X;). Then\n",
      "\n",
      "2\n",
      "B(X,) =, VO%n)=— and E(S{) =o.\n",
      "\n",
      "If X and Y are random variables, then the covariance and correlation\n",
      "between X and Y measure how strong the linear relationship is between X\n",
      "\n",
      "and Y.\n",
      "\n",
      " \n",
      "\n",
      "Definition 4.17 Let X and Y be random variables with means x and\n",
      "by and standard deviations ox and oy. Define the covariance between\n",
      "\n",
      "X andY by\n",
      "Cov(X,Y) = E[(X — px)(¥ — py) (4.11)\n",
      "and the correlation by\n",
      "“3 Cov(X, Y\n",
      "P= pxy =AX,Y) = Co) (4.12)\n",
      "Ox0y\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 4.18 The covariance satisfies:\n",
      "Cov(X,Y) = E(XY) — E(X)E(¥).\n",
      "\n",
      "The correlation satisfies:\n",
      "\n",
      "-1< (X,Y) <1\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-067.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "4.4, EXPECTATION AND VARIANCE OF IMPORTANT RANDOM VARIABLES75\n",
      "\n",
      "If Y =a+bX for some constants a and b then p(X,Y) = 1 ifb > 0 and\n",
      "p(X, Y) =—-1ifb <0. IfX andY are independent, then Cov(X,Y) = p=0.\n",
      "The converse is not true in general.\n",
      "\n",
      "Theorem 4.19 V(X + Y) = V(X) + V(Y) + 2Cov(X,Y) and V(X —Y) =\n",
      "V(X)+V(Y)—2Cov(X, Y). More generally, for random variables X,,...,Xn,\n",
      "\n",
      "Vv (= oa = Ss a?V(X;) +2 S S aja; Cov(X;, Xj).\n",
      "\n",
      "i<j\n",
      "\n",
      "4.4 Expectation and Variance of Important\n",
      "Random Variables\n",
      "\n",
      "Here we record the expectation of some important random variables.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Distribution Mean Variance\n",
      "\n",
      "Point mass at a a 0\n",
      "\n",
      "Bernoulli (p) p p(1-p)\n",
      "\n",
      "Binomial (n,p) p n p (1-p)\n",
      "Geometric (p) 1/p (l—p)/r\n",
      "\n",
      "Poisson (A) a a\n",
      "\n",
      "Uniform (a,b) (a+b) /2 (b—a)?/12\n",
      "\n",
      "Normal (1,07) Lb o\n",
      "\n",
      "Exponential (() B fom\n",
      "\n",
      "Gamma (a, 3) aB ap?\n",
      "\n",
      "Beta (a, 8) a/(a+ 8) aB/((a+ 8)(a+8+1))\n",
      "ty O(ify >1) v/(v—2) (ifv > 2)\n",
      "Pe P 2p\n",
      "\n",
      "Multinomial (n, p) np see below\n",
      "Multivariate Normal (1,2) ps x\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "We derived E(X) and V(X) for the binomial in the previous section. The\n",
      "calculations for some of the others are in the excercises.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-068.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n",
      "76 CHAPTER 4. EXPECTATION\n",
      "\n",
      "The last two entries in the table are multivariate models which involve a\n",
      "random vector X of the form\n",
      "\n",
      "X\n",
      "X=| :\n",
      "Xk\n",
      "The mean of a random vector X is defined by\n",
      "Ma E(X1)\n",
      "w=| i f=]:\n",
      "Mk E(Xx)\n",
      "The variance-covariance matrix » is defined to be\n",
      "V(Xi) Cov(X1,X2) +++ Cov(X1, Xx)\n",
      "v(x) = Conk) nial : . Cov(X2, Xe)\n",
      "Cov(Xx,X1) Cov( Xz, X2) +++ V(Xx)\n",
      "If X ~ Multinomial(n, p) then E(X) = np = n(pi,-.-., px) and\n",
      "npi(l— pi) —npipe +++ NPL\n",
      "v(x) = ~npeps nas — pe) ” ~nPeP\n",
      "—NPKP1 —NDKP2 +++ mpr(1 — pr)\n",
      "\n",
      "To see this, note that the marginal distribution of any one component of\n",
      "the vector is binomial, that is X; ~ Binomial(n,p;). Thus, E(X;) = np;\n",
      "and V(X;) = np;(1— pi). Note that X; + X; ~ Binomial(n,p; + p;). Thus,\n",
      "V(X; + X;) = n(p; + pj) — [p; + pj])- On the other hand, using the formula\n",
      "for the variance of a sum, we have that V(X; + X;) = V(X;) + V(X;) +\n",
      "2Cov(X;, X;) = npi(1 — pi) + npj(1 — pj) + 2Cov(X;, X;). If you equate this\n",
      "formula with n(p; + p;)(pi + pj) and solve, one gets Cov(X;, X;) = —npip;.\n",
      "\n",
      "Finally, here is a lemma that can be useful for finding means and variances\n",
      "of linear combinations of multivariate random vectors.\n",
      "\n",
      "Lemma 4.20 Jf a is a vector and X is a random vector with mean js and\n",
      "variance © then E(a?X) = aT and V(a? X) = a7Xa. If A is a matrix then\n",
      "E(AX) = Ay and V(AX) = ASAT.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-069.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C85C8>\n",
      "4.5. CONDITIONAL EXPECTATION 77\n",
      "\n",
      "4.5 Conditional Expectation\n",
      "\n",
      "Suppose that X and Y are random variables. What is the mean of X\n",
      "among those times when Y = y? The answer is that we compute the mean\n",
      "of X as before but we substitute fxjy(x|y) for fx(x) in the definition of\n",
      "expectation.\n",
      "\n",
      " \n",
      "\n",
      "Definition 4.21 The conditional expectation of X given Y = y is\n",
      "\n",
      "Ye fxiy(ely) dx discrete case\n",
      "\n",
      "4.13\n",
      "[2 fxjy(zly) dx continuous case. (4.13)\n",
      "\n",
      "nar=0={\n",
      "\n",
      "If r(x, y) is a function of x and y then\n",
      "\n",
      "Sr(a,y) fxjy (aly) da discrete case\n",
      "\n",
      "r(@y) fxiy(aly) dx continuous case.\n",
      "(4.14)\n",
      "\n",
      "Biri yi = 0) = {\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Whereas, E(X) is a number, E(X|Y = y) is a function of y. Before we\n",
      "observe Y, we don’t know the value of E(X|Y = y) so it is a random variable\n",
      "which we denote E(X|Y). In other words, E(X|Y) is the random variable\n",
      "whose value is E(X|Y = y) when y = y. Similarly, E(r(X,Y)|Y) is the\n",
      "random variable whose value is E(r(X,Y)|Y = y) when y = y. This isa\n",
      "very confusing point so let us look at an example.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 4.22 Suppose we draw X ~ Unif(0,1). After we observe X = z,\n",
      "we draw Y|\\X = x ~ Unif(z,1). Intuitively, we expect that E(Y|X = x) =\n",
      "(1+ .2)/2. In fact, fy\\x(y|x) =1/(l-«) fora <y <1 and\n",
      "\n",
      " \n",
      "\n",
      "1 1 1 1 +2\n",
      "E(Y|X = 2) -[ y Fyix (yla)dy = —/ ydy =\n",
      "© 1-gJ, 2\n",
      "as expected. Thus, E(Y|X) = (1+X)/2. Notice that E(Y|X) = (1+ X)/2 is\n",
      "a random variable whose value is the number E(Y|X = x) = (1+ 2)/2 once\n",
      "X =x is observed. @\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-070.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "78 CHAPTER 4. EXPECTATION\n",
      "\n",
      "Theorem 4.23 (The rule of iterated expectations.) For random variables X and\n",
      "Y, assuming the expectations exist, we have that\n",
      "\n",
      "E[E(Y|X)]=E(Y) and E[E(X|Y)] = E(X). (4.15)\n",
      "More generally, for any function r(x, y) we have\n",
      "\n",
      "E([E(r(X,Y)|X)] =E(r(X,Y)) and E[B(r(X,Y)|X)] = E(r(X,Y)).\n",
      "(4.16)\n",
      "\n",
      "ProoF. We'll prove the first equation. Using the definition of conditional\n",
      "expectation and the fact that f(,y) = f(x)f(y|z),\n",
      "\n",
      "E(E(Y|X)]\n",
      "\n",
      "[eix=aisteyae= ff veculaaysteyar\n",
      "| [vtolore@aca = [ [vile naedy = 80. 7\n",
      "\n",
      "Example 4.24 Consider example 4.22. How can we compute E(Y)? One\n",
      "method is to find the joint density f(x,y) and then compute E(Y) = f fy f(z, y)dxdy.\n",
      "An easier way is to do this in two steps. First, we already know that E(Y|X) =\n",
      "\n",
      "(1+ X)/2. Thus,\n",
      "\n",
      " \n",
      "\n",
      "1 RB Hd (es 0) = ORO) 40/2) ayy\n",
      "\n",
      "Definition 4.25 The conditional variance is defined as\n",
      "\n",
      "Vorix =a) = fw ne))*70la)ay (417)\n",
      "where u(x) = E(Y|X = 2).\n",
      "Theorem 4.26 For random variables X and Y,\n",
      "\n",
      "V(Y) = EV(Y|X) + VE(Y|X).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-071.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891699B48>\n",
      "4.6. TECHNICAL APPENDIX 79\n",
      "\n",
      "Example 4.27 Draw a county at random from the United States. Then draw\n",
      "n people at random from the county. Let X be the number of those people\n",
      "who have a certain disease. If Q denotes the proportion of of people in that\n",
      "county with the disease then Q is also a random variable since it varies from\n",
      "county to county. Given Q = q, we have that X ~ Binomial(n,q). Thus,\n",
      "E(X|Q = q) = ng and V(X|Q = q) = ng(1—q). Suppose that the random\n",
      "variable P has a Uniform (0,1) distribution. Then, E(X) = EE(X|Q) =\n",
      "E(nQ) = nE(Q) = n/2. Let us compute the variance of X. Now, V(X) =\n",
      "EV(X|Q) + VE(X|Q). Let’s compute these two terms. First, EV(X|Q) =\n",
      "E[nQ(— Q)] = nE(Q(1—Q)) =n f alt —a) fa)da =n fi q(d —g)dq = 1/6.\n",
      "Nett, VE(X|Q) = V(nQ) = n?V(Q) = n? f (q— (1/2))?dq = n?/12. Hence,\n",
      "V(X) = (n/6) + (n?/12).\n",
      "\n",
      "4.6 Technical Appendix\n",
      "\n",
      "4.6.1 Expectation as an Integral\n",
      "\n",
      "The integral of a measurable function r(x) is defined as follows. First sup-\n",
      "pose that r is simple, meaning that it takes finitely many values ay,..., ax\n",
      "over a partition Ai,...,Ag- Then fr(z)dF(«) = WW, aP(r(X) € Aj).\n",
      "The integral of a positive measurable function r is defined by f r()dF (a) =\n",
      "lim, f r;(x)dF(a) where r; is a sequence of simple functions such that r;(x) <\n",
      "r(z) and ri(z) > r(x) as i + co. This does not depend on the partic-\n",
      "ular sequence. The integral of a measurable function r is defined to be\n",
      "[r(@)dF(x) = frt(@)dF(z) — [r-(z)dF (x) assuming both integrals are\n",
      "finite, where r*(x) = max{r(x),0} and r-(z) = — min{r(z), O}.\n",
      "\n",
      "4.6.2 Moment Generating Functions\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-072.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E92C8>\n",
      "80 CHAPTER 4. EXPECTATION\n",
      "\n",
      " \n",
      "\n",
      "Definition 4.28 The moment generating function (mgf), or Laplace|\n",
      "transform, of X is defined by\n",
      "\n",
      "x(t) = E(e*) = [exarc)\n",
      "\n",
      "where t varies over the real numbers.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "In what follows, we assume that the megf is well defined for all ¢ in small\n",
      "neighborhood of 0. A related function is the characteristic function, defined\n",
      "by E(e*) where i = /—I. This function is always well defined for all t. The\n",
      "ingf is useful for several reasons. First, it helps us compute the moments of\n",
      "a distribution. Second, it helps us find the distribution of sums of random\n",
      "variables. Third, it is used to prove the central limit theorem which we\n",
      "discuss later.\n",
      "\n",
      " \n",
      "\n",
      "When the mef is well defined, it can be shown that we change interchange\n",
      "the operations of differentiation and “taking expectation.” This leads to\n",
      "d\n",
      "\n",
      "d\n",
      "y'(0) = ae\" | =E ae] =E[Xe*],_, =E(X).\n",
      "dt t=0 dt t=0 Jiao\n",
      "\n",
      " \n",
      "\n",
      "By taking for derivatives we conclude that 7“) (0) = E(X*). This gives us a\n",
      "method for computing the moments of a distribution.\n",
      "\n",
      "Example 4.29 Let X ~ Exp(1). For anyt <1,\n",
      "\n",
      "20 0\n",
      "x(t) = Eé* = | edz = [ eV dz = —_.\n",
      "0 0 1—t\n",
      "\n",
      "The integral is divergent if t >1. So, wx(t)=1/(1—t) for allt < 1. Now,\n",
      "w'(0) = 1 and w\"(0) = 2. Hence, E(X) = 1 and V(X) = E(X?) — 2 =\n",
      "21 1.\n",
      "\n",
      "Lemma 4.30 Properties of the mgf.\n",
      "\n",
      "(1) IfY = aX +6 then dy (t) = eux (at).\n",
      "\n",
      "(2) If X1,...,Xn are independent and Y = SY, X; then py (t) = TI, vi(t)\n",
      "where w; is the maf of X;.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-073.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E1B88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7. EXCERCISES 81\n",
      "\n",
      "Example 4.31 Let X ~ Binomial(n,p). As before we know that X = 0; X;\n",
      "where P(X; = 1) = p and P(X; = 0) =1—p. Now y(t) = Ee*\" = (px e')+\n",
      "((1—p)) = pe'+q where q=1-—p. Thus, x(t) = [], v(t) = (pe +g)”.\n",
      "Theorem 4.32 Let X and Y be random variables. If yx (t) = Wy(t) for all t\n",
      "in an open interval around 0, then X 4y.\n",
      "\n",
      "Example 4.33 Let X ~ Binomial(n1,p) and X ~ Binomial(ng, p) be indepen-\n",
      "dent. Let Y = X, +X. Now\n",
      "\n",
      "by (t) = Yi (t)do() = (pel + 4) (pel +g)” = (pel +g)\"\n",
      "\n",
      "and we recognize the latter as the mgf of a Binomial(n; + n2,p) distribution.\n",
      "Since the mgf characterizes the distribution (i.e. there can’t be another ran-\n",
      "dom variable which has the same mgf) we conclude that Y ~ Bin(ni+nz, p)-\n",
      "\n",
      "Moment Generating Function for Some Common Distributions\n",
      "\n",
      "Distribution mgf\n",
      "Bernoulli (p) pe’ + (1— p)\n",
      "Binomial (n,p) (pe’ + (1 — p))”\n",
      "\n",
      "Poisson (A) exe)\n",
      "\n",
      " \n",
      "\n",
      "242\n",
      "\n",
      "Normal (4,0) exp {ut + sh\n",
      "\n",
      "Gamma (a,,3) (4)* fort < B\n",
      "\n",
      "4.7 Excercises\n",
      "\n",
      "1. Suppose we play a game where we start with c dollars. On each play of\n",
      "the game you either double or half your money, with equal probability.\n",
      "What is your expected fortune after n trials?\n",
      "\n",
      "2. Show that V(X) = 0 if and only if there is a constant c such that\n",
      "P(X =c)=1.\n",
      "\n",
      "3. Let Xi,...,X, ~ Uniform(0,1) and let Y, = max{X),...,X,}. Find\n",
      "E(Y,).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-074.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907BAA88>\n",
      "82\n",
      "\n",
      "10.\n",
      "\n",
      "11.\n",
      "\n",
      "CHAPTER 4. EXPECTATION\n",
      "\n",
      ". A particle starts at the origin of the real line and moves along the\n",
      "\n",
      "line in jumps of one unit. For each jump the probability is p that the\n",
      "particle will jump one unit to the left and the probability is 1 — p that\n",
      "the particle will jump one unit to the right. Let X,, be the position of\n",
      "the particle after n units. Find E(X,,) and V(X,,). (This is known as\n",
      "a random walk.)\n",
      "\n",
      ". A fair coin is tossed until a head is obtained. What is the expected\n",
      "\n",
      "number of tosses that will be required?\n",
      "\n",
      ". Prove Theorem 4.6 for discrete random variables.\n",
      "\n",
      ". Let X be a continuous random variable with cDF F. Suppose that\n",
      "\n",
      "P(X > 0) = 1 and that E(X) exists. Show that E(X) = f>°P(X >\n",
      "a)dz.\n",
      "\n",
      "Hint: Consider integrating by parts. The following fact is helpful: if\n",
      "E(X) exists then limy_,.. a[1 — F(2)] = 0.\n",
      "\n",
      ". Prove Theorem 4.16.\n",
      "\n",
      ". (Computer Experiment.) Let X1,X2,...,X, be N(0,1) random vari-\n",
      "\n",
      "ables and let X, = n7+ Wh Xi. Plot Xp versus n for n = 1,..., 10,000.\n",
      "Repeat for X1,.Xo,...,X, ~ Cauchy. Explain why there is such a dif-\n",
      "ference.\n",
      "\n",
      "Let X ~ N(0,1) and let Y = e*. Find E(Y) and V(Y).\n",
      "\n",
      "(Computer Experiment: Simulating the Stock Market.) Let Yi, Yo,... be\n",
      "independent random variables such that P(Y; = 1) = P(Y¥; = -1) =\n",
      "1/2. Let X, = 0, Yi. Think of Y; = 1 as “the stock price increased\n",
      "by one dollar”, Y; = —1 as “the stock price decreased by one dollar”\n",
      "and X,, as the value of the stock on day n.\n",
      "\n",
      "(a) Find E(X,) and V(X,,).\n",
      "(b) Simulate X,, and plot X,, versus n for n = 1,2,...,10,000. Repeat\n",
      "\n",
      "the whole simulation several times. Notice two things. First, it’s easy\n",
      "to “see” patterns in the sequence even though it is random. Second,\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-075.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148A1C23048>\n",
      "4.7.\n",
      "\n",
      "12.\n",
      "\n",
      "13.\n",
      "\n",
      "14.\n",
      "\n",
      "16.\n",
      "\n",
      "EXCERCISES 83\n",
      "\n",
      "you will find that the four runs look very different even though they\n",
      "were generated the same way. How do the calculations in (a) explain\n",
      "the second observation?\n",
      "\n",
      "Prove the formulas given in the table at the beginning of Section 4.4\n",
      "for the Bernoulli, Poisson, Uniform, Exponential, Gamma and Beta.\n",
      "Here are some hints. For the mean of the Poisson, use the fact that\n",
      "e* = 2, a*/a!. To compute the variance, first compute E(X (X —1)).\n",
      "For the mean of the Gamma, it will help to multiply and divide by\n",
      "T(a+1)/8°*' and use the fact that a Gamma density integrates to 1.\n",
      "For the Beta, multiply and divide by ['(a + 1)I'(8)/T(a+ 8+ 1).\n",
      "\n",
      "Suppose we generate a random variable X in the following way. First\n",
      "we flip a fair coin. If the coin is heads, take X to have a Unif(0,1)\n",
      "distribution. If the coin is tails, take X to have a Unif(3,4) distribution.\n",
      "\n",
      "(a) Find the mean of X.\n",
      "(b) Find the standard deviation of X.\n",
      "\n",
      "Let X1,...,Xm and Yi,...,¥Y, be random variables and let a1,...,@m\n",
      "and b;,...,6, be constants. Show that\n",
      "\n",
      "Cov (Soo, Ysx;) = 323 aibCov(X:,¥)).\n",
      "ro j=l\n",
      "\n",
      "i=1 j=l\n",
      "\n",
      " \n",
      "\n",
      ". Let\n",
      "\n",
      "(uty) 0OSa@<10<y<2\n",
      "otherwise.\n",
      "\n",
      "Ixy(a,y) = { 3\n",
      "Find V(2X — 3Y +8).\n",
      "Let r(x) be a function of x and let s(y) be a function of y. Show that\n",
      "E(r(X)s(¥)|X) = r(X)E(s()|X).\n",
      "\n",
      "Also, show that E(r(X)|X) = r(X).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-076.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3508>\n",
      "84\n",
      "\n",
      "17.\n",
      "\n",
      "18.\n",
      "\n",
      "19.\n",
      "\n",
      "20.\n",
      "\n",
      "CHAPTER 4. EXPECTATION\n",
      "\n",
      "Prove that\n",
      "\n",
      "V(Y) =EV(Y | X)+ VE(Y | X).\n",
      "Hint: Let m = E(Y) and let b(x) = E(Y|X = x). Note that E(b(X)) =\n",
      "EE(Y |X) = E(Y) = m. Bear in mind that b is a function of x. Now\n",
      "write V(Y) = E(Y — m)? = E((Y — b(X)) + (b(X) — m))?. Expand the\n",
      "square and take the expectation. You then have to take the expectation\n",
      "\n",
      " \n",
      "\n",
      "of three terms. In each case, use the rule of the iterated expectation:\n",
      "ie. E(stuff) = E(E(stuff|X)).\n",
      "\n",
      "Show that if E(X\n",
      "uncorrelated.\n",
      "\n",
      " \n",
      "\n",
      "Y = y) =c for some constant c then X and Y are\n",
      "\n",
      "This question is to help you understand the idea of a sampling dis-\n",
      "tribution. Let X,,...,X, be IID with mean jp and variance 0. Let\n",
      "X, =n\" dyX;. Then X,, is a statistic, that is, a function of the\n",
      "data. Since X,, is a random variable, it has a distribution. This distri-\n",
      "bution is called the sampling distribution of the statistic. Recall from\n",
      "Theorem 4.16 that E(X,,) = and V(X,,) = 0?/n. Don’t confuse the\n",
      "distribution of the data fx and the distribution of the statistic FR, To\n",
      "make this clear, let X,,...,X, ~ Uniform(0,1). Let fx be the density\n",
      "of the Uniform(0, 1). Plot fy. Now let X, = n7! 7, Xj. Find E(X,,)\n",
      "and V(X,,). Plot them as a function of n. Comment. Now simulate the\n",
      "distribution of X,, for n = 1,5, 25, 100. Check that the simulated values\n",
      "of E(X,,) and V(X,,) agree with your theoretical calculations. What\n",
      "do you notice about the sampling distribution of X,, as n increases?\n",
      "\n",
      "Prove Lemma 4.20.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-077.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891699B48>\n",
      "Chapter 5\n",
      "\n",
      "Inequalities\n",
      "\n",
      "5.1 Markov and Chebychev Inequalities\n",
      "\n",
      "Inequalities are useful for bounding quantities that might otherwise be\n",
      "hard to compute. They will also be used in the theory of convergence which\n",
      "is discussed in the next chapter. Our first inequality is Markov’s inequality.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 5.1 (Markov’s Inequality.) Let X be a non-negative random\n",
      "variable and suppose that E(X) exists. For any t > 0,\n",
      "\n",
      "P(X >2)< ae (5.1)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Proor. E(X) = f° af(a)dx = fi af(x)de+f xf (a)dx > f° xf (x)de >\n",
      "th f(a)de = 1P(X >t).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-078.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "86 CHAPTER 5. INEQUALITIES\n",
      "\n",
      " \n",
      "\n",
      "Theorem 5.2 (Chebyshev’s inequality.) Let p= E(X) and o? = V(X).\n",
      "Then,\n",
      "\n",
      "a 1 7\n",
      "z and P(|Z|>k)< Rp (5.2)\n",
      "where Z = (X — p)/a. In particular, P(|Z| > 2) < 1/4 and P(|Z| >\n",
      "3) < 1/9.\n",
      "\n",
      "P(X -p)>4) <\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "PRooF. We use Markov’s inequality to conclude that\n",
      "\n",
      "E(X — p)? o\n",
      "P(X — pl > 0) =P(X— pl? > 8) < PAW oe\n",
      "\n",
      "The second part follows by setting t= ko.\n",
      "\n",
      "Example 5.3 Suppose we test a prediction method, a neural net for example,\n",
      "on a set of n new test cases. Let X; = 1 if the predictor is wrong and X; =0\n",
      "if the predictor is right. Then X, = n-* SLi Xi is the observed error rate.\n",
      "Each X; may be regarded as a Bernoulli with unknown mean p. We would\n",
      "like to know the true, but unknown error rate p. Intuitively, we expect that\n",
      "X, should be close to p. How likely is X,, to not be within € of p? We have\n",
      "that V(X,) = V(X1)/n? = p(1— p)/n and\n",
      "\n",
      "ba V(Xn) _ pp) 1\n",
      "P(|X >e< <\n",
      "(| P| 6) = e ne — Ane?\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "since p(l—p) < + for all p. For € = .2 andn = 100 the bound is .0625.\n",
      "\n",
      "5.2 Hoeffding’s Inequality\n",
      "\n",
      "Hoeffding’s inequality is similar in spirit to Markov’s inequality but it is\n",
      "a sharper inequality. We present the result here in two parts. The proofs are\n",
      "in the technical appendix.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-079.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891699B48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2. HOEFFDING’S INEQUALITY 87\n",
      "\n",
      " \n",
      "\n",
      "Theorem 5.4 Let ¥i,...,Y, be independent observations such that\n",
      "E(¥%j) =0 anda; < ¥; < b;. Lete > 0. Then, for any t > 0,\n",
      "\n",
      "P (= ¥i> ‘ Se Tae, (5.3)\n",
      "1, i=1\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 5.5 Let X,,...,X, ~ Bernoulli(p). Then, for any € > 0,\n",
      "P ([X,—p| > 6) $26\" (5.4)\n",
      "\n",
      "where X, =n oy, Xj.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 5.6 Let X,,...,X, ~ Bernoulli(p). Let n = 100 ande = .2. We\n",
      "saw that Chebyshev’s yielded\n",
      "\n",
      "P(|X, —p| > €) < .0625.\n",
      "According to Hoeffding’s inequality,\n",
      "P([X — p| > .2) < 26-7” = 00067\n",
      "which is much smaller than .0625.\n",
      "\n",
      "Hoeffding’s inequality gives us a simple way to create a confidence in-\n",
      "terval for a binomial parameter p. We will discuss confidence intervals later\n",
      "but here is the basic idea. Fix a > 0 and let\n",
      "\n",
      "_ 1 7 2 1/2\n",
      "én = 1 5 los | 3\n",
      "By Hoeffding’s inequality,\n",
      "\n",
      "P(X, — pl > &n) < 26°\" =a.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-080.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3C48>\n",
      "88 CHAPTER 5. INEQUALITIES\n",
      "\n",
      "Let C = (X,, —¢,X, +). Then, P(C ¢ p) = P(|X, —p| > ©) < a. Hence,\n",
      "P(p € C) > 1—a, that is, the random interval C traps the true parameter\n",
      "value p with probability 1 — a; we call C a 1 — a confidence interval. More\n",
      "on this later.\n",
      "\n",
      "5.3 Cauchy-Schwarz and Jensen Inequalities\n",
      "\n",
      "This section contains two inequalities on expected values that are often\n",
      "useful.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 5.7 (Cauchy-Schwarz inequality.) If X and Y have finite vari-\n",
      "\n",
      "ances then\n",
      "E|XY| < /EXX)EQ”). (5.5)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Recall that a function g is convex if for each x, y and each a € [0,1],\n",
      "glax + (1— a)y) < ag(x) + (1 — a)g(y)-\n",
      "\n",
      "If g is twice differentiable, then convexity reduces to checking that g\"(x) > 0\n",
      "for all x. It can be shown that if g is convex then it lies above any line that\n",
      "touches g at some point, called a tangent line. A function g is concave if\n",
      "—g is convex. Examples of convex functions are g(z) = 2° and g(x) = e.\n",
      "Examples of concave functions are g(x) = —2” and g(x) = logz.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 5.8 (Jensen’s Inequality.) If g is convex then\n",
      "Eg(X) > g(EX). (56)\n",
      "\n",
      "If g is concave then\n",
      "Eq(X) < g(BX). (6.7)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Proor. Let L(x) = a+ bz be a line, tangent to g(x) at the point E(X).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-081.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E11C8>\n",
      "5.4. TECHNICAL APPENDIX: PROOF OF HOEFFDING’S INEQUALITY89\n",
      "Since g is convex, it lies above the line L(x). So,\n",
      "Eg(X) > EL(X) = E(a+ bX) =a + bE(X) = L(E(X)) = g(EX).\n",
      "\n",
      "From Jensen’s inequality we see that EX? > (EX)? and E(1/X) >\n",
      "1/E(X). Since log is concave, E(log X) < logE(X). For example, suppose\n",
      "that X ~ N(3,1). Then E(1/X) > 1/3.\n",
      "\n",
      "5.4 Technical Appendix: Proof of Hoeffding’s\n",
      "Inequality\n",
      "We will make use of the exact form of Taylor’s theorem: if g is a smooth\n",
      "\n",
      "function, then there is a number € € (0, u) such that g(u) = g(0) + ug'(0) +\n",
      "eon\n",
      "+9 (é)-\n",
      "\n",
      "PRrooF of Theorem 5.4. For any t > 0, we have, from Markov’s inequality,\n",
      "that\n",
      "\n",
      "S)\n",
      "\n",
      "é=1\n",
      "\n",
      "P (ox > “) =P Cone > \")\n",
      "a1\n",
      "\n",
      "e*E (¢=m%) =e“ Ee). (5.8)\n",
      "\n",
      "IA\n",
      "\n",
      "Since a; < Y; < bi, we can write Y; as a convex combination of a; and bj,\n",
      "namely, Y; = ab;+(1—a)a; where a = (Y;—a;)/(b;-a). So, by the convexity\n",
      "of e” we have\n",
      "\n",
      "Y= Gi othe b= Yi tas,\n",
      "\n",
      "bj -— a; bi — ay\n",
      "\n",
      " \n",
      "\n",
      "Take expectations of both sides and use the fact that E(Y;) =0 to get\n",
      "\n",
      "ee a et = es) (5.9)\n",
      "\n",
      "Eé¥ < -\n",
      "i — Bi by — a;\n",
      "\n",
      "where u = t(b; — aj), g(u) = —yu+ log(1 — y+ ye\") and y = —a;/(b; — ai).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-082.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n",
      "90 CHAPTER 5. INEQUALITIES\n",
      "\n",
      "Note that g(0) = g'(0) = 0. Also, g (u) < 1/4 for all u > 0. By Taylor’s\n",
      "theorem, there is a € € (0, u) such that\n",
      "\n",
      "g(u) = g(0)+ug'(0) +\n",
      "\n",
      "Hence,\n",
      "\n",
      "The result follows from (5.8). ll\n",
      "\n",
      "Proor of Theorem 5.5. Let Y¥; = (1/n)(X; — p). Then E(Y;) = 0 and\n",
      "a < Y¥; < b where a = —p/n and b = (1—p)/n. Also, (b- a)? = 1/n?.\n",
      "Applying the last Theorem we get\n",
      "\n",
      "P(X, -p>o= »(¥ > ‘) meet NS)\n",
      "i\n",
      "\n",
      "The above holds for any t > 0. In particular, take t = 4ne and we get\n",
      "P(X, -p>e< e?\" By a similar argument we can show that P(X, -p<\n",
      "—c) <e-*\"©. Putting these together we get P ([X,-pl>e < 26-2?\n",
      "\n",
      "5.5 Bibliographic Remarks\n",
      "An excellent reference on probability inequalities and their use in statistics\n",
      "\n",
      "and pattern recognition is Devroye, Gy6rfi and Lugosi (1996). The proof of\n",
      "Hoeffding’s inequality is from that text.\n",
      "\n",
      "5.6 Excercises\n",
      "\n",
      "1. Let X ~ Exponential(@). Find P(|X —yx| > kox) for k > 1. Compare\n",
      "this to the bound you get from Chebyshev’s inequality.\n",
      "\n",
      "2. Let X ~ Poisson(A). Use Chebyshev’s inequality to show that P(X >\n",
      "2) <1/A.\n",
      "\n",
      "3. Let X1,...,X, ~ Bernoulli(p) and X,, = n-! iL, X;. Bound P(|Xn—-\n",
      "p| > ©) using Chebyshev’s inequality and using Hoeffding’s inequality.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-083.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "5.6. EXCERCISES 91\n",
      "\n",
      "Show that, when n is large, the bound from Hoeffding’s inequality is\n",
      "smaller than the bound from Chebyshev’s inequality.\n",
      "\n",
      "4. Let X,,...,Xn ~ Bernoulli(p).\n",
      "(a) Let a > 0 be fixed and define\n",
      "\n",
      "= fio (2\n",
      "&u = 4/5, log ( )-\n",
      "\n",
      "Let p, = n7* 77, Xi. Define C, = (Pa — €n, Bn + €n). Use Hoeffding’s\n",
      "inequality to show that\n",
      "\n",
      "P(C,, contains p) > 1— a.\n",
      "\n",
      "We call C,, a 1 — a confidence interval for p. In practice, we truncate\n",
      "the interval so it does not go below 0 or above 1.\n",
      "\n",
      "(b) (Computer Experiment.) Let’s examine the properties of this confi-\n",
      "dence interval. Let a = 0.05 and p = 0.4. Conduct a simulation study\n",
      "to see how often the interval contains p (called the coverage). Do this\n",
      "for various values of n between 1 and 10000. Plot the coverage versus\n",
      "n.\n",
      "\n",
      "(c) Plot the length of the interval versus n. Suppose we want the length\n",
      "of the interval to be no more than .05. How large should n be?\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-084.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3C48>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-085.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "4.\n",
      "6\n",
      "\n",
      "Convergence of Random Variables\n",
      "\n",
      "6.1 Introduction\n",
      "\n",
      "The most important aspect of probability theory concerns\n",
      "the behavior of sequences of random variables. This part of\n",
      "probability is called large sample theory or limit theory\n",
      "or asymptotic theory. This material is extremely important\n",
      "for statistical inference. The basic question is this: what can we\n",
      "say about the limiting behavior of a sequence of random vari-\n",
      "ables X1, X2,X3,...? Since statistics and data mining are all\n",
      "about gathering data, we will naturally be interested in what\n",
      "happens as we gather more and more data.\n",
      "\n",
      "In calculus we say that a sequence of real numbers x, con-\n",
      "verges to a limit x if, for every « > 0, |v, — 2| < € for all\n",
      "large n. In probability, convergence is more subtle. Going back\n",
      "to calculus for a moment, suppose that 2, = 2 for all n. Then,\n",
      "trivially, lim, x, = x. Consider a probabilistic version of this\n",
      "example. Suppose that X,, X2,... is a sequence of random vari-\n",
      "ables which are independent and suppose each has a N(0,1)\n",
      "distribution. Since these all have the same distribution, we are\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "This is page 89\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-086.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "90 6. Convergence of Random Variables\n",
      "\n",
      "tempted to say that X, “converges” to X ~ N(0,1). But this\n",
      "\n",
      "can’t quite be right since P(X,, = Z) = 0 for all n. (Two con-\n",
      "\n",
      "tinuous random variables are equal with probability zero.)\n",
      "Here is another example. Consider X,,X2,... where X; ~\n",
      "\n",
      "N(0,1/n). Intuitively, X;, is very concentrated around 0 for large\n",
      "\n",
      "n. But P(X, = 0) = 0 for all n. This chapter develops appro-\n",
      "\n",
      "priate methods of discussing convergence of random variables.\n",
      "There are two main ideas in this chapter:\n",
      "\n",
      "1. The law of large numbers says that sample average\n",
      "X, =n! So, X; converges in probability to the ex-\n",
      "pectation pp = E(X).\n",
      "\n",
      "2. The central limit theorem says that sample average\n",
      "has approximately a Normal distribution for large n. More\n",
      "precisely, /n(Xn — 2) converges in distribution to a\n",
      "Normal(0, 0”) distribution, where 0? = V(X).\n",
      "\n",
      "6.2 Types of Convergence\n",
      "\n",
      "The two main types of convergence are defined as follows.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-087.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2 Types of Convergence\n",
      "\n",
      "91\n",
      "\n",
      " \n",
      "\n",
      "Definition 6.1 Let X,,Xo,... be a sequence of random vari-\n",
      "ables and let X be another random variable. Let F,, de-\n",
      "note the CDF of X, and let F denote the CDF of X.\n",
      "\n",
      "if, for every € > 0,\n",
      "P(\\X, —X|>e) 0\n",
      "asn— oo.\n",
      "\n",
      "if,\n",
      "lim F,(t) = F(t)\n",
      "\n",
      "n—=00\n",
      "\n",
      "at allt for which F is continuous.\n",
      "\n",
      "1. X,;, converges to X in probability, written X23 X%,\n",
      "\n",
      "(6.1)\n",
      "\n",
      "2. X, converges to X in distribution, written X, ~~ X,\n",
      "\n",
      "(6.2)\n",
      "\n",
      " \n",
      "\n",
      "There is another type of convergence which we introduce\n",
      "\n",
      "mainly because it is useful for proving convergence in proba-\n",
      "\n",
      "bility.\n",
      "\n",
      " \n",
      "\n",
      "Definition 6.2 X,, converges to X in quadratic mean\n",
      "(also called convergence in Lz), written XS X, if,\n",
      "\n",
      "E(X, — X)? +0 (6.3)\n",
      "\n",
      "asn— oo.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "qm\n",
      "\n",
      "f X is a point mass at c — that is P(X = c) = 1 — we write\n",
      "\n",
      "am. a . > P\n",
      "X,— ¢ instead of X—> X. Similarly, we write X,—+ c and\n",
      "\n",
      "Xe.\n",
      "\n",
      "Example 6.3 Let X,, ~ N(0,1/n). Intuitively, X,, is concentrat-\n",
      "ing at 0 so we would like to say that X,, ~+ 0. Let’s see if this\n",
      "is true. Let F be the distribution function for a point mass at\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-088.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E19C8>\n",
      "92 6. Convergence of Random Variables\n",
      "\n",
      "0. Note that /nXn ~ N(0,1). Let Z denote a standard normal\n",
      "random variable. For t < 0, F,(t) = P(X, < t) = P(/nxX, <\n",
      "Jnt) = P(Z < Wnt) — 0 since nt — —oo. Fort > 0,\n",
      "F,(t) = P(X, < t) = P( nx, < Jnt) = P(Z < nt) 41\n",
      "since ,/nt — oo. Hence, F,(t) > F(t) for allt 4 0 and so\n",
      "X, ~ 0. But notice that F,(0) = 1/2 4 F(1/2) = 1 so con-\n",
      "vergence fails at t = 0. But that doesn’t matter because t = 0 is\n",
      "not a continuity point of F and the definition of convergence\n",
      "distribution only requires convergence at continuity points.\n",
      "\n",
      "in\n",
      "\n",
      " \n",
      "\n",
      "The next theorem gives the relationship between the types of\n",
      "convergence. The results are summarized in Figure 6.1.\n",
      "\n",
      "Theorem 6.4 The following relationships hold:\n",
      "\n",
      "(a) pea ® implies that os Kes\n",
      "\n",
      "(b) X,—+X implies that X,, + X.\n",
      "\n",
      "(c) If X, ~~ X and if P(X =c) =1 for some real number c,\n",
      "then Xp—2> X.\n",
      "\n",
      "In general, none of the reverse implications hold except the\n",
      "special case in (c).\n",
      "\n",
      "PROOF. We start by proving (a). Suppose that X, “5 X. Fix\n",
      "€ > 0. Then, using Chebyshev’s inequality,\n",
      "\n",
      "E|X, — X[?\n",
      "aie AL\n",
      "\n",
      "P(|Xn— X| > 6) = P([Xn— XP > 2) $5 0.\n",
      "\n",
      "€\n",
      "Proof of (b). This proof is a little more complicated. You may\n",
      "skip if it you wish. Fix € > 0 and let a be a continuity point of\n",
      "F. Then\n",
      "F(t) = P(X, <2) =P(X, <2,X <at+e)+P(X, <2,X >r+6)\n",
      "P(X <2+e6)+P(|X,-X|>6)\n",
      "F(x +6) +P(|X, — X| >).\n",
      "\n",
      "IA\n",
      "\n",
      "ll\n",
      "\n",
      "Also.\n",
      "\n",
      "F(@-6€) = P(X <a-e) =P(X <a—-e€,X, <2) + P(X <4+6,X, >2)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-089.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891699B48>\n",
      "6.2 Types of Convergence 93\n",
      "< F(z) +P(|Xn — X| > ©).\n",
      "Hence,\n",
      "F(z—€)—P(|Xn—X| > €) S Fa(z) < F(a+e)+P(|Xn—X| > ©).\n",
      "Take the limit as n — oo to conclude that\n",
      "\n",
      "F(a —«) < liminf F,(x) < limsup F(x) < F(a +).\n",
      "\n",
      "n—00 n—00\n",
      "\n",
      "This holds for all ¢ > 0. Take the limit as € > 0 and use the fact\n",
      "that F is continuous at « and conclude that lim, F(a) = F(a).\n",
      "Proof of (c). Fix € > 0. Then,\n",
      "\n",
      "P(\\Xn-cl>e) = P(X, <c—e)+P(X, >c+6)\n",
      "\n",
      "< P(X, <e-6) + P(X > ete)\n",
      "= F,(c-69)+1-F,(c+e¢\n",
      "> F(e-6é)+1-F(e+e\n",
      "\n",
      "= 0+1-0=0.\n",
      "\n",
      "Let us now show that the reverse implications do not hold.\n",
      "\n",
      "CONVERGENCE IN PROBABILITY DOES NOT IMPLY CON-\n",
      "VERGENCE IN QUADRATIC MEAN. Let U ~ Unif(0,1) and let\n",
      "Xn = VnToa/n)(U)- Then P(|Xn| > €) = P(VnIo1/m)(U) >\n",
      "6) = BPO <U <1/n) = 1/n = 0. Hence, Then X,,++0. But\n",
      "E(X2) = nfo\" du = 1 for all n so X,, does not converge in\n",
      "quadratic mean.\n",
      "\n",
      "CONVERGENCE IN DISTRIBUTION DOES NOT IMPLY CON-\n",
      "VERGENCE IN PROBABILITY. Let X ~ N(0,1). Let X, = —X\n",
      "for n = 1,2,3,...; hence X, ~ N(0,1). X;, has the same distri-\n",
      "bution function as X for all n so, trivially, lim, F(x) = F(x)\n",
      "for all x. Therefore, X,,  X. But P(|X, —X| > €) = P(|2X| >\n",
      "©) = P(|X| > €/2) £0. So X,, does not tend to X in probability.\n",
      "a\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-090.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "94 6. Convergence of Random Variables\n",
      "\n",
      " \n",
      "\n",
      "waar\n",
      "quadratic mean ————— = probability — distribution\n",
      "\n",
      "FIGURE 6.1. Relationship between types of convergence.\n",
      "\n",
      "Warning! One might conjecture that if Kys b then E(X,,) >\n",
      "We can conclude b. This is not true. Let X,, be a random variable defined by\n",
      "that E(X,) —> b P(X, =n?) =1/n and P(X, =0) = 1 -(1/n). Now, P(|Xn| <\n",
      "if Xn is uniformly e) = P(X, = 0) = 1- (1/n) - 1. Hence, Z*+0. However,\n",
      "integrable. See the Bex. ) = In? x (1/n)]+[0x (1—-(1/n))] =n. Thus, E(X,,) — 00.\n",
      "technical appendix.\n",
      "\n",
      "Summary. Stare at Figure 6.1.\n",
      "\n",
      "Some convergence properties are preserved under transforma-\n",
      "\n",
      "tions.\n",
      "\n",
      "Theorem 6.5 Let X,,X,Yn,Y be random variables. Let g be a\n",
      "continuous function.\n",
      "\n",
      "(a) If X,—+X and ¥,—>Y, then X,»+Y¥n2>X+Y.\n",
      "\n",
      "(b) If Xn X and ¥,22Y, then X,+V¥neX+Y.\n",
      "\n",
      "(c) If X, ~~» X and Y, ~ ¢, then X,+Yn~ X +e.\n",
      "\n",
      "(d) If Xp—7>X and Y¥,—>Y, then X,Yn—> XY.\n",
      "\n",
      "(e) If Xn ~» X and Y, ~ ¢, then XnYn ~> cX.\n",
      "\n",
      "(f) If Xp2+X then g(Xp)+ 9(X).\n",
      "\n",
      "(9) If X, ~» X then g(X,) ~ g({X).\n",
      "\n",
      "6.3 The Law of Large Numbers\n",
      "\n",
      "Now we come to a crowning achievement in probability, the law\n",
      "of large numbers. This theorem says that the mean of a large\n",
      "sample is close to the mean of the distribution. For example,\n",
      "the proportion of heads of a large number of tosses is expected\n",
      "to be close to 1/2. We now make this more precise.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-091.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E92C8>\n",
      "6.3 The Law of Large Numbers 95\n",
      "\n",
      "Let X1,X2,..., be an 1D sample and let yp = E(X,) and\n",
      "o? = V(X;). Recall that the sample mean is defined as X, =\n",
      "n-! SYj- Xj and that E(X,,) = and V(X,,) = 0?/n.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 6.6 (The Weak Law of Large Numbers (WLLN).)\n",
      "If X1,...,Xn are UD , then pomeagy\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Interpretation of WLLN: The distribution of X,, be-\n",
      "comes more concentrated around ju as n gets large.\n",
      "\n",
      "PRoor. Assume that ¢ < oo. This is not necessary but it\n",
      "simplifies the proof. Using Chebyshev’s inequality,\n",
      "V(Xn) _\n",
      "\n",
      "P(|X,-vl>e) <\n",
      "\n",
      " \n",
      "\n",
      "e ne\n",
      "\n",
      "which tends to 0 as n — oo.\n",
      "\n",
      "Example 6.7 Consider flipping a coin for which the probability\n",
      "of heads is p. Let X; denote the outcome of a single toss (0\n",
      "or 1). Hence, p = P(X; = 1) = E(X;j). The fraction of heads\n",
      "after n tosses is X,. According to the law of large numbers,\n",
      "X,, converges to p in probability. This does not mean that X,\n",
      "will numerically equal p. It means that, when n is large, the\n",
      "distribution of X,, is tightly concentrated around p. Suppose that\n",
      "p= 1/2. How large should n be so that P(.4< Xn < .6) > .7?\n",
      "\n",
      "First, E(X,,) = p = 1/2 and V(X,,) = 0?/n = p(l—p)/n =\n",
      "1/(4n). From Chebyshev’s inequality,\n",
      "\n",
      "P(4<X, <6) = P(X,-p| <1)\n",
      "1-—P(|Xn — pl] > 1)\n",
      "oo Lo t- 25\n",
      "~ 4n(.1)? n-\n",
      "\n",
      "ll\n",
      "\n",
      "ll\n",
      "\n",
      "The last expression will be larger than .7 ifn = 84.\n",
      "\n",
      "Note that po =\n",
      "E(X;) is the same\n",
      "for all i so we can\n",
      "define yp = E(X;)\n",
      "for any i. By con-\n",
      "vention, we often\n",
      "write js = E(X,).\n",
      "\n",
      "There is a stronger\n",
      "theorem in the ap-\n",
      "pendix called the\n",
      "strong law of large\n",
      "numbers.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-092.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148A1C23048>\n",
      "96 6. Convergence of Random Variables\n",
      "6.4 The Central Limit Theorem\n",
      "\n",
      "Suppose that X,,...,X, are iid with mean yz and variance o.\n",
      "The central limit theorem (CLT) says that X, = n-1 >>, X;\n",
      "has a distribution which is approximately Normal with mean pu\n",
      "and variance o?/n. This is remarkable since nothing is assumed\n",
      "about the distribution of X;, except the existence of the mean\n",
      "and variance.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 6.8 (The Central Limit Theorem (CLT).) Let X1].\n",
      "be ID with mean ps and variance o?. Let X, =n! oP, X}.\n",
      "\n",
      "Then _\n",
      "7, = VIX) og\n",
      "o\n",
      "\n",
      "where Z ~ N(0,1). In other words,\n",
      "\n",
      "1 2\n",
      "= 27/24\n",
      "€ it\n",
      "Van\n",
      "\n",
      "n—00\n",
      "\n",
      "lim P(Z, < z) = O(z) = [\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Interpretation: Probability statements about X, can\n",
      "be approximated using a Normal distribution. It’s the\n",
      "probability statements that we are approximating, not\n",
      "the random variable itself.\n",
      "\n",
      "In addition to Z, ~+ N(0,1), there are several forms of nota-\n",
      "tion to denote the fact that the distribution of Z,, is converging\n",
      "to a Normal. They all mean the same thing. Here they are:\n",
      "\n",
      "Z, ~ N(01)\n",
      "Zz\n",
      "\n",
      "x & N(u “)\n",
      "n\n",
      "\n",
      "(05)\n",
      "\n",
      "n\n",
      "\n",
      "Xn-p ® N\n",
      "N (0, 02)\n",
      "\n",
      "4\n",
      "al\n",
      "|\n",
      "gv\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-093.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1EC8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4 The Central Limit Theorem 97\n",
      "\n",
      "Vi(Xn — #)\n",
      "\n",
      "x= N(0,1).\n",
      "\n",
      "Example 6.9 Suppose that the number of errors per computer\n",
      "program has a Poisson distribution with mean 5. We get 125 pro-\n",
      "grams. Let Xy,..., Xj25 be the number of errors in the programs.\n",
      "We want to approximate P(X < 5.5). Let w= E(X1) =A =5\n",
      "and 0? = V(X,) =A=5. Then,\n",
      "\n",
      "_ Ky — i(5.5 — ju\n",
      "P(X <5.5) =P ( = H) sa “) = P(Z < 2.5) = .9938.\n",
      "\n",
      " \n",
      "\n",
      "The central limit theorem tells us that Z, = //n(X — p)/o\n",
      "is approximately N(0,1). However, we rarely know o. We can\n",
      "estimate o? from X,,...,X, by\n",
      "\n",
      "1 n _\n",
      "\n",
      "Le =).\n",
      "\n",
      "Wo\n",
      "\n",
      "This raises the following question: if we replace o with S', is the\n",
      "central limit theorem still true? The answer is yes.\n",
      "\n",
      "Theorem 6.10 Assume the same conditions as the CLT. Then,\n",
      "\n",
      "Vil(Xn = 1)\n",
      "\n",
      "o> NOD).\n",
      "\n",
      "You might wonder, how accurate the normal approximation\n",
      "is. The answer is given in the Berry-Essten theorem.\n",
      "\n",
      "Theorem 6.11 (Berry-Esséen.) Suppose that E|.X,|* < oo. Then\n",
      "\n",
      "(6.4)\n",
      "\n",
      " \n",
      "\n",
      "There is also a multivariate version of the central limit theo-\n",
      "rem.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-094.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148A1C23048>\n",
      "98 6. Convergence of Random Variables\n",
      "\n",
      "Theorem 6.12 (Multivariate central limit theorem) Let X;\n",
      "be UD random vectors where\n",
      "\n",
      "Xi\n",
      "X= as\n",
      "Xki\n",
      "with mean\n",
      "MM E(Xii)\n",
      "He E(X2;)\n",
      "L= = :\n",
      "Mi E(Xii)\n",
      "and variance matria X. Let\n",
      "X,\n",
      "x-|\n",
      "Xi\n",
      "\n",
      "where X; =n! 0\", Xy;. Then,\n",
      "\n",
      "Vn(X — 1) ~» N(0,5).\n",
      "\n",
      "6.5 The Delta Method\n",
      "\n",
      "If Y,, has a limiting Normal distribution then the delta method\n",
      "allows us to find the limiting distribution of g(Y,,) where g is\n",
      "any smooth function.\n",
      "\n",
      "Theorem 6.13 (The Delta Method) Suppose that\n",
      "Ven =), (0,1)\n",
      "o\n",
      "and that g is a differentiable function such that g'(w) £0. Then\n",
      "\n",
      "ve NOD\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-095.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "6.5 The Delta Method 99\n",
      "\n",
      "In other words,\n",
      "2\n",
      "\n",
      "me »(m *) = 9%) © x(a wre).\n",
      "\n",
      "Example 6.14 Let X,,...,Xn be 1D with finite mean ju and fi-\n",
      "nite variance o?. By the central limit theorem, \\/n(Xn)/o ~>\n",
      "N(0,1). Let W, = eX, Thus, W, = g(Xn) where g(s) = e°.\n",
      "Since g'(s) = e°, the delta method implies that W, ~ N(e\",e?\"0?/n).\n",
      "a\n",
      "\n",
      "There is also a multivariate version of the delta method.\n",
      "Theorem 6.15 (The Multivariate Delta Method) Suppose that\n",
      "Yn = (Yni,---,Ynk) is a sequence of random vectors such that\n",
      "\n",
      "Vail — pn)» N(0,3).\n",
      "\n",
      "Let g: R* > R ane let\n",
      "dg\n",
      "\n",
      "Oy\n",
      "\n",
      "Voy)= |:\n",
      "ag.\n",
      "Oi\n",
      "\n",
      "Let V,, denote Vg(y) evaluated at y = js and assume that the\n",
      "elements of V,, are non-zero. Then\n",
      "\n",
      "Vn(g(¥n) ~ 9()) > N (0, Vi=V,) -\n",
      "Example 6.16 Let\n",
      "Xu X12 Xin\n",
      "Xo J? \\ Xe J? \\ Xan\n",
      "\n",
      "be 1D random vectors with mean f= (}t1, 2)? and variance D.\n",
      "\n",
      "Let h h\n",
      "X= nt X= 7\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-096.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1EC8>\n",
      "100 6. Convergence of Random Variables\n",
      "\n",
      "and define Y, = X1X2. Thus, Yn = g(X1, X») where g(s1, 82) =\n",
      "$189. By the central limit theorem,\n",
      "\n",
      "X2— be\n",
      "og\n",
      "3 -(2)\n",
      "og\n",
      "Os2 a\n",
      "\n",
      "VIDV. = (Mo be ou 13 Be) x Boy + 2p p2012 + peor.\n",
      "pov Gin my (2 09 I M2011 1 44 2012 TF Hy O22\n",
      "\n",
      "vii( Xi i ) ~» N(0,5).\n",
      "\n",
      "Now\n",
      "Va(s)\n",
      "\n",
      "Il\n",
      "\n",
      "and so\n",
      "\n",
      "Therefore,\n",
      "\n",
      "Vn(X Xo — [i fl2) N (0,130 + Qu pooi + pon). i\n",
      "\n",
      "6.6 Bibliographic Remarks\n",
      "\n",
      "Convergence plays a central role in modern probability the-\n",
      "ory. For more details, see Grimmet and Stirzaker (1982), Karr\n",
      "(1993) and Billingsley (1979). Advanced convergence theory is\n",
      "explained in great detail in van der Vaart and Wellner (1996)\n",
      "and van der Vaart (1998).\n",
      "\n",
      "6.7 Technical Appendix\n",
      "\n",
      "6.7.1 Almost Sure and L, Convergence\n",
      "\n",
      "We say that X, converges almost surely to X, written\n",
      "X,— > X, if\n",
      "P({s: Xn(s) + X(s)}) =1.\n",
      "\n",
      "We say that X,, converges in L; to X, written aE, X, if\n",
      "\n",
      "E|X, —X|—0\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-097.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907A7088>\n",
      "6.7 Technical Appendix 101\n",
      "\n",
      "asn— oo.\n",
      "Theorem 6.17 Let X,, and X be random vaiables. Then:\n",
      "(a) X,—+ X implies that Xue x\n",
      "(b) X,“4.X implies that Nig\n",
      "(c) X,25 xX implies that Xe XxX.\n",
      "\n",
      "The weak law of large numbers says that X,, converges to\n",
      "EX; in probability. The strong law asserts that this is also true\n",
      "almost surely.\n",
      "\n",
      "Theorem 6.18 (The strong law of large numbers.) Let X,,Xo,...\n",
      "be tid. If 1p = E|.X1| < oo then se\n",
      "\n",
      "A sequence X,, is asymptotically uniformly integrable if\n",
      "\n",
      "Jim lim sup E (|X,,|Z(|X,,| > M)) = 0.\n",
      "M=00\n",
      "\n",
      "n—00\n",
      "\n",
      "If X,—> b and X,, is asymptotically uniformly integrable, then\n",
      "E(X;) = b.\n",
      "\n",
      "6.7.2 Proof of the Central Limit Theorem\n",
      "\n",
      "If X is a random variable, define its moment generating func-\n",
      "\n",
      "tion (mgf) by Wx(t) = Ee'*. Assume in what follows that the\n",
      "mef is finite in a neighborhood around t = 0.\n",
      "Lemma 6.19 Let Z,,Z,... be a sequence of random variables.\n",
      "Let Ww, the mgf of Z,. Let Z be another random variable and\n",
      "denote its mgf by w. If Un(t) + w(t) for all t in some open\n",
      "interval around 0, then Z, ~+ Z.\n",
      "\n",
      "PROOF OF THE CENTRAL LIMIT THEOREM. Let Y; = (X; —\n",
      "p)/o. Then, Z, = n-¥/? do Vi. Let v(t) be the mef of Y;. The\n",
      "megf of >, Y; is (u(t))” and mef of Z,, is [v(t//n)]”\" = &.(t).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-098.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1F08>\n",
      "102 6. Convergence of Random Variables\n",
      "\n",
      "Now w'(0) = E(¥i) = 0, w\"(0) = E(Y?) = Var(Yi) = 1. So,\n",
      "\n",
      "2 3\n",
      "w(t) = (0) +4v\"(0) + su\") + 7\") fives\n",
      "\n",
      "a.\n",
      "\n",
      "= 1404042 wo\n",
      "= ta 3 (0) +-++\n",
      "\n",
      "Pb\n",
      "= lt yt qQva+\n",
      "\n",
      "2\n",
      "\n",
      "Now,\n",
      "\n",
      " \n",
      "\n",
      "which is the mgf of a N(0,1). The result follows from the previous\n",
      "Theorem. In the last step we used the fact that, if a, — a then\n",
      "\n",
      "Gn\\”\n",
      "(1 + =) ee.\n",
      "n\n",
      "\n",
      "6.8 Excercises\n",
      "\n",
      "1. Let X4,..., X,, be iid with finite mean pp = E(X,) and\n",
      "finite variance 0? = V(X,). Let X,, be the sample mean\n",
      "and let S? be the sample variance.\n",
      "\n",
      "(a) Show that E(S?) = 0?.\n",
      "\n",
      "(b) Show that se, o?. Hint: Show that $2 = can~! 0\", X?—\n",
      "dX, where c, — 1 and d, — 1. Apply the law of large\n",
      "numbers to n~! )77_, X? and to X,,. Then use part (e) of\n",
      "Theorem 6.5.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-099.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907BAA88>\n",
      ". Let X1,...\n",
      "\n",
      "6.8 Excercises 103\n",
      "\n",
      ". Let X1,X2,... be a sequence of random variables. Show\n",
      "\n",
      "that X,,“ b if and only if\n",
      "\n",
      "lim E(X,) =6 and lim V(X,) =0.\n",
      "\n",
      "n—00 n—00\n",
      "\n",
      ". Let X4,..., X,, be iid and let pp = E(X,). Suppose that\n",
      "\n",
      "the variance is finite. Show that X,,\"> pu.\n",
      "\n",
      ". Let X1,X2,... be a sequence of random variables such\n",
      "that\n",
      "1 1 1\n",
      "P(x = *) =1- 72 and P(X, =n)= ae\n",
      "\n",
      "Does X,, converge in probability? Does X,, converge in\n",
      "quadratic mean?\n",
      "\n",
      "X, ~ Bernoulli(p). Prove that\n",
      "\n",
      "n\n",
      "\n",
      "ig 1 im\n",
      "~S7 x? Foy and —S>X?Sp.\n",
      "n f=1 n\n",
      "\n",
      "i=l\n",
      "\n",
      ". Suppose that the height of men has mean 68 inches and\n",
      "\n",
      "standard deviation 4 inches. We draw 100 men at ran-\n",
      "dom. Find (approximately) the probability that the aver-\n",
      "age height of men in our sample will be at least 68 inches.\n",
      "\n",
      ". Let A, = 1/n for n = 1,2,.... Let X,, ~ Poisson(A,).\n",
      "\n",
      "(a) Show that Xe s0.\n",
      "(b) Let Y;, = nX,,. Show that Y,—> 0.\n",
      "\n",
      ". Suppose we have a computer program consisting of n =\n",
      "\n",
      "100 pages of code. Let X; be the number of errors on the i”\n",
      "page of code. Suppose that the X/s are Poisson with mean\n",
      "1 and that they are independent. Let Y = 7, X; be the\n",
      "total number of errors. Use the central limit theorem to\n",
      "approximate P(Y < 90).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-100.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "104\n",
      "\n",
      "9:\n",
      "\n",
      "10.\n",
      "\n",
      "11.\n",
      "\n",
      "12.\n",
      "\n",
      "6. Convergence of Random Variables\n",
      "\n",
      "Suppose that P(X = 1) = P(X = —1) = 1/2. Define\n",
      "\n",
      "n\n",
      "\n",
      "with probability 2.\n",
      "\n",
      "n\n",
      "\n",
      "X= { X~ with probability 1— +\n",
      "\n",
      "e\n",
      "\n",
      "Does X,, converge to X in probability? Does X,, converge\n",
      "to X in distribution? Does E(X — X,,)? converge to 0?\n",
      "\n",
      "Let Z~ N(0,1). Let t > 0.\n",
      "(a) Show that, for any k > 0,\n",
      "\n",
      "E|Z|*\n",
      "\n",
      "P(Z|>#) <=.\n",
      "\n",
      " \n",
      "\n",
      "(b) (Mill's inequality.) Show that\n",
      "9)?\n",
      "P(|Z|>t)<4-\n",
      "(a>n< {zh\n",
      "\n",
      "Hint. Note that P(|Z| > t) = 2P(Z > t). Now write out\n",
      "what P(Z > t) means and note that x/t > 1 whenever\n",
      "z>t.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Suppose that X, ~ N(0,1/n) and let X be a random\n",
      "variable with distribution F(x) = 0 if a <0 and F(x) =1\n",
      "if a > 0. Does X, converge to X in probability? (Prove or\n",
      "disprove). Does X,, converge to X in distribution? (Prove\n",
      "or disprove).\n",
      "\n",
      "Let X,X), X2, X3,... be random variables that are posi-\n",
      "tive and integer valued. Show that X,, ~» X if and only\n",
      "if\n",
      "\n",
      "lim P(X, =k) = P(X =k)\n",
      "\n",
      "noo\n",
      "\n",
      "for every integer k.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-101.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.\n",
      "\n",
      "14.\n",
      "\n",
      "6.8 Excercises 105\n",
      "\n",
      "Let Z, Z2,... be ii.d., random variables with density f.\n",
      "Suppose that P(Z; > 0) = 1 and that \\ = lim, \\9 f(x) > 0.\n",
      "Let\n",
      "\n",
      "X, =nmin{Z,...,Z,}.\n",
      "\n",
      "Show that X,, ~+ Z where Z has an exponential distribu-\n",
      "tion with mean 1/A.\n",
      "\n",
      "Let X,,...,X,, ~ Uniform(0, 1). Let Y, = X?. Find the\n",
      "limiting distribution of Y,,.\n",
      "\n",
      "Xu X12 Xin\n",
      "Xa J? \\ X22 J \\ Xen\n",
      "be iid random vectors with mean js = (/41, /2) and variance\n",
      "x. Let\n",
      "i 1\n",
      "X= 5% Ras Xa\n",
      "\n",
      "and define Y,, = X1/X». Find the limiting distribution of\n",
      "Yn\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-102.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "Part II\n",
      "\n",
      "Statistical Inference\n",
      "\n",
      "103\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-103.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-104.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-105.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907BAA88>\n",
      "7\n",
      "\n",
      "Models, Statistical Inference and\n",
      "Learning\n",
      "\n",
      "7.1 Introduction\n",
      "\n",
      "Statistical inference, or “learning” as it is called in computer\n",
      "science, is the process of using data to infer the distribution\n",
      "that generated the data. The basic statistical inference problem\n",
      "is this:\n",
      "\n",
      "We observe X1,...,X, ~ F. We want to infer (or\n",
      "estimate orlearn) F orsome feature of F' such as its\n",
      "\n",
      "mean.\n",
      "\n",
      "7.2. Parametric and Nonparametric Models\n",
      "\n",
      "A statistical model is a set of distributions (or a set of\n",
      "densities) §. A parametric model is a set § that can be pa-\n",
      "rameterized bya finite nunber of parameters. For example, if\n",
      "we assume that the data come from a Normal distribution then\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-106.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1F08>\n",
      "The distinction\n",
      "between parametric\n",
      "and nonparametric\n",
      "is more subtle than\n",
      "this but we don’t\n",
      "\n",
      "need a_ rigorous\n",
      "definition for our\n",
      "purposes.\n",
      "\n",
      "106 7. Models, Statistical Inference and Learning\n",
      "\n",
      "the model is\n",
      "\n",
      "=\n",
      "\n",
      "{fleima) =e\n",
      "\n",
      "ex {sale wt », HER o> of :\n",
      "\n",
      "(7.1)\n",
      "This is a two-parameter model. We have written the density\n",
      "as f(2;p,0) to show that x is a value of the random variable\n",
      "whereas ji and o are parameters. In general, a parametric model\n",
      "takes the form\n",
      "\n",
      "B= { se): eco} (7.2)\n",
      "where @ is an unknown parameter (or vector of parameters) that\n",
      "can take values in the parameter space 0. If @ is a vector but\n",
      "we are only interested in one component of @, we call the re-\n",
      "maining parameters nuisance parameters. A nonparamet-\n",
      "ric model is a set § that cannot be parameterized by a finite\n",
      "number of parameters. For example, $a, = {all CDF 's} is non-\n",
      "parametric.\n",
      "\n",
      "Example 7.1 (One-dimensional Parametric Estimation.) Let Xj,...\n",
      "be independent Bernoulli(p) observations. The problem is to es-\n",
      "timate the parameter p. fl\n",
      "\n",
      "Example 7.2 (Two-dimensional Parametric Estimation.) Suppose that\n",
      "X1,...,Xn~ F and we assume that the PDF f € § where § is\n",
      "given in (7.1). In this case there are two parameters, jt and o.\n",
      "The goal is to estimate the parameters from the data. If we are\n",
      "only interested in estimating p then jt is the parameter of inter-\n",
      "est and o is a nuisance parameter. Wl\n",
      "\n",
      "Example 7.3 (Nonparametric estimation of the cdf.) Let X,,...,Xp\n",
      "be independent observations from a cdf F. The problem is to es-\n",
      "timate F assuming only that F € §ay, = {all cpr 's}.\n",
      "\n",
      "Example 7.4 (Nonparametric density estimation.) Let X,,...,Xn\n",
      "\n",
      "be independent observations from a cdf F and let f = F\"' be the\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-107.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n",
      "7.2 Parametric and Nonparametric Models 107\n",
      "\n",
      "PDF . Suppose we want to estimate the PDF f. It is not pos-\n",
      "sible to estimate f assuming only that F € Sau. We need to\n",
      "assume some smoothness on f. For example, we might assume\n",
      "that f € § = ®pens() Ssow where Fpens is the set of all proba-\n",
      "bility density functions and\n",
      "\n",
      "son = {s : fu\"opar< oo}.\n",
      "\n",
      "The class §sop is called a Sobolev space; it is the set of func-\n",
      "tions that are not “too wiggly.” 1\n",
      "\n",
      "Example 7.5 (Nonparametric estimation of functionals.) Let Xj,...\n",
      "F. Suppose we want to estimate pp = E(Xi) = f rdF (x) as-\n",
      "\n",
      "suming only that 4s exists. The mean js may be thought of as a\n",
      "\n",
      "function of F: we can write y = T(F) = f «dF (x). In general,\n",
      "\n",
      "any function of F is called a statistical functional. Other\n",
      "\n",
      "examples of functions are the variance T(F) = f 2°dF(x) —\n",
      "\n",
      "(f adF(zx))” and the median T(F) = F-‘/?,\n",
      "\n",
      "Example 7.6 (Regression, prediction and classification.) Suppose we\n",
      "observe pairs of data (X,,Y;),---(Xn,Yn). Perhaps X; is the\n",
      "blood pressure of subject i and Y; is how long they live. X is\n",
      "called a predictor or regressor or feature or independent\n",
      "variable. Y is called the outcome or the response variable\n",
      "or the dependent variable. We call r(x) = E(Y|X = 2) the\n",
      "regression function. If we assume that f € § where § is fi-\n",
      "nite dimensional — the set of straight lines for example — then\n",
      "\n",
      " \n",
      "\n",
      "we have a parametric regression model. If we assume that\n",
      "f € where § is not finite dimensional then we have a para-\n",
      "metric regression model. The goal of predicting Y for a new\n",
      "patient based on their X value is called prediction. If Y is dis-\n",
      "crete (for example, live or die) then prediction is instead called\n",
      "classification. If our goal is to estimate the functin f, then we\n",
      "call this regression or curve estimation. Regression models\n",
      "\n",
      "Xn~\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-108.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891699B48>\n",
      "108 7. Models, Statistical Inference and Learning\n",
      "are sometimes written as\n",
      "Y =f(X) +e (7.3)\n",
      "\n",
      "where E(e) = 0. We can always rewrite a regression model this\n",
      "way. To see this, define e = Y — f(X) and hence Y = Y +\n",
      "f(X)—f(X) = f(X)+e. Moreover, E(e) = EE(e|X) = E(E(Y—\n",
      "F(X)IX) = EEX) — f(X)) = E(F(X) — F(X) =0.\n",
      "\n",
      "Wuat’s NEXT? It is traditional in most introductory courses\n",
      "\n",
      " \n",
      "\n",
      "to start with parametric inference. Instead, we will start with\n",
      "nonparametric inference and then we will cover parametric in-\n",
      "ference. In some respects, nonparametric inference is easier to\n",
      "understand and is more useful than parametric inference.\n",
      "\n",
      "FREQUENTISTS AND BAYESIANS. There are many approaches\n",
      "to statistical inference. The two dominant approaches are called\n",
      "frequentist inference and Bayesian inference. We'll cover\n",
      "both but we will start with frequentist inference. We'll postpone\n",
      "a discussion of the pro’s and con’s of these two until later.\n",
      "\n",
      "Some Noration. If § = {f(x; 0) : @ € O} is a parametric\n",
      "model, we write Po(X € A) = f, f(x; @)dx and Eg(r(X)) =\n",
      "[r(2)f(2; 9)dx. The subscript @ indicates that the probability\n",
      "or expectation is with respect to f(z; @); it does not mean we\n",
      "are averaging over @. Similarly, we write Vy for the variance.\n",
      "\n",
      " \n",
      "\n",
      "7.3 Fundamental Concepts in Inference\n",
      "\n",
      "Many inferential problems can be identified as being one of\n",
      "three types: estimation, confidence sets or hypothesis testing.\n",
      "We will treat all of these problems in detail in the rest of the\n",
      "book. Here, we give a brief introduction to the ideas.\n",
      "\n",
      "7.8.1 Point Estimation\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-109.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1F08>\n",
      "7.3 Fundamental Concepts in Inference 109\n",
      "\n",
      "Point estimation refers to providing a single “best guess”\n",
      "of some quantity of interest. The quantity of interest could be a\n",
      "parameter in a parametric model, aCDF F, a probability density\n",
      "function f, a regression function r, or a prediction for a future\n",
      "value Y of some random variable.\n",
      "\n",
      "By convention, we denote a point estimate of @ by 0.\n",
      "Remember that @ is a fixed, unknown quantity. The es-\n",
      "timate 0 depends on the data so @ is a random variable.\n",
      "\n",
      "More formally, let X;,...,X, be n 1D data point from some\n",
      "distribution F. A point estimator 6, of a parameter @ is some\n",
      "function of X,,...,X,:\n",
      "\n",
      "6, =9(&,..\n",
      "\n",
      " \n",
      "\n",
      "_X,).\n",
      "\n",
      "We define\n",
      "bias (8,,) = E9(0,) — 0 (7.4)\n",
      "\n",
      "to be the bias of G,,. We say that 6, is unbiased if E(0,) =.\n",
      "Unbiasedness used to receive much attention but these days it is\n",
      "not considered very important; many of the estimators we will\n",
      "use are biased. A point estimator 8, of a parameter 0 is con-\n",
      "sistent if On—+ @. Consistency is a reasonable requirement for\n",
      "estimators. The distribution of 6, is called the sampling dis-\n",
      "tribution. The standard deviation of 6, is called the standard\n",
      "error, denoted by se:\n",
      "\n",
      "se =se (,) = 1/V(8,)- (7.5)\n",
      "\n",
      "Often, it is not possible to compute the standard error but usu-\n",
      "ally we can estimate the standard error. The estimated standard\n",
      "error is denoted by sé.\n",
      "\n",
      "Example 7.7 Let X;,...,X, ~ Bernoulli(p) and let p,, =n! 7,Xj.\n",
      "Then E(pn) =n7! 3), E(X;) =p so py is unbiased. The standard\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-110.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E1B88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 7. Models, Statistical Inference and Learning\n",
      "\n",
      "error is se = \\/V(pn) = /p(1—p)/n. The estimated standard\n",
      "error is Sé = \\/p(1—p)/n.\n",
      "\n",
      "The quality of a point estimate is sometimes assessed by the\n",
      "mean squared error, or MSE, defined by\n",
      "\n",
      "MSE = E,(0,, — 0)?.\n",
      "\n",
      "Recall that E,(-) refers to expectation with respect to the dis-\n",
      "tribution\n",
      "\n",
      "F(21,---52n3 0) =] Foes 8)\n",
      "i=l\n",
      "\n",
      "that generated the data. It does not mean we are averaging over\n",
      "\n",
      " \n",
      "\n",
      "a density for 0.\n",
      "\n",
      "Theorem 7.8 The MSE can be written as\n",
      "\n",
      " \n",
      "\n",
      "MSE = bias (6,,)? + Vo(On).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "PrRoor. Let 9, = Ey(0n). Then\n",
      "Eo(@n — 9)? = E(u —On + On — 0)”\n",
      "E,(0, — 9)? + 2(0n — 9)\"Eo(8, — 9) +E gn — 0)?\n",
      "(8, — 0)? +E9(On — On)?\n",
      "bias? + V(6,). i\n",
      "\n",
      "Theorem 7.9 If bias — 0 and se — 0 as n —+ 00 then 0, is\n",
      "consistent, that is, 6,2 0.\n",
      "\n",
      "ProorF. If bias — 0 and se —+ 0 then, by Theorem 7.8,\n",
      "MSE -> 0. It follows that 0,4. (Recall definition 6.3.) The\n",
      "result follows from part (b) of Theorem 6.4. ll\n",
      "\n",
      "Example 7.10 Returning to the coin flipping example, we have\n",
      "\n",
      "that E,(p,) =p so that bias = p—p = 0 andse = /p(1 — p)/n >\n",
      "\n",
      "0. Hence, Pn—>D, that is, Py is a consistent estimator.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-111.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "7.3 Fundamental Concepts in Inference 111\n",
      "\n",
      "Many of the estimators we will encounter will turn out to\n",
      "have, approximately, a Normal distribution.\n",
      "\n",
      " \n",
      "\n",
      "Definition 7.11 An estimator is asymptotically Nor-\n",
      "mal if\n",
      "\n",
      "6, —\n",
      "\n",
      "- 8. N(O,1). (7.7)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "7.8.2 Confidence Sets\n",
      "\n",
      "A 1—a confidence interval for a parameter 6 is an interval\n",
      "C,, = (a,b) where a = a(X1,...,X,) and b = b(X,,...,X,) are\n",
      "functions of the data such that.\n",
      "\n",
      "P,(@EC,) >1—a, for all@€O. (7.8)\n",
      "\n",
      "In words, (a,b) traps 9 with probability 1 — a. We call 1 — a\n",
      "the coverage of the confidence interval. Commonly, people use\n",
      "95 per cent confidence intervals which corresponds to choosing\n",
      "a =0.05. Note: C,, is random and @ is fixed! If 6 is a vector\n",
      "then we use a confidence set (such a sphere or an ellipse) instead\n",
      "of an interval.\n",
      "\n",
      "Warning! There is much confusion about how to interpret\n",
      "a confidence interval. A confidence interval is not a probability\n",
      "statement about @ since @ is a fixed quantity, not a random\n",
      "variable. Some texts interpret confidence intervals as follows: if\n",
      "I repeat the experiment over and over, the interval will contain\n",
      "the parameter 95 per cent of the time. This is correct but useless\n",
      "since we rarely repeat the same experiment over and over. A\n",
      "better interpretation is this:\n",
      "\n",
      "On day 1, you collect data and construct a 95 per\n",
      "cent confidence interval for a parameter 6,. On day\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-112.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "112 7. Models, Statistical Inference and Learning\n",
      "\n",
      "2, you collect new data and construct a 95 per cent\n",
      "confidence interval for an unrelated parameter 02. On\n",
      "day 3, you collect new data and construct a 95 per\n",
      "cent confidence interval for an unrelated parameter 63.\n",
      "You continue this way constructing confidence inter-\n",
      "vals for a sequence of unrelated parameters 6}, 02,...\n",
      "Then 95 per cent your intervals will trap the true pa-\n",
      "rameter value. There is no need to introduce the idea\n",
      "of repeating the same experiment over and over.\n",
      "\n",
      "Example 7.12 Every day, newspapers report opinion polls. For\n",
      "example, they might say that “83 per cent of the population fa-\n",
      "vor arming pilots with guns.” Usually, you will see a statement\n",
      "like “this poll is accurate to within 4 points 95 per cent of the\n",
      "time.” They are saying that 83 +4 is a 95 per cent confidence\n",
      "interval for the true but unknown proportion p of people who\n",
      "favor arming pilots with guns. If you form a confidence inter-\n",
      "val this way everyday for the rest of your life, 95 per cent of\n",
      "your intervals will contain the true parameter. This is true even\n",
      "though you are estimating a different quantity (a different poll\n",
      "question) every day.\n",
      "\n",
      "Later, we will discuss Bayesian methods in which we treat\n",
      "@ as if it were a random variable and we do make probability\n",
      "statements about 0. In particular, we will make statements like\n",
      "“the probability that @ is C,, given the data, is 95 per cent.”\n",
      "However, these Bayesian intervals refer to degree-of-belief prob-\n",
      "abilities. These Bayesian intervals will not, in general, trap the\n",
      "parameter 95 per cent of the time.\n",
      "Example 7.13 In the coin flipping setting, let Cn = (Pn—€n, Dnt\n",
      "€n) where €2 = log(2/a)/(2n). From Hoeffding’s inequality (5.4)\n",
      "it follows that\n",
      "\n",
      "P(pEC,) >1—a\n",
      "\n",
      "for every p. Hence, Cn is al —a confidence interval.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-113.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E92C8>\n",
      "7.3 Fundamental Concepts in Inference 113\n",
      "\n",
      "As mentioned earlier, point estimators often have a limiting\n",
      "Normal distribution, meaning that equation (7.7) holds, that\n",
      "is, 0, & N(0,”). In this case we can construct (approximate)\n",
      "confidence intervals as follows.\n",
      "\n",
      "Theorem 7.14 (Normal-based Confidence Interval.) Suppose that\n",
      "0, ~ N(0,s7). Let ® be the CDF of a standard Normal and\n",
      "let 2a/2 = ®-'(1 — (a/2)), that is, P(Z > 2aj2) = a/2 and\n",
      "P(—2a)2 < Z < 2a/2) =1—a where Z ~ N(0,1). Let\n",
      "Qe (On — 20/2, On + 2/28). (7.9)\n",
      "Then\n",
      "P(@EC,) 3 1—a. (7.10)\n",
      "\n",
      "Proor. Let Z, = (0, —0)/S. By assumption Z, ~+ Z where\n",
      "Z ~ N(0,1). Hence,\n",
      "\n",
      "Po(O€ Cn) = Po (On zap ® <0 <b, + xap8)\n",
      "\n",
      "6,—9\n",
      "= Pyl- ” z\n",
      "»(-2a< a <=]\n",
      "\n",
      "+ P(-2za2 < Z < zap)\n",
      "= l-a. @\n",
      "\n",
      " \n",
      "\n",
      "For 95 per cent confidence intervals, a = 0.05 and zy). =\n",
      "1.96 = 2 leading to the approximate 95 per cent confidence\n",
      "interval 6, +28€. We will discuss the construction of confidence\n",
      "intervals in more generality in the rest of the book.\n",
      "\n",
      "Example 7.15 Let X,,...,X, ~ Bernoulli(p) and let p, =n-! Sy, Xi.\n",
      "Then V(f,) =n\", V(X) = 2-2 pll—p) = nap —\n",
      "\n",
      "p) =p(1—p)/n. Hence, se = \\/p(1— p)/n and & = \\/p, (1 —p,)/n.\n",
      "By the Central Limit Theorem, fy © N(p,&°). Therefore, an\n",
      "approximate 1— a confidence interval is\n",
      "\n",
      " \n",
      "\n",
      "PH 2p =P+ rap\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-114.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8708>\n",
      "114 7. Models, Statistical Inference and Learning\n",
      "\n",
      "Compare this with the confidence interval in the previous exam-\n",
      "ple. The Normal-based interval is shorter but it only has approx-\n",
      "imately (large sample) correct coverage. Mi\n",
      "\n",
      "7.8.8 Hypothesis Testing\n",
      "\n",
      "In hypothesis testing, we start with some default theory\n",
      "— called a null hypothesis — and we ask if the data provide\n",
      "sufficient evidence to reject the theory. If not we retain the null\n",
      "The term “retaining hypothesis.\n",
      "the null hypothesis” Example 7.16 (Testing if a Coin is Fair) Suppose X,,...,X, ~ Bernoulli(p)\n",
      "is due to Chris Gen- denote n independent coin flips. Suppose we want to test if the\n",
      "\n",
      "owese: Other termi- coin is fair. Let Hy denote the hypothesis that the coin is fair\n",
      "nology is “accepting\n",
      "\n",
      "the null” or “failing\n",
      "to reject the null.”\n",
      "\n",
      "and let H, denote the hypothesis that the coin is not fair. Ho\n",
      "is called the null hypothesis and H, is called the alternative\n",
      "hypothesis. We can write the hypotheses as\n",
      "\n",
      "Hy:p=1/2 versus H,:p#1/2.\n",
      "\n",
      "It seems reasonable to reject Hy if T = |p, — (1/2)| is large.\n",
      "When we discuss hypothesis testing in detail, we will be more\n",
      "precise about how large T should be to reject Ho.\n",
      "\n",
      "7.4 Bibliographic Remarks\n",
      "\n",
      "Statistical inference is covered in many texts. Elementary\n",
      "texts include DeGroot and Schervish (2001) and Larsen and\n",
      "Marx (1986). At the intermediate level I recommend Casella\n",
      "and Berger (2002) and Bickel and Doksum (2001). At the ad-\n",
      "vanced level, Lehmann and Casella (1998), Lehmann (1986) and\n",
      "van der Vaart (1998).\n",
      "\n",
      "7.5 Technical Appendix\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-115.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n",
      "7.5 Technical Appendix 115\n",
      "\n",
      "Our definition of confidence interval requires that P(@ €\n",
      "Cn) > 1— a for all 6 € ©. An pointwise asymptotic con-\n",
      "fidence interval requires that lim inf, ,.. Pg(@ € C,,) > 1—a for\n",
      "all 9 € ©. An uniform asymptotic confidence interval requres\n",
      "that lim inf, 4. infyee Po(@ € Cn) > 1— a. The approximate\n",
      "Normal-based interval is a pointwise asymptotic confidence in-\n",
      "terval. In general, it might not be a uniform asymptotic confi-\n",
      "dence interval.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-116.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907A7088>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 7. Models, Statistical Inference and Learning\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-117.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E1948>\n",
      "8\n",
      "\n",
      "Estimating the CDF and Statistical\n",
      "Functionals\n",
      "\n",
      "The first inference problem we will consider is nonparametric\n",
      "\n",
      "estimationthh cpF F and functions of the CDF.\n",
      "\n",
      "8.1 The Empirical Distribution Function\n",
      "\n",
      "Let X1,...,X, ~ F be 1D where F is a distribution function\n",
      "\n",
      "on the real line. We will estimate F with the empirical distribu-\n",
      "\n",
      "tion function, which is defined as follo ws.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Definition 8.1 The empirical distribution function F,\n",
      "is the ODF that puts mass 1/n at e ach data pint X;. For-\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "mally,\n",
      "\n",
      "di nl Xy <\n",
      "\n",
      "Fic) = Die (Xi < 2)\n",
      "\n",
      "n\n",
      "nunber of observations less than or equal BE\n",
      "—\n",
      "n\n",
      "where\n",
      "\n",
      ". 1 ifX;<2\n",
      "1x<a)={ 4 if Xj >\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-118.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C85C8>\n",
      "Actually,\n",
      "\n",
      "sup, |F,(x) — F(x)\n",
      "\n",
      "converges to\n",
      "almost surely.\n",
      "\n",
      "0\n",
      "\n",
      "118 8. Estimating the CDF and Statistical Functionals\n",
      "\n",
      "Example 8.2 (Nerve Data.) Cor and Lewis (1966) reported 799\n",
      "waiting times between successive pulses along a nerve fibre. The\n",
      "first plot in Figure 8.1 shows the a “toothpick plot” where each\n",
      "toothpick shows the location of one data point. The second plot\n",
      "shows that empirical CDF F,. Suppose we want to estimate the\n",
      "fraction of waiting times between .4 and .6 seconds. The estimate\n",
      "is F,(.6) — F,(.4) =.93— .84 =.09.\n",
      "\n",
      "The following theorems give some properties of F',(z7).\n",
      "\n",
      "Theorem 8.3 At any fixed value of x,\n",
      "\n",
      "E(Fi(z)) = F(o) and V (Fala) =\n",
      "Thus,\n",
      "\n",
      "F(a) = Fle),\n",
      "\n",
      "F(z)(1- F(a)\n",
      "\n",
      "MSE = > 0\n",
      "\n",
      "and hence, F,,(%)——> F (2).\n",
      "\n",
      "Theorem 8.4 (Glivenko-Cantelli Theorem.) Let X,,...,X,~ F.\n",
      "Then\n",
      "sup |F,(x) — F(x)|—+0.\n",
      "\n",
      "8.2 Statistical Functionals\n",
      "\n",
      "A statistical functional T(F) is any function of F. Examples\n",
      "are the mean p. = f rdF (x), the variance o? = f(a — 1)*dF (x\n",
      "and the median m = F-!(1/2).\n",
      "\n",
      "a\n",
      "\n",
      " \n",
      "\n",
      "Definition 8.5 The plug-in estimator of 0 = T(F) is\n",
      "defined by\n",
      "\n",
      "6, =T(F,).\n",
      "\n",
      "In other words, just plug in F, for the unknown F.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-119.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "8.2 Statistical Functionals 119\n",
      "\n",
      " \n",
      "\n",
      "Reteseesaccuucasesceagencccuovaseesneaengvcssreaeenay(eau yCereeiteetUU ne)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\n",
      "\n",
      "time\n",
      "\n",
      " \n",
      "\n",
      "cdf\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "0.0 04 08\n",
      "\n",
      " \n",
      "\n",
      "time\n",
      "\n",
      "FIGURE 8.1. Nerve data. The solid line in the middle is the empirical\n",
      "distribution function. The lines above and below the middle line are\n",
      "a 95 per cent confidence band. The confidence band is explained in\n",
      "the appendix.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-120.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E9308>\n",
      "120 8. Estimating the CDF and Statistical Functionals\n",
      "\n",
      "A functional of the form f r(«)dF(2) is called a linear func-\n",
      "tional. Recall that fr(x)dF(x) is defined to be [r(2) f(x)dx\n",
      "in the continuous case and }),r(2;)f(x;) in the discrete. The\n",
      "empirical cdf F,(x) is discrete, putting mass 1/n at each X;.\n",
      "Hence, if T(F) = fr(x)dF(2) is a linear functional then we\n",
      "have:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The plug-in estimator for linear functional T(F) = fr(2)dF (x) is:\n",
      "\n",
      "(8.2)\n",
      "\n",
      " \n",
      "\n",
      "Sometimes we can find the estimated standard error s@ of\n",
      "T(E,) by doing some calculations. However, in other cases it\n",
      "is not obvious how to estimate the standard error. In the next\n",
      "chapter, we will discuss a general method for finding sé. For\n",
      "now, let us just assume that somehow we can find sé. In many\n",
      "\n",
      "cases, it turns out that\n",
      "T(F,) © N(T(F), &).\n",
      "\n",
      "By equation (7.10), an approximate 1 — a confidence interval\n",
      "for T(F) is then\n",
      "\n",
      " \n",
      "\n",
      "T (Fy) £ 20/2 %.\n",
      "\n",
      " \n",
      "\n",
      "(8.3)\n",
      "\n",
      " \n",
      "\n",
      "We will call this the Normal-based interval. For a 95 per cent\n",
      "confidence interval, 24/2 = 2.05/2 = 1.96 © 2 so the interval is\n",
      "\n",
      "T(F,) £28.\n",
      "\n",
      "Example 8.6 (The mean.) Let = T(F) = f cdF (2). The plug-\n",
      "in estimator is fi = [vdF,(z) = X,. The standard error is\n",
      "\n",
      "se = \\/V(Xn) = o/Yn. If & denotes an estimate of o, then\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-121.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "8.2 Statistical Functionals 121\n",
      "\n",
      "the estimated standard error is G/\\/n. (In the next example, we\n",
      "shall see how to estimate 0.) A Normal-based confidence interval\n",
      "forpisX,+ Za/2 se’.\n",
      "\n",
      "Example 8.7 (The Variance) Leto? = T(F) = V(X) = f a?dF(x)—\n",
      "(f adF(z))”. The plug-in estimator is\n",
      "\n",
      "2 [ec a (fsa)\n",
      "EGE)\n",
      "= 1 4-%y\n",
      "\n",
      "Another reasoable estimator of 0? is the sample variance\n",
      "\n",
      "1 “ _\n",
      "Xo)\n",
      "Seno\n",
      "\n",
      "In practice, there is little difference between 6? and S? and you\n",
      "\n",
      "oe\n",
      "\n",
      "n\n",
      "\n",
      " \n",
      "\n",
      "can use either one. Returning the our last example, we now see\n",
      "that the estimated standard error of the estimate of the mean is\n",
      "\n",
      "®@=3/Vn.\n",
      "\n",
      "Example 8.8 (The Skewness) Let js and o? denote the mean and\n",
      "variance of a random variable X. The skewness is defined to be\n",
      "\n",
      "E(X — p)3 [(e— w)?dF (2)\n",
      "\n",
      "o {[(@— dF (2) ye\n",
      "The skewness measure the lack of symmetry of a distribution.\n",
      "To find the plug-in estimate, first recall that ji = n7 yO, Xi and\n",
      "@ =n1D)(X;— ji)’. The plug-in estimate of « is\n",
      "\n",
      "fle w'MFa) 206-0)\n",
      "as 3/2 oe .\n",
      "{ f(x — PdFa(a)}\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "k=\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-122.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3C48>\n",
      "122 8. Estimating the CDF and Statistical Functionals\n",
      "\n",
      "Example 8.9 (Correlation.) Let Z = (X,Y) and let p=T(F) =\n",
      "E(X—px)(¥—py)/(720y) denote the correlation between X and\n",
      "\n",
      "Y, where F(x, y) is bivariate. We can write T(F) = a(T; (F), To(F), T3(F), Ti(F), Ts(F))\n",
      "where\n",
      "\n",
      "TF) =fadF(z2) T(F)=fydF(2) T(F) = f cydF (2)\n",
      "Ti(F) = f a2dF(z) Ts te =f ydF(z)\n",
      "and bh hb\n",
      "Replace F with F, in T,(F), ..., Ts(F), and take\n",
      "p= a(T,(F,),T2(F,), Ts(Fa), Ti(Fr), Ts(F,))-\n",
      "We get\n",
      "\n",
      "Dili — Xn)(¥i — Yn)\n",
      "asi X-¥,2/OH-Y.)\n",
      "\n",
      "which is called the sample correlation. Hi\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 8.10 (Quantiles.) Let F be strictly increasing with den-\n",
      "sity f. The T(F) = F7'(p) be the p quantile. The estimate\n",
      "if T(F) is F>'\\(p). We have to be a bit careful since F, is\n",
      "not invertible. To avoid ambiguity we define F>'(p) = inf{a :\n",
      "F(x) > Pp}. We call F-'(p) the p\"\" sample quantile. ll\n",
      "\n",
      "Only in the first example did we compute a standard error or\n",
      "a confidence interval. How shall we handle the other examples.\n",
      "When we discuss parametric methods, we will develop formulae\n",
      "for standard errors and confidence intervals. But in our non-\n",
      "parametric setting we need something else. In the next chapter,\n",
      "we will introduce two methods — the jackknife and the bootstrap\n",
      "— for getting standard errors and confidence intervals.\n",
      "\n",
      "Example 8.11 (Plasma Cholesterol.) Figure 8.2 shows histograms\n",
      "for plasma cholesterol (in mg/dl) for 371 patients with chest\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-123.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "8.2 Statistical Functionals 123\n",
      "\n",
      "pain (Scott et al. 1978). The histograms show the percentage of\n",
      "patients in 10 bins. The first histogram is for 51 patients who\n",
      "had no evidence of heart disease while the second histogram is\n",
      "for 820 patients who had narrowing of the arteries. Is the mean\n",
      "cholesterol different in the two groups? Let us regard these data\n",
      "as samples from two distributions F, and F9. Let ju, = f adF (2)\n",
      "and [2 = [ vdF2(2) denote the means of the two populations.\n",
      "The plug-in estimates are fi, = frdF,,(«) = Xna = 195.27\n",
      "and fiz = f tdF,2(2) = Xn2 = 216.19. Recall that the standard\n",
      "error of the sample mean fi = 437\", X; is\n",
      "\n",
      "(f= v(t>%) = vx) (ees\n",
      "\n",
      "which we estimate by\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "where\n",
      "\n",
      " \n",
      "\n",
      "For the two groups this yields sé(ji;) = 5.0 and sé (fiz) = 2.4.\n",
      "Approximate 95 per cent confidence intervals for j, and 12 are\n",
      "fi; + 28 (fix) = (185, 205) and fin + 288 (fig) = (211, 221).\n",
      "\n",
      "Now, consider the functional @ = T(F,)—T(F,) whose plug-in\n",
      "estimate is 0 = Ji2— fi, = 216.19 — 195.27 = 20.92. The standard\n",
      "error of 0 is\n",
      "\n",
      " \n",
      "\n",
      "se = VV (fiz — fis) = VV (fa) + V(ii) = V/(se (fix)? + (se (fia)?\n",
      "\n",
      "and we estimate this by\n",
      "\n",
      "sé = \\/ (se (f71))? + (sé (fiz)? = 5.55.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-124.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "124 8. Estimating the CDF and Statistical Functionals\n",
      "\n",
      "An approximate 95 per cent confidence interval for 0 is 0428 =\n",
      "(9.8, 32.0). This suggests that cholesterol is higher among those\n",
      "with narrowed arteries. We should not jump to the conclusion\n",
      "(from these data) that cholesterol causes heart disease. The leap\n",
      "from statistical evidence to causation is very subtle and is dis-\n",
      "cussed later in this text. 1\n",
      "\n",
      "8.3 Technical Appendix\n",
      "\n",
      "In this appendix we explain how to construct a confidence\n",
      "band for the CDF .\n",
      "Theorem 8.12 (Dvoretzky-Kiefer-Wolfowitz (DKW) inequality.) Let\n",
      "X,...,X, be iid from F. Then, for any € > 0,\n",
      "\n",
      "(sup Fla) Fla)|>e) <2, (8\n",
      "\n",
      "From the DKW inequality, we can construct a confidence set.\n",
      "Let €? = log(2/a)/(2n), L(x) = max{F;,(x)—€n, 0} and U(x) =\n",
      "min{F,,(2) + €,, 1}. It follows from (8.4) that for any F,\n",
      "\n",
      "P(FEC,) >1-a.\n",
      "\n",
      "Thus, C,, is a nonparametric 1—a confidence set for F’. A better\n",
      "name for C’, is a confidence band. To summarize:\n",
      "\n",
      " \n",
      "\n",
      "A 1—a nonparametric confidence band for F is (L(x), U(a)) where\n",
      "\n",
      "L(x) = max{F,(x) — €,, 0}\n",
      "U(r) = ait oie 1}\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-125.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E9308>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.3 Technical Appendix 125\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "[ T T T T T I\n",
      "100 150 200 250 300 350 400\n",
      "\n",
      "plasma cholesterol for patients without heart disease\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "4\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "[ T T T T T 1\n",
      "100 150 200 250 300 350 400\n",
      "\n",
      " \n",
      "\n",
      "plasma cholesterol for patients with heart disease\n",
      "\n",
      "FIGURE 8.2. Plasma cholesterol for 51 patients with no heart disease\n",
      "and 320 patients with narrowing of the arteries.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-126.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1988>\n",
      "126 8. Estimating the CDF and Statistical Functionals\n",
      "\n",
      "Example 8.13 The dashed lines in Figure 8.1 give a 95 per cent\n",
      "\n",
      "confidence band using €n =\n",
      "\n",
      " \n",
      "\n",
      "8.4 Bibliographic Remarks\n",
      "\n",
      "The Glivenko-Cantelli theorem is the tip of the iceberg. The\n",
      "theory of distribution functions is a special case of what are\n",
      "called empirical processes which underlie much of modern statis-\n",
      "tical theory. Some references on empirical processes are Shorack\n",
      "and Wellner (1986) and van der Vaart and Wellner (1996).\n",
      "\n",
      "8.5 Exercises\n",
      "\n",
      "1. Prove Theorem 8.3.\n",
      "\n",
      "2. Let X),...,X, ~ Bernoulli(p) and let Y¥1,.--,¥n ~ Bernoulli(q).\n",
      "Find the plug-in estimator and estimated standard error\n",
      "for p. Find an approximate 90 per cent confidence interval\n",
      "for p. Find the plug-in estimator and estimated standard\n",
      "error for p—q. Find an approximate 90 per cent confidence\n",
      "interval for p— q.\n",
      "\n",
      "3. (Computer Experiment.) Generate 100 observations from\n",
      "a N(0,1) distribution. Compute a 95 per cent confidence\n",
      "band for the CDF F’. Repeat this 1000 times and see how\n",
      "often the confidence band contains the true distribution\n",
      "function. Repeat using data from a Cauchy distribution.\n",
      "\n",
      "4. Let X),...,X, ~ F and let F,,(x) be the empirical distri-\n",
      "bution function. For a fixed x, use the central limit theo-\n",
      "rem to find the limiting distribution of F,,(2).\n",
      "\n",
      " \n",
      "\n",
      "5. Let « and y be two distinct points. Find Cov(Fi,(«), Fa(y))-\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-127.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907A7088>\n",
      "10.\n",
      "\n",
      "8.5 Exercises 127\n",
      "\n",
      ". Let X,,...,X, ~ F and let F be the empirical distri-\n",
      "\n",
      "bution function. Let a < b be fixed numbers and define\n",
      "0 =T(F) =F(b)— F(a). Let 0=T(F,) = F,(b) — F(a).\n",
      "\n",
      "Find the estimated standard error of 6. Find an expression\n",
      "for an approximate 1 — a confidence interval for 0.\n",
      "\n",
      ". Data on the magnitudes of earthquakes near Fiji are avail-\n",
      "\n",
      "able on the course website. Estimate the cdf F(a). Com-\n",
      "pute and plot a 95 per cent confidence envelope for F’.\n",
      "Find an approximate 95 per cent confidence interval for\n",
      "F(4.9) — F(4.3).\n",
      "\n",
      ". Get the data on eruption times and waiting times between\n",
      "\n",
      "eruptions of the old faithful geyser from the course web-\n",
      "site. Estimate the mean waiting time and give a standard\n",
      "error for the estimate. Also, give a 90 per cent confidence\n",
      "interval for the mean waiting time. Now estimate the me\n",
      "dian waiting time. In the next chapter we will see how to\n",
      "get the standard error for the median.\n",
      "\n",
      ". 100 people are given a standard antibiotic to treat an in-\n",
      "\n",
      "fection and another 100 are given a new antibiotic. In the\n",
      "first group, 90 people recover; in the second group, 85 peo-\n",
      "ple recover. Let p; be the probability of recovery under\n",
      "the standard treatment and let py be the probability of\n",
      "recovery under the new treatment. We are interested in\n",
      "estimating @ = p, — po. Provide an estimate, standard er-\n",
      "ror, an 80 per cent confidence interval and a 95 per cent\n",
      "confidence interval for 0.\n",
      "\n",
      "In 1975, an experiment was conducted to see if cloud seed-\n",
      "ing produced rainfall. 26 clouds were seeded with silver\n",
      "nitrate and 26 were not. The decision to seed or not was\n",
      "made at random. Get the data from\n",
      "\n",
      "http://lib.stat.cmu.edu/DASL/Stories/CloudSeeding.html\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-128.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E9308>\n",
      "128 8. Estimating the CDF and Statistical Functionals\n",
      "\n",
      "Let @ be the difference in the median precipitation from\n",
      "the two groups. Estimate @. Estimate the standard error of\n",
      "the estimate and produce a 95 per cent confidence interval.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-129.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "9\n",
      "The Bootstrap\n",
      "\n",
      "The bootstrap is a nonparametric method for estimating stan-\n",
      "dard errors and computing confidence intervals. Let\n",
      "\n",
      "Th =Wg (Kis ong Xp)\n",
      "\n",
      "be a statistic, that is, any function of the data. Suppose we\n",
      "want to know V;(T;,), the variance of T,,. We have written V»\n",
      "to emphasize that the variance usually depends on the unknown\n",
      "distribution function F. For example, if T,, =n! iL, X; then\n",
      "Vp(T,) = 0?/n where o? = [(2— p)?dF (x) and p= f rdF (x).\n",
      "The bootstrap idea has two parts. First we estimate V;-(T,,) with\n",
      "Va,(Tn)- Thus, for T,, = n7! SO\", X; we have Vg (Tn) = G?/n\n",
      "where 6? = n7! So\", (X; — X,,). However, in moremplicated\n",
      "cases we cannot write down a simple formula for Vj, (T,,). This\n",
      "leads us to the second step which is to approximate Vet)\n",
      "using simulation. Before proceeding, let us discuss the idea of\n",
      "simulation.\n",
      "\n",
      "9.1 Simulation\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-130.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1988>\n",
      "130 9. The Bootstrap\n",
      "\n",
      "Suppose we draw an IID sample Yj,..., Y, from a distribution\n",
      "G. By the law of large numbers,\n",
      "\n",
      "B\n",
      "eZ [vac =20)\n",
      "j=l\n",
      "\n",
      "as B — oo. So if we draw a large sample from G,, we can use the\n",
      "sample mean Y,, to approximate E(Y). In a simulation, we can\n",
      "make B as large as we like in which case, the difference between\n",
      "Y,, and E(Y) is negligible. More generally, if h is any function\n",
      "with finite mean then\n",
      "\n",
      "Few [rac = E(hA(Y))\n",
      "as B — ov. In particular,\n",
      "Lorry 3 3 3 - (3 oy) / yPdF (y)— (/ wlF ad) =v).\n",
      "\n",
      "Hence, we can use the sample variance of the simulated values\n",
      "\n",
      " \n",
      "\n",
      "to approximate V(Y).\n",
      "9.2 Bootstrap Variance Estimation\n",
      "\n",
      "According to what we just learned, we can approximate Vp (Th)\n",
      "by simulation. Now Vella) means “the variance of T), if the\n",
      "distribution of the data is Fv How can we simulate from the\n",
      "distribution of T,, when the data are assumed to have distribu-\n",
      "tion F.? The answer is to simulate X7,..., X% from F, and then\n",
      "compute T* = g(Xj,...,X*). This constitutes one draw from\n",
      "the distribution of T,,. The idea is illustrated in the following\n",
      "diagram:\n",
      "\n",
      "Real world Fo = > X,...,Xn => Th=9(X1,---;Xn)\n",
      "Bootstrap world F, => Xj,...,.Xi => Te =g(X},...,X%)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-131.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C85C8>\n",
      "9.2 Bootstrap Variance Estimation 131\n",
      "\n",
      "How do we simulate X7,..., X% from F,,? Notice that F, puts\n",
      "mass 1/n at each data point X;,...,X,. Therefore, drawing an\n",
      "observation from F,, is equivalent to drawing one point\n",
      "at random from the original data set. Thu, to simulate\n",
      "DF gccong RE ES F, it suffices to draw n observations with re-\n",
      "placement from X,,...,X,. Here is a summary:\n",
      "\n",
      " \n",
      "\n",
      "Boostrap Variance Estimation\n",
      "1. Draw X},...,X*~ F,.\n",
      "2. Compute T* = g(Xf,..., X})-\n",
      "3. Repeat steps 1 and 2, B times, to get Tras . The\n",
      "\n",
      "4. Let\n",
      "\n",
      "B 2\n",
      "1 * 1 =e\n",
      "Upoot = B > (x a B yn.) m (9.1)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 9.1 The following pseudo-code shows how to use the\n",
      "bootstrap to estimate the standard of the median.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-132.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148A1C23048>\n",
      "132 9. The Bootstrap\n",
      "\n",
      " \n",
      "\n",
      "Bootstrap for The Median\n",
      "\n",
      "Given data X = (X(1), ..., X(m)):\n",
      "\n",
      "T <- median(X)\n",
      "Tboot <- vector of length B\n",
      "for(i in 1:i){\n",
      "\n",
      "Tboot [i] <- median(Xstar)\n",
      "}\n",
      "\n",
      "se <- sqrt (variance(Tboot) )\n",
      "\n",
      " \n",
      "\n",
      "Xstar <- sample of size n from X (with replacement)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The following schematic diagram will remind you that we are\n",
      "using two approximations:\n",
      "\n",
      "not so small small\n",
      "\n",
      "Vi(In) 73> Vig (Tn) Vboot-\n",
      "\n",
      "Example 9.2 Consider the nerve data. Let 0 = T(F) = f(a—-\n",
      "y)dF(x)/o> be the skewness. The skewness is a measure of\n",
      "asymmetry. A Normal distribution, for example, has skewness\n",
      "0. The plug-in estimate of the skewness is\n",
      "\n",
      "15nd OnE\n",
      "\n",
      "el = 1.76.\n",
      "oO\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "To estimate the standard error with the bootstrap we follow the\n",
      "same steps as with the medin example except we compute the\n",
      "skewness from each bootstrap sample. When applied to the nerve\n",
      "data, the bootstrap, based on B = 1000 replications, yields a\n",
      "standard error for the estimated skewness of .16.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-133.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E1988>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.3 Bootstrap Confidence Intervals 133\n",
      "9.3 Bootstrap Confidence Intervals\n",
      "\n",
      "There are several ways to construct bootstrap confidence in-\n",
      "tervals. Here we discuss three methods.\n",
      "\n",
      "Normal Interval. The simplest is the Normal interval\n",
      "\n",
      "Ty + 2a/2 B hoot (9.2)\n",
      "\n",
      " \n",
      "\n",
      "where S€}oo is the bootstrap estimate of the standard error.\n",
      "This interval is not accurate unless the distribution of T,, is\n",
      "close to Normal.\n",
      "\n",
      "Pivotal Intervals. Let @ = T(F) and 0, = T(F,) and define the\n",
      "\n",
      "pivot R, = On — 6. Let 0, cess Os denote bootstrap replica-\n",
      "tions of 6,,. Let H(r) denote the CDF of the pivot:\n",
      "H(r) =Pp(Rn <1). (9.3)\n",
      "\n",
      "Define C* = (a,b) where\n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "a\n",
      "\n",
      " \n",
      "\n",
      "8, —H™ (1-5) and 6, —H-' (£). (9.4)\n",
      "\n",
      "Tt follows that\n",
      "P(a<0<b) = Paa-#,<\n",
      "= P@,-b\n",
      "\n",
      "ll\n",
      "=\n",
      "=\n",
      "I\n",
      "= 1A IA\n",
      "Es)\n",
      "g\n",
      "IA\n",
      "§\n",
      ";\n",
      "&\n",
      "\n",
      "Il\n",
      "za\n",
      "i\n",
      "y\n",
      "nw] s\n",
      "=e\n",
      "|\n",
      "=\n",
      "c\n",
      "as\n",
      "\n",
      "|\n",
      "a\n",
      "\n",
      "|\n",
      "\n",
      "mls\n",
      "\n",
      "Il\n",
      "Hw\n",
      "\n",
      "|\n",
      "2\n",
      "\n",
      "Hence, C* is an exact 1 — a confidence interval for 6. Unfortu-\n",
      "nately, a and b depend on the unknown distribution H but we\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-134.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1988>\n",
      "134 9. The Bootstrap\n",
      "\n",
      "can form a bootstrap estimate of H:\n",
      "\n",
      "B\n",
      "=5 <r) (9.5)\n",
      "b=1\n",
      "\n",
      "where Ri, = 0, —6,. Let rj denote the 8 sample qauntile\n",
      "Of (Ri yguos Rt) and let 0% denote the 6 sample qauntile of\n",
      "(5 1,-+-,93,n)- Note that rh = 05 - @,,. It follows that an ap-\n",
      "\n",
      "proximate 1 — a confidence interval is C, = (@,) where\n",
      "\n",
      "a 6, — (1-$) =O, — Thuja = 2, — Of aps\n",
      "\n",
      "b= bh\n",
      "\n",
      "Il\n",
      "|\n",
      ")\n",
      "b\n",
      "I\n",
      "ee\n",
      "|\n",
      "=>\n",
      "|\n",
      "=\n",
      "=\n",
      "iS\n",
      "|\n",
      "w\n",
      "S\n",
      "5)\n",
      ">\n",
      "e)\n",
      "3\n",
      "\n",
      "In summary:\n",
      "\n",
      " \n",
      "\n",
      "The 1 — a bootstrap pivotal confidence interval is\n",
      "\n",
      "Cn = (28, — Baja, 28n — 8p) -\n",
      "\n",
      " \n",
      "\n",
      "Typically, this is a pointwise, asymptotic confidence interval.\n",
      "\n",
      "(9.6)\n",
      "\n",
      " \n",
      "\n",
      "Theorem 9.3 Under weak conditions onT(F), Pe(T(F) € Cr) 4\n",
      "\n",
      "1—aasn- oo, where C,, is given in (9.6).\n",
      "\n",
      "Percentile Intervals. The bootstrap percentile interval is\n",
      "defined by\n",
      "\n",
      "Cr = (0% aja» Oi a/2) +\n",
      "\n",
      "The justification for this interval is given in the appendix.\n",
      "\n",
      "Example 9.4 For estimating the skewness of the nerve data, here\n",
      "are the various confidence intervals.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-135.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "9.3 Bootstrap Confidence Intervals 135\n",
      "\n",
      "Method 95% Interval\n",
      "Normal (1.44, 2.09)\n",
      "Percentile (1.42, 2.03)\n",
      "Pivotal (1.48, 2.11)\n",
      "\n",
      "All these confidence intervals are approximate. The probabil-\n",
      "ity that T(F) is in the interval is not exactly 1— a. All three\n",
      "intervals have the same level of accuracy. There are more accu-\n",
      "rate bootstrap confidence intervals but they are more compli-\n",
      "cated and we will not discuss them here.\n",
      "\n",
      "Example 9.5 (The Plasma Cholesterol Data.) Let us return to the\n",
      "cholesterol data. Suppose we are interested in the difference of\n",
      "the medians. Pseudo-code for the bootstrap analysis is as follows:\n",
      "\n",
      "xi <- first sample\n",
      "\n",
      "x2 <- second sample\n",
      "\n",
      "ni <- length(x1)\n",
      "\n",
      "n2 <- length(x2)\n",
      "\n",
      "th.hat <- median(x2) - median(x1)\n",
      "\n",
      "B <- 1000\n",
      "\n",
      "Tboot <- vector of length B\n",
      "\n",
      "for(i in 1:B){\n",
      "xxi <- sample of size ni with replacement from x1\n",
      "xx2 <- sample of size n2 with replacement from x2\n",
      "Tboot [i] <- median(xx2) - median(xx1)\n",
      "3\n",
      "\n",
      "se <- sqrt (variance (Tboot))\n",
      "\n",
      "Normal <- (th.hat - 2*xse, th.hat + 2*se)\n",
      "\n",
      "percentile <- (quantile(Tboot,.025), quantile(Tboot, .975))\n",
      "\n",
      "pivotal <- ( 2*th.hat-quantile(Tboot, .975), 2*th.hat-quantile(Tboot, .025)\n",
      "\n",
      "The point estimate is 18.5, the bootstrap standard error is 7.42\n",
      "and the resulting approzimate 95 per cent confidence intervals\n",
      "are as follows:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-136.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E9308>\n",
      "136 9. The Bootstrap\n",
      "\n",
      "Method 95% Interval\n",
      "\n",
      "Percentile (5.0, 33.3)\n",
      "\n",
      "Pivotal (5.0, 34.0)\n",
      "Since these intervals exclude 0, it appears that the second group\n",
      "has higher cholesterol although there is considerable uncertainty\n",
      "about how much higher as reflected in the width of the intervals.\n",
      "\n",
      "The next two examples are based on small sample sizes. In\n",
      "practice, statistical methods based on very small sample sizes\n",
      "might not be reliable. We include the examples for their peda-\n",
      "gogical value but we do want to sound a note of caution about\n",
      "interpreting the results with some skepticism.\n",
      "\n",
      "Example 9.6 Here is an example that was one of the first used\n",
      "to illustrate the bootstrap by Bradley Efron, the inventor of the\n",
      "bootstrap. The data are LSAT scores (for entrance to law school)\n",
      "and GPA.\n",
      "\n",
      "LSAT 576 635 558 578 666 580 555 661\n",
      "651 605 653. 575 545 572 594\n",
      "\n",
      "GPA 3.39 3.30 2.81 3.03 3.44 3.07 3.00 3.43\n",
      "3.36 3.13 3.12 2.74 2.76 2.88 3.96\n",
      "\n",
      "Each data point is of the form X; = (Yj, Z;) where Y; = LSAT;\n",
      "and Z; = GPA;. The law school is interested in the correlation\n",
      "\n",
      "ga [fy -— uy) — wz) dF ly, 2) ;\n",
      "Vu = ay)24P (y) fe = nzPaF (2)\n",
      "The plug-in estimate is the sample correlation\n",
      "<. BM-Wez\n",
      "YDM- YP DZ\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-137.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "9.3 Bootstrap Confidence Intervals 137\n",
      "\n",
      "The estimated correlation is 0 = .776. The bootstrap based on\n",
      "B =1000 gives sé = .137. Figure 9.1 shows the data and a his-\n",
      "togram of the bootstrap replications 6%, cess 0%. This histogram is\n",
      "an approximation to the sampling distribution of 0. The Normal-\n",
      "based 95 per cent confidence interval is .78 + 2se = (.51,1.00)\n",
      "while the percentile interval is (.46,.96). In large samples, the\n",
      "\n",
      "two methods will show closer agreement.\n",
      "\n",
      "Example 9.7 This example is borrowed from An Introduction to\n",
      "the Bootstrap by B. Efron and R. Tibshirani. When drug com-\n",
      "panies introduce new medications, they are sometimes requires\n",
      "to show bioequivalence. This means that the new drug is not\n",
      "substantially different than the current treatment. Here are data\n",
      "on eight subjects who used medical patches to infuse a hormone\n",
      "into the blood. Each subject received three treatments: placebo,\n",
      "old-patch, new-patch.\n",
      "\n",
      "subject placebo old new old-placebo new-old\n",
      "fi 9243 17649 16449 8406 -1200\n",
      "2 9671 12013 14614 2342 2601\n",
      "3 11792 19979 17274 8187 2705\n",
      "4 13357 21816 23798 8459 1982\n",
      "5 9055 13850 12560 4795 -1290\n",
      "6 6290 9806 10157 3516 351\n",
      "Z4 12412 17208 16570 4796 -638\n",
      "8 18806 29044 26325 10238 -2719\n",
      "\n",
      "Let Z = old — placebo and Y = new — old. The Food and\n",
      "Drug Administration (FDA) requirement for bioequivalence is\n",
      "that |0| < .20 where\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-138.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E1A48>\n",
      "138 9. The Bootstrap\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "=~ J 7\n",
      "oo . *\n",
      "™ J\n",
      "2\n",
      "x= = .\n",
      "oO .\n",
      "° .\n",
      "2 | ‘\n",
      "cd .\n",
      "o | ‘\n",
      "“N .\n",
      "560 580 600 620 640 660\n",
      "LSAT\n",
      "B\n",
      "\n",
      "100\n",
      "\n",
      "50\n",
      "\n",
      " \n",
      "\n",
      "0.2 0.4 0.6 0.8 1.0\n",
      "\n",
      "Bootstrap Samples\n",
      "\n",
      "FIGURE 9.1. Law school data.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-139.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CDB48>\n",
      "9.4 Bibliographic Remarks 139\n",
      "The estimate of 0 is\n",
      "\n",
      "Y 452.3\n",
      "\n",
      "Z 6342\n",
      "\n",
      ">)\n",
      "\n",
      "= = —.0713.\n",
      "\n",
      "The bootstrap standard error is 5 = .105. To answer the bioe-\n",
      "quivalence question, we compute a confidence interval. From\n",
      "B = 1000 bootstrap replications we get the 95 per cent inter-\n",
      "val is (-.24,.15). This is not quite contained in [-.20,.20] so at\n",
      "the 95 per cent level we have not demonstrated bioequivalence.\n",
      "Figure 9.2 shows the histogram of the bootstrap values.\n",
      "\n",
      "9.4 Bibliographic Remarks\n",
      "\n",
      "The boostrap was invented by Efron (1979). There are several\n",
      "books on these topics including Efron and Tibshirani (1993),\n",
      "Davison and Hinkley (1997), Hall (1992), and Shao and Tu\n",
      "(1995). Also, see section 3.6 of van der Vaart and Wellner (1996).\n",
      "9.5 Technical Appendix\n",
      "\n",
      "9.5.1 The Jackknife\n",
      "\n",
      "There is another method for computing standard errors called\n",
      "the jackknife, due to Quenouille (1949). It is less computa-\n",
      "tionally expensive than the bootstrap but is less general. Let\n",
      "T, =T(X,...,Xn) be a statistic and T_; denote the statistic\n",
      "with the i** observation removed. Let T,, =n! 77, Tia. The\n",
      "jackknife estimate of var(T,,) is\n",
      "\n",
      " \n",
      "\n",
      "n\n",
      "\n",
      "n-1 tN\n",
      "Vjack = n Cre) _ Gy\n",
      "\n",
      "i=l\n",
      "\n",
      " \n",
      "\n",
      "and the jackknife estimate of the standard error is S€ jack =\n",
      "/Tjack- Under suitable conditions on T, it can be shown that\n",
      "\n",
      "Ujack Consistently estimates var(T;,) in the sense that vyack/var(Tn) 7,\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-140.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 9. The Bootstrap\n",
      "\n",
      "ih,\n",
      "T T T T T 1\n",
      "\n",
      "-0.3 -0.2 -0.1 0.0 0.1 0.2\n",
      "\n",
      "60\n",
      "|\n",
      "\n",
      "40\n",
      "|\n",
      "\n",
      "20\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Bootstrap Samples\n",
      "\n",
      "FIGURE 9.2. Patch data.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-141.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907A7088>\n",
      "9.6 Excercises 141\n",
      "\n",
      "1. However, unlike the bootstrap, the jackknife does not produce\n",
      "consistent estimates of the standard error of sample quantiles.\n",
      "\n",
      "9.5.2 Justification For The Percentile Interval\n",
      "\n",
      "Suppose there exists a monotone transformation U = m(T)\n",
      "such that U ~ N(@,c*) where ¢ = m(0). We do not suppose we\n",
      "know the transformation, only that one exist. Let Uf = m(6%,,)-\n",
      "Let u%Z be the 6 sample quantile of the U;’s. Since a mono-\n",
      "tone transformation preserves quantiles, we have that Usp =\n",
      "m(O%)- Also, since U ~ N(¢,c?), the a/2 quantile of U is\n",
      "@— Za/2c. Hence Uap = o- 2a/2¢. Similarly, Ula/a = 0+ Za/2€.\n",
      "Therefore,\n",
      "\n",
      "PO. SOS Oop) =\n",
      "\n",
      " \n",
      "\n",
      "An exact normalizing transformation will rarely exist but there\n",
      "may exist approximate normalizing transformations.\n",
      "9.6 Excercises\n",
      "\n",
      "1. Consider the data in Example 9.6. Find the plug-in esti-\n",
      "mate of the correlation coefficient. Estimate the standard\n",
      "error using the bootstrap. Find a 95 per cent confidence\n",
      "inerval using all three methods.\n",
      "\n",
      "2. (Computer Experiment.) Conduct a simulation to compare\n",
      "the four bootstrap confidence interval methods. Let n =\n",
      "50 and let T(F) = f(a — p)*dF(x)/o* be the skewness.\n",
      "Draw Y\\,..-,Xn ~ N(0,1) and set X; =e%, i=1,...,n.\n",
      "Construct the four types of bootstrap 95 per cent intervals\n",
      "for T(F) from the data X,,...,X,. Repeat this whole\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-142.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1EC8>\n",
      "142\n",
      "\n",
      "9. The Bootstrap\n",
      "\n",
      "thing many times and estimate the true coverage of the\n",
      "four intervals.\n",
      "\n",
      "Let\n",
      "Xi,..-,Xn~ ts\n",
      "\n",
      "where n = 25. Let 0 = T(F) = (q75 — ¢.25)/1.34 where q,\n",
      "denotes the p“” quantile. Do a simulation to compare the\n",
      "coverage and length of the following confidence intervals\n",
      "for 8: (i) Normal interval with standard error from the\n",
      "bootstrap, (ii) bootstrap percentile interval.\n",
      "\n",
      "Remark: The jackknife does not give a consistent estima-\n",
      "tor of the variance of a quantile.\n",
      "\n",
      ". Let X\\,...,X, be distinct observations (no ties). Show\n",
      "\n",
      "that there are\n",
      "2n-1\n",
      "n\n",
      "distinct bootstrap samples.\n",
      "\n",
      "Hint: Imagine putting n balls into n buckets.\n",
      "\n",
      "denote a bootstrap sample and let Ke = wT XP\n",
      "Find: E(X;|¥1,.--, Xn), V(X4|X1,.--,Xn), E(X,) and\n",
      "V(X,)-\n",
      "\n",
      "Computer Experiment.) Let X1,...,.X;, Normal(j,1). Let\n",
      "0 =! and let 0 = e* be the mle. Create a data set (using\n",
      "jt = 5) consisting of n=100 observations.\n",
      "\n",
      " \n",
      "\n",
      "a) Use the bootstrap to get the seand 95 percent confi-\n",
      "dence interval for 0.\n",
      "\n",
      "(b) Plot a histogram of the bootstrap replications for the\n",
      "parametric and nonparametric bootstraps. These are es-\n",
      "timates of the distribution of 0. Compare this to the true\n",
      "sampling distribution of 6.\n",
      "\n",
      "Let X,,..., Xn be distinct observations (no ties). Let Xj,..., X*\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-143.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8708>\n",
      "9.6 Excercises 143\n",
      "\n",
      "7. Let Xj, ...,X, Unif(0, 0). The mle is @= Ximar = max{X,..., Xp}.\n",
      "Generate a data set of size 50 with @ = 1.\n",
      "\n",
      "(a) Find the distribution of 0. Compare the true distri-\n",
      "bution of 6 to the histograms from the parametric and\n",
      "nonparametric bootstraps.\n",
      "\n",
      "(b) This is a case where the nonparametric bootstrap does\n",
      "very poorly. In fact, we can prove that this is the case.\n",
      "Show that, for the parametric bootstrap P(6* = 0) = 0\n",
      "but for the nonparametric bootstrap P(0* = 0) ~ .632.\n",
      "Hint: show that, P(@* = 0) = 1 — (1 —(1/n))” then take\n",
      "\n",
      "the limit as n gets large.\n",
      "\n",
      "8. Let Th = x, w= E(X), ax = f |e — w|*dF (2) and\n",
      "& = 4 (i - X,,|*. Show that\n",
      "\n",
      "=~ | ane\n",
      "4X Go 4 4X03 4\n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Vboot =\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-144.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n",
      "144 9. The Bootstrap\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-145.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "10\n",
      "\n",
      "Parametric Inference\n",
      "\n",
      "We now turn our attention to parametric models, that is,\n",
      "models of the form\n",
      "\n",
      "t= { F(239) :0€ o} (10.1)\n",
      "\n",
      "where the © C R* is the parameter space and 6 = (61,..., 0)\n",
      "is the parameter. The problem of inference then reduces to the\n",
      "problem of estimating the parameter 0.\n",
      "\n",
      "Students learning statistics often ask: how would we ever\n",
      "know that the distribution that generated the data is in some\n",
      "parametric model? This is an excellent question. Indeed, we\n",
      "would rarely havesuch knowledge which is why nonparametric\n",
      "methods are preferable. Still, studying methods for parametric\n",
      "models is useful for two reasons. First, there are some cases\n",
      "where background knowledge suggests that a parametric model\n",
      "provides a reasonable approximation. F orexample, counts of\n",
      "traffic accidents are known from prior experience to follow ap-\n",
      "proximately a P oissonmodel. Second, the inferential concepts\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-146.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "146 10. Parametric Inference\n",
      "\n",
      "for parametric models provide background for understanding\n",
      "certain nonparametric methods.\n",
      "\n",
      "We will begin with a brief dicussion about parameters of in-\n",
      "terest and nuisance parameters in the next section, then we will\n",
      "discuss two methods for estimating 6, the method of moments\n",
      "and the method of maximum likelihood.\n",
      "\n",
      "10.1 Parameter of Interest\n",
      "\n",
      "Often, we are only interested in some function T(0). For ex-\n",
      "\n",
      " \n",
      "\n",
      "ample, if X ~ N(y,07) then the parameter is @ = (1,0). If our\n",
      "goal is to estimate ys then pp = T(@) is called the parameter\n",
      "of interest and ¢ is called a nuisance parameter. The pa-\n",
      "rameter of interest can be a complicated function of 6 as in the\n",
      "following example.\n",
      "\n",
      "Example 10.1 Let X),...,X, ~ Normal(j,0?). The parameter\n",
      "is0=(p,0) is O={(p,0): w ER o> O}. Suppose that X;\n",
      "is the outcome of a blood test and suppose we are interested in\n",
      "T, the fraction of the population whose test score is larger than\n",
      "1. Let Z denote a standard Normal random variable. Then\n",
      "\n",
      " \n",
      "\n",
      "X-po1-p\n",
      "T = P(X >1)=1-P(x <1) =1-P( H< 1H)\n",
      "oO oO\n",
      "\n",
      "1-n l-n\n",
      "1-P(z< t) -1-9(2< “).\n",
      "oO oO\n",
      "\n",
      "The parameter of interest is T =T(p,0) =1—((1—)/c).\n",
      "\n",
      " \n",
      "\n",
      "Example 10.2 Recall that X has a Gamma(a, 8) distribution if\n",
      "\n",
      "1\n",
      "\n",
      " \n",
      "\n",
      "x e-t/6 a > 0\n",
      "\n",
      "f(x; 4,8) =\n",
      "\n",
      "BT (a)\n",
      "\n",
      "where a, 3 >0 and\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-147.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "10.2 The Method of Moments 147\n",
      "\n",
      "is the Gamma function. The parameter is 0 = (a, 3). The Gamma\n",
      "distribution is sometimes used to model lifetimes of people, an-\n",
      "imals, and electronic equipment. Suppose we want to estimate\n",
      "the mean lifetime. Then T(a, 8) = Eg(X1) =f.\n",
      "\n",
      "10.2 The Method of Moments\n",
      "\n",
      "The first method for generating parametric estimators that\n",
      "we will study is called the method of moments. We will see\n",
      "that these estimators are not optimal but they are often easy to\n",
      "compute. They are are also useful as starting values for other\n",
      "methods that require iterative numerical routines.\n",
      "\n",
      "Suppose that the parameter 6 = (0),...,0,) has k compo-\n",
      "nents. For 1 <j <k, define the j** moment\n",
      "\n",
      "0; = 0,(0) =Ey(X4) = / aldF a(x) (10.2)\n",
      "and the j*\" sample moment\n",
      "\n",
      "~ Igy ‘\n",
      "aj = ioe (10.3)\n",
      "\n",
      " \n",
      "\n",
      "Definition 10.3 The method of moments estimator 6,\n",
      "is defined to be the value of 0 such that\n",
      "\n",
      " \n",
      "\n",
      "ay (6,) = a\n",
      "a2(O,) = @\n",
      "ax (On) = a. (10.4)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Formula (10.4) defines a system of k equations with k un-\n",
      "\n",
      "knowns.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-148.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1EC8>\n",
      "148 10. Parametric Inference\n",
      "\n",
      "Example 10.4 Let X,,\n",
      "pand@,=n-\"!Syr XxX\n",
      "\n",
      "~ 1,\n",
      "P= Xe\n",
      "\n",
      "Example 10.5 Let X1,...,Xn ~ Normal(j, 07). Then, ay = Eg(X1) =\n",
      "Recall that V(X) = ys and ay = Eg(X?) = Vo(X1) + (Eo(X1))? = 0? +7. We need\n",
      "E(X?) — (E (X))?. to solve the equations\n",
      "Hence, E(X?\n",
      "\n",
      ") »\n",
      "V(X) + (a) a - 10x,\n",
      "\n",
      "X,, ~ Bernoulli(p). Thena, = E,(X) =\n",
      "i. By equating these we get the estimator\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      ": 1g\n",
      "72\n",
      "o+h = nee\n",
      "This is a system of 2 equations with 2 unknowns. The solution\n",
      "is\n",
      "i= X,\n",
      "P= Foe ;-X,)?.\n",
      "Theorem 10.6 Let 0, denote the method of moments estimator.\n",
      "\n",
      "Under the the conditions given in the appendix, the following\n",
      "statements hold:\n",
      "\n",
      "1. The estimate 0, exists with probability tending to 1.\n",
      "2. The estimate is consistent: 0,5 0.\n",
      "3. The estimate is asymptotically Normal:\n",
      "\n",
      "n(n — 8) ~» N(0,5)\n",
      "\n",
      "where\n",
      "D = gEe(YY\")g\"\n",
      "\n",
      "Y = (X,X7,..., X97, 9 = (gi... -. 9x) and g; = da; '(0)/00.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-149.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C85C8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3 Maximum Likelihood 149\n",
      "\n",
      "The last statement in the Theorem above can be used to find\n",
      "standard errors and confidence intervals. However, there is an\n",
      "easier way: the bootstrap. We defer discussion of this until the\n",
      "end of the chapter.\n",
      "\n",
      "10.3. Maximum Likelihood\n",
      "\n",
      "The most common method for estimating parameters in a\n",
      "parametric model is the maximum likelihood method. Let\n",
      "X\\,.--,Xn be UD with PDF f(zx;6).\n",
      "\n",
      " \n",
      "\n",
      "Definition 10.7 The likelihood function is defined by\n",
      "\n",
      "n\n",
      "\n",
      "Lo(0) = TP £58). (10.5)\n",
      "\n",
      "i=l\n",
      "\n",
      "The log-likelihood function is defined by ¢,,(0) = log Ln{@).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The likelihood function is just the joint density of the data,\n",
      "except that we treat it is a function of the parameter 6.\n",
      "Thus £, : © > [0,00). The likelihood function is not a density\n",
      "function: in general, it is not true that £,,(@) integrates to 1.\n",
      "\n",
      " \n",
      "\n",
      "Definition 10.8 The maximum likelihood estimator\n",
      "MLE , denoted by 6,, is the value of 0 that maximizes\n",
      "\n",
      "Lal(6).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The maximum of ¢,,(@) occurs at the same place as the max-\n",
      "\n",
      " \n",
      "\n",
      "imum of £,,(9), so maximizing the log-likelihood leads to the\n",
      "same answer as maximizing the likelihood. Often, it is easier to\n",
      "work with the log-likelihood.\n",
      "\n",
      "Remark 10.9 If we multiply £,(0) by any positive constant c (not\n",
      "depending on 0) then this will not change the MLE . Hence, we\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-150.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1EC8>\n",
      "150 10. Parametric Inference\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "S 3\n",
      "5\n",
      "Q #1\n",
      "34 |\n",
      "\n",
      " \n",
      "\n",
      "00 02 04 06 08 10\n",
      "Pp\n",
      "\n",
      "FIGURE 10.1. Likelihood function for Bernoulli with n = 20 and\n",
      "\n",
      "yi\", Xj =12. The Mux is p, = 12/20 = 0.6.\n",
      "\n",
      "shall often be sloppy about dropping constants in the likelihood\n",
      "function.\n",
      "\n",
      "Example 10.10 Suppose that X\\,...,X, ~ Bernoulli(p). The prob-\n",
      "ability function is f(x;p) = p*(1— p)'~* for x =0,1. The un-\n",
      "known parameter is p. Then,\n",
      "\n",
      "L,(p) = [] xe p= TT“ —p) S=p*(1—p)\n",
      "\n",
      "where S = \\), Xj. Hence,\n",
      "\n",
      "ln(p) = Slogp + (n — S) log(1 — p).\n",
      "\n",
      "Take the derivative of ¢,(p), set it equal to 0 to find that the\n",
      "MLE is py, = S/n. See Figure 10.1. Wl\n",
      "\n",
      "Example 10.11 Let X,,...,Xn ~ N(,07). The parameter is\n",
      "6 = (1,0) and the likelihood function is (ignoring some con-\n",
      "stants)\n",
      "\n",
      "1\n",
      "\n",
      "Ln(p,o) = Tse {zai 0}\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-151.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CD9C8>\n",
      "10.3 Maximum Likelihood 151\n",
      "<n 1 - 2\n",
      "ania 2 a wee — 1)\n",
      "\n",
      "_n nS? n(X — p)?\n",
      "o \"exp ) —>y f EXP) —— a\n",
      "\n",
      "where X =n-! D>, X; is the sample mean and §? = n7! 0,(Xj—\n",
      "X)?. The last equality above follows from the fact that V(X -\n",
      "pH)? =nS? +n(X—p)? which can be verified by writing (X;—\n",
      "bw)? = (Xi; -X +X —)? and then expanding the square. The\n",
      "log-likelihood is\n",
      "\n",
      "ns? n(X — p)?\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "&(u,0) =—nlogo —\n",
      "\n",
      "20? 20?\n",
      "Solving the equations\n",
      "Ol( pL, 7) O( 1,0)\n",
      "EAP q Skee\n",
      "Ou 0 an aa 0\n",
      "\n",
      "we conclude that ji =X and@ = S. It can be verified that these\n",
      "are indeed global maxima of the likelihood.\n",
      "\n",
      "Example 10.12 (A Hard Example.) Here is the example that con-\n",
      "fuses everyone. Let X,,...,X, ~ Unif (0,0). Recall that\n",
      "4 0<a<o\n",
      "\n",
      "Has) ={3\n",
      "\n",
      "0 otherwise.\n",
      "\n",
      "Consider a fied value of 0. Suppose @ < X; for some i. Then,\n",
      "f(Xi;0) = 0 and hence Ln (0) = |]; f (Xi; 0) = 0. It follows that\n",
      "L,(0) = 0 if any X; > 0. Therefore, £L,(0) = 0 if 0 < Xn)\n",
      "where X(n) = max{X,,..-,X,}. Now consider any 0 > Xn).\n",
      "For every X; we then have that f(X;;0) =1/0 so that £,(0) =\n",
      "TI, f(X% 0) = 0. In conclusion,\n",
      "(4)\" 90> X\n",
      "= 7) = h(n)\n",
      "\n",
      "£n(@) { 0 0< Xn).\n",
      "See Figure 10.2. Now L£,(0) is strictly decreasing over the inter-\n",
      "val [X (mn), 00). Hence, 0, = X(n). Ml\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-152.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891699B48>\n",
      "152 10. Parametric Inference\n",
      "\n",
      " \n",
      "\n",
      "18\n",
      "15\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "FIGURE 10.2. Likelihood function for Uniform (0,0). The\n",
      "vertical lines show the observed data. The first three\n",
      "plots show f(«;0) for three different values of 6. When\n",
      "9 < Xn) = max{X1,...,Xp}, as in the first plot, f(X(,);0) = 0\n",
      "and hence £,(0) = []j_; f(Xi;0) = 0. Otherwise f(X;; 0) = 1/0\n",
      "for each i and hence £,(0) = []j-1 f(Xi; 4) = (1/0)”. The last plot\n",
      "shows the likelihood function.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-153.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148A1C23048>\n",
      "10.4 Properties of Maximum Likelihood Estimators. 153\n",
      "\n",
      "10.4 Properties of Maximum Likelihood\n",
      "Estimators.\n",
      "\n",
      "Under certain conditions on the model, the maximum likeli-\n",
      "hood estimator 6, possesses many properties that make it an ap-\n",
      "pealing choice of estimator. The main properties of the MLE are:\n",
      "(1) It is consistent: 6,7 0, where 6, denotes the true value\n",
      "of the parameter 0;\n",
      "\n",
      "(2) It is equivariant: if 6, is the MLE of @ then g(@,) is the\n",
      "MLE of g(9);\n",
      "\n",
      "(3) It is asymptotically Normal: \\/n(6— 6,)/ ~» N(0,1)\n",
      "where sé can be computed analytically;\n",
      "\n",
      "(4) It is asymptotically optimal or efficient: roughly, this\n",
      "means that among all well behaved estimators, the MLE has the\n",
      "\n",
      " \n",
      "\n",
      "smallest variance, at least for large samples.\n",
      "(5) The mle is approximately the Bayes estimator. (To be\n",
      "explained later.)\n",
      "\n",
      "We will spend some time explaining what these properties\n",
      "mean and why they are good things. In sufficiently complicated\n",
      "problems, these properties will no longer hold and the MLE will\n",
      "no longer be a good estimator. For now we focus on the simpler\n",
      "situations where the MLE works well. The properties we discuss\n",
      "only hold if the model satisfies certain regularity conditions.\n",
      "These are essentially smoothness conditions on f(2;0). Unless\n",
      "otherwise stated, we shall tacitly assume that these con-\n",
      "ditions hold.\n",
      "\n",
      "10.5 Consistency of Maximum Likelihood\n",
      "Estimators.\n",
      "\n",
      "Consistency means that the MLE converges in probability to\n",
      "the true value. To proceed, we need a definition. If f and g are\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-154.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CD9C8>\n",
      "This is not a dis-\n",
      "tance in the formal\n",
      "sense.\n",
      "\n",
      "154 10. Parametric Inference\n",
      "\n",
      "PDF ’s, define the Kullback-Leibler distance between f and\n",
      "\n",
      "g to be\n",
      "D(f,9) = [re log (42) da. (10.6)\n",
      "\n",
      "It can be shown that D(f,g) > 0 and D(f,f) = 0. For any\n",
      "6,u € © write D(@,w) to mean D(f (2; @), f(z; w)). We will\n",
      "assume that 0 # w implies that D(@,w) > 0.\n",
      "\n",
      "Let @, denote the true value of 9. Maximizing ¢,,(0) is equiv-\n",
      "alent to maximizing\n",
      "\n",
      "M,,( =7 Lhe: a ood\n",
      "By the law of large numbers, M,,() converges to\n",
      "(Xi 8) ) / (7 J (238) ?\n",
      "Ey, { lo = lo. x; 0,)dx\n",
      "wf °F) 8 FG 2) £ (5 44)\n",
      "\n",
      "= —D(6,,0).\n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Hence, M,,(0) & —D(@,, 0) which is maximized at 0, since —D(0,,0,) =\n",
      "Oand —D(0,,0.) <0 for 0 4 6,. Hence, we expect that the max-\n",
      "imizer will tend to 6,. To prove this formally, we need more than\n",
      "M,,(0)—3 — D(6,,0). We need this convergence to be uniform\n",
      "\n",
      "over 9. We also have to make sure that the function D(0,,0) is\n",
      "\n",
      "well behaved. Here are the formal details.\n",
      "\n",
      "Theorem 10.13 Let 0, denote the true value of 0. Define\n",
      "u (Xz 9)\n",
      "M,(@) =— > lo =\n",
      "(@) ye SFX)\n",
      "and M(0) = —D(0,,0). Suppose that\n",
      "\n",
      "sup |M,,(@) — M(6)|+30 (10.7)\n",
      "\n",
      "060\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-155.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C85C8>\n",
      "10.6 Equivariance of the MLE 155\n",
      "\n",
      "and that, for every € > 0,\n",
      "\n",
      "sup M(0) < M(6,). (10.8)\n",
      "0:|0-0.|>€\n",
      "\n",
      "Let 0, denote the mle. Then O23 Ops\n",
      "\n",
      "PROOF. See appendix. ll\n",
      "\n",
      "10.6 Equivariance of the MLE\n",
      "\n",
      "Theorem 10.14 Let 7 = g(0) be a one-to-one function of 0. Let\n",
      "6, be the MLE of 0. Then 7, = g(On) is the MLE of rT.\n",
      "\n",
      "ProoF. Let h = g~! denote the inverse of g. Then , =h(Tn)-\n",
      "For any 7, L(r) = [], f(wish(7)) = T], f(@is0) = £(@) where\n",
      "\n",
      "6 =h(r). Hence, for ana 7, £,(7) = £(0) < £(0) = £,(7).\n",
      "\n",
      "Example 10.15 Let X),...,X, ~N(0,1). The mle for 0 is 0, =\n",
      "Xn. Let t =e\". Then, the mle fort is? =e’ =e*. Hl\n",
      "\n",
      "10.7. Asymptotic Normality\n",
      "\n",
      "It turns out that 8, is approximately Normal and we can\n",
      "compute its variance analytically. To explore this, we first need\n",
      "a few definitions.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-156.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3508>\n",
      "156 10. Parametric Inference\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Definition 10.16 The score function is defined to be\n",
      "s(X;0) = =a fy (10.9)\n",
      "The Fisher information is defined to be\n",
      "(0) = Wo e)\n",
      "i=l\n",
      "= Sve (Xi3)) (10.10)\n",
      "For n = 1 we will sometimes write [() instead of I, (6).\n",
      "\n",
      "It can be shown that E,(s(X;0@)) = 0. It then follows that\n",
      "Vo(s(X;0)) = Eo(s?(X;0)). In fact a further simplification of\n",
      "I,,(@) is given in the next result.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 10.17 I,,(0) = nJ(@) and\n",
      "\n",
      "10) = -n, (Pees 0)\n",
      "\n",
      "_ -|[ (EE tte Ade. (10.11)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-157.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CDB48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.7 Asymptotic Normality 157\n",
      "\n",
      " \n",
      "\n",
      "Theorem 10.18 (Asymptotic Normality of the MLE .) Under appr\n",
      "priate regularity conditions, the following hold:\n",
      "\n",
      "1. Let se = \\/1/I,(0). Then,\n",
      "\n",
      "(On = 9)\n",
      "\n",
      "a+ N(0,1). (10.12)\n",
      "2. Let & = 4/1/In(On). Then,\n",
      "\n",
      "es ~ N(0,1). (10.13)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The proof is in the appendix. The first statement says that\n",
      "0, ~ N(0,se) where the standard error of 0, is se = \\/1/T,,(0).\n",
      "The second statement says that this is still true even if we re-\n",
      "\n",
      " \n",
      "\n",
      "place the standard error by its estimated standard error sé =\n",
      "\n",
      "Informally, the theorem says that the distribution of the MLE can\n",
      "be approximated with N (0, se), From this fact we can construct\n",
      "an (asymptotic) confidence interval.\n",
      "\n",
      "Theorem 10.19 Let\n",
      "C, = (6, — zp 8, by, + 20/28).\n",
      "Then, P9(9 € Cn) + 1—a asn oo.\n",
      "\n",
      "Proor. Let Z denote a standard normal random variable.\n",
      "Then,\n",
      "\n",
      "Po(9€Cn) = Py (6, = 0/288 <9 <6, +228)\n",
      "\n",
      "6,—8\n",
      "= Pyl—zan<— <2,\n",
      "( af2S—~Z_ Ss »2)\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-158.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3C48>\n",
      "158 10. Parametric Inference\n",
      "> P(-2ajp<Z<2ay2)=1l-a. Of\n",
      "\n",
      "For a = .05, Za/2 = 1.96 & 2, so:\n",
      "\n",
      " \n",
      "\n",
      "0, $28\n",
      "\n",
      "is an approximate 95 per cent confidence interval.\n",
      "\n",
      " \n",
      "\n",
      "(10.14)\n",
      "\n",
      " \n",
      "\n",
      "When you read an opinion poll in the newspaper, you often\n",
      "see a statement like: the poll is accurate to within one point,\n",
      "95 per cent of the time. They are simply giving a 95 per cent\n",
      "confidence interval of the form 6, + 286.\n",
      "\n",
      "Example 10.20 Let X,,...,X, ~ Bernoulli(p). The MLE is p, =\n",
      "Yo; Xi/n and f (x;p) = p*(1—p)'™*, log f(x; p) = xlogp + (1—\n",
      "1) log(1—p), s(X;p) = (x/p)—(1—2)/(1—p), and —s'(X;p) =\n",
      "(x/p?) + (1 x)/(L—p)?. Thus,\n",
      "\n",
      " \n",
      "\n",
      "Hence,\n",
      "\n",
      " \n",
      "\n",
      "~_ 1 1 sf p= 3)\n",
      "8 = eas em { m }\n",
      "\n",
      "An approximate 95 per cent confidence interval is\n",
      "\n",
      "oy 1/2\n",
      "as tL\n",
      "j+2 {HOD\n",
      "n\n",
      "Compare this with the Hoeffding interval.\n",
      "\n",
      "Example 10.21 Let X,,...,X, ~ N(@, 10\") where o? is known.\n",
      "The score function is s(X;0) = (X 9)/o? and s'(X;0) =\n",
      "—1/o? so that I,(0) = 1/02. The MLE is 6, = X,. According\n",
      "to Theorem 10.18, Xn &% N(0,0?/n). In fact, in this case, the\n",
      "distribution is exact.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-159.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8708>\n",
      "10.8 Optimality 159\n",
      "\n",
      "Example 10.22 Let X,,...,X, ~ Poisson(\\). Then a =X,\n",
      "and some calculations show that I,(A) = 1/2, so\n",
      "\n",
      " \n",
      "\n",
      "Therefore, an approximate 1 — a confidence interval for X is\n",
      "\n",
      "Xe t 2a) n/n. a\n",
      "\n",
      " \n",
      "\n",
      "10.8 Optimality\n",
      "\n",
      "Suppose that X,,...,X, ~ N(@,07). The MLB is 6, =X,.\n",
      "Another reasonable estimator is the sample median Oy. The\n",
      "MLE satisfies\n",
      "\n",
      "Vnl6, — 0) + N(0,0°).\n",
      "\n",
      "It can be proved that the median satisfies\n",
      "Vin, — 0) ~~ N (0.0?) .\n",
      "\n",
      "This means that the median converges to the right value but\n",
      "has a larger variance than the MLE .\n",
      "More generally, consider two estimators T,, and U,, and sup-\n",
      "\n",
      "pose that\n",
      "\n",
      "Vn(Tp, — 0) ~» N(0, t?)\n",
      "and that\n",
      "\n",
      "Jn(U, — 0) ~ N(0,u?).\n",
      "We define the asymptotic relative efficiency of U to T by ARE(U,\n",
      "(?/u2. In the Normal example, ARE(6,,6,) = 2/7 = .63. The\n",
      "interpretation is that if you use the median, you are only using\n",
      "about 63 per cent of the available data.\n",
      "Theorem 10.23 If 6, is the MLE and 6, is any other estimator\n",
      "then\n",
      "\n",
      "Ts\n",
      "\n",
      "The result is ac-\n",
      "tually more subtle\n",
      "than this but we\n",
      "needn't worry about\n",
      "the details.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-160.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "160 10. Parametric Inference\n",
      "ARE(,,0,) <1.\n",
      "\n",
      "Thus, the MLE has the smallest (asymptotic) variance and we\n",
      "say that MLE is efficient or asymptotically optimal.\n",
      "\n",
      "This result is predicated upon the assumed model being cor-\n",
      "rect. If the model is wrong, the MLE may no longer be optimal.\n",
      "We will discuss optimality in more generality when we discuss\n",
      "decision theory.\n",
      "\n",
      "10.9 The Delta Method.\n",
      "\n",
      "Let rt = g(@) where g is a smooth function. The maximum\n",
      "\n",
      "likelihood estimator of 7 is F = g(@). Now we address the fol-\n",
      "lowing question: what is the distribution of T?\n",
      "\n",
      " \n",
      "\n",
      "Theorem 10.24 (The Delta Method) Jf rt = g(0) where g is differ-\n",
      "entiable and g'(0) #0 then\n",
      "\n",
      "viii, = 7)\n",
      "\n",
      "Ba NOD (10.15)\n",
      "where F = (On) and\n",
      "% (F,) = |9'()| 8 (6) (10.16)\n",
      "Hence, if\n",
      "On = (F — 2/28 (Fn), Tn + 20/28 ()) (10.17)\n",
      "\n",
      "then P(t € Cy) + 1—a asn—- oo.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 10.25 Let X,,...,X, ~ Bernoulli(p) and let y = g(p) =\n",
      "log(p/(1—p)). The Fisher information function is I(p) = 1/(p(—\n",
      "p)) 80 the standard error of the MLE Py isse = {Pn(1 — pn)/n}'”.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-161.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907A7088>\n",
      "10.9 The Delta Method. 161\n",
      "\n",
      " \n",
      "\n",
      "The MLE of w is yy = logp/(1—p). Since, g(p) =1/(p/(1—p)),\n",
      "according to the delta method\n",
      "ae ae ee SS L\n",
      "$6 (Un) = |9' (Bn) |S (Px) = —————.\n",
      "\"Pn (1 — Pn)\n",
      "\n",
      "An approximate 95 per cent confidence interval is\n",
      "\n",
      " \n",
      "\n",
      "Example 10.26 Let X),...,X, ~ N(j,07). Suppose that ps is\n",
      "known, 0 is unknown and that we want to estimate w = logo.\n",
      "The log-likelihood is €(o) = —nlogo — sy 0 ,(a; — 1)?. Differ-\n",
      "entiate and set equal to 0 and conclude that\n",
      "\n",
      "= {2 oa\n",
      "\n",
      "C=\n",
      "n\n",
      "\n",
      "To get the standard error we need the Fisher information. First,\n",
      "\n",
      "log f (X;0) =— logo — om\n",
      "with second derivative\n",
      "1 3(xX- hy?\n",
      "o ot\n",
      "and hence i. 34 9\n",
      "i a\n",
      "\n",
      "Hence, & = G,/V2n. Let = g(o) = log(a). Then, by, =\n",
      "logG,. Since, g' = 1/o,\n",
      "\n",
      "6 1\n",
      "$e (Wn) =\n",
      "\n",
      "van Vn\n",
      "\n",
      "and an approximate 95 per cent confidence interval is On +\n",
      "\n",
      "2/\\/2n.\n",
      "\n",
      "iy) Rad\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-162.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E9308>\n",
      "162 10. Parametric Inference\n",
      "10.10 Multiparameter Models\n",
      "\n",
      "These ideas can directly be extended to models with several\n",
      "parameters. Let 9 = (0,,...,;) and let @ = (01,...,;) be the\n",
      "MLE . Let £, = )5j_, log f (Xs 0),\n",
      "\n",
      "Oe, ln\n",
      "\n",
      "H, and Ae = 3950,\n",
      "0},\n",
      "\n",
      "= OBE\n",
      "J\n",
      "\n",
      "Define the Fisher Information Matrix by\n",
      "\n",
      "Eo(Hi1) Eg(Hiz) +++ Eg(ix)\n",
      "1,0) =- Beit) mt) a “ Malta (tos)\n",
      "Ey(Hi1) Eo(Hy2) +++ Eo(Hex)\n",
      "\n",
      "Let J,(0) = 17'(@) be the inverse of I,,.\n",
      "Theorem 10.27 Under appropriate regularity conditions,\n",
      "\n",
      "Vnl0 — 0) = N(0, Jn).\n",
      "\n",
      "Also, if 6; is the j'\" component of 0, then\n",
      "\n",
      "6, = 9j)\n",
      "\n",
      " \n",
      "\n",
      "where &; = Jn(j,j) is the j** diagonal element of Jn. The ap-\n",
      "protimate covariance of 0; and 0, is Cov(0;, Ox) x In( j,k).\n",
      "\n",
      "There is also a multiparameter delta method. Let rT = g(01,---, 9%)\n",
      "be a function and let\n",
      "cm\n",
      "00\n",
      "Vg= ‘\n",
      "oo\n",
      "00;.\n",
      "\n",
      "be the gradient of g.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-163.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3C48>\n",
      "10.11 The Parametric Bootstrap 163\n",
      "\n",
      "Theorem 10.28 (Multiparameter delta method) Suppose that\n",
      "Vg evaluated at 0 is not 0. Let 7 = g(@). Then\n",
      "\n",
      " \n",
      "\n",
      "where\n",
      "\n",
      "& (7) = V (Vg) FAV 9), (10.19)\n",
      "Tn = In (On) and Vo is Vg evaluated at 0 = 0.\n",
      "\n",
      "Example 10.29 Let X1,...,Xn ~ N(p,07). Let rT = g(p,o) =\n",
      "a/p. In homework question 8 you will show that\n",
      "\n",
      "Hence,\n",
      "\n",
      " \n",
      "\n",
      "Thus,\n",
      "\n",
      " \n",
      "\n",
      "10.11 The Parametric Bootstrap\n",
      "\n",
      "For parametric models, standard errors and confidence in-\n",
      "tervals may also be estimated using the bootstrap. There is\n",
      "only one change. In the nonparametric bootstrap, we sampled\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-164.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E9308>\n",
      "164 10. Parametric Inference\n",
      "\n",
      "X{,...,X; from the empirical distribution F,. In the paramet-\n",
      "ric bootstrap we sample instead from f (2; 0,,). Here, 6, could\n",
      "be the MLE or the method of moments estimator.\n",
      "\n",
      "Example 10.30 Consider example 10.29. To get to bootstrap stan-\n",
      "dard error, simulate X1,...,Xz ~ N(fi,6?), compute ji* =\n",
      "n-1 30, X7 and 6* = n7' 30,(X} — 7*)?. Then compute 7 =\n",
      "g(t, 6*) = 6*/j*. Repeating this B times yields bootstrap repli-\n",
      "cations\n",
      "\n",
      "oo a\n",
      "Ty yess TB\n",
      "\n",
      "and the estimated standard error is\n",
      "\n",
      "2\n",
      "$€ boot = B _\n",
      "\n",
      "The bootstrap is much easier than the delta method. On the\n",
      "other hand, the delta method has the advantage that it gives a\n",
      "closed form expression for the standard error.\n",
      "\n",
      "10.12 Technical Appendix\n",
      "\n",
      "10.12.1 Proofs\n",
      "\n",
      "PROOF OF THEOREM 10.13. Since 6, maximizes M,,(@), we\n",
      "\n",
      "have M,,(0,,) > M,,(6,). Hence,\n",
      "\n",
      "M(6.) — M(@,) M,(0,) — M(6,) + M(6,) — Mn(6.)\n",
      "\n",
      "M,(8) — M(0n) + M(0.) — Mn (0x)\n",
      "sup |M,,(0) — M(0)| + M(0.) — Mn(0.)\n",
      "0\n",
      "\n",
      "[PAA\n",
      "\n",
      "0\n",
      "\n",
      "where the last line follows from (10.7). It follows that, for any\n",
      "5>0,\n",
      "P (1G) <M) - d) 30.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-165.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3508>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.12 Technical Appendix 165\n",
      "\n",
      "Pick any € > 0. By (10.8), there exists 6 > 0 such that |—0,| > €\n",
      "implies that W(@) < M(0,) — 6. Hence,\n",
      "\n",
      "P(|0, —%| >) <P (1@.) < M(0,) - °) 30. ff\n",
      "Next we want to prove Theorem 10.18. First we need a Lemma.\n",
      "Lemma 10.31 The score function satisfies\n",
      "Ep [s(X;4)] =0.\n",
      "\n",
      "Proor. Note that 1 = f f(2;0)dx. Differentiate both sides\n",
      "of this equation to conclude that\n",
      "\n",
      " \n",
      "\n",
      "0 = wf seme f Zresoar\n",
      "ae a Dog (29) 6 1\n",
      "Ca al ( O)dz = ——y (2 O)dx\n",
      "\n",
      "[ s(osoyrtes0)ae = E,ys(X;0).\n",
      "\n",
      "PROOF OF THEOREM 10.18. Let (0) = log £(0). Then,\n",
      "0=£(0) = C0) + (-9)0\"(0).\n",
      "\n",
      "Rearrange the above equation to get @— 6 = —0'(0)/0\"(8) or, in\n",
      "other words,\n",
      "\n",
      "Fal) TOP\n",
      "Va _\n",
      "G9) ae OE\n",
      "Wale—8) —17\"(@) ~ BOTTOM\n",
      "\n",
      "Let Y; = 0 log f (X;;0)/00. Recall that E(Y;) =0 from the pre-\n",
      "vious Lemma and also V(Y;) = 1(@). Hence,\n",
      "\n",
      "TOP =n7'? S°Y; = VnY = Vn(¥ -0) ~ W~ NOD)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-166.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E92C8>\n",
      "166 10. Parametric Inference\n",
      "\n",
      "by the central limit theorem. Let A; = —0? log f (Xj;0)/00?.\n",
      "Then E(A;) =1(@) and\n",
      "\n",
      "BOTTOM = A—> (6)\n",
      "\n",
      "by the law of large numbers. Apply Theorem 6.5 part (e), to\n",
      "conclude that\n",
      "\n",
      "~ Ww 1\n",
      "Vn(0 — 0) “THAN (0.75):\n",
      "\n",
      "Assuming that [(@) is a continuous function of @, it follows that\n",
      "1(6,) 4 1(8). Now\n",
      "\n",
      " \n",
      "\n",
      "—— = val'?(On)(On — 9)\n",
      "\n",
      "~. 1/2\n",
      "= {vare@@, 0} te} ;\n",
      "\n",
      "The first terms tends in distribution to N(0,1). The second term\n",
      "tends in probability to 1. The result follows from Theorem 6.5\n",
      "part (e).\n",
      "\n",
      "OUTLINE OF PROOF OF THEOREM 10.24. Write,\n",
      "\n",
      " \n",
      "\n",
      "F = (8) = (0) + (0 —8)9'(9) =7 + 0 — A)g'(0).\n",
      "\n",
      "Thus, .\n",
      "Vil? — 7) = V/nl(8 — 0) 9'(0)\n",
      "and hence\n",
      "nl(O)(F—T) - ~\n",
      "Fe) z I(0)(0 — 0).\n",
      "\n",
      "Theorem 10.18 tells us that the right hand side tends in distri-\n",
      "bution to a N(0,1). Hence,\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-167.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n",
      "10.12 Technical Appendix 167\n",
      "\n",
      "or, in other words,\n",
      "\n",
      "where Woy\n",
      "a2\\)_ VY\n",
      "se7(7,) = “nl @)\n",
      "The result remains true if we substitute 0 for 0 by Theorem 6.5\n",
      "part (ec).\n",
      "\n",
      "10.12.2 Sufficiency\n",
      "\n",
      "A statistic is a function T(X\") of the data. A sufficient statis-\n",
      "tic is a statistic that contains all the information in the data.\n",
      "To make this more formal, we need some defintions.\n",
      "\n",
      " \n",
      "\n",
      "Definition 10.32 Write 2” © y” if f(x\";0) = c f(y\";@)\n",
      "for some constant c that might depend on x\" and y” but\n",
      "not 6. A statistic T(a”) is sufficient if T(x”) «+ T(y\")\n",
      "implies that 2” <> y\".\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Notice that if x” < y” then the likelihood function based on\n",
      "\n",
      "x” has the same shape as the likelihood function based on y\".\n",
      "\n",
      "Roughly speaking, a statistic is sufficient if we can calculate the\n",
      "likelihood function knowing only T(X\").\n",
      "\n",
      "Example 10.33 Let X),...,X, ~ Bernoulli(p). Then L(p)\n",
      "ps(1—p)\"-* where S = yo, X; so S is sufficient.\n",
      "\n",
      "Example 10.34 Let X1,...,X;, ~ N(,0) and let T = (X,S).\n",
      "Then,\n",
      "\n",
      "f(X\"sp,0) = []f4in0\n",
      "\n",
      "II o 7 oP {-s des - wh\n",
      "\n",
      "_ 1 _ ns? a _n(X = p)?\n",
      "= (se) PP) age GF\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-168.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "168 10. Parametric Inference\n",
      "\n",
      "The last expression depends on the data only through T and\n",
      "therefore, T = (X,S) is a sufficient statistic. Note that U =\n",
      "(17X,S) is also a sufficient statistic. If I tell you the value of U\n",
      "then you can easily figure out T and then compute the likelihood.\n",
      "Sufficient statistics are far from unique. Consider the following\n",
      "statistics for the N(,0?) model:\n",
      "\n",
      "TX\") = (X1,---, Xn)\n",
      "T,(X\") = (X,S)\n",
      "T;(X\") x\n",
      "\n",
      "Ti(X\") = (X,S, Xs).\n",
      "\n",
      "The first statistic is just the whole data set. This is sufficient.\n",
      "The second is also sufficient as we proved above. The third is not\n",
      "sufficient: you can’t compute L(p,0) if I only tell you X. The\n",
      "fourth statistic T, is sufficient. The statistics T; and T, are suffi-\n",
      "cient but they contain redundant information. Intuitively, there\n",
      "is a sense in which Ty is a “more concise” sufficient statistic\n",
      "than either T; or Ty. We can express this formally by noting\n",
      "that Tz is a function of T, and similarly, T> is a function of Ty.\n",
      "For example, T, = g(T;) where g(a,,@2,a3) = (a), a).\n",
      "\n",
      " \n",
      "\n",
      "Definition 10.35 A statistic T is minimally sufficient if\n",
      "(i) it is sufficient and (ii) it is a function of every other\n",
      "sufficient statistic.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 10.36 T is minimal sufficient if T(a\") =T(y\") if and\n",
      "only if a\" 4 y”.\n",
      "\n",
      "A statistic induces a partition on the set if outcomes. We can\n",
      "think of sufficiency in terms of these partitions.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-169.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3C48>\n",
      "10.12 Technical Appendix 169\n",
      "\n",
      "Example 10.37 Let X,,X2,X3 ~ Bernoulli(@). Let V = X,\n",
      "T = 30,X; and U = (T,X). Here is the set of outcomes and\n",
      "the statistics:\n",
      "\n",
      "Xi X\n",
      "\n",
      "’ T U\n",
      "\n",
      " \n",
      "\n",
      "Then V is not sufficient but T and U are sufficient. T is min-\n",
      "imal sufficient; U is not minimal since if x2” = (1,0,1) and\n",
      "y” = (0,1,1), then 2” 4 y” yet U(x\") # U(y\"). The statistic\n",
      "W =17T generates the same partition as T. It is also minimal\n",
      "sufficient. il\n",
      "\n",
      "Example 10.38 For a N(j,07) model, T = (X,S) is a minimally\n",
      "sufficient statistic. For the Bernoulli model, T = 3); X; is a\n",
      "minimally sufficient statistic. For the Poisson model, T = )), Xj\n",
      "is a minimally sufficient statistic. Check that T = (30; X;,X1)\n",
      "is sufficient but not minimal sufficient. Check that T = X, is\n",
      "not sufficient. Hi\n",
      "\n",
      "I did not give the usual definition of sufficiency. The usual\n",
      "definition is this: T is sufficient if the distribution of X” given\n",
      "T(X\") =t does not depend on 0.\n",
      "\n",
      "Example 10.39 Two coin flips. Let X = (X,, Xz) ~ Bernoulli(p).\n",
      "Then T = X, + Xq is sufficient. To see this, we need the dis-\n",
      "tribution of (X,,X2) given T = t. Since T can take 3 possible\n",
      "values, there are 3 conditional distributions to check. They are:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-170.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CD9C8>\n",
      "170 10. Parametric Inference\n",
      "(i) the distribution of (X1, Xz) given T = 0:\n",
      "\n",
      "P(X; =0,X2.=\n",
      "\n",
      "  \n",
      "\n",
      "t= 0) =1, P(X, =0,Xy = 1|t=0) =0,\n",
      "\n",
      "P(X, =1,Xy =0|t =0) =0, P(X, =1,.X, = 1t=0) =0\n",
      "(ii) the distribution of (X,,X2) given T =1:\n",
      "\n",
      "1\n",
      "\n",
      "P(X; =0, Xz =0lf = 1) = 0, P(X =0, Xx == 1) =5,\n",
      "\n",
      "1\n",
      "\n",
      "P(X, = 1, X2 = 0|t=1) =5,P(X1 = 1, Xp = 1 = 1) =0\n",
      "\n",
      "(iii) the distribution of (X,,X2) given T = 2:\n",
      "\n",
      "P(X, = 0, X_ = Olt = 2) = 0, P(X, = 0, X» = 1\\t = 2) =0,\n",
      "\n",
      " \n",
      "\n",
      "P(X; =1, Xz = 0|t = 2) =0, P(X =1, Xe = 1|t = 2) = 1.\n",
      "None of these depend on the parameter p. Thus, the distribution\n",
      "\n",
      "of X,,X2|T does not depend on @ so T is sufficient. Wl\n",
      "\n",
      "Theorem 10.40 (Factorization Theorem) T is sufficient if and\n",
      "only if there are functions g(t,0) and h(x) such that f(2\";0) =\n",
      "\n",
      "g(t(2\"), @)h(2\").\n",
      "Example 10.41 Return to the two coin flips. Let t = a, 4+ £9.\n",
      "Then\n",
      "\n",
      "F (1130) f (v2; 9)\n",
      "Mao eo\n",
      "= g(t, A)h(x1, x2)\n",
      "\n",
      "where g(t,0) = 0'(1 —0)?-' and h(x1,22) =1. Therefore, T =\n",
      "X, +X is sufficient. HH\n",
      "\n",
      "Ff (a1, #238)\n",
      "\n",
      "Now we discuss an implication of sufficiency in point estima-\n",
      "tion. Let 0 be an estimator of 9. The Rao-Blackwell theorem says\n",
      "that an estimator should only depend on the sufficient statistic,\n",
      "\n",
      "otherwise it can be improved. Let R(0,0) = E,[( — @)?] denote\n",
      "the MSE of the estimator.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-171.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3C48>\n",
      "10.12 Technical Appendix 171\n",
      "\n",
      "Theorem 10.42 (Rao-Blackwell) Let @ be an estimator and\n",
      "let T be a sufficient statistic. Define a new estimator by\n",
      "\n",
      "6 =E(6|T).\n",
      "Then, for every 0, R(0,0) < R(6,@).\n",
      "Example 10.43 Consider flipping a coin twice. Let d= X. This\n",
      "is a well defined (and unbiased) estimator. But it is not a func-\n",
      "tion of the sufficient statistic T = X, + Xz. However, note\n",
      "that 0 = E(X,|T) = (X, + X2)/2. By the Rao-Blackwell Theo-\n",
      "rem, 0 has MSE at least as small as @ = X,. The same applies\n",
      "with n coin flips. Again define @ = X, and T = \\>,X;. Then\n",
      "6 =E(X,|T) =n! So, X; has improved vse . Wl\n",
      "\n",
      "10.12.8 Exponential Families\n",
      "\n",
      "Most of the parametric models we have studied so far are spe-\n",
      "cial cases of a general class of models called exponential families.\n",
      "We say that {f(2;0;0 € QO} is a one-parameter exponential\n",
      "family if there are functions 7(0), B(@), T(x) and h(x) such\n",
      "that:\n",
      "\n",
      "f(a30) = h(x)en@T@)-BO |\n",
      "\n",
      "It is easy to see that T(X) is sufficient. We call T the natural\n",
      "sufficient statistic.\n",
      "Example 10.44 Let X ~ Poisson(@). Then\n",
      "\n",
      " \n",
      "\n",
      "Oe? 1c togo-0\n",
      "f (230) = a = ae e\n",
      "\n",
      " \n",
      "\n",
      "and hence, this is an exponential family with n(0) = log 0, B(@) =\n",
      "6, T(x) =2, h(x) =1/c!.\n",
      "\n",
      "Example 10.45 Let X ~ Bin(n,@). Then\n",
      "\n",
      "(230) = (“Jera—oy = (\") exp {tog (4) + nlog(1 — a} .\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-172.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C85C8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172 10. Parametric Inference\n",
      "\n",
      "In this case,\n",
      "\n",
      "and\n",
      "\n",
      "a\n",
      "We can rewrite an exponential family as\n",
      "flesn) = h(e)e\"o-a\n",
      "\n",
      "where 7 = 7(9) is called the natural parameter and\n",
      "A(n) = tog f h(a)et™ar.\n",
      "\n",
      "For example a Poisson can be written as f(2;7) = e™—°\"/z!\n",
      "where the natural parameter is 7 = log@.\n",
      "\n",
      "Let X\\,...,X,, be iid froma exponential family. Then f(x”; 4)\n",
      "is an exponential family:\n",
      "\n",
      "f(s\" 0) =h, (a? i, a ee\n",
      "\n",
      "where h,,(2\") = J],;h(#), Tr(w\") = 30,7 (2;) and B,(@) =\n",
      "nB(@). This implies that }>,T(X;) is sufficient.\n",
      "\n",
      "Example 10.46 Let X),...,X, ~ Uniform(0,0). Then\n",
      "sf 1\n",
      "f@5O)= gel (Xn) <0)\n",
      "\n",
      "where I is 1 if the term inside the brackets is true and 0 other-\n",
      "wise, and x(n) = max{x,,...,%n}. Thus T(X\") = mar{X,,...,X,}\n",
      "is sufficient. But since T(X\") # 3),T (Xj), this cannot be an ex-\n",
      "ponential family.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-173.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907A7088>\n",
      "10.12 Technical Appendix 173\n",
      "\n",
      "Theorem 10.47 Let X have density in an exponential family.\n",
      "Then,\n",
      "E(T(X)) = A'(n), V(T(X)) = A\"(n)-\n",
      "\n",
      "If 0 = (,...,0;) is a vector, then we say that f(2;@) has\n",
      "exponential family form if\n",
      "\n",
      "k\n",
      "f (230) = h(x) exp {= nj(0)T; (2) — ovo) :\n",
      "jel\n",
      "Again, T = (T),.-..,T7),) is sufficient and n iid samples also has\n",
      "exponential form with sufficient statistic ()>; Ti (Xj), -.., 0; Tk(X))-\n",
      "\n",
      "Example 10.48 Consider the normal family with 0 = (1,0). Now,\n",
      "\n",
      "(230) = ef He - a -2 (4 + lox( na?) } .\n",
      "\n",
      "202 2 \\o?\n",
      "\n",
      "This is exponential with\n",
      "\n",
      "m(0)= 4, Til) =«\n",
      "m0) = ~9g2? T(x) = 2°\n",
      "B(d) = B (5 + lo(2x0?)) , A(z) =1.\n",
      "\n",
      "Hence, with n iid samples, (37;Xi, >, X?) is sufficient. i\n",
      "\n",
      "As before we can write an exponential family as\n",
      "\n",
      "f(a3n) = h(x) exp {17 (a)n — A(m)}\n",
      "\n",
      "where A(y) = f h(z)e™” \"dz. It can be shown that\n",
      "\n",
      "where the first expression is the vector of partial derivatives and\n",
      "the second is the matrix of second derivatives.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-174.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891850C48>\n",
      "174\n",
      "\n",
      "10. Parametric Inference\n",
      "\n",
      "10.13 Exercises\n",
      "\n",
      "1.\n",
      "\n",
      "Let X),...,X, ~ Gamma(a, 3). Find the method of mo-\n",
      "ments estimator for a and .\n",
      "\n",
      ". Let X\\,...,X, ~ Uniform(a,b) where a and 6 are un-\n",
      "\n",
      "known parameters and a < b.\n",
      "\n",
      "(a) Find the method of moments estimators for a and b.\n",
      "(b) Find the MLE @ and 6.\n",
      "\n",
      "(c) Let r = f xdF (x). Find the MLB of 7.\n",
      "\n",
      "(d) Let 7 be the MLE from (1bc). Let 7 be the nonpara-\n",
      "metric plug-in estimator of 7 = f dF (x). Suppose that\n",
      "a =1,b=3 and n= 10. Find the MsE of 7 by simulation.\n",
      "Find the MSE of 7 analytically. Compare.\n",
      "\n",
      ". Let X1,...,Xn ~ N(p,07). Let 7 be the .95 percentile,\n",
      "\n",
      "ie. P(X < 7) =.95.\n",
      "(a) Find the MLE of r.\n",
      "\n",
      "(b) Find an expression for an approximate 1—a confidence\n",
      "interval for T.\n",
      "\n",
      "(c) Suppose the data are:\n",
      "\n",
      "3.23 -2.50 1.88 -0.68 4.43 0.17\n",
      "1.03 -0.07 -0.01 0.76 1.76 3.18\n",
      "0.33 -0.31 0.30 -0.61 1.52 5.43\n",
      "1.54 2.28 0.42 2.33 -1.03 4.00\n",
      "0.39\n",
      "\n",
      "Find the mle 7. Find the standard error using the delta\n",
      "method. Find the standard error using the parametric\n",
      "bootstrap.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-175.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3C48>\n",
      "10.13 Exercises 175\n",
      "\n",
      "4. Let X,,...,X, ~ Uniform(0,0). Show that the MLE is\n",
      "\n",
      "consistent. Hint: Let Y = max{Xj,...,X,}.. For any c,\n",
      "PY <c) = P(X, < ¢,X) < ¢...,X, <c) = P(X <\n",
      "c)P(X2 < c)...P(X, < c).\n",
      "\n",
      "- Let X,,...,X, ~ Poisson(\\). Find the method of mo-\n",
      "\n",
      "ments estimator, the maximum likelihood estimator and\n",
      "the Fisher information I().\n",
      "\n",
      ". Let Xy,...,X, ~ N(0,1). Define\n",
      "\n",
      "yall XxX >0\n",
      "‘= L0 if X) <0.\n",
      "\n",
      "Let  =P(% =1).\n",
      "\n",
      "a) Find the maximum likelihood estimate bof v.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "(b) Find an approximate 95 per cent confidence interval\n",
      "for w.\n",
      "\n",
      "(c) Define = (1/n) >, ¥i- Show that ® is a consistent\n",
      "estimator of y.\n",
      "\n",
      "(d) Compute the asymptotic relative efficiency of wb tov.\n",
      "Hint: Use the delta method to get the standard error of the\n",
      "MLE . Then compute the standard error (i.e. the standard\n",
      "deviation) of 1.\n",
      "\n",
      "(ec ) Suppose that the data are not really normal. Show that\n",
      "w is not consistent. What, if anything, does o converge to?\n",
      "\n",
      ". (Comparing two treatments.) n, people are given treat-\n",
      "ment 1 and nz people are given treatment 2. Let X; be\n",
      "the number of people on treatment 1 who respond fa-\n",
      "yorably to the treatment and let Xj be the number of\n",
      "people on treatment 2 who respond favorably. Assume\n",
      "that X, ~ Binomial(n;,p,) X2 ~ Binomial(ng, pz). Let\n",
      "w= Ppi — pe-\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-176.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n",
      "176 10. Parametric Inference\n",
      "\n",
      "(a) Find the MLE for w.\n",
      "\n",
      "(b) Find the Fisher information matrix I(P,, p2).\n",
      "(\n",
      "\n",
      "t\n",
      "\n",
      "c) Use the multiparameter delta method to find the asymp-\n",
      "otic standard error of W.\n",
      "\n",
      "(d) Suppose that nj = ny = 200, X; = 160 and Xy = 148.\n",
      "\n",
      "Find w. Find an approximate 90 percent confidence inter-\n",
      "\n",
      "val for u) using (i) the delta method and (ii) the parametric\n",
      "\n",
      "bootstrap.\n",
      "\n",
      "8. Find the Fisher information matrix for Example 10.29.\n",
      "\n",
      " \n",
      "\n",
      "9. Let Xj,...,X, Normal(j,1). Let 6 = e” and let 0\n",
      "be the mle. Create a data set (using 4 = 5) consisting of\n",
      "n=100 observations.\n",
      "\n",
      "(a) Use the delta method to get sé and 95 percent confi-\n",
      "dence interval for 6. Use the parametric bootstrap to get\n",
      "sé and 95 percent confidence interval for 0. Use the non-\n",
      "parametric bootstrap to get sé and 95 percent confidence\n",
      "interval for 6. Compare your answers.\n",
      "\n",
      "(b) Plot a histogram of the bootstrap replications for the\n",
      "parametric and nonparametric bootstraps. These are esti-\n",
      "mates of the distribution of @. The delta method also gives\n",
      "an approximation to this distribution namely, Normal (9, se”).\n",
      "Compare these to the true sampling distribution of 6 (whihe\n",
      "you can get by simulation). Which approximation, para-\n",
      "metric bootstrap, bootstrap, or delta method is closer to\n",
      "the true distribution?\n",
      "\n",
      "10. Let X),...,X, Unif(0, 0). The MLE is? = X(q = max{Xi,..., Xn}.\n",
      "Generate a data set. of size 50 with @=1.\n",
      "\n",
      "(a) Find the distribution of @ analytically. Compare the\n",
      "true distribution of @ to the histograms from the paramet-\n",
      "ric and nonparametric bootstraps.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-177.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "10.13 Exercises 177\n",
      "\n",
      "(b) This is a case where the nonparametric bootstrap\n",
      "does very poorly. Show that, for the parametric bootstrap\n",
      "P(6* = 0) =0 but for the nonparametric bootstrap P(O* =\n",
      "0) ~ .632. Hint: show that, P(@* = 0) =1 - (1—(1/n))\"\n",
      "then take the limit as n gets large. What is the implication\n",
      "of this?\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-178.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CD9C8>\n",
      "178 10. Parametric Inference\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-179.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "11\n",
      "Hypothesis Testing and p-values\n",
      "\n",
      "Suppose we want to know if a certain chemical causes cancer.\n",
      "We take some rats and randomly divide them into two groups.\n",
      "We expose one group to the chemical and then we compare\n",
      "the cancer rate in the two groups. Consider the following two\n",
      "hypotheses:\n",
      "\n",
      "The Null Hypothesis: The cancer rate is the same\n",
      "in the two groups\n",
      "\n",
      "The Alternative Hypothesis: The cancer rate is\n",
      "not the same in the two groups.\n",
      "\n",
      "If the exposed group has a much higher rate of cancer than the\n",
      "unexposed group then we will reject the null hypothesis and\n",
      "conclude that the evidence favors the alternative h ypothesis;\n",
      "in other words we will conclude that there is evidence that the\n",
      "chemical causes cancer. This is an example of hypothesis testing.\n",
      "\n",
      "More formally, suppose that we partition the parameter space\n",
      "Q into two disjoint sets O9 and ©, and that we wish to test\n",
      "\n",
      "Ho:0€@o versus H,:0€O4. (11.1)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-180.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891850C48>\n",
      "180 11. Hypothesis Testing and p-values\n",
      "\n",
      "We call Hp the null hypothesis and H, the alternative hy-\n",
      "pothesis.\n",
      "\n",
      "Let X be a random variable and let Y be the range of X. We\n",
      "test a hypothesis by finding an appropriate subset. of outcomes\n",
      "RC # called the rejection region. If X € R we reject the\n",
      "null hypothesis, otherwise, we do not reject the null hypothesis:\n",
      "\n",
      "XeER => reject Ho\n",
      "X ¢R => retain (do not reject) Ho\n",
      "\n",
      "Usually the rejection region R is of the form\n",
      "\n",
      "Rafer T(x) > e}\n",
      "\n",
      "where T is a test statistic and c is a critical value. The prob-\n",
      "lem in hypothesis testing is to find an appropriate test statistic\n",
      "T and an appropriate cutoff value c.\n",
      "\n",
      "Warning! There is a tendency to use hypthesis testing meth-\n",
      "ods even when they are not appropriate. Often, estimation and\n",
      "confidence intervals are better tools. Use hypothesis testing only\n",
      "when you want to test a well defined hypothesis.\n",
      "\n",
      "Hypothesis testing is like a legal trial. We asume someone is\n",
      "innocent unless the evidence strongly suggests that he is guilty.\n",
      "Similarly, we retain Ho unless there is strong evidence to reject.\n",
      "Hy. There are two types of errors when can make. Rejecting Ho\n",
      "when Hp is true is called a type I error. Rejecting H; when\n",
      "#1, is true is called a type II error. The possible outcomes for\n",
      "hypothesis testing are summarized in the Table below:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-181.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891699B48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11. Hypothesis Testing and p-values 181\n",
      "\n",
      "Retain Null Reject Null\n",
      "Ho true | / type I error\n",
      "H, true | type ILerror /\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Summary of outcomes of hypothesis testing.\n",
      "\n",
      "Definition 11.1 The power function of a test with rejection re-\n",
      "gion R is defined by\n",
      "\n",
      "B(0) = Py(X € R). (11.2)\n",
      "The size of a test is defined to be\n",
      "\n",
      "a= sup (8). (11.3)\n",
      "\n",
      "0E@0\n",
      "\n",
      "A test is said to have level a if its size is less than or equal to\n",
      "Q.\n",
      "\n",
      "A hypothesis of the form 0 = @p is called a simple hypoth-\n",
      "esis. A hypothesis of the form @ > 69 or @ < @ is called a\n",
      "composite hypothesis. A test of the form\n",
      "\n",
      "Ao:0=0605 versus H,:04 6\n",
      "is called a two-sided test. A test of the form\n",
      "AHyo:0<0) versus H,;:0> 4\n",
      "\n",
      "or\n",
      "Ho :0>6) versus H,:0< 4%\n",
      "\n",
      "is called a one-sided test. The most common tests are two-\n",
      "sided.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-182.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3C48>\n",
      "182 11. Hypothesis Testing and p-values\n",
      "\n",
      "Example 11.2 Let X),...,X, ~ N(y,0) where o is known. We\n",
      "want to test Hy: <0 versus H, : p> 0. Hence, Og = (—ov, 0]\n",
      "and ©, = (0,00). Consider the test:\n",
      "\n",
      "reject Hy if T >c\n",
      "\n",
      "where T =X. The rejection region is R= {x 2 (GP) &. a}.\n",
      "Let Z denote a standard normal random variable. The power\n",
      "function is\n",
      "\n",
      "Bu) = Pr (X>e)\n",
      "~ 7, (A=, view)\n",
      "\n",
      "o o\n",
      "\n",
      "_ P(z> a)\n",
      "_ 1-0(4),\n",
      "\n",
      "o\n",
      "\n",
      "This function is increasing in w. Hence\n",
      "\n",
      " \n",
      "\n",
      "sive sup (i) = 60) =1-0( ° ).\n",
      "\n",
      "u<d\n",
      "To get a size a test, set this equal to a and solve for c to get\n",
      "\n",
      "od\"(L—a)\n",
      "\n",
      "—\n",
      "\n",
      " \n",
      "\n",
      "So we reject when X > oO-!(1—a)/ Jn. Equivalently, we reject\n",
      "\n",
      "when —\n",
      "Vu gg\n",
      "o\n",
      "Finding most powerful tests is hard and, in many cases, most\n",
      "powerful tests don’t even exist. Instead of searching for most\n",
      "powerful tests, we'll just consider three widely used tests: the\n",
      "Wald test, the y? test and the permutation test. A fourth test,\n",
      "the likelihood ratio test, is dicussed in the appendix.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-183.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "11.1 The Wald Test 183\n",
      "\n",
      ".1 The Wald Test\n",
      "\n",
      "Let @ be a scalar parameter, let @ be an estimate of @ and let\n",
      "be the estimated standard error of @.\n",
      "\n",
      " \n",
      "\n",
      "Definition 11.3 The Wald Test\n",
      "Consider testing\n",
      "\n",
      "Hy:9=0 versus H,: 046.\n",
      "Assume that 0 is asymptotically Normal:\n",
      "\n",
      "0-4\n",
      "vind — 60) ~ N(0,1).\n",
      "sé\n",
      "The size a Wald test is: reject Hy when |W| > Za/2\n",
      "where\n",
      "—%\n",
      "\n",
      "se\n",
      "\n",
      "W=\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 11.4 Asymptotically, the Wald test has size a, that is,\n",
      "\n",
      "Po, (|Z| > 2a/2) 3 @\n",
      "\n",
      "as n —+ &.\n",
      "\n",
      "PrRooF. Under 6 = 60, (0- 0)/sé ~» N(0,1). Hence, the\n",
      "\n",
      "probability of rejecting when the null 6 = 6p is true is\n",
      "\n",
      "0-6\n",
      "Py, (W|> zap) = Po ( > =)\n",
      "\n",
      "+ P(|N(O,1)| > 2/2)\n",
      "= a. —\n",
      "\n",
      "Remark 11.5 Most texts define the Wald test slightly differently.\n",
      "They use the standard error computed at @ = 09 rather then at\n",
      "\n",
      "the estimated value 0. Both versions are valid.\n",
      "\n",
      "The test is named\n",
      "after Abraham Wald\n",
      "(1902-1950), who\n",
      "was a very influen-\n",
      "tial — mathematical\n",
      "statistician.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-184.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8708>\n",
      "184 11. Hypothesis Testing and p-values\n",
      "\n",
      "Let us consider the power of the Wald test when the null\n",
      "hypothesis is false.\n",
      "\n",
      "Theorem 11.6 Suppose the true value of 0 is 0, #09. The power\n",
      "B(0,) - the probability of correctly rejecting the null hypothesis\n",
      "— is given (approximately) by\n",
      "\n",
      "9% — 8. % —8\n",
      "of 7 * 42a) +0( = tan). (11.4)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "se\n",
      "\n",
      "Recall that sé tends to 0 as the sample size increases. Inspect-\n",
      "ing (11.4) closely we note that: (i) the power is large if 0, is far\n",
      "from 99 and (ii) the power is large if the sample size is large.\n",
      "\n",
      "Example 11.7 (Comparing Two Prediction Algorithms) We test a\n",
      "prediction algorithm on a test set of sizem and we test a second\n",
      "prediction algorithm on a second test set of size n. Let X be\n",
      "the number of incorrect predictions for algorithm 1 and let Y\n",
      "be the number of incorrect predictions for algorithm 2. Then\n",
      "X ~ Binomial(m,p,) and Y ~ Binomial(n, p2). To test the null\n",
      "hypothesis that p; = po write\n",
      "\n",
      "Ho:5=0 versus H,;:6#40\n",
      "\n",
      "where 6 = py — py. The MLB is f= Pi — py with estimated\n",
      "standard error\n",
      "\n",
      " \n",
      "\n",
      "5-0 Di — De\n",
      "se AG-A) | é\n",
      "Vee +\n",
      "\n",
      "The power of this test will be largest when p, is far from py and\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "when the sample sizes are large.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-185.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148A1C23048>\n",
      "11.1 The Wald Test 185\n",
      "\n",
      "What if we used the same test set to test both algorithms?\n",
      "The two samples are no longer independent. Instead we use the\n",
      "following strategy. Let X; = 1 if algorithm 1 is correct on test\n",
      "case i and X; = 0 otherwise. Let Y; =1 if algorithm 2 is correct\n",
      "on test case i Y; = 0 otherwise. A typical data set will look\n",
      "something like this:\n",
      "\n",
      "Test Case] X; Y; Dj; =X;-Y;\n",
      "\n",
      " \n",
      "\n",
      "i TO 7\n",
      "2 1 1 0\n",
      "3 1 1 0\n",
      "4 0 1 -1\n",
      "5 0 0 0\n",
      "n 0 1 Fr\n",
      "\n",
      "Let\n",
      "\n",
      "5=E(D) =E(X) —E(¥) =\n",
      "Then 6 =D =n“! 1 Dj and sé (6) = S/Yn where S? =\n",
      "ne (Di —D)?. To test Hy :6 =0 versus H,:6 #0 we use\n",
      "W =0/sé and reject Ho if |W| > Zqj2- This is called a paired\n",
      "comparison.\n",
      "\n",
      "Example 11.8 (Comparing Two Means.) Let X\\,...,X  andYj,...\n",
      "\n",
      "be two independent samples from populations with means 1, and\n",
      "Ha, respectively. Let’s test the null hypothesis that pf, = [lo.\n",
      "Write this as Hy : 6 = 0 versus H, : 6 #0 where 6 = py — po.\n",
      "Recall that the nonparametric plug-in estimate of 6 is 6=X-Y\n",
      "\n",
      " \n",
      "\n",
      "with estimated standard error\n",
      "\n",
      " \n",
      "\n",
      "Vn\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-186.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3508>\n",
      "186 11. Hypothesis Testing and p-values\n",
      "\n",
      "where s{ and s3 are the sample variances. The size a Wald test\n",
      "rejects Hy when |W| > Zaj2 where\n",
      "-_ 5-0\n",
      "W= =\n",
      "\n",
      "sé\n",
      "\n",
      "    \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 11.9 (Comparing Two Medians.) Consider the previous\n",
      "example again but let us test whether the medians of the two\n",
      "distributions are the same. Thus, Ho : 6 = 0 versus H, : 6 #\n",
      "0 where 6 = v4, — 2 where 4 and 12 are the medians. The\n",
      "nonparametric plug-in estimate of 6 is g= DV, —D where V, and\n",
      "Dy are the sample medians. The estimated standard error sé of\n",
      "5 can be obtained from the bootstrap. The Wald test statistic is\n",
      "W=6/s.\n",
      "\n",
      "There is a relationship between the Wald test and the 1 — a\n",
      "asymptotic confidence interval 6 + sé Za/2-\n",
      "\n",
      "Theorem 11.10 The size a Wald test rejects Hy : 0 = 09 versus\n",
      "H,:0 4 if and only if 0 € C where\n",
      "\n",
      "C= (0-8 Zea /2s 048 aj)\n",
      "\n",
      "Thus, testing the hypothesis is equivalent to checking whether\n",
      "the null value is in the confidence interval.\n",
      "\n",
      "11.2 p-values\n",
      "\n",
      "Reporting “reject Ho” or “retain Ho” is not very informative.\n",
      "Instead, we could ask, for every a, whether the test rejects at\n",
      "that level. Generally, if the test rejects at level a it will also\n",
      "reject at level a’ > a. Hence, there is a smallest a at which the\n",
      "test rejects and we call this number the p-value.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-187.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E92C8>\n",
      "11.2 p-values 187\n",
      "\n",
      " \n",
      "\n",
      "Definition 11.11 Suppose that for every a € (0,1) we have\n",
      "a size a test with rejection region Ra. Then,\n",
      "\n",
      "p-value = inf{a : T(X\")eE Ral.\n",
      "\n",
      "That is, the p-value is the smallest level at which we can\n",
      "reject Ho.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Informally, the p-value is a measure of the evidence against\n",
      "Hg: the smaller the p-value, the stronger the evidence against\n",
      "A. Typically, researchers use the following evidence scale:\n",
      "\n",
      "p-value evidence\n",
      "\n",
      "< 01 very strong evidence againt Ho\n",
      "-01- .05 strong evidence againt Ho\n",
      "-05- .10 weak evidence againt Ho\n",
      "\n",
      ">i little or no evidence againt Ho\n",
      "\n",
      " \n",
      "\n",
      "Warning! A large p-value is not strong evidence in favor of\n",
      "Ho. A large p-value can occur for two reasons: (i) Ho is true or\n",
      "(ii) Ho is false but the test has low power.\n",
      "\n",
      "But do not confuse the p-value with P(Ho|Data). The p- We discuss quanti-\n",
      "\n",
      "value is not the probability that the null hypothesis is ties like P(Ho|Data)\n",
      "true. in the chapter on\n",
      "\n",
      "The following result explains how to compute the p-value. Bayesian inference.\n",
      "\n",
      "Theorem 11.12 Suppose that the size a test is of the form\n",
      "reject Ho if and only if T(X\") > cq.\n",
      "\n",
      "Then,\n",
      "\n",
      "p-value = sup Py(T(X\") > T(2\")).\n",
      "0€00\n",
      "\n",
      "In words, the p-value is the probability (under Hg) of observing\n",
      "a value of the test statistic as or more extreme than what was\n",
      "actually observed.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-188.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 11. Hypothesis Testing and p-values\n",
      "\n",
      "For a Wald test, W has an aproximate N(0,1) distribution\n",
      "under Ho. Hence, the p-value is\n",
      "\n",
      "pevalue ~ P(|Z| > \\~) = 2P(Z < —|wl) = 20(Z < —|wl)\n",
      "(11.5)\n",
      "where Z ~ N(0,1) and w = (— 0)/& is the observed value of\n",
      "the test statistic.\n",
      "Here is an important property of p-values.\n",
      "\n",
      "Theorem 11.13 If the test statistic has a continuous distribu-\n",
      "tion, then under Hy : 0 = 9, the p-value has a Uniform (0,1)\n",
      "distribution.\n",
      "\n",
      "If the p-value is less than .05 then people often report that\n",
      "“the result is statistically significant at the 5 per cent level.”\n",
      "This just means that the null would be rejected if we used a size\n",
      "a = 0.05 test. In general, the size a test rejects if and only if\n",
      "p-value < a.\n",
      "\n",
      "Example 11.14 Recall the cholesterol data from Example 8.11.\n",
      "To test of the means ard different we compute\n",
      "\n",
      "6-0 X-Y _ 2162-1953\n",
      "\n",
      "Pape 38\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "min\n",
      "\n",
      "To compute the p-value, let Z ~ N(0,1) denote a standard Nor-\n",
      "mal random variable. Then,\n",
      "\n",
      "p-value = P(|Z| > 3.78) = 2P(Z < —3.78) = .0002\n",
      "\n",
      "which is very strong evidence against the null hypothesis. To\n",
      "test if the medians are different, let 7, and? denote the sample\n",
      "medians. Then,\n",
      "\n",
      "D-H 225-194 |\n",
      "\n",
      "Woe ap\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-189.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "11.3 The x? distribution 189\n",
      "\n",
      "where the standard error 7.7 was found using the boostrap. The\n",
      "p-value is\n",
      "\n",
      "p-value = P(|Z| > 2.4) = 2P(Z < —2.4) = .02\n",
      "which is strong evidence against the null hypothesis.\n",
      "\n",
      "Warning! A result might be statistically significant and yet\n",
      "the size of the effect might be small. In such a case we have a\n",
      "result that is statistically significant but not practically signifi-\n",
      "cant. It is wise to report a confidence interval as well.\n",
      "\n",
      "11.3. The x? distribution\n",
      "\n",
      "Let Z,...,Z, be independent, standard normals. Let V =\n",
      "ae Z?. Then we say that V has a x? distribution with k de-\n",
      "grees of freedom, written V ~ xZ. The probability density of V\n",
      "\n",
      "1s .\n",
      "pk/2)-1e-v/2\n",
      "\n",
      "f= Saar\n",
      "for v > 0. It can be shown that E(V) =k and V(k) = 2k. We\n",
      "define the upper a quantile ew = F-'(1— a) where F is the\n",
      "CDF . That is, P(xj > Xf.) =\n",
      "\n",
      "11.4 Pearson’s x? Test For Multinomial Data\n",
      "\n",
      "Pearson’s x” test is used for multinomial data. Recall that\n",
      "X = (X,...,X;) has a multinomial distribution if\n",
      "\n",
      " \n",
      "\n",
      "n\n",
      "\n",
      ": = 1... th\n",
      "S(t1,---,K5 pP) = (7, )ei Di\n",
      "\n",
      "(..\"..)\"am\n",
      "\n",
      "The MLE of p is P= (i,.--, Px) = (Xi/\n",
      "\n",
      "where\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-190.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1EC8>\n",
      "190 11. Hypothesis Testing and p-values\n",
      "\n",
      "Let (poi,---,Pox) be some fixed set of probabilities and sup-\n",
      "pose we want to test\n",
      "\n",
      "Ho: (P1,---5 Pe) = (Por,---,Pox) versus Hy : (p1,---, pe) # (Dor,---sPox)-\n",
      "\n",
      "Pearson’s x? statistic is\n",
      "\n",
      "T x Xj = npo;) Pay (= 5\n",
      "jal\n",
      "\n",
      "ny\n",
      "Po; j=l J\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "where O; = X; is the observed data and E; = E(Xj) = npo; is\n",
      "the expected value of Xj under Ho.\n",
      "\n",
      "Theorem 11.15 Under Ho, T ~+ x%_,. Hence the test: reject Ho\n",
      "if T > Xi-1q has asymptotic level a. The p-value is P(x% > t)\n",
      "where t is the observed value of the test statistic.\n",
      "\n",
      "Example 11.16 (Mendel’s peas.) Mendel bred peas with round yel-\n",
      "low seeds and wrinkled green seeds. There are four types of progeny:\n",
      "round yellow, wrinkled yellow, round green and (4) wrinkled\n",
      "green. The number of each type is multinomial with probability\n",
      "(Pi, P2,P3, Ps). His theory of inheritance predicts that\n",
      "\n",
      "(2% 3 3 1\\_\n",
      "?~\\ ie 16’ 16’ 16) ~?\"\n",
      "\n",
      "In n = 556 trials he observed X = (315,101, 108,32). Since,\n",
      "npio = 312.75, npo = np3o = 104.25 and npyo = 34.75, the test\n",
      "statistic is\n",
      "» _ (315 — 312.75)? (101 — 104.25)? (108 — 104.25)? (32 — 34.75)?\n",
      "~ 312.75 Tolas. SCOOT\n",
      "The a = .05 value for a x3 is 7.815. Since 0.47 is not larger\n",
      "than 7.815 we do not reject the null. The p-value is\n",
      "\n",
      "= 0.47.\n",
      "\n",
      " \n",
      "\n",
      "p-value = P(x} > .47) =.07\n",
      "\n",
      "which is only moderate evidence againt Hy. Hence, the data do\n",
      "not contradict Mendel’s theory. Interestingly, there is some con-\n",
      "troversey about whether Mendel’s results are “too good.”\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-191.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "11.5 The Permutation Test 191\n",
      "11.5 The Permutation Test\n",
      "\n",
      "The permutation test is a nonparametric method for test-\n",
      "ing whether two distribution are the same. This test is “exact”\n",
      "meaning that it is not based on large sample theory approxima-\n",
      "tions. Suppose that X1,...,Xm~ Fy and Yi,...,Y¥; ~ Fy are\n",
      "two independent samples and Hp is the hypothesis that the two\n",
      "samples are identically distributed. This is the type of hypothe-\n",
      "sis we would consider when testing whether a treatment differs\n",
      "from a placebo. More precisely we are testing\n",
      "\n",
      "Ay: Fy =Fy versus H,: Fy #4 Fy.\n",
      "Let T(21,---,2m;Yi,---;Yn) be some test statistic, for example,\n",
      "T (Xi, Xm Yis--a¥a) = [Kn Pale\n",
      "\n",
      "Let N = m+n and consider forming all N! permutations of\n",
      "the data X1,..., Xm, Yi,---, Yn. For each permutation, compute\n",
      "the test statistic T. Denote these values by T\\,..., Ty. Under\n",
      "the null hypothesis, each of these values is equally likely. The\n",
      "distribution Pj that puts mass 1/N! on each T; is called the\n",
      "permutation distribution of T. Let to}, be the observed value\n",
      "of the test statistic. Assuming we reject when T is large, the p-\n",
      "value is\n",
      "\n",
      "N!\n",
      "\n",
      "1\n",
      "p-value = Po(T' > toys) = MI ice Stops)\n",
      "\n",
      "ja\n",
      "\n",
      "Example 11.17 Here is a toy example to make the idea clear.\n",
      "Suppose the data are: (X,, X2, Yi) = (1,9, 3). Let T(X, X2,Y) =\n",
      "[X-Y\n",
      "\n",
      " \n",
      "\n",
      "= 2. The permutations are:\n",
      "\n",
      "Under the null hy-\n",
      "pothesis, given the\n",
      "ordered data values,\n",
      "\n",
      "Xi, ++) Xm: Vis.\n",
      "\n",
      "Vn\n",
      "\n",
      "is uniformly — dis-\n",
      "\n",
      "tributed\n",
      "\n",
      "over\n",
      "\n",
      "the\n",
      "\n",
      "N! permutations of\n",
      "\n",
      "the data.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-192.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "192 11. Hypothesis Testing and p-values\n",
      "\n",
      "permutation value of T probability\n",
      "\n",
      " \n",
      "\n",
      "(2,9,3) 2 1/6\n",
      "(9,1,3) 2 1/6\n",
      "(1,3,9) 7 1/6\n",
      "(3,1,9) 7 1/6\n",
      "(3,9,1) 5 1/6\n",
      "(9,3,1) 5 1/6\n",
      "\n",
      "The p-value is P(T > 2) = 4/6.\n",
      "\n",
      "Usually, it is not practical to evaluate all N! permutations.\n",
      "We can approximate the p-value by sampling randomly from\n",
      "the set of permutations. The fraction of times Tj > t,,. among\n",
      "these samples approximates the p-value.\n",
      "\n",
      " \n",
      "\n",
      "Algorithm for Permutation Test\n",
      "\n",
      "1. Compute the observed value of the test statistic to, =\n",
      "P(X 26, Xue Yigees Ya)\n",
      "\n",
      "2. Randomly permute the data. Compute the statistic again\n",
      "using the permuted data.\n",
      "\n",
      "3. Repeat the previous step B times and let T,,...,T de-\n",
      "note the resulting values.\n",
      "\n",
      "4. The approximate p-value is\n",
      "\n",
      "B\n",
      "BoM (Tj > tobs)-\n",
      "\n",
      "_ol=\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 11.18 DNA microarrays allow researchers to measure\n",
      "the expression levels of thousands of genes. The data are the lev-\n",
      "els of messenger RNA (mRNA) of each gene, which is thought\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-193.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "11.6 Multiple Testing 193\n",
      "\n",
      "to provide a measure how much protein that gene produces.\n",
      "Roughly, the larger the number, the more active the gene. The\n",
      "table below, reproduced from Efron, et. al. (JASA, 2001, p. 1160)\n",
      "shows the expression levels for genes from two types of liver can-\n",
      "cer cells. There are 2,638 genes in this experiment but here we\n",
      "show just the first two. The data are log-ratios of the intensity\n",
      "levels of two different color dyes used on the arrays.\n",
      "\n",
      " \n",
      "\n",
      "Type I Type II\n",
      "Patient 7 2 3 4 5| 6 7 8 9 10\n",
      "Gene 1 230.0 -1,350 -1,580.0 -400 -760|970 110 -50 -190 -200\n",
      "\n",
      "Gene 2 470.0 -850 -.8 -280 120) 390 -1730 -1360\n",
      "\n",
      "Let’s test whether the median level of gene 1 is different be-\n",
      "tween the two groups. Let 1, denote the median level of gene 1\n",
      "of Type I and let v, denote the median level of gene 1 of Type II.\n",
      "The absolute difference of sample medians is T = | —V.| = 710.\n",
      "Now we estimate the permutation distribution by simulation and\n",
      "we find that the estimated p-value is .045. Thus, if we use a\n",
      "a = .05 level of significance, we would say that there is evidence\n",
      "to reject the null hypothesis of no difference. Mi\n",
      "\n",
      "In large samples, the permutation test usually gives similar\n",
      "results to a test that is based on large sample theory. The per-\n",
      "mutation test is thus most useful for small samples.\n",
      "\n",
      "11.6 Multiple Testing\n",
      "\n",
      "In some situations we may conduct many hypothesis tests. In\n",
      "example 11.18, there were actually 2,638 genes. If we tested for a\n",
      "difference for each gene, we would be conducting 2,638 separate\n",
      "hypothesis tests. Suppose each test is conducted at level a. For\n",
      "any one test, the chance of a false rejection of the null is a.\n",
      "But the chance of at least one false rejection is much higher.\n",
      "\n",
      "-.8 -330\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-194.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148A1C23048>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194 11. Hypothesis Testing and p-values\n",
      "\n",
      "This is the multiple testing problem. The problem comes up\n",
      "\n",
      "in many data mining situations where one may end up testing\n",
      "\n",
      "thousands or even millions of hypotheses. There are many ways\n",
      "\n",
      "to deal with this problem. Here we discuss two methods.\n",
      "Consider m hypothesis tests:\n",
      "\n",
      "Ao; versus Hy;, i =1,...,m\n",
      "\n",
      "and let P,,...,P,, denote m p-values for these tests.\n",
      "\n",
      " \n",
      "\n",
      "The Bonferroni Method\n",
      "\n",
      " \n",
      "\n",
      "Given p-values P;,...,Pmm, reject null hypothesis Ho; if P; < a/m.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 11.19 Using the Bonferroni method, the probability of\n",
      "falsely rejecting any null hypotheses is less than or equal to a.\n",
      "\n",
      "ProoF. Let R be the event that at least one null hypotheses\n",
      "is falsely rejected. Let R; be the event that the 7\" null hypoth-\n",
      "esis is falsely rejected. Recall that if A,,...,A, are events then\n",
      "P( Ba Aj) < whi P(Aj). Hence,\n",
      "\n",
      "m\n",
      "\n",
      "P(R) =P (Ux) < SPUR) =f =a\n",
      "\n",
      "i=l\n",
      "\n",
      "from Theorem 11.13.\n",
      "\n",
      "Example 11.20 In the gene example, using a = .05, we have\n",
      "that .05/2638 = .00001895375. Hence, for any gene with p-value\n",
      "less than .00001895375, we declare that there is a significant\n",
      "difference.\n",
      "\n",
      "The Bonferroni method is very conservative because it is try-\n",
      "ing to make it unlikely that you would make even one false\n",
      "rejection. Sometimes, a more reasonable idea is to control the\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-195.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "11.6 Multiple Testing 195\n",
      "\n",
      "false discovery rate (FDR) which is defined as the mean of the\n",
      "number of false rejections divided by the number of rejections.\n",
      "\n",
      "Suppose we reject all null hypotheses whose p-values fall be-\n",
      "low some threshold. Let mo be the number of null hypotheses\n",
      "are true and m; = m— mo null hypotheses are false. The tests\n",
      "can be categorize in a 2 x 2 as in the following table.\n",
      "\n",
      "Hy Not Rejected Hp Rejected Total\n",
      "\n",
      "Hy True U Vv mo\n",
      "Ho False T S my\n",
      "Total m—R R m\n",
      "\n",
      "Define the False Discovery Proportion (FDP)\n",
      "\n",
      "V/R ifR>0\n",
      "ERP = { 0 ifR=0.\n",
      "The FDP is the proportion of rejections that are incorrect. Next\n",
      "define FDR = E(FDP).\n",
      "\n",
      " \n",
      "\n",
      "The Benjamini-Hochberg (BH) Method\n",
      "\n",
      "1. Let Pay < +++ < Pom) denote the ordered p-values.\n",
      "\n",
      "2. Define\n",
      "\n",
      "t “and R= maxi 7 Pe = ei} (11.6)\n",
      "\n",
      " \n",
      "\n",
      "Cmm\n",
      "\n",
      "where C,,, is defined to be 1 if the p-values are independent\n",
      "and Om = 07, (1/i) otherwise.\n",
      "\n",
      "3. Let ¢ = Pim; we call t the BH rejection threshold.\n",
      "\n",
      "4. Rejects all null hypotheses Ho; for which P; < t.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-196.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C85C8>\n",
      "196 11. Hypothesis Testing and p-values\n",
      "\n",
      "Theorem 11.21 (Benjamini and Hochberg) If the procedure\n",
      "above is applied, then regardless of how many nulls are true\n",
      "and regardless of the distribution of the p-values when the null\n",
      "hypthesis is false,\n",
      "\n",
      "FDR =E(FDP) < wa <a.\n",
      "\n",
      "Example 11.22 Figure 11.1 shows 7 ordered p-values plotted as\n",
      "vertical lines. If we tested at level a without doing any corec-\n",
      "tion for mutiple testing, we would reject all hypotheses whose\n",
      "p-values are less than a. In this case, the 5 hypotheses corre-\n",
      "sponding to the 5 smallest p-values are rejected. The Bonferroni\n",
      "method rejects all hypotheses whose p-values are less than a/m.\n",
      "In this example, this leads to no rejections. The BH threshold\n",
      "corresponds to the last p-value that falls under the line with slope\n",
      "a. This leads to three hypotheses being rejected in this case. Wi\n",
      "\n",
      "SumMARY. The Bonferonni method controls the probability\n",
      "of a single false rejection. This is very strict and leads to low\n",
      "power when there are many tests. The FDR method controls the\n",
      "fraction of false discoveries which is a more reasonable criterion\n",
      "when there are many tests.\n",
      "\n",
      "11.7 Technical Appendix\n",
      "\n",
      "11.7.1 The Neyman-Pearson Lemma\n",
      "\n",
      "In the special case of a simple null Ho : 8 = 4 and a simple\n",
      "alternative H; : 9 = 6, we can say precisely what the most\n",
      "powerful test is.\n",
      "\n",
      "Theorem 11.23 (Neyman-Pearson.) Suppose we test Hy : 0 =\n",
      "0) versus H,: 0 =0,. Let\n",
      "L£(91) haf (aes 1)\n",
      "\n",
      "—_ _ st.\n",
      "T= £60) ~ [tas Flas Bo)\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-197.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3C48>\n",
      "11.7 Technical Appendix 197\n",
      "\n",
      "p-values\n",
      "\n",
      "a/m\n",
      "\n",
      " \n",
      "\n",
      "0 reject don’t reject 1\n",
      "\n",
      "threshold\n",
      "\n",
      "FIGURE 11.1. Schematic illustration of Benjamini-Hochberg pro-\n",
      "cedure. All hypotheses corresponding to the last undercrossing are\n",
      "rejected.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-198.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "198 11. Hypothesis Testing and p-values\n",
      "\n",
      "Suppose we reject Hy when T > k. If we choose k so that\n",
      "Py,(I > k) = a then this test is the most powerful, size a\n",
      "test. That is among all tests with size a, this test maximizes the\n",
      "power 8(6).\n",
      "\n",
      "11.7.2 Power of the Wald Test\n",
      "PROOF OF THEOREM 11.6.\n",
      "Let Z ~ N(0,1). Then,\n",
      "\n",
      "Power = {(0,)\n",
      "\n",
      "Py, (Reject Ho)\n",
      "\n",
      "= Py(\\W|> 2a)\n",
      "\n",
      "O— 6\n",
      "= Po (“ | > sn]\n",
      "se\n",
      "0-8, 0-8,\n",
      "= Py ( = > an) + Po, ( — <-~san)\n",
      "se se\n",
      "\n",
      "= Po. ( > +S un) 4% (0 < % — & 20/2)\n",
      "\n",
      "6-98, > ft — 6. ef! — 86.\n",
      "= n(S * oe at un) + Po, G5 = <3 = - sa)\n",
      "\n",
      "p(z> Be boa) +P ze! * can)\n",
      "\n",
      "0\n",
      "1-0(45 + son) +0 (95 - 0) rT]\n",
      "\n",
      "11.7.3 The t-test\n",
      "\n",
      "To test Ho : 1 = {to where ji is the mean, we can use the Wald\n",
      "test. When the data are assumed to be Normal and the sample\n",
      "size is small, it is common instead to use the t-test. A random\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "z\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "variable T has a t-distribution with k degrees of freedom if it has\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-199.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "11.7 Technical Appendix 199\n",
      "\n",
      "density\n",
      "r()\n",
      "vie (9 (1+ ey\n",
      "\n",
      "When the degrees of freedom k — oo, this tends to a Normal\n",
      "distribution. When k = 1 it reduces to a Cauchy.\n",
      "\n",
      "Let X1,..., Xn ~ N(,07) where 6 = (1,07) are both un-\n",
      "known. Suppose we want to test js = jo versus ps A fio. Let\n",
      "\n",
      "f=\n",
      "\n",
      "Jri(Xn = Ho)\n",
      "f= ee\n",
      "By\n",
      "\n",
      "where $? is the sample variance. For large samples T = N(0, 1)\n",
      "under Ho. The exact distribution of T under Ho is t,_;. Hence\n",
      "if we reject when |T| > ty 1.0/2 then we get a sizea test.\n",
      "\n",
      "11.7.4 The Likelihood Ratio Test\n",
      "\n",
      "Let 0 = (01,.--,99,9q41,---,9,) and suppose that Qo consists\n",
      "of all parameter values @ such that (0,41, .--,4+) = (Oo,¢41)---+%,r)-\n",
      "\n",
      " \n",
      "\n",
      "Definition 11.24 Define the likelihood ratio statistic\n",
      "\n",
      "by\n",
      "21o SUD 9co L(8) —2lo L(8)\n",
      "A= 2le (= am) = 208 (=)\n",
      "\n",
      "where @ is the MLE and % is the MLE when 0 is restricted\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "to lie in Oo. The likelihood ratio test is: reject Ho\n",
      "when (2\") > X} ge\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "For example, if @ = (0;, 0, 03,04) and we want to test the null\n",
      "hypothesis that 0; = 6, = 0 then the limiting distribution has\n",
      "4 —2=2 degrees of freedom.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-200.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8708>\n",
      "200\n",
      "\n",
      "11. Hypothesis Testing and p-values\n",
      "\n",
      "Theorem 11.25 Under Ho,\n",
      "\n",
      "E ny 4.2\n",
      "2log A(x\") > x7_,.\n",
      "\n",
      "Hence, asymptotically, the LR test is level a.\n",
      "\n",
      "11.8 Bibliographic Remarks\n",
      "\n",
      "The most complete book on testing is (1986). See also Casella\n",
      "and Berger (1990, Chapter 8). The FDR method is due to Ben-\n",
      "jamini and Hochberg (1995).\n",
      "\n",
      "11.9 Exercises\n",
      "\n",
      "1. Prove Theorem 11.13.\n",
      "\n",
      "2. Prove Theorem 11.10.\n",
      "\n",
      "3. Let Xy,..., Xn ~ Uniform(0, 0) and let Y = max{Xj,..., X,}-\n",
      "\n",
      "We want to test:\n",
      "Ho: 9 =1/2 versus H, : 0 > 1/2.\n",
      "\n",
      "The Wald test is not appropriate since Y does not converge\n",
      "to a Normal. Suppose we decide to test this hypothesis by\n",
      "rejecting Hy when Y > c.\n",
      "\n",
      "(a) Find the power function.\n",
      "(b) What choice of ¢ will make the size of the test .05?\n",
      "\n",
      "(c) In a sample of size n = 20 with Y=0.48 what is the\n",
      "p-value? What conclusion about Ho would you make?\n",
      "\n",
      "(d) In a sample of size n = 20 with Y=0.52 what is the\n",
      "p-value? What conclusion about Ho would you make?\n",
      "\n",
      ". There is a theory that people can postpone their death\n",
      "\n",
      "until after an important event. To test the theory, Phillips\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-201.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1EC8>\n",
      "6.\n",
      "\n",
      "11.9 Exercises 201\n",
      "\n",
      "and King (1988) collected data on deaths around the Jew-\n",
      "ish holiday Passover. Of 1919 deaths, 922 died the week\n",
      "before the holiday and 997 died the week after. Think\n",
      "of this as a binomial and test the null hypothesis that\n",
      "6 = 1/2. Report and interpret the p-value. Also construct\n",
      "a confidence interval for 0.\n",
      "\n",
      "Reference:\n",
      "Phillips, D.P. and King, E.W. (1988).\n",
      "\n",
      "Death takes a holiday: Mortality surrounding major social\n",
      "occasions.\n",
      "\n",
      "The Lancet, 2, 728-732.\n",
      "\n",
      ". In 1861, 10 essays appeared\n",
      "\n",
      "Crescent. They were signed “\n",
      "and some people suspected t\n",
      "Mark Twain. To investigate t\n",
      "\n",
      "in the New Orleans Daily\n",
      "Quintus Curtuis Snodgrass”\n",
      "hey were actually written by\n",
      "is, we will consider the pro-\n",
      "\n",
      "portion of three letter words found in an author’s work.\n",
      "\n",
      "From eight Twain essays we\n",
      "\n",
      "-225 .262 .217 .240 .230 .229 .235 .217\n",
      "\n",
      " \n",
      "\n",
      "ave:\n",
      "\n",
      "From 10 Snodgrass essays we have:\n",
      "-209 .205 .196 .210 .202 .207 .224 .223 .220 .201\n",
      "(source: Rice xxxx)\n",
      "\n",
      "(a) Perform a Wald test for equality of the means. Use\n",
      "the nonparametric plug-in estimator. Report the p-value\n",
      "and a 95 per cent confidence interval for the difference of\n",
      "means. What do you conclude?\n",
      "\n",
      "(b) Now use a permutation test to avoid the use of large\n",
      "\n",
      "sample methods. What is your conclusion?\n",
      "Let X1,...,Xn~ N(0,1). Consider testing\n",
      "\n",
      "Ho : 0 =0 versus 6 = 1.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-202.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891850C48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202 11. Hypothesis Testing and p-values\n",
      "Let the rejection region be R = {a\" : T(a\") > c} where\n",
      "Te) =a yy Xe\n",
      "(a) Find c so that the test has size a.\n",
      "(b) Find the power under Hy, i.e. find 8(1).\n",
      "(c) Show that 8(1) 4 1 as n > ov.\n",
      "\n",
      "7. Let be the MLE ofa parameter 0 and let & = {nI(9)}~'/?\n",
      "where J(0) is the Fisher information. Consider testing\n",
      "\n",
      "Hy : 0 = 6 versus 0 F 6.\n",
      "Consider the Wald test with rejection region R = {x” :\n",
      "|Z| > 2aj2} where Z = (6 — 0%)/Se. Let 0, > % be some\n",
      "alternative. Show that 3(0:) > 1.\n",
      "8. Here are the number of elderly Jewish and Chinese women\n",
      "\n",
      "who died just before and after the Chinese Harvest Moon\n",
      "Festival.\n",
      "\n",
      "Week | Chinese Jewish\n",
      "\n",
      " \n",
      "\n",
      "-2 55 141\n",
      "-1 33 145\n",
      "dL. 70 139\n",
      "2 49 161\n",
      "\n",
      "Compare the two mortality patterns.\n",
      "\n",
      "9. A randomized, double-blind experiment was conducted to\n",
      "assess the effectiveness of several drugs for reducing post-\n",
      "operative nausea. The data are as follows.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Number of Patients | Incidence of Nausea.\n",
      "Placebo 80 45\n",
      "Chlorpromazine 75 26\n",
      "Dimenhydrinate 85 52\n",
      "Pentobarbital (100 mg) 67 35\n",
      "Pentobarbital (150 mg) 85 37\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-203.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E1D08>\n",
      "10.\n",
      "\n",
      "11.9 Exercises 203\n",
      "\n",
      "(source: )\n",
      "\n",
      "(a) Test each drug versus the placebo at the 5 per cent\n",
      "level. Also, report the estimated odds-ratios. Summarize\n",
      "your findings.\n",
      "\n",
      "(b) Use the Bonferoni and the FDR method to adjust for\n",
      "multiple testing.\n",
      "\n",
      "Let Xj, ...,Xn ~ Poisson().\n",
      "(a) Let Ag > 0. Find the size a Wald test for\n",
      "\n",
      "Ay:X =X versus Hy: AF Ao.\n",
      "\n",
      "(b) (Computer Experiment.) Let Ay = 1, n = 20 anda =\n",
      ".05. Simulate X,,...,X, ~ Poisson(A9) and perform the\n",
      "Wald test. Repeat many times and count how often you\n",
      "reject the null. How close is the type I error rate to .05?\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-204.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1F08>\n",
      "204 11. Hypothesis Testing and p-values\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-205.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E92C8>\n",
      "ib\n",
      "\n",
      "Bayesian Inference\n",
      "\n",
      "12.1 The Bay esian Philosophy\n",
      "\n",
      "The statistical theory and methods that we have discussed\n",
      "so far are known as frequentist (or classical) inference. The\n",
      "frequentist point of view is based on the following postulates:\n",
      "\n",
      "(F1) Probability refers to limiting relative frequencies. Proba-\n",
      "bilities are objective properties of the real world.\n",
      "\n",
      "(F2) P arametersare fixed, (usually unknown) constants. Be-\n",
      "cause they are not fluctuating, no probability statements can\n",
      "be made about parameters.\n",
      "\n",
      "(F3) Statistical procedures should be designed to have well de-\n",
      "fined long run frequency properties. F or example, a 95 per cei\n",
      "confidence in terwal should trap the true value of the parameter\n",
      "with limiting frequency at least 95per cent.\n",
      "\n",
      "There is another approach to inference called Bay esian in-\n",
      "ference. The Bay esianapproach is based on the following pos-\n",
      "tulates:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-206.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1EC8>\n",
      "206 12. Bayesian Inference\n",
      "\n",
      "(B1) Probability describes degree of belief, not limiting fre-\n",
      "quency. As such, we can make probability statements about lots\n",
      "of things, not just data which are subject to random variation.\n",
      "For example, I might say that ‘the probability that Albert Ein-\n",
      "stein drank a cup of tea on August 1 1948” is .35. This does not\n",
      "refer to any limiting frequency. It reflects my strength of belief\n",
      "that the proposition is true.\n",
      "\n",
      "(B2) We can make probability statements about parameters,\n",
      "even though they are fixed constants.\n",
      "\n",
      "(B3) We make inferences about a parameter 0, by producing\n",
      "a probability distribution for 6. Inferences, such as point esti-\n",
      "mates and interval estimates may then be extracted from this\n",
      "distribution.\n",
      "\n",
      "Bayesian inference is a controversial approach because it in-\n",
      "herently embraces a subjective notion of probability. In general,\n",
      "Bayesian methods provide no guarantees on long run perfor-\n",
      "mance. The field of Statistics puts more emphasis on frequentist\n",
      "methods although Bayesian methods certainly have a presence.\n",
      "Certain data mining and machine learning communuties seem to\n",
      "embrace Bayesian methods very strongly. Let’s put aside philo-\n",
      "sophical arguments for now and see how Bayesian inference is\n",
      "done. We'll conclude this chapter with some discussion on the\n",
      "strengths and weaknesses of each approach.\n",
      "\n",
      "12.2 The Bayesian Method\n",
      "\n",
      "Bayesian inference is usually carried out in the following way.\n",
      "1. We choose a probability density f(#) - called the prior\n",
      "distribution — that expresses our degrees of beliefs about\n",
      "\n",
      " \n",
      "\n",
      "a parameter @ before we see any data.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-207.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8708>\n",
      "12.2 The Bayesian Method 207\n",
      "\n",
      "2. We choose a statistical model f(2|@) that reflects our be-\n",
      "liefs about x given 6. Notice that we now write this as\n",
      "f (|) instead of f (230).\n",
      "\n",
      "3. After observing data X,,...,X,, we update our beliefs\n",
      "and form the posterior distribution f(0|X1,...,Xn)-\n",
      "\n",
      "To see how the third step is carried out, first, suppose that 6 is\n",
      "discrete and that there is a single, discrete observation XY. We\n",
      "should use a capital letter now to denote the parameter since\n",
      "\n",
      "we are treating it like a random variable so let O denote the\n",
      "parameter. Now, in this discrete setting,\n",
      "\n",
      "P(X =2,0 =0) P(X =2|0 =0)P(O0 =80)\n",
      "\n",
      "20 08s\") = Fire a) ~ 35, P(X =2]0 =0)P(O=9)\n",
      "\n",
      " \n",
      "\n",
      "which you may recognize from earlier in the course as Bayes’\n",
      "\n",
      " \n",
      "\n",
      "theorem. The version for continuous variables is obtained by\n",
      "using density functions:\n",
      "\n",
      "f(2l0) £0)\n",
      "F(0|2) = LO) _ (121)\n",
      "= Te) Fa\n",
      "If we have n IID observations X),...,X,, we replace f(2|@)\n",
      "with f(r,-..,%,|0) = TT}, f(#i|@). Let us write X” to mean\n",
      "(Xq,...,X,) and 2” to mean (21,...,2,). Then\n",
      "f(2\" |) F(8) L£n(8)£(9)\n",
      "\n",
      "Me) = Tremp gaae ~ Tele) F(aaa * oO\n",
      "(12.2)\n",
      "\n",
      "In the right hand side of the last equation, we threw away the\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "denominator [ £,(0)f(@)d0 which is a constant that does not\n",
      "depend on 0; we call this quantity the normalizing constant.\n",
      "We can summarize all this by writing:\n",
      "\n",
      "“posterior is proportional to likelihood times prior.” (12.3)\n",
      "\n",
      "You might wonder, doesn’t it cause a problem to throw away\n",
      "the constant f £,,(0)f(0)d0? The answer is that we can always\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-208.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "208 12. Bayesian Inference\n",
      "\n",
      "recover the constant is since we know that f f(0|x\")d0 = 1.\n",
      "Hence, we often omit the constant until we really need it.\n",
      "What do we do with the posterior? First, we can get a point\n",
      "estimate by summarizing the center of the posterior. Typically,\n",
      "we use the mean or mode of the posterior. The posterior mean\n",
      "\n",
      "7 wy £ PLn(8)F (8)\n",
      "I, = [ ortoje \\d8 = a Ohas\n",
      "\n",
      "We can also obtain a Bayesian interval estimate. Define a and b\n",
      "by [°F (O|a\")do = f° f(0|2\")dd = a/2. Let C = (a,b). Then\n",
      "\n",
      "(12.4)\n",
      "\n",
      "PO €C|z”) = [soa =l-a\n",
      "\n",
      "so C is a 1 —a posterior interval.\n",
      "\n",
      "Example 12.1 Let X,,...,X, ~ Bernoulli(p). Suppose we take\n",
      "the uniform distribution f (p) = 1 as a prior. By Bayes’ theorem\n",
      "the posterior has the form\n",
      "\n",
      "F(p|2\")  f(e)La(p) = p'(L =p)\" = ph = pyr\n",
      "\n",
      "where 8 = )),2; is the number of heads. Recall that random\n",
      "variable has a Beta distribution with parameters a and 6 if its\n",
      "density is\n",
      "\n",
      "T+) 44\n",
      "\n",
      "Toray a2\n",
      "\n",
      "f(p; a, 8) =\n",
      "\n",
      "We see that the posterior for p is a Beta distribution with pa-\n",
      "rameters s +1 andn—s +1. That is,\n",
      "\n",
      "n) _ T(n +2) Ghhid: Vine fij2\n",
      "fle\") = Foy prmss aye fe Ce) nae\n",
      "\n",
      "We write this as\n",
      "\n",
      "pia” ~ Beta(s +1,n—s +1).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-209.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1988>\n",
      "12.2 The Bayesian Method 209\n",
      "\n",
      "Notice that we have figured out the normalizing constant without\n",
      "actually doing the integral f Ln(p)f(p)dp. The mean of a Beta\n",
      "(a, 8) is a/(a + B) so the Bayes estimator is\n",
      "\n",
      "— sti\n",
      "Pena?\n",
      "It is instructive to rewrite the estimator as\n",
      "\n",
      "B=Anpt (1 —An)p\n",
      "\n",
      "where p = s/n is the mle, p =1/2 is the prior mean and \\,, =\n",
      "n/(n +2) 1. A 95 per cent posterior interval can be obtained\n",
      "by numerically finding a and b such that SL? Flp|z”) dp = 95.\n",
      "\n",
      "Suppose that instead of a uniform prior, we use the prior p ~\n",
      "Beta(a, 3). If you repeat the calculations above, you will see that\n",
      "pia” ~ Beta(a +s,8+n—s). The flat prior is just the special\n",
      "case with a = 8 =1. The posterior mean is\n",
      "\n",
      "a+s n a4 a+,\n",
      "at+B+n \\atB+n)”' \\atp+n)™\n",
      "\n",
      "where po = a/(a + 8) is the prior mean.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "In the previous example, the prior was a Beta distribution\n",
      "and the posterior was a Beta distribution. When the prior and\n",
      "the posterior are in the same family, we say that the prior is\n",
      "conjugate.\n",
      "\n",
      "Example 12.2 Let Xi,...,Xn ~ N(0,0°). For simplicity, let us\n",
      "assume that o is known. Suppose we take as a prior 0 ~ N(a, b?).\n",
      "In problem 1 in the homework, it is shown that the posterior for\n",
      "0 is\n",
      "\n",
      "O|X\" ~ N(a,b’) (12.5)\n",
      "\n",
      "where\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-210.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E1AC8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 12. Bayesian Inference\n",
      "\n",
      " \n",
      "\n",
      "where A\n",
      "oy i eel\n",
      "w=>z* 7 and G=atG\n",
      "wtRP T se 6\n",
      "\n",
      "and se = o/,/n is the standard error of the mle X. This is\n",
      "another example of a conjugate prior. Note that w — 1 and\n",
      "t/se +1 asn — oo. So, for large n, the posterior is approx-\n",
      "imately NO, se”). The same is true if n is fixed but b + x,\n",
      "which corresponds to letting the prior become very flat.\n",
      "\n",
      "Continuing with this example, let is find C = (c,d) such that\n",
      "Pr(9 € C|X\") = .95. We can do this by choosing c such that\n",
      "Pr(@<¢|X\") = .025 and Pr(@ > d|X\") = 025. So, we want to\n",
      "find c such that\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "0-0 c-#O\n",
      "PW) <clX\") = P( «é |»)\n",
      "T T\n",
      "\n",
      "=P (z < —) = 025.\n",
      "T\n",
      "\n",
      "Now, we know that P(Z < —1.96) = .025. So\n",
      "\n",
      "c-@\n",
      "= —1.96\n",
      "\n",
      " \n",
      "\n",
      "implying that c = @—1.967. By similar arguments, d = 0 +1.96.\n",
      "So a 95 per cent Bayesian interval is 0 +1967. Since 0 = a\n",
      "and tT & se, the 95 per cent Bayesian interval is approximated\n",
      "by 941.96 se which is the frequentist confidence interval. Mi\n",
      "\n",
      " \n",
      "\n",
      "12.3 Functions of Parameters\n",
      "\n",
      "How do we make inferences about a function T = (0)? Re\n",
      "member in Chapter 3 we solved the following problem: given\n",
      "the density fx for X, find the density for Y = g(X). We now\n",
      "simply apply the same reasoning. The posterior CDF for T is\n",
      "\n",
      "H(rla\") = P(al®) <7) = ff s(ele\")a0\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-211.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "12.4 Simulation 211\n",
      "\n",
      "where A = {@: g(@) <7}. The posterior density is h(r|2\") =\n",
      "H'(r|x\").\n",
      "\n",
      "Example 12.3 Let Xy,...,X, ~ Bernoulli(p) and f(p) = 1 s\n",
      "that p|X\" ~ Beta(s +1,n—s8 +1) with s = SO, 2;. Let y =\n",
      "log(p/(1— p)). Then\n",
      "\n",
      ")\n",
      "\n",
      "3\n",
      "\n",
      " \n",
      "\n",
      "P(W < pa\") =P (108 (+) <p\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "ev\n",
      "=P (? < )\n",
      "l1+e?\n",
      "e¥ /(L+e)\n",
      "= [sole\n",
      "0\n",
      "\n",
      "P(n +2) et Ate)\n",
      "_ fy ay\n",
      "rates | Pi — py\" dp\n",
      "\n",
      "and\n",
      "A(d|a\") = H'(W|2\")\n",
      "= acca (os) (rin) (OO\n",
      "\n",
      "I(s+1T(n—s+1) \\l+er¥ 1+e¥\n",
      "\n",
      "- raroreon (ree) (as) (Ge)\n",
      "\n",
      "i\n",
      "_ T(n +2) aye 1 Vee\n",
      "~ T(s+)Dr(n—s4+l) \\i+ee 1+e\"\n",
      "\n",
      "forvEeRE\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "12.4 Simulation\n",
      "\n",
      "The posterior can often be approximated by simulation. Sup-\n",
      "pose we draw 0;,...,93 ~ p(6|2\"). Then a histogram of 01, ...,45\n",
      "approximates the posterior density p(@|2”). An approximation\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-212.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1988>\n",
      "212 12. Bayesian Inference\n",
      "\n",
      "to the posterior mean 0, = E(6|x”) is B-! Ya 0;. The poste-\n",
      "rior 1 — a interval can be approximated by (0/2, 41—-a/2) where\n",
      "9,/2 is the a/2 sample quantile of (,...,9p-\n",
      "\n",
      "Once we have a sample 6;,...,9, from f(0|2\"), let 7; = 9(0;).\n",
      "Then 7,...,7, is a sample from f(r|x\"). This avoids the need\n",
      "to do any analytical calculations. Simulation is discussed in more\n",
      "detail later in the book.\n",
      "\n",
      "Example 12.4 Consider again Example 12.8. We can approxi-\n",
      "mate the posterior for y without doing any calculus. Here are\n",
      "the steps:\n",
      "\n",
      "1. Draw P,,...,Pg ~ Beta(s+1,n—s +1).\n",
      "2. Let pj =log(P;)/(1— P,)) fori =1,...,B.\n",
      "\n",
      "Now ,---,Up are ID draws from h(w|2\"). A histogram of\n",
      "these values provides an estimate of h(w|2\"). Ml\n",
      "\n",
      "   \n",
      "\n",
      "12.5 Large Sample Properties of Bayes’\n",
      "Procedures.\n",
      "\n",
      "In the Bernoulli and Normal examples we saw that the pos-\n",
      "terior mean was close to the MLE . This is true in greater gen-\n",
      "erality.\n",
      "\n",
      "Theorem 12.5 Under appropriate regularity conditions, we have\n",
      "that the posterior is approximately NO, &”) where 6, is the\n",
      "MLE and & =1/\\/nI(0,). Hence, 0, © 0,. Also, if C = (On —\n",
      "2a [288 , On +208) is the asymptotic frequentist 1—a confidence\n",
      "interval, then C,, is also an approtimate 1—a Bayesian posterior\n",
      "interval:\n",
      "\n",
      "P(9EC|X\") > 1-a.\n",
      "There is also a Bayesian delta method. Let r = g(@). Then\n",
      "\n",
      "T|X\" = N(F, 5€”)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-213.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "12.6 Flat Priors, Improper Priors and “Noninformative” Priors. 213\n",
      "\n",
      "where 7 = g(@) and se = se |9'(8)|.\n",
      "\n",
      "12.6 Flat Priors, Improper Priors and\n",
      "“Noninformative” Priors.\n",
      "\n",
      "A big question in Bayesian inference is: where do you get\n",
      "the prior f(@)? One school of thought, called “subjectivism”\n",
      "says that the prior should reflect our subjective opinion about\n",
      "@ before the data are collected. This may be possible in some\n",
      "cases but seems impractical in complicated problems especially\n",
      "if there are many parameters. An alternative is to try to define\n",
      "some sort of “noninformative prior.” An obvious candidate for\n",
      "a noninformative prior is to use a “flat” prior f(@) oc constant.\n",
      "\n",
      "In the Bernoulli example, taking f(p) = 1 leads to p|X” ~\n",
      "Beta(s +1,n—s +1) as we saw earlier which seemed very rea-\n",
      "sonable. But unfettered use of flat priors raises some questions.\n",
      "\n",
      "IMPROPER PRIORS. Consider the N(0,1) example. Suppose\n",
      "we adopt a flat prior f() «x c¢ where c > 0 is a constant. Note\n",
      "that f f(0)d0 = oo so this is not a real probability density\n",
      "in the usual sense. We call such a prior an improper prior.\n",
      "Nonetheless, we can still carry out Bayes’ theorem and compute\n",
      "the posterior density f(@) « L,(0)f (0) « L£,(0). In the nor-\n",
      "mal example, this gives 0|X\" ~ N(X,o?/n) and the resulting\n",
      "point and interval estimators agree exactly with their frequen-\n",
      "tist counterparts. In general, improper priors are not a problem\n",
      "as long as the resulting posterior is a well defined probability\n",
      "distribution.\n",
      "\n",
      "FLAT PRIORS ARE Not INVARIANT. Go back to the Bernoulli\n",
      "example and consider using the flat prior f(p) = 1. Recall that\n",
      "a flat prior presumably represents our lack of information about\n",
      "p before the experiment. Now let = log(p/(1 — p)). This is a\n",
      "transformation and we can compute the resulting distribution\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-214.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "214 12. Bayesian Inference\n",
      "\n",
      "for w. It turns out that:\n",
      "\n",
      "ev\n",
      "¢%) = ——:.\n",
      "fol) = oa\n",
      "But one could argue that if we are ignorant about p then we are\n",
      "also ignorant about ¢ so shouldn’t we use a flat prior for 7)? This\n",
      "contradicts the prior fy(w) for that is implied by using a flat\n",
      "prior for p. In short, the notion ofa flat prior is not well-defined\n",
      "because a flat prior on a parameter does not imply a flat prior\n",
      "on a transformed version of the parameter. Flat priors are not\n",
      "transformation invariant.\n",
      "\n",
      "JEFFREYS’ PRIOR. Jeffreys came up with a “rule” for cre-\n",
      "ating priors. The rule is: take f (0) oc I(@)'/? where I(0) is the\n",
      "Fisher information function. This rule turns out to be transfor-\n",
      "mation invariant. There are various reasons for thinking that\n",
      "this prior might be a useful prior but we will not go into details\n",
      "here.\n",
      "\n",
      "Example 12.6 Consider the Bernoulli (p). Recall that\n",
      "\n",
      "at\n",
      "p—p)\n",
      "Jeffrey’s rule says to use the prior\n",
      "\n",
      "f(p) « VT(p) = p7'? (1 =p).\n",
      "\n",
      "This is a Beta (1/2,1/2) density. This is very close to a uniform\n",
      "density.\n",
      "\n",
      "I(p) =\n",
      "\n",
      "In a multiparameter problem, the Jeffreys’ prior is defined to\n",
      "be f(0) x \\/detI(@) where det(A) denotes the determinant of\n",
      "a matrix A.\n",
      "\n",
      "12.7 Multiparameter Problems\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-215.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n",
      "12.7 Multiparameter Problems 215\n",
      "\n",
      "In principle, multiparameter problems are handled the same\n",
      "way. Suppose that @ = (6;,...,4,). The posterior density is still\n",
      "given by\n",
      "\n",
      "(Olax) x Ln(9)F (4).\n",
      "The question now arises of how to extract inferences about one\n",
      "parameter. The key is find the marginal posterior density for\n",
      "the parameter of interest. Suppose we want to make inferences\n",
      "about 0;. The marginal posterior for 0; is\n",
      "\n",
      "f(6: |x”) = foe fH. eet).\n",
      "\n",
      "In practice, it might not be feasible to do this integral. Simula-\n",
      "tion can help. Draw randomly from the posterior:\n",
      "\n",
      "O',...,0% ~ f(O|x\")\n",
      "\n",
      "where the superscripts index the different draws. Each @” is a\n",
      "vector 67 = (6},...,03). Now collect together the first compo-\n",
      "nent of each draw:\n",
      "\n",
      "0... 08.\n",
      "These are a sample from f(6,|2\") and we have avoided doing\n",
      "any integrals.\n",
      "Example 12.7 (Comparing two binomials.) Suppose we have n, con-\n",
      "trol patients and nz treatment patients and that X, control pa-\n",
      "tients survive while Xj treatment patients survive. We want to\n",
      "estimate T = g(pi,p2) = p2— pi. Then,\n",
      "\n",
      "X,~ Binomial(ni,pi) and X2 ~ Binomial(na, p2).\n",
      "\n",
      "Suppose we take f(pi,p2) =1. The posterior is\n",
      "\n",
      "ny—21 to\n",
      "\n",
      "J (pi, p2|e1, 22) % py\" (1 — pi)\" \"p37 (1 — pe\n",
      "\n",
      "jpaoe\n",
      "\n",
      "Notice that (p,,p2) live on a rectangle (a square, actually) and\n",
      "that\n",
      "\n",
      "J (pi, p2|1, %2) = f (pilei) f (p2|x2)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-216.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E1608>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 12. Bayesian Inference\n",
      "\n",
      "where\n",
      "\n",
      "f (piles) x py = pi)\" and f (pele) x py? (L = pe)\"\n",
      "which implies that py and pz are independent under the pos-\n",
      "terior. Also, pi|a, ~ Beta(a, + 1,m — 2 +1) and polar ~\n",
      "Beta(a2.+1, n2—224+1). If we simulate P,1,..., Pi,g ~ Beta(xy+\n",
      "1,m—2,4+1) and P2,,..., Po,3 ~ Beta(22+1,n2—22 41) then\n",
      "7) = Poy — Pip, b=1,...,B, is a sample from f(r\\r1, 22).\n",
      "\n",
      " \n",
      "\n",
      "12.8 Strengths and Weaknesses of Bayesian\n",
      "Inference\n",
      "\n",
      "Bayesian inference is appealing when prior information is avail-\n",
      "able since Bayes’ theorem is a natural way to combine prior in-\n",
      "formation with data. Some people find Bayesian inference psy-\n",
      "chologically appealing because it allows us to make probability\n",
      "statements about parameters. In contrast, frequentist inference\n",
      "provides confidence sets C, which trap the parameter 95 per\n",
      "cent of the time, but we cannot say that P(@ € C,,|X\") is .95.\n",
      "In the frequentist approach we can make probability statements\n",
      "\n",
      " \n",
      "\n",
      "about C,, not 6. However, psychological appeal is not really an\n",
      "argument for using one type of inference over another.\n",
      "\n",
      "In parametric models, with large samples, Bayesian and fre-\n",
      "quentist methods give approximately the same inferences. In\n",
      "general, they need not agree. Consider the following example.\n",
      "\n",
      "Example 12.8 Let X ~ N(6,1) and suppose we use the prior\n",
      "6 ~ N(0,7?). From (12.5), the posterior is\n",
      "£ 1\n",
      "Ola ~N (— a) = N (ez, c)\n",
      "\n",
      "where c = 17/(7? +1). A 1— a per cent posterior interval is\n",
      "C = (a,b) where\n",
      "\n",
      "a=cxr— VeZa/2 and b=cxr+ Vezap2-\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-217.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C85C8>\n",
      "12.9 Appendix 217\n",
      "\n",
      "Thus, P(@ € C|X) =1—a. We can now ask, from a frequntist\n",
      "perspective, what is the coverage of C, that is, how often will\n",
      "this interval contain the true value? The answer is\n",
      "\n",
      " \n",
      "\n",
      "Py(a<@0<b) = Po(cX — ZajaVe <0 <cX + Za/2V0)\n",
      "_ py (ev cg <a)\n",
      "fa é\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "- % (“a - Be Fapve og 2@l4 to)\n",
      "_ 9 (“ =o) 4 Zane) 6 (“ =9 = suv\")\n",
      "\n",
      "where Z ~ N(0,1). Figure 12.1 shows the coverage as a function\n",
      "of 0 fort =1 anda = 05. Unless the true value of 6 is close to\n",
      "0, the coverage can be very small. Thus, upon repeated use, the\n",
      "Bayesian 95 per cent interval might contain the true value with\n",
      "frequency near 0! In contrast, a confidence interval has coverage\n",
      "95 per cent coverage no matter what the true value of 6 is.\n",
      "\n",
      "What should we conclude from all this? The important thing\n",
      "is to understand that frequentist and Bayesian methods are an-\n",
      "swering different questions. To combine prior beliefs with data\n",
      "in a principled way, use bayesian inference. To construct proce-\n",
      "dures with guaranteed long run performance, such as confidence\n",
      "intervals, use frequentist methods. It is worth remarking that it\n",
      "is possible to develop nonparametric Bayesian methods similar\n",
      "to plug-in estimation and the bootstrap. be forewarned, how-\n",
      "ever, that the frequency properties of nonparametric Bayesian\n",
      "methods can sometimes be quite poor.\n",
      "\n",
      "12.9 Appendix\n",
      "\n",
      "Proof of Theorem 12.5.\n",
      "It can be shown that the effect of the prior diminishes as\n",
      "n increases so that f(0|X\") « Lnr(O)f(@) = Ln(0). Hence,\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-218.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E9308>\n",
      "218 12. Bayesian Inference\n",
      "\n",
      " \n",
      "\n",
      "08\n",
      "\n",
      "coverage\n",
      "\n",
      "04\n",
      "\n",
      "02\n",
      "L\n",
      "\n",
      " \n",
      "\n",
      "0.0\n",
      "L\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "FIGURE 12.1. Frequentist coverage of 95 per cent Bayesian posterior\n",
      "interval as a function of the true value 6. The dotted line marks the\n",
      "95 per cent level.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-219.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891850C48>\n",
      "12.10 Bibliographic Remarks 219\n",
      "\n",
      "log f (|X) = £(8). Now, (8) & £(6) + (8 = 8)e(8) + [(0 —\n",
      "0)? /2\\0\"(8) = €(0) + [(0 — 0)?/2)¢\"(0) since ¢'(0) = 0. Exponen-\n",
      "tiating, we get approximately that\n",
      "\n",
      "(0X) oc exp {4 =|\n",
      "\n",
      "on\n",
      "\n",
      " \n",
      "\n",
      "where 02 = — 1/0\", ,)- So the posterion of @ is approximately\n",
      "Normal with mean @ and variance o?. Let ¢; = log f (X;|0), then\n",
      "\n",
      "oy? exe 6G) = Y- G00)\n",
      "\n",
      "Bs WE 0\", n) Ey | — €1(6,)]\n",
      "@,)\n",
      "\n",
      "and hence 0, © sc(0). i\n",
      "\n",
      "12.10 Bibliographic Remarks\n",
      "\n",
      "Some references on Bayesian inference include Carlin and\n",
      "Louis (1996), Gelman, Carlin, Stern and Rubin (1995), Lee\n",
      "(1997), Robert (1994) and Schervish (1995). See Cox (1997), Di-\n",
      "aconis and Freedman (1996), Freedman (2001), Barron, Schervish\n",
      "and Wasserman (1999), Ghosal, Ghosh and van der Vaart (2001),\n",
      "Shen and Wasserman (2001) and Zhao (2001) for discussions of\n",
      "some of the technicalities of nonparametric Bayesian inference.\n",
      "\n",
      "12.11 Exercises\n",
      "1. Verify (12.5).\n",
      "\n",
      "2. Let X,,...,X, Normal(j,1). (a) Simulate a data set (using\n",
      "j= 5) consisting of n=100 observations.\n",
      "\n",
      "(b) Take f(s.) = 1 and find the posterior density. Plot the\n",
      "density.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-220.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3508>\n",
      "220\n",
      "\n",
      "12. Bayesian Inference\n",
      "\n",
      "(c) Simulate 1000 draws from the posterior. Plot a his-\n",
      "togram of the simulated values and compare the histogram\n",
      "to the answer in (b).\n",
      "\n",
      "(d) Let @ = e“. Find the posterior density for @ analytically\n",
      "and by simulation.\n",
      "\n",
      "(e) Find a 95 per cent posterior interval for 0.\n",
      "\n",
      "(f) Find a 95 per cent confidence interval for 0.\n",
      "\n",
      ". Let X1,...,X, Uniform(0,9). Let f(@) « 1/0. Find the\n",
      "\n",
      "posterior density.\n",
      "\n",
      ". Suppose that 50 people are given a placebo and 50 are\n",
      "\n",
      "given a new treatment. 30 placebo patients show improve-\n",
      "ment while 40 treated patients show improvement. Let\n",
      "T =p2—p where py is the probability of improving under\n",
      "treatment and p, is the probability of improving under\n",
      "placebo.\n",
      "\n",
      "(a) Find the mle of 7. Find the standard error and 90 per\n",
      "cent confidence interval using the delta method.\n",
      "\n",
      " \n",
      "\n",
      "(b) Find the standard error and 90 per cent confidence\n",
      "interval using the parametric bootstrap.\n",
      "\n",
      "(c) Use the prior f(p1,p2) = 1. Use simulation to find the\n",
      "posterior mean and posterior 90 per cent interval for 7.\n",
      "\n",
      "(d) Let\n",
      "Y = log (4) ~ (=)\n",
      "\n",
      "be the log-odds ratio. Note that q = 0 if p, = po. Find\n",
      "the MLE of w. Use the delta method to find a 90 per cent:\n",
      "confidence interval for 4).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "(e) Use simulation to find the posterior mean and posterior\n",
      "90 per cent interval for y.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-221.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CDB48>\n",
      "12.11 Exercises 221\n",
      "\n",
      "5. Consider the Bernoulli(p) observations\n",
      "0101000000\n",
      "Plot the posterior for p using these priors: Beta(1/2,1/2),\n",
      "Beta(1,1), Beta(10,10), Beta(100,100).\n",
      "\n",
      "6. Let X),...,X, ~ Poisson(,).\n",
      "\n",
      "(a) Let A ~ Gamma(a, 8) be the prior. Show that the\n",
      "posterior is also a Gamma. Find the posterior mean.\n",
      "\n",
      "(b) Find the Jeffreys’ prior. Find the posterior.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-222.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E9308>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-223.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E92C8>\n",
      "13\n",
      "\n",
      "Statistical Decision Theory\n",
      "\n",
      "13.1 Preliminaries\n",
      "\n",
      "We have considered several point estimators such as the max-\n",
      "imum likelihood estimator, the method of moments estimator\n",
      "and the posterior mean. In fact, there are many other ways to\n",
      "generate estimators. How do we choose among them? The an-\n",
      "swer is found in decision theory which is a formal theory for\n",
      "comparing statistical procedures.\n",
      "\n",
      "Consider a parameter 6 which lives in a parameter space O.\n",
      "Let 0 be an estimator of 0. In the language of decision theory, a\n",
      "estimator is sometimes called a decision rule and the possible\n",
      "values of the decision rule are called actions.\n",
      "\n",
      "We shall measure the discrepancy between 0 and @ using a\n",
      "loss function L(6,8). Formally, L maps © x © into R. Here\n",
      "\n",
      "This is page 227\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-224.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CD9C8>\n",
      "228 13. Statistical Decision Theory\n",
      "\n",
      "are some examples of loss functions:\n",
      "\n",
      "L(6,0) = (9 — 0)? squared error loss,\n",
      "L(0,0) = |@— | absolute error loss,\n",
      "= |0—9/P L, loss,\n",
      "\n",
      "=0if 0=@and1if 0 # 9  zero-one loss,\n",
      "\n",
      "= Slog (422) f(a; 9)dx  Kullback-Leibler loss.\n",
      "\n",
      "w\n",
      "aes\n",
      "B) DS) BS) BS) B)\n",
      "\n",
      "Bear in mind in what follows that an estimator @ is a function\n",
      "of the data. To emphasize this point, sometimes we will write 6\n",
      "\n",
      "as 0(X). To assess an estimator, we evaluate the average loss or\n",
      "tisk.\n",
      "\n",
      " \n",
      "\n",
      "Definition 13.1 The risk of an estimator 0 is\n",
      "\n",
      "R(0,0) = Ey (z(0,8)) = [ 400.60) 104;0)ae.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "When the loss function is squared error, the risk is just the\n",
      "MSE (mean squared error):\n",
      "R(0,0) = Ey(0— 6)? = sz = Vp(6) + bias3().\n",
      "In the rest of the chapter, if we do not state what loss func-\n",
      "\n",
      "tion we are using, assume the loss function is squared\n",
      "error.\n",
      "\n",
      "13.2 Comparing Risk Functions\n",
      "\n",
      "To compare two estimators we can compare their risk func-\n",
      "tions. However, this does not provide a clear answer as to which\n",
      "estimator is better. Consider the following examples.\n",
      "\n",
      "Example 13.2 Let X ~ N(0,1) and assume we are using squared\n",
      "error loss. Consider two estimators: 0; = X and 6, = 3. The\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-225.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E92C8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.2 Comparing Risk Functions. 229\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "0 1 2 3 4 5\n",
      "FIGURE 13.1. Comparing two risk functions. Neither dominates the\n",
      "other at all values of 6.\n",
      "\n",
      "risk functions are R(0,0,) = Ey(X — 0)? = 1 and R(0,0,) =\n",
      "E,(3— 6)? = (3 — 6)?. Notice that, if 2 <6 <4 then R(0, 02) <\n",
      "R(0,0;) otherwise R(0,0,) < R(0,0,). Neither estimator uni-\n",
      "formly dominates the other; see Figure 13.1. Obviously 0 isa\n",
      "ridiculous estimator but it serves to illustrate the point that it\n",
      "is not obvious how to compare two risk functions.\n",
      "\n",
      "Example 13.3 Let X,...,Xn ~ Bernoulli(p). Consider squared\n",
      "error loss and let p; = X. Since this has 0 bias, we have that\n",
      "\n",
      "- pa-p\n",
      "Rep,fi) = VX) = PAP).\n",
      "Another estimator is\n",
      "wx Yo\n",
      "ee ot Bon\n",
      "where Y = Dn X; anda and £ are positive constants. This is\n",
      "the posterior mean using a Beta (a, 8) prior. Now,\n",
      "\n",
      "R(p,P) = Vp(B2) + (bias p(P2))” ; ;\n",
      "v. (Seas) * ( Geren) ?)\n",
      "np(1 — p) +( np +a -»).\n",
      "\n",
      "f@+Bent \\atBan\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-226.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C85C8>\n",
      "230 13. Statistical Decision Theory\n",
      "\n",
      " \n",
      "\n",
      "RO\n",
      "\n",
      "Risk\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "FIGURE 13.2. Risk functions for p, and 2 in Example 13.3.\n",
      "\n",
      "Now leta = 8= y/n/4. (In Example 13.13 we will explain this\n",
      "choice.) The resulting estimator is\n",
      "\n",
      "n+/J/n\n",
      "and risk function is\n",
      "A n\n",
      "R(p, Po) = Ta+ Vane\n",
      "\n",
      "The risk functions are plotted in figure 13.2. As we can see,\n",
      "neither estimator uniformly dominates the other.\n",
      "\n",
      "These examples highlight the need to be able to compare risk\n",
      "functions. To do so, we need a one-number summary of the risk\n",
      "\n",
      "function. Two such summaries are the maximum risk and the\n",
      "Bayes risk.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-227.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891850C48>\n",
      "13.2 Comparing Risk Functions. 231\n",
      "\n",
      " \n",
      "\n",
      "Definition 13.4 The maximum risk is\n",
      "\n",
      "R(9) = sup R(8, 0) (13.1)\n",
      "0\n",
      "and the Bayes risk is\n",
      "r(n,8) = / R(6, dyn (0)d8 (13.2)\n",
      "\n",
      "where (0) is a prior for 0.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 13.5 Consider again the two estimators in Example 13.3.\n",
      "We have\n",
      "\n",
      "a pi-p)_ 1\n",
      "RU) = BK = an\n",
      "and\n",
      "Rf) = max ——__ = —_\" __\n",
      "\n",
      "p A(n+ Jn)? 4(n+ Vn)?”\n",
      "Based on maximum risk, Py is a better estimator since R(p) <\n",
      "R(fi). However, when n is large, R(P,) has smaller risk except\n",
      "for a small region in the parameter space near p = 1/2. Thus,\n",
      "many people prefer p, to p2. This illustrates that one-number\n",
      "summaries like maximum risk are imperfect. Now consider the\n",
      "Bayes risk. For illustration, let us take 7(p) = 1. Then\n",
      "\n",
      "(fi) = [ ®@.5)0= [2 o- i.\n",
      "\n",
      "and\n",
      "n\n",
      "\n",
      "Po) = | R(p,p2)dp = ——— ,.\n",
      "\n",
      "v(m, D2) | (p, p2)dp Tn+ vnp\n",
      "For n > 20, r(a,p2) > (7, pi) which suggests that p, is a better\n",
      "estimator. This might seem intuitively reasonable but this an-\n",
      "swer depends on the choice of prior. The advtantage of using\n",
      "mazimum risk, despite its problems, is that it does not require\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-228.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E1888>\n",
      "232 13. Statistical Decision Theory\n",
      "\n",
      "one to choose a prior. In high-dimensional, complex problems,\n",
      "choosing a defensible prior can be extremely difficult. Ml\n",
      "\n",
      "These two summaries of the risk function suggest two dif-\n",
      "ferent methods for devising estimators: choosing @ to minimize\n",
      "the maximum risk leads to minimax estimators; choosing 0 to\n",
      "minimize the Bayes risk leads to Bayes estimators.\n",
      "\n",
      " \n",
      "\n",
      "Definition 13.6 A decision rule that minimizes the Bayes\n",
      "risk is called a Bayes rule. Formally, 0 isa Byes rule\n",
      "for prior w if\n",
      "R(6,0) = inf r(x, 6) (13.3)\n",
      "0\n",
      "\n",
      "where the infimum is over all estimators 8.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Definition 13.7 An estimator that minimizes the mazi-\n",
      "mum risk is called a minimax rule. Formally, 0 is min-\n",
      "imaz if\n",
      "\n",
      "R(0, 0) = inf sup R(6, @) (13.4)\n",
      "a 0\n",
      "\n",
      "where the infimum is over all estimators 8.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "13.3 Bayes Estimators\n",
      "\n",
      "Let 7 be a prior. From Bayes’ theorem, the posterior density\n",
      "is\n",
      "L(al)(0) _ __ F(xl@)r(@)\n",
      "m(x) J Fal) (@)a0\n",
      "\n",
      "where m(x) = f f(z,0)d0 = f f(x\\@)r(0)d0 is the marginal\n",
      "distribution of X. Define the posterior risk of an estimator\n",
      "\n",
      " \n",
      "\n",
      "f(|x) = (13.5)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-229.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CD9C8>\n",
      "13.3 Bayes Estimators 233\n",
      "O(x) by\n",
      "r@le) = f 1(6,8(e)) p(0.2)20. (13.6)\n",
      "Theorem 13.8 The Bayes risk r(x, 0) satisfies\n",
      "\n",
      "r(a,0) = J r@eym(c) ac.\n",
      "\n",
      "Let 6(x) be the value of @ that minimizes r(6\\x). Then @ is the\n",
      "Bayes estimator.\n",
      "\n",
      "Proor. We can rewrite the Bayes risk as follows:\n",
      "\n",
      "r(m, 8) [Re 0,0)n(0)d0 = [UJ setenven)-0s\n",
      "{fusion (2,0) jazao = ff 24 (0,0(2)) f(0|x)m\n",
      "\n",
      "If we choose 6(«) to be the value of @ that minimizes r(6|x) then\n",
      "we will minimize the integrand at every x and thus minimize the\n",
      "integral [r(0|x)m(x)dx.\n",
      "\n",
      "Now we can find an explicit formula for the Bayes estimator\n",
      "for some specific loss functions.\n",
      "\n",
      "Theorem 13.9 If L(0,8) = (0- 6) then the Bayes estimator is\n",
      "(x) = [esa =E(6|X = 2). (13.7)\n",
      "\n",
      "If L(0, 8) = |0- 4) then the Bayes estimator is the median of\n",
      "the posterior f(@|x). If L(0,0) is zero-one loss, then the Bayes\n",
      "estimator is the mode of the posterior f(0|x).\n",
      "\n",
      "PROOF. We will prove the theorem for squared error loss. The\n",
      "Bayes rule 0(x) minimizes r(6|x) = [(9—0(a))? f(@|x)d0. Taking\n",
      "\n",
      "(x)dad0\n",
      "\n",
      "[U 1(0,8(2)f(0\\2)a® n(x) ae = [oom eae\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-230.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1F08>\n",
      "234 13. Statistical Decision Theory\n",
      "\n",
      "the derivative of r(x) with respect to 6() and setting it equal\n",
      "to 0 yields the equation 2 [ (9 — 0(x)) f(@|x)d# = 0. Solving for\n",
      "\n",
      "O(x) we get 13.7.\n",
      "\n",
      "Example 13.10 Let X1,...,Xn ~ N(u,07) where o? is known.\n",
      "Suppose we use a N(a,b’) prior for . The Bayes estimator with\n",
      "respect to squared error loss is the posterior mean, which is\n",
      "\n",
      "a Pe 2\n",
      "Bion lll) — ee Tig\n",
      "\n",
      "7\n",
      "\n",
      " \n",
      "\n",
      "13.4 Minimax Rules\n",
      "\n",
      "The problem of finding minimax rules is complicated and we\n",
      "cannot attempt a complete coverage of that theory here but\n",
      "we will mention a few key results. The main message to take\n",
      "away from this section is: Bayes estimators with a constant risk\n",
      "function are minimax.\n",
      "\n",
      "Theorem 13.11 Let 6* be the Bayes rule for some prior 7:\n",
      "\n",
      "r(q, 6\") = inf r(z, 0). (13.8)\n",
      "D\n",
      "\n",
      "Suppose that\n",
      "R(0,0\") <r(a,6\") for all 6. (13.9)\n",
      "Then 6 is minimax and 7 is called a least favorable prior.\n",
      "\n",
      "PROOF. Suppose that 6\" is not minimax. Then there is an-\n",
      "other rule such that sup, R(0, 0) < sup, R(0, 6\"). Since the\n",
      "average of a function is always less than or equal to its maxi-\n",
      "mum, we have that r(z, 00) < sup, R(O, 00). Hence,\n",
      "\n",
      "r(,8)) < sup R(6, 6) < sup RO, 6*) < r(n, 6\")\n",
      "0 0\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-231.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CDB48>\n",
      "13.4 Minimax Rules 235\n",
      "\n",
      "which contradicts (13.8). i\n",
      "\n",
      " \n",
      "\n",
      "Theorem 13.12 Suppose that @ is the Bayes rule with re-\n",
      "spect to some prior n. Suppose further that 0 has constant\n",
      "risk: R(0,0) = c for some c. Then 0 is minimac.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Proor. The Bayes risk is r(7,4) = f R(0,0)n(0)d0 = c and\n",
      "\n",
      "hence R(0,@) < r(z, 9) for all 6. Now apply the previous theo-\n",
      "rem.\n",
      "\n",
      "Example 13.13 Consider the Bernoulli model with squared error\n",
      "loss. In example 13.3 we showed that the estimator\n",
      "\n",
      "p(X\") = Die Xi + vn/A\n",
      "n+ /J/n\n",
      "\n",
      "has a constant risk function. This estimator is the posterior\n",
      "mean, and hence the Bayes rule, for the prior Beta(a, 8) with\n",
      "a= B= /n/4. Hence, by the previous theorem, this estimator\n",
      "is minimas.\n",
      "\n",
      "Example 13.14 Consider again the Bernoulli but with loss func-\n",
      "tion\n",
      "\n",
      "~ _ (p-d)\n",
      "\n",
      "Bae pp)\n",
      "Let y\n",
      "AX\") = P= —\n",
      "\n",
      "The risk is\n",
      "\n",
      "R(p,p) = E (o*) = a (4) ~ :\n",
      "\n",
      "which, as a function of p, is constant. It can be shown that, for\n",
      "this loss function, p(X\") is the Bayes estimator under the prior\n",
      "w(p) = 1. Hence, p is minimac.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-232.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1F08>\n",
      "“Well-behaved”\n",
      "means that the level\n",
      "sets must be convex\n",
      "and symmetric\n",
      "about the origin.\n",
      "The result holds up\n",
      "to sets of measure\n",
      "0.\n",
      "\n",
      "Typically, the\n",
      "squared bias is order\n",
      "O(n) while the\n",
      "variance is of order\n",
      "\n",
      "O(n\").\n",
      "\n",
      "236 13. Statistical Decision Theory\n",
      "\n",
      "A natural question to ask is: what is the minimax estimator\n",
      "for a Normal model?\n",
      "\n",
      "Theorem 13.15 Let X1,...,X, ~ N(@,1) and let @=X. Then\n",
      "0 is minimax with respect to any well-behaved loss function. It\n",
      "is the only estimator with this property.\n",
      "\n",
      "If the parameter space is restricted, the theorem above does\n",
      "not apply as the next example shows.\n",
      "\n",
      "Example 13.16 Suppose that X ~ N(0,1) and that 0 is known\n",
      "to lie in the interval [-m,m] where 0 < m < 1. The unique,\n",
      "minimas estimator under squared error loss is\n",
      "\n",
      "6(X) = mtanh(mX)\n",
      "\n",
      "where tanh(z) = (e* —e~*)/(e* + e7*). It can be shown that this\n",
      "is the Bayes rule with respect to the prior that puts mass 1/2 at\n",
      "m and mass 1/2 at —m. Moreover, it can be shown that the risk\n",
      "\n",
      "is not constant but it does satisfy R(@, 0) <r(n, 6) for all 0; see\n",
      "\n",
      "Figure 13.3. Hence, Theorem 13.11 implies that 0 is minima.\n",
      "|\n",
      "\n",
      "13.5 Maximum Likelihood, Minimax and Bayes\n",
      "\n",
      "For parametric models that satisfy weak regularity conditions,\n",
      "the maximum likelihood estimator is approximately minimax.\n",
      "Consider squared error loss which is squared bias plus variance.\n",
      "In parametric models with large samples, it can be shown that\n",
      "the variance term dominates the bias so the risk of the MLE 0\n",
      "roughly equals the variance:\n",
      "\n",
      "R(8,0) = Vo(@) + bias? = Vo(8).\n",
      "\n",
      "As we saw in the Chapter on parametric models, the variance\n",
      "of the MLE is approximately\n",
      "\n",
      " \n",
      "\n",
      "2)\n",
      "\n",
      "vo\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-233.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907BAA88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.5 Maximum Likelihood, Minimax and Bayes 237\n",
      "\n",
      "@\n",
      "FIGURE 13.3. Risk functions for constrained Normal with m=.5.\n",
      "The two short lines show the least favorable prior which puts its\n",
      "mass at two points.\n",
      "\n",
      " \n",
      "\n",
      "where J(@) is the Fisher information. Hence,\n",
      "\n",
      "a a\n",
      "\n",
      "nR(0,8) = > Oh\n",
      "\n",
      "v\n",
      "\n",
      "(13.10)\n",
      "\n",
      "For any other estimator 6’, it can be shown that for large n,\n",
      "R(0,6') > R(@,9). More precisely,\n",
      "1\n",
      "\n",
      "limlimsup sup nR(0',0) > —. 13.11\n",
      "200 meee aoatee (2 1(0) ( }\n",
      "\n",
      "This says that, in a local, large sample sense, the MLE is min-\n",
      "imax. It can also be shown that the MLE is approximately the\n",
      "Bayes rule.\n",
      "\n",
      "In summary, in parametric models with large samples, the\n",
      "MLE is approximately minimax and Bayes. There is a caveat:\n",
      "these results break down when the number of parameters is\n",
      "large as the next example shows.\n",
      "\n",
      "Example 13.17 (Many Normal means) Let Y; ~ N(0;,07/n), i\n",
      "1,...,n. Let Y = (¥%,...,¥,) denote the data and let 0\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-234.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148A1C23048>\n",
      "The many Normal\n",
      "means problem is\n",
      "more general than\n",
      "it looks. Many\n",
      "nonparametric es-\n",
      "timation problems\n",
      "are mathematically\n",
      "equivalent to this\n",
      "model.\n",
      "\n",
      "238 13. Statistical Decision Theory\n",
      "\n",
      "(01,---,9n) denote the unknown parameters. Assume that\n",
      "\n",
      "0€O,= { (+0045) ase}\n",
      "1\n",
      "\n",
      "for some c > 0. In this model, there are as many parameters\n",
      "as observations. The MLE is 0 = Y = (Y%1,---, Yn). Under the\n",
      "loss function L(0,0) = re G — 0;)?, the risk of the MLE is\n",
      "RO, 0) =o. It can be shown that the minimas risk is approsi-\n",
      "mately o?/(0?+¢) and one can find an estimator 6 that achieves\n",
      "this risk. Since o?/(o?+@) < 02, we see that 6 has smaller risk\n",
      "than the MLE . In practice, the difference between the risks can\n",
      "be substantial. This shows that maximum likelihood is not an\n",
      "optimal estimator in high dimensional problems.\n",
      "\n",
      "13.6 Admissibility\n",
      "Minimax estimator and Bayes estimator are “good estimator”\n",
      "\n",
      "in the sense that they have small risk. Sometimes it is also useful\n",
      "to characterize bad estimator.\n",
      "\n",
      " \n",
      "\n",
      "Definition 13.18 An estimator @ is inadmissible if there\n",
      "exists another rule 6’ such that\n",
      "\n",
      "R(6,0’) < R(0,0) forall @ and\n",
      "R(0,0\") < R(0,0) for at least one 0.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 13.19 Let X ~ N(0,1) and consider estimating 0 with\n",
      "squared error loss. Let 0(X) = 3. We will show that 0 is ad-\n",
      "missible. Suppose not. Then there exists a different rule 0 with\n",
      "smaller risk. In particular, R(3, 0) < R(3, 0) = 0. Hence, 0 =\n",
      "R(3, 6) S@o) — 3)? f(x; 3)dz. Thus, 6'(x) = 3. So there is\n",
      "no rule that beats 6. Even though @ is admissible it is clearly a\n",
      "bad decision rule.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-235.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1EC8>\n",
      "13.6 Admissibility 239\n",
      "\n",
      "A prior “ow has full support if for every 6 and every\n",
      "€>0, fy) 1(0)d0 > 0.\n",
      "\n",
      "Theorem 13.20 (Bayes’ rules are admissible.) Suppose that\n",
      "9 Cc R and that R(6, 0) is a continuous function of 0 for every\n",
      "@. Let x be a prior density with full support and let 8 be the\n",
      "Bayes’ rule. If the Bayes risk is finite then 6\" is admissible.\n",
      "\n",
      "PROOF. Suppose 6\" is inadmissible. Then there exists a better\n",
      "rule @ such that R(,0) < R(0,47) for all 6 and R(6,0) <\n",
      "R(6, 6\") for some Oy. Let v = R(6,6*) — R(6,0) > 0. Since R\n",
      "is continuous, there is an > 0 such that R(0,6*)—R(8, 0) > v/2\n",
      "for all 0 € (0 —€, 0 +). Now,\n",
      "\n",
      "r(, 0\") — r(r, 8) / R(0,0\")n(0)d0 — / R(0, 0) (0)d0\n",
      "/ [2,8*) — R(0,8)] x(0)a0\n",
      "\n",
      "IV\n",
      "\n",
      "Bo+e\n",
      "| [Re, #) — RO, 8) 1 (0)d0\n",
      "6\n",
      "\n",
      "lo-€\n",
      "\n",
      "v ote\n",
      "of n(0)d8\n",
      "\n",
      "o-€\n",
      "\n",
      "> 0.\n",
      "\n",
      "Hence, r (7, 6\") >r(r, 6). This implies that 4\" does not minimize\n",
      "r(m,0) which contradicts the fact that 6* is the Bayes rule. ll\n",
      "\n",
      "Theorem 13.21 Let X1,...,Xn ~ N(u, 07). Under squared er-\n",
      "ror loss, X is admissible.\n",
      "\n",
      "The proof of the last theorem is quite technical and is omitted\n",
      "but the idea is as follows. The posterior mean is admissible for\n",
      "any strictly positive prior. Take the prior to be N(a, 0”). When\n",
      "b is very large, the posterior mean is approximately equal to\n",
      "x.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-236.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "240 13. Statistical Decision Theory\n",
      "\n",
      "How are minimaxity and admissibility linked? In general, a\n",
      "rule may be one, both or neither. But here are some facts linking\n",
      "admissibility and minimaxity.\n",
      "\n",
      "Theorem 13.22 Suppose that @ has constant risk and is admis-\n",
      "sible. Then it is minimas.\n",
      "\n",
      "PROOF. The risk is R(@,0) = c for some c. If @ were not\n",
      "minimax then there exists a rule 6’ such that\n",
      "\n",
      "R(6, 6) < sup R(0, 6\") < sup R@,6) = c.\n",
      "0 0\n",
      "\n",
      "This would imply that 6 is inadmissible. ll\n",
      "Now we can prove a restricted version of Theorem 13.15 for\n",
      "squared error loss.\n",
      "\n",
      "Theorem 13.23 Let Xi,...,Xn ~ N(0,1). Then, under squared\n",
      "error loss, 0 =X is minimas.\n",
      "\n",
      "Proor. According to Theorem 13.21, @ is admissible. The\n",
      "tisk of @ is 1/n which is constant. The result follows from The-\n",
      "orem 13.22.\n",
      "\n",
      "Although minimax rules are not guaranteed to be admissi-\n",
      "ble they are “close to admissible.” Say that Gis strongly in-\n",
      "admissible if there exists a rule 6’ and an « > 0 such that\n",
      "R(0, 6\") < R(6,) — e for all 6.\n",
      "\n",
      "Theorem 13.24 Ife is minimaz then it is not strongly inadmis-\n",
      "sible.\n",
      "\n",
      "13.7 Stein’s Paradox\n",
      "\n",
      "Suppose that X ~ N(6,1) and consider estimating @ with\n",
      "squared error loss. From the previous section we know that\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-237.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1988>\n",
      "13.8 Bibliographic Remarks 241\n",
      "\n",
      "6(X) = X is admissible. Now consider estimating two, unre-\n",
      "lated quantities 6 = (01, 62) and suppose that X; ~ N (6,1) and\n",
      "X2 ~ N(O2, 1) independently, with loss L(0,0) = 7}_,(0;-;).\n",
      "Not surprisingly, 6(X) = X is again admissible where X =\n",
      "(Xi, X2). Now consider the generalization to k normal means.\n",
      "Let 0 = (01,...,x), X =(Xi,...,X) with X; ~ N(G;,1) (in-\n",
      "dependent) and loss L(0,0) = ~* (0; — 0,)?. Stein astounded\n",
      "\n",
      "j=l\n",
      "\n",
      "everyone when he proved that, if k > 3, then O(X) = X is inad-\n",
      "missible. It can be shown that the following estimator, known\n",
      "\n",
      "as the James-Stein estimator, has smaller risk:\n",
      "\n",
      "5(X) = (1 Ane *y (13.12)\n",
      "_ ixXi) 7\" :\n",
      "where (z)t = max{z,0}. This estimator shrinks the X;’s to-\n",
      "\n",
      "wards 0. The message is that, when estimating many param-\n",
      "eters, there is great value in “shrinking” the estimates. This\n",
      "observation plays an important role in modern nonparametric\n",
      "function estimation.\n",
      "\n",
      "13.8 Bibliographic Remarks\n",
      "\n",
      "It is difficult to find books that cover modern decision the-\n",
      "ory in great detail. Aspects of decision theory can be found in\n",
      "Casella and Berger (2002), Berger (1985), Ferguson (1967) and\n",
      "Lehmann and Casella (1998).\n",
      "\n",
      "13.9 Exercises\n",
      "\n",
      "1. In each of the following models, find (i) the Bayes risk and\n",
      "the Bayes estimator, using squared error loss.\n",
      "\n",
      "(a) X ~ Binomial(n, p), p ~ Beta(a, 8).\n",
      "(b) X ~ Poisson(\\), \\ ~ Gamma(a, 8).\n",
      "\n",
      "(c) X ~ N(0,07) where o? is known and 6 ~ N(a, 7).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-238.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907BAA88>\n",
      "242\n",
      "\n",
      "13. Statistical Decision Theory\n",
      "\n",
      "Let Xi,...,Xn ~ N(0,07) and suppose we estimate @\n",
      "with loss function L(0,@) = (@ — 0)?/o?. Show that X is\n",
      "admissible and minimax.\n",
      "\n",
      "Let © = {6;,...,0,} be a finite parameter space. Prove\n",
      "that the posterior mode is the Bayes estimator under zero-\n",
      "one loss.\n",
      "\n",
      "(Casella and Berger.) Let X1,...,Xn be a sample from a\n",
      "distribution with variance o?. Consider estimators of the\n",
      "form }S? where S? is the sample variance. Let the loss\n",
      "function for estimating a? be\n",
      "ped oi\n",
      "a a Gi\n",
      "L(o?, 6\") = aT 1— log (=) :\n",
      "Find the optimal value of b that minimizes the risk for all\n",
      "\n",
      "et\n",
      "\n",
      "(Berliner, 1983). Let X ~ Binomial(n, p) and suppose the\n",
      "\n",
      "loss function is\n",
      "p 2\n",
      "un.) = (1-4)\n",
      "P\n",
      "\n",
      "where 0 < p < 1. Consider the estimator A(X) = 0. This\n",
      "estimator falls outside the parameter space (0,1) but we\n",
      "will allow this. Show that p(X) = 0 is the unique, minimax\n",
      "tule.\n",
      "\n",
      "(Computer Experiment.) Compare the risk of the mle and\n",
      "the James-Stein estimator (13.12) by simulation. Try var-\n",
      "ious values of n and various vectors 6. Summarize your\n",
      "results.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-239.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part III\n",
      "\n",
      "Statistical Models and\n",
      "Methods\n",
      "\n",
      "243\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-240.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3508>\n",
      "This is page 244\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-241.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891699B48>\n",
      "14\n",
      "\n",
      "Linear Regression\n",
      "\n",
      "Regression is a method for studying the relationship be-\n",
      "tween a response variable Y and a covariates X. The co-\n",
      "variate is also called a predictor variable or a feature. Later,\n",
      "we will generalize and allow for more than one covariate. The\n",
      "data are of the form\n",
      "\n",
      "(%4,.%1),---5 (Yas Xn):\n",
      "\n",
      "One way to summarize the relationship between X and Y is\n",
      "through the regression function\n",
      "\n",
      "re) = BOX =2) = fusule)dy. (04a)\n",
      "\n",
      "Most of this chapter is concerned with estimating the regression\n",
      "function.\n",
      "14.1 Simple Linear Regression\n",
      "\n",
      "The simplest version of regression is when X; is simple (a\n",
      "scalar not a vector) and r(x) is assumed to be linear:\n",
      "\n",
      "r(x) = Bot fiz.\n",
      "\n",
      "This is page 245\n",
      "Printer: Opaque this\n",
      "\n",
      "The term “regres-\n",
      "sion” is due to\n",
      "Sir Francis Galton\n",
      "(1822-1911) who\n",
      "noticed that tall\n",
      "and short men tend\n",
      "to have sons with\n",
      "heights closer to the\n",
      "mean. He called this\n",
      "“regression towards\n",
      "the mean.”\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-242.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CDB48>\n",
      "246 14. Linear Regression\n",
      "\n",
      "This model is called the the simple linear regression model.\n",
      "\n",
      "Let €; = Y; — (89 + 8:X;). Then,\n",
      "\n",
      "E(e|Xi) = E(¥;— (0 + &Xi)|Xi) =E(%\n",
      "= (Xj) — (Bo + AX)\n",
      "\n",
      "(60 + P:Xj) — (Bo + A Xi)\n",
      "\n",
      "= 0.\n",
      "\n",
      " \n",
      "\n",
      "Xi) — (80 + 1 Xi)\n",
      "\n",
      "Let o?(a) = V(e,|X = x). We will make the further simplifying\n",
      "assumption that o?(7) = 0? does not depend on x. We can thus\n",
      "write the linear regression model as follows.\n",
      "\n",
      " \n",
      "\n",
      "Definition 14.1 The Linear Regresion Model\n",
      "\n",
      "Yi = Bot AX +; (14.2)\n",
      "where E(e;|X;) = 0 and V(e,|X;) = 07.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 14.2 Figure 14.1 shows a plot of Log surface tempera-\n",
      "ture (Y) versus Log light intensity (X) for some nearby stars.\n",
      "Also on the plot is an estimated linear regression line which will\n",
      "be explained shortly.\n",
      "\n",
      "The unknown parameters in the model are the intercept [Jo\n",
      "and the slope }; and the variance o?. Let 89 and '; denote\n",
      "estimates of 39 and 3. The fitted line is defined to be\n",
      "\n",
      "F(a) = Bo + Bye. (14.3)\n",
      "\n",
      "The predicted values or fitted values are ¥, = 7(X;) and the\n",
      "residuals are defined to be\n",
      "\n",
      "a-Y¥,-f=-y- (5 + AX). (14.4)\n",
      "\n",
      "The residual sums of squares or RSS is defined by\n",
      "\n",
      "78\n",
      "Rss =).\n",
      "i=1\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-243.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891699B48>\n",
      "Log surface temperature (Y)\n",
      "\n",
      " \n",
      "\n",
      "4.1\n",
      "\n",
      "4.0\n",
      "\n",
      "14.1 Simple Linear Regression 247\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "T T T T\n",
      "4.0 45 5.0 5.5\n",
      "\n",
      "Log light intensity (X)\n",
      "\n",
      "FIGURE 14.1. Data on stars in nearby stars.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-244.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "248 14. Linear Regression\n",
      "\n",
      "The quantity RSS measures how well the fitted line fits the data.\n",
      "\n",
      " \n",
      "\n",
      "Definition 14.3. The least squares estimates are the val-\n",
      "\n",
      "ues By and 3, that minimize RSS = v\"_,@\n",
      "\n",
      "i=l i\"\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 14.4 The least squares estimates are given by\n",
      "\n",
      "3, = Sei FN-7.)\n",
      "-kP\n",
      "\n",
      "2\n",
      "\n",
      "Bo = Yn—BiXn- (14.6)\n",
      "\n",
      "(14.5)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "An unbiased estimate of o? is\n",
      "\n",
      "Be (5) ve (14.7)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 14.5 Consider the star data from Example 14.2. The\n",
      "least squares estimates are 39 = 3.58 and 3; = 0.166. The fitted\n",
      "line F(x) = 3.58 + 0.1662 is shown in Figure 14.1. Wl\n",
      "\n",
      "Example 14.6 (The 2001 Presidential Election.) Figure 14.2 shows\n",
      "the plot of votes for Buchanan (Y) versus votes for Bush (X)\n",
      "in Florida. The least squares estimates (omitting Palm Beach\n",
      "County) and the standard errors are\n",
      "\n",
      "By = 66.0991 €((y) = 17.2926\n",
      "B, = 00.0035 €(,) = 0.0002.\n",
      "\n",
      "The fitted line is\n",
      "Buchanon = 66.0991 + .0035 Bush.\n",
      "\n",
      "(We will see later how the standard errors were computed.) Fig-\n",
      "ure 14.2 also shows the residuals. The inferences from linear\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-245.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1988>\n",
      "14.2 Least Squares and Maximum Likelihood 249\n",
      "\n",
      "regression are most accurate when the residuals behave like ran-\n",
      "dom normal numbers. Based on the residual plot, this is not the\n",
      "case in this example. If we repeat the analysis replacing votes\n",
      "with log(votes) we get\n",
      "\n",
      "By = —2.3298\n",
      "B, = 0.730300\n",
      "\n",
      "& (Bo) = -3529\n",
      "ea\n",
      "\n",
      "& (31) = 0.0358.\n",
      "This gives the fit\n",
      "log(Buchanon) = —2.3298 + .7303 log(Bush).\n",
      "\n",
      "The residuals look much healthier. Later, we shall address two\n",
      "interesting questions: (1) how do we see if Palm Beach County\n",
      "has a statistically plausible outcome? (2) how do we do this prob-\n",
      "lem nonparametrically ?\n",
      "\n",
      "14.2 Least Squares and Maximum Likelihood\n",
      "\n",
      "Suppose we add the assumption that €;|X; ~ N(0,07), that\n",
      "\n",
      "is,\n",
      "\n",
      "  \n",
      "\n",
      "{|X ~ N (ui, 07)\n",
      "where fj = 39 + 3,X;. The likelihood function is\n",
      "\n",
      "n\n",
      "\n",
      "Tv“. = Tt fvix WiLX)\n",
      "\n",
      "i=l\n",
      "Ti x I Frix(¥iLX)\n",
      "sa] ek\n",
      "\n",
      "Li xX Lo\n",
      "\n",
      "where £; = JJ}, fx(Xj) and\n",
      "\n",
      "Lo= Il fyix(VilX). (14.8)\n",
      "\n",
      "i=1\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-246.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1F08>\n",
      "250\n",
      "\n",
      "Buchanon\n",
      "\n",
      "log Buchanon\n",
      "\n",
      "3000\n",
      "\n",
      "1000\n",
      "\n",
      "ie)\n",
      "\n",
      "2345 6/7 8\n",
      "\n",
      "14. Linear Regression\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "0 50000 150000 250000\n",
      "\n",
      "Bush\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "log Bush\n",
      "\n",
      "FIGURE 14.2. Voting Data for Election 2000.\n",
      "\n",
      "Residuals\n",
      "\n",
      "Residuals\n",
      "\n",
      "Oo 200\n",
      "\n",
      "-400\n",
      "\n",
      "45\n",
      "\n",
      "0.0\n",
      "\n",
      "-1.0\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "0\n",
      "\n",
      "50000\n",
      "\n",
      "150000 250000\n",
      "\n",
      "Bush\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "log Bush\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-247.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "14.3 Properties of the Least Squares Estimators 251\n",
      "\n",
      "The term £; does not involve the parameters (39 and (3,. We shall\n",
      "focus on the second term £2 which is called the conditional\n",
      "likelihood, given by\n",
      "\n",
      "“ a na 1 .\n",
      "L = £(3o, 10) = TT] fix Wil X) 0 0\" exp {st eH - wr}\n",
      "\n",
      "i=l\n",
      "\n",
      "The conditional log-likelihood is\n",
      "; Loy, xa\\\"\n",
      "(80, 81,0) = —nlogo — at — (Bo + 6.X,)) - (14.9)\n",
      "i=\n",
      "\n",
      "To find the MLE of (8, 5) we maximize ¢(/J, 31,0). From (14.9)\n",
      "we see that maximizing the likelihood is the same as minimizing\n",
      "2\n",
      "the RSS SO\", (i am (Go+4.%;)) . Therefore, we have shown the\n",
      "\n",
      "following.\n",
      "\n",
      "Theorem 14.7 Under the assumption of Normality, the least squares\n",
      "estimator is also the maximum likelihood estimator.\n",
      "\n",
      "We can also maximize ¢((%, 3;,0) over o yielding the MLE\n",
      "Es 1\n",
      "P= = 2 (14.10)\n",
      "\n",
      "This estimator is similar to, but not identical to, the unbiased\n",
      "estimator. Common practice is to use the unbiased estimator\n",
      "(14.7).\n",
      "\n",
      "14.3 Properties of the Least Squares Estimators\n",
      "\n",
      "We now record the standard errors and limiting distribution\n",
      "of the least squares estimator. In regression problems, we usu-\n",
      "ally focus on the properties of the estimators conditional on\n",
      "X”\" =(X,...,X,,). Thus, we state the means and variances as\n",
      "\n",
      " \n",
      "\n",
      "conditional means and variances.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-248.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3508>\n",
      "252 14. Linear Regression\n",
      "\n",
      " \n",
      "\n",
      "Theorem 14.8 Let 7 = (Bo, BJT denote the least squares\n",
      "estimators. Then,\n",
      "\n",
      "E(3|X\") =\n",
      "\n",
      "V(B|X\") =\n",
      "\n",
      " \n",
      "\n",
      "2 psn (yy 2\n",
      "where sy =n fet (Kp oMg)*\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The estimated standard errors of {39 and 3, are obtained by\n",
      "\n",
      " \n",
      "\n",
      "taking the square roots of the corresponding diagonal terms of\n",
      "V(8|X\") and inserting the estimate & for o. Thus,\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "& (Bo) = — (14.12)\n",
      "Bh) = @ (14.13)\n",
      "\n",
      " \n",
      "\n",
      "sx Jn\n",
      "\n",
      "We should really write these as sé (Bo|X\") and sé (B:|X\") but\n",
      "we will use the shorter notation sé (f) and sé (3;).\n",
      "\n",
      "Theorem 14.9 Under appropriate conditions we have:\n",
      "1. (Consistency): Bo—> Bo and i> fr.\n",
      "\n",
      "2. (Asymptotic Normality):\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Bo =o N(0,1) and Br SH sg N(0,1).\n",
      "$e (0) se (81)\n",
      "\n",
      "8. Approximate 1 —a confidence intervals for 89 and 8, are\n",
      "Bo + 2a /2 B (30) and Br + 2a 9 & (i). (14.14)\n",
      "\n",
      "The Wald test statistic for testing Ho: 8; =0 versus H, :\n",
      "Bi £0 is: reject Hy if W > 2a where W = §1/sé (G1).\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-249.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891850C48>\n",
      "14.4 Prediction 253\n",
      "\n",
      "Example 14.10 For the election data, on the log scale, a 95 per\n",
      "cent confidence interval is .7303 + 2(.0358) = (.66,.80). The\n",
      "fact that the interval excludes 0 The Wald statistics for testing\n",
      "Ho : 6, = 0 versus H, : 8; £0 is W = |.7303—0|/.0358 = 20.40\n",
      "with a p-value of P(|Z| > 20.40) © 0. This is strong evidence\n",
      "that that the true slope is not 0.\n",
      "\n",
      " \n",
      "\n",
      "14.4 Prediction\n",
      "\n",
      "Suppose we have estimated a regression model 7(x) = Bo + Bi\n",
      "from data (X,,¥i),---,(Xn, Yn). We observe the value X = x,\n",
      "of the covariate for a new subject and we want to predict their\n",
      "outcome Y,. An estimate of Y, is\n",
      "\n",
      "¥, = Bo +B X.. (14.15)\n",
      "\n",
      "Using the formula for the variance of the sum of two random\n",
      "variables,\n",
      "\n",
      "V(¥,) = V(B + Bia.) = V(Bo) + 22V(B,) + 22,Cov(fo, 31).\n",
      "\n",
      "Theorem 14.8 gives the formulas for all the terms in this equa-\n",
      "tion. The estimated standard error sé (Y,) is the square root of\n",
      "this variance, with G in place of 0”. However, the confidence\n",
      "\n",
      "interval for Y, is not of the usual form y. +2a/2- The appendix\n",
      "\n",
      " \n",
      "\n",
      "explains why. The correct form of the confidence interval is given\n",
      "in the following Theorem. We call the interval a prediction in-\n",
      "terval.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-250.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254 14. Linear Regression\n",
      "\n",
      " \n",
      "\n",
      "Theorem 14.11 (Prediction Interval) Let\n",
      "\n",
      "2 = a(F)+e\n",
      "= toa qa (AG — X\n",
      "ny(Xi—X)?\n",
      "\n",
      "An approximate 1 —a predicition interval for Y, is\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "¥, + zajatn- (14.17)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 14.12 (Election Data Revisited.) On the log-scale, our lin-\n",
      "ear regression gives the following prediction equation: log( Buchanon) =\n",
      "—2.3298 + .7303log(Bush). In Palm Beach, Bush had 152954\n",
      "votes and Buchanan had 3467 votes. On the log scale this is\n",
      "11.93789 and 8.151045. How likely is this outcome, assuming\n",
      "our regression model is appropriate? Our prediction for log Buchanan\n",
      "votes -2.3298 + .7803 (11.93789)=6. 388441. Now 8.151045 is\n",
      "bigger than 6.388441 but is is “significantly” bigger? Let us com-\n",
      "pute a confidence interval. We find that &, = .093775 and the ap-\n",
      "protimate 95 per cent confidence interval is (6.200, 6.578) which\n",
      "clearly excludes 8.151. Indeed, 8.151 is nearly 20 standard er-\n",
      "rors from ¥,. Going back to the vote scale by exponentiating, the\n",
      "confidence interval is (493,717) compared to the actual number\n",
      "\n",
      "of votes which is 3467.\n",
      "\n",
      "14.5 Multiple Regression\n",
      "\n",
      "Now suppose we have k covariates X;,...,X,. The data are\n",
      "of the form\n",
      "\n",
      "(¥1, 1), -+- (Vis Xa), +s (Ys Xn)\n",
      "\n",
      "where\n",
      "Ky = [Xilinx og Nye)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-251.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "14.5 Multiple Regression 255\n",
      "\n",
      "Here, X; is the vector of k covariate values for the i*® observa-\n",
      "tion. The linear regression model is\n",
      "\n",
      "k\n",
      "Vi= Do Bi Xy + €i (14.18)\n",
      "\n",
      "j=l\n",
      "\n",
      "for i=1,...,n, where E(e;|X1,,...,X4;) = 0. Usually we want\n",
      "to include an intercept in the model which we can do by setting\n",
      "Xj, =1 fori =1,...,n. At this point it will be more convenient\n",
      "to express the model in matrix notation. The outcomes will be\n",
      "denoted by\n",
      "Y\n",
      "Yo\n",
      "Yo .\n",
      "Yn\n",
      "\n",
      "and the covariates will be denoted by\n",
      "\n",
      "Xi Xi ... Xp\n",
      "ye Xn Xn oe Xu\n",
      "Xn Xn2 «+. Xnk\n",
      "\n",
      "Each row is one observation; the columns correspond to the k\n",
      "covariates. Thus, X is a (n x k) matrix. Let\n",
      "\n",
      "fern 1\n",
      "B= : and €= s\n",
      "Br En\n",
      "Then we can write (14.18) as\n",
      "Y=NXB+e (14.19)\n",
      "\n",
      "Theorem 14.13 Assuming that the (k x k) matrix X7X is in-\n",
      "vertible, the least squares estimate is\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-252.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3508>\n",
      "256 14. Linear Regression\n",
      "\n",
      " \n",
      "\n",
      "B=(XTX)-UXTY. (14.20)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The estimated regression function is\n",
      "is ~\n",
      "F(x) = So Bjx;. (14.21)\n",
      "j=l\n",
      "\n",
      "The variance-covariance matrix of Bis\n",
      "V(3|X\") = 0? (XTX).\n",
      "\n",
      "Under appropriate conditions,\n",
      "\n",
      " \n",
      "\n",
      "where € = XB —Y is the vector of residuals. An approximate\n",
      "1 —a confidence interval for 8; is\n",
      "\n",
      "Bj + 228 (3;) (14.22)\n",
      "\n",
      "where sé 2(B;) is the j diagonal element of the matrix G?(X™X)-!.\n",
      "\n",
      "Example 14.14 Crime data on 47 states in 1960 can be obtained\n",
      "at http://lib. stat.cmu. edu/DASL/Stories/USCrime.html. If we\n",
      "fit a linear regression of crime rate on 10 variables we get the\n",
      "following:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-253.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "14.6 Model Selection 257\n",
      "\n",
      "ak\n",
      "*\n",
      "\n",
      "HK\n",
      "\n",
      "Covariate Least Estimated t value p-value\n",
      "\n",
      "Squares — Standard\n",
      "\n",
      "Estimate Error\n",
      "(Intercept) -589.39 167.59  -3.51 0.001\n",
      "Age 1.04 0.45 2.33 0.025\n",
      "Southern State 11.29 13.24 0.85 0.399\n",
      "Education 1.18 0.68 1.7 0.093\n",
      "Expenditures 0.96 0.25 3.86 0.000\n",
      "Labor 0.11 0.15 0.69 0.493\n",
      "Number of Males 0.30 0.22 1.36 0.181\n",
      "Population 0.09 O14 0.65 0.518\n",
      "Unemployment (14-24) -0.68 0.48 -1.4 0.165\n",
      "Unemployment (25-39) 2.15 0.95 2.26 0.030\n",
      "Wealth -0.08 0.09 -0.91 0.367\n",
      "\n",
      "This table is typical of the output of a multiple regression pro-\n",
      "gram. The “t-value” is the Wald test statistic for testing Ho :\n",
      "8; = 0 versus H, : 6; # 0. The asterisks denote “degree of\n",
      "significance” with more asterisks being significant at a smaller\n",
      "level. The example raises several important questions. In partic-\n",
      "ular: (1) should we eliminate some variables from this model?\n",
      "(2) should we interpret this relationships as causal? For exam-\n",
      "ple, should we conclude that low crime prevention expenditures\n",
      "cause high crime rates? We will address question (1) in the next\n",
      "section. We will not address question (2) until a later Chapter.\n",
      "\n",
      "14.6 Model Selection\n",
      "\n",
      "Example 14.14 illustrates a problem that often arises in mul-\n",
      "tiple regression. We may have data on many covariates but we\n",
      "may not want to include all of them in the model. A smaller\n",
      "model with fewer covariates has two advantages: it might give\n",
      "better predictions than a big model and it is more parsimonious\n",
      "(simpler). Generally, as you add more variables to a regression,\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-254.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891850C48>\n",
      "258 14. Linear Regression\n",
      "\n",
      "the bias of the predictions decreases and the variance increases.\n",
      "Too few covariates yields high bias; too many covariates yields\n",
      "high variance. Good predictions result from achieving a good\n",
      "balance between bias and variance.\n",
      "\n",
      "In model selection there are two problems: (i) assigning a\n",
      "“score” to each model which measures, in some sense, how good\n",
      "the model is and (ii) searching through all the models to find\n",
      "the model with the best score.\n",
      "\n",
      "Let us first discuss the problem of scoring models. Let S C\n",
      "{1,...,k} and let Ys = {X;: j € S} denote a subset of the\n",
      "covariates. Let 6s denote the coefficients of the corresponding\n",
      "set of covariates and let Bs denote the least squares estimate of\n",
      "Bg. Also, let Xs denote the X matrix for this subset of covari-\n",
      "ates and define 7s(x) to be the estimated regression function\n",
      "from (14.21). The predicted valus from model § are denoted by\n",
      "Y,(S) =7s(X,). The prediction risk is defined to be\n",
      "\n",
      "R(S) = STE) - 9) (14.23)\n",
      "\n",
      "where Y,* denotes the value of a future observation of Y; at\n",
      "covariate value X;. Our goal is to choose S to make R(S) small.\n",
      "The training error is defined to be\n",
      "\n",
      "Ru(S) = S(%i(8) —\n",
      "\n",
      "This estimate is very biased and under-estimates R(S).\n",
      "Theorem 14.15 The training error is a downward biased esti-\n",
      "mate of the prediction risk:\n",
      "\n",
      "E(Ru(S)) < R(S).\n",
      "In fact,\n",
      "\n",
      "bias (R..(S)) =E(R.(S)) —R(S) = (¥;,.¥)). (14.24)\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-255.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1F08>\n",
      "14.6 Model Selection 259\n",
      "\n",
      "The reason for the bias is that the data are being used twice:\n",
      "to estimate the parameters and to estimate the risk. When\n",
      "we fit a complex model with many parameters, the covariance\n",
      "Cov(¥;, ¥;) will be large and the bias of the training error gets\n",
      "worse. In summary, the training error is a poor estimate of risk.\n",
      "Here are some better estimates.\n",
      "\n",
      "Mallow’s C;, statistic is defined by\n",
      "\n",
      "R(S) = R.(S) +2\n",
      "\n",
      " \n",
      "\n",
      "S| (14.25)\n",
      "\n",
      "where |S] denotes the number of terms in S and G? is the es\n",
      "timate of o? obtained from the full model (with all covariates\n",
      "in the model). This is simply the training error plus a bias cor\n",
      "rection. This estimate is named in honor of Colin Mallows who\n",
      "invented it. The first term in (14.25) measures the fit of the\n",
      "model while the second measure the complexity of the model.\n",
      "Think of the C,, statistic as:\n",
      "\n",
      "lack of fit + complexity penalty.\n",
      "\n",
      "Thus, finding a good model involves trading off fit and\n",
      "complexity.\n",
      "\n",
      "A related method for estimating risk is AIC (Akaike Infor-\n",
      "mation Criterion). The idea is to choose S to maximize\n",
      "\n",
      "es —|3| (14.26)\n",
      "\n",
      "where ¢s is the log-likelihood of the model evaluated at the\n",
      "MLE . This can be thought of “goodness of fit” minus “com-\n",
      "plexity.” In linear regression with Normal errors, maximizing\n",
      "AIC is equivalent to minimizing Mallow’s C,; see exercise 8.\n",
      "Yet another method for estimating risk is leave-one-out\n",
      "cross-validation. In this case, the risk estimator is\n",
      "\n",
      "n\n",
      "\n",
      "Rev(S) = 3% - Yo)?\n",
      "\n",
      "i=l\n",
      "\n",
      "(14.27)\n",
      "\n",
      "Some texts use a\n",
      "\n",
      "slightly different\n",
      "definition of AIC\n",
      "which involves\n",
      "multiplying the\n",
      "definition here by\n",
      "\n",
      "2 or -2. This has\n",
      "no effect on which\n",
      "model is selected.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-256.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148A1C23048>\n",
      "260 14. Linear Regression\n",
      "\n",
      "where Yu) is the prediction for Y; obtained by fitting the model\n",
      "with Y; omitted. It can be shown that\n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      "_ nly _=\n",
      "Rev(3) => (8 (14.28)\n",
      "i=l\n",
      "where Uj;(S) is the i\" diagonal element of the matrix\n",
      "U(S) =Xs(XPXs)1 XE. (14.29)\n",
      "\n",
      "Thus, one need not actually drop each observation and re-fit\n",
      "the model. A generalization is k-fold cross-validation. Here\n",
      "we divide the data into k groups; often people take k = 10. We\n",
      "omit one group of data and fit the models to the remaining data.\n",
      "We use the fitted model to predict the data in the group that\n",
      "was omitted. We then estimate the risk by )>,(¥j — Y,)? where\n",
      "the sum is over the the data points in the omitted group. This\n",
      "process is repeated for each of the k groups and the resulting\n",
      "risk estimates are averaged.\n",
      "\n",
      "For linear regression, Mallows C,, and cross-validation often\n",
      "yield essentially the same results so one might as well use Mal-\n",
      "\n",
      " \n",
      "\n",
      "lows’ method. In some of the more complex problems we will\n",
      "discuss later, cross-validation will be more useful.\n",
      "\n",
      "Another scoring method is BIC (Bayesian information crite-\n",
      "rion). Here we choose a model to maximize\n",
      "\n",
      "BIC(S) = rss (S) + 2|5|6?. (14.30)\n",
      "\n",
      "The BIC score has a Bayesian interpretation. Let S = {S,,..., Sin}\n",
      "denote a set of models. Suppose we assign the prior P(S;) = 1/m\n",
      "over the models. Also, assume we put a smooth prior on the pa-\n",
      "rameters within each model. It can be shown that the posterior\n",
      "probability for a model is approximately,\n",
      "\n",
      "eBIC(S))\n",
      "\n",
      "P(S;|data) = > ares\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-257.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CD9C8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.6 Model Selection 261\n",
      "\n",
      "Hence, choosing the model with highest BIC is like choosing the\n",
      "model with highest posterior probability. The BIC score also has\n",
      "an information-theoretic interpretation in terms of something\n",
      "called minimum description length. The BIC score is identical\n",
      "to Mallows C, except that it puts a more severe penalty for\n",
      "complexity. It thus leads one to choose a smaller model than\n",
      "the other methods.\n",
      "\n",
      "Now let us turn to the problem of model search. If there are k\n",
      "covariates then there are 2\" possible models. We need to search\n",
      "rough all these models, assign a score to each one, and choose\n",
      "he model with the best score. If k is not too large we can do\n",
      "a complete search over all the models. When k is large, this is\n",
      "infeasible. In that case we need to search over a subset of all\n",
      "\n",
      "a a0\n",
      "\n",
      " \n",
      "\n",
      "the models. Two common methods are forward and back-\n",
      "ward stepwise regression. In forward stepwise regression, we\n",
      "start with no covariates in the model. We then add the one vari-\n",
      "able that leads to the best score. We continue adding variables\n",
      "one at a time until the score does not improve. Backwards step-\n",
      "\n",
      " \n",
      "\n",
      "wise regression is the same except that we start with the biggest\n",
      "model and drop one variable at a time. Both are greedy searches;\n",
      "nether is guaranteed to find the model with the best score. An-\n",
      "other popular method is to do random searching through the\n",
      "set of all models. However, there is no reason to expect this to\n",
      "\n",
      " \n",
      "\n",
      "be superior to a deterministic search.\n",
      "\n",
      "Example 14.16 We apply backwards stepwise regression to the\n",
      "crime data using AIC. The following was obtained from the pro-\n",
      "gram R. This program uses minus our version of AIC. Hence,\n",
      "we are seeking the smallest possible AIC. This is the same is\n",
      "minimizing Mallows Cy.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-258.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "262 14. Linear Regression\n",
      "\n",
      "The full model (which includes all covariates) has AIC= 310.37.\n",
      "The AIC scores, in ascending order, for deleting one variable are\n",
      "as follows:\n",
      "\n",
      "variable || Pop Labor South Wealth Males U1 Educ. U2 Age _Bxpend\n",
      "\n",
      " \n",
      "\n",
      "AIC l 308 = 309 309 309 310 310 312 814 315 324\n",
      "\n",
      "For example, if we dropped Pop from the model and kept the\n",
      "other terms, then the AIC score would be 308. Based on this in-\n",
      "formation we drop “population” from the model and the current\n",
      "AIC score is 308. Now we consider dropping a variable from the\n",
      "current model. The AIC scores are:\n",
      "\n",
      "variable || South Labor Wealth Males U1 Education U2 Age LExpend\n",
      "AIC l 308 308 308 309 = 309 310 3138 3138 329\n",
      "\n",
      "We then drop “Southern” from the model. This process is con-\n",
      "\n",
      "tinued until there is no gain in AIC by dropping any variables.\n",
      "\n",
      "In the end, we are left with the following model:\n",
      "\n",
      " \n",
      "\n",
      "Crime = 1.2 Age + .75 Education + .87 Expenditure + .34 Males — .86 U1 + 2.31 U2.\n",
      "\n",
      "Warning! This does not yet address the question of which vari-\n",
      "ables are causes of crime.\n",
      "\n",
      "14.7. The Lasso\n",
      "\n",
      "There is an easier model search method although it addresses\n",
      "a slightly different question. The method, due to Tibshirani, is\n",
      "called the Lasso. In this section we assume that the covari-\n",
      "ates have all been rescaled to have the same variance. This\n",
      "puts each covariate on the same scale. Consider estimating 6 =\n",
      "(61,---,8,) by minimizing the loss function\n",
      "n\n",
      "\n",
      "k\n",
      "SH =H? +a SP 13) (14.31)\n",
      "\n",
      "i=l\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-259.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8708>\n",
      "14.8 Technical Appendix 263\n",
      "\n",
      "where \\ > 0. The idea is to minimize the sums of squares but\n",
      "we include a penalty that gets large if any of the Bis are large.\n",
      "The solution B = (Bi, --+5Bx) can be found numerically and\n",
      "will depend on the choice of A. It can be shown that some of\n",
      "the Bis will be 0. We interpret this is meaning that the j is\n",
      "omitted from the model. Hence, we are doing estimation and\n",
      "model selection simultaneously. We need to choose a value of\n",
      "A. We can do this by estimating the prediction risk R(A) as\n",
      "a function of \\ and choosing \\ to minimze the estimated risk.\n",
      "For example, we can estimate the risk using leave-one-out cross-\n",
      "validation.\n",
      "\n",
      "Example 14.17 Returning to the crime data, Figure 14.3 shows\n",
      "the results of the lasso. The first plot shows the leave-one-out\n",
      "cross-validation score as a function of X. The minimum. occurs\n",
      "at \\ = .7. The second plot shows the estimated coefficients as a\n",
      "function of X. You can see how some estimated parameters are\n",
      "zero until A gets larger. At \\ = .7, all the B; are non-zero so the\n",
      "Lasso chooses the full model.\n",
      "\n",
      "14.8 Technical Appendix\n",
      "\n",
      "Why is the predicition interval of a different form than the\n",
      "other confidence intervals we have seen? The reason is that the\n",
      "quantity we want to estimate, Y,, is not a fixed parameter, it\n",
      "is a random variable. To understand this point better, let 6 =\n",
      "Bo t+ A,X, and let 0 = 8) + B.X,. Thus, Y, = @ while Y, =0 +e.\n",
      "Now, Ox) (0,se”) where\n",
      "\n",
      "se? = V(8) = V(4y + B24).\n",
      "\n",
      " \n",
      "\n",
      "Note that V(@) is the same as V(Y,). Now, 04 2\\/Var(0) is a\n",
      "95 per cent confidence interval for 9 = 89+ 612, using the usual\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-260.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CD9C8>\n",
      "264 14. Linear Regression\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "    \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "2\n",
      "g 4\n",
      "EA 4\n",
      "3° 4\n",
      "284\n",
      "“a o\n",
      "a T T T T\n",
      "é\n",
      "5 0.2 04 0.6 0.8\n",
      "d\n",
      "Oo\n",
      "be 9-0-0-5-8-8-6-0-9-0-0-0-0-6\n",
      ". 4+ yee\n",
      "BO) 5 |e” e-\n",
      "| 9-0-0262828202058263\n",
      "2 4\n",
      "1\n",
      "\n",
      " \n",
      "\n",
      "0.2 0.4 0.6\n",
      "\n",
      "FIGURE 14.3. The Lasso applied to the crime data.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-261.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CDB48>\n",
      "14.8 Technical Appendix 265\n",
      "\n",
      "argument for a confidence interval. It is not a valid confidence\n",
      "interval for Y,. To see why, let’s compute the probability that\n",
      "\n",
      "y. + 2/V(¥.) contains Y,.\n",
      "\n",
      "P(Y, —28 < Y, < ¥, +2s)\n",
      "\n",
      " \n",
      "\n",
      "2\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "-2< N(0,1)-N (0 =) < 2)\n",
      "\n",
      "2\n",
      "-2 <n (01+) <2)\n",
      "Ss\n",
      "# 95.\n",
      "\n",
      "The problem is that the quantity of interest Y, is equal\n",
      "\n",
      "to a parameter @ plus a random variable. We can fix this\n",
      "by defining\n",
      "\n",
      "=VN,)+07= as —- + ie\n",
      "\n",
      "In practice, we substitute G for o and we denote the resulting\n",
      "quantity by é.. Now consider the interval ¥, +26, Then,\n",
      "\n",
      "P(Y, — 26, <¥% < ¥, 426) =\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-262.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148A1C23048>\n",
      "266\n",
      "\n",
      "14. Linear Regression\n",
      "\n",
      "= P(-2< N(0,1) < 2) =.95.\n",
      "\n",
      " \n",
      "\n",
      "Of course, a 1 — a interval is given by ¥, + Zajren-\n",
      "14.9 Exercises\n",
      "\n",
      "sles\n",
      "\n",
      "2.\n",
      "\n",
      "Prove Theorem 14.4.\n",
      "\n",
      "Prove the formulas for the standard errors in Theorem\n",
      "14.8. You should regard the X;’s as fixed constants.\n",
      "\n",
      ". Consider the regression through the origin model:\n",
      "\n",
      "Y, = BX; +e.\n",
      "\n",
      "Find the least squares estimate for 6. Find the standard\n",
      "error of the estimate. Find conditions that guarantee that\n",
      "the estimate is consistent.\n",
      "\n",
      ". Prove equation (14.24).\n",
      "\n",
      ". In the simple linear regression model, construct a Wald\n",
      "\n",
      "test for Ho : 6; = 179 versus H, : 6; #17).\n",
      "\n",
      ". Get the passenger car mileage data from\n",
      "\n",
      "http://lib.stat.cmu.edu/DASL/Datafiles/carmpgdat.html\n",
      "\n",
      "(a) Fit a simple linear regression model to predict MPG\n",
      "(miles per gallon) from HP (horsepower). Summarize your\n",
      "analysis including a plot of the data with the fitted line.\n",
      "(b) Repeat the analysis but use log(MPG) as the response.\n",
      "Compare the analyses.\n",
      "\n",
      ". Get the passenger car mileage data from\n",
      "\n",
      "http://lib.stat.cmu.edu/DASL/Datafiles/carmpgdat.html\n",
      "\n",
      "(a) Fit a multiple linear regression model to predict MPG\n",
      "(miles per gallon) from HP (horsepower). Summarize your\n",
      "analysis.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-263.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E92C8>\n",
      "14.9 Exercises 267\n",
      "\n",
      "(b) Use Mallow C, to select a best sub-model. To search\n",
      "trhough the models try (i) all possible models, (ii) for-\n",
      "ward stepwise, (ili) backward stepwise. Summarize your\n",
      "findings.\n",
      "\n",
      "(c) Repeat (b) but use BIC. Compare the results.\n",
      "\n",
      "(d) Now use the Lasso and compare the results.\n",
      "\n",
      ". Assume that the errors are Normal. Show that the model\n",
      "with highest AIC (equation (14.26)) is the model with the\n",
      "lowest Mallows C, statistic.\n",
      "\n",
      ". In this question we will take a closer look at the AIC\n",
      "method. Let X1,...,X,, be iid observations. Consider two\n",
      "models Mp and M,. Under Mo the data are assumed to\n",
      "be N(0,1) while under M, the data are assumed to be\n",
      "N(6,1) for some unknown @ € R:\n",
      "\n",
      "Mo: Xi,---;Xn ~ N(0,1)\n",
      "Mi: Xi--,Xn ~ N(O,1), OER\n",
      "\n",
      "This is just another way to view the hypothesis testing\n",
      "problem: Ho : 9 = 0 versus H; : 0 4 0. Let 2,(0) be\n",
      "the log-likelihood function. The AIC score for a model is\n",
      "the log-likelihood at the mle minus the number of param-\n",
      "eters. (Some people multiply this score by 2 but that is\n",
      "irrelevant.) Thus, the AIC score for Mo is AICo = ¢,(0)\n",
      "and the AIC score for M, is AIC; = £,(0) — 1. Suppose\n",
      "we choose the model with the highest AIC score. Let J,\n",
      "denote the selected model:\n",
      "j= { 0 if AIC) > AIC;\n",
      "mn (1 if AIC, > AIC).\n",
      "\n",
      "(a) Suppose that Mo is the true model, i.e. 9 = 0. Find\n",
      "\n",
      "lim P (Jn = 0).\n",
      "\n",
      "n—00\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-264.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n",
      "\n",
      "14. Linear Regression\n",
      "\n",
      "Now compute lim,_,.. P(J, =0) when 0 4 0.\n",
      "\n",
      "(b) The fact that lim;,.P (J, =0) 4 1 when 0 = 0 is\n",
      "why some people say that AIC “overfits.” But this is not\n",
      "quite true as we shall now see. Let @g(2) denote a Normal\n",
      "density function with mean @ and variance 1. Define\n",
      "\n",
      "=. f do(x) if Jn =0\n",
      "hey ={ ola) if Jn =1.\n",
      "\n",
      "If 0 =0, show that D(do, f,) 20 as n + 00 where\n",
      "\n",
      "D(f.9)= fH) 108 (42) dx\n",
      "\n",
      "is the Kullback-Leibler distance. Show also that D($9, fn) 2\n",
      "0if@ #0. Hence, AIC consistently estimates the true den-\n",
      "sity even if it “overshoots” the correct model.\n",
      "\n",
      "REMARK: If you are feeling ambitious, repeat this anal-\n",
      "ysis for BIC which is the log-likelihood minus (p/2) logn.\n",
      "where p is the number of parameters and n is sample size.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-265.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891699B48>\n",
      "5\n",
      "Multivariate Models\n",
      "\n",
      "In this chapter we revisit the Multinomial model and the mul-\n",
      "tivariate Normal, as we will need them in future chapters.\n",
      "\n",
      "Let us first review some notation from linear algebra. Re-\n",
      "call that if « and y are vectors then 2’ y = Ms xy; If Aisa\n",
      "matrix then det(A) denotes the determinant of A, A? denotes\n",
      "the transpose of A and A~ denotes the the inverse of A (if the\n",
      "inverse exists). The trace of a square matrix A — denoted by\n",
      "tr(A) — is the sum of its diagonal elements. The trace satis-\n",
      "fies tr(AB) = tr(BA) and tr(A) + tr(B). Also, tr(a) = a if a\n",
      "is a scalar (i.e. a real number). A matrix is positive definite if\n",
      "x? Sx > 0 for all non-zero vectors x. If a matrix © is symmet-\n",
      "ric and positive definite, there exists a matrix y!/2 — called the\n",
      "square root of © — with the following properties:\n",
      "\n",
      "1. ©? is symmetric;\n",
      "2. D=DIeyV?,\n",
      "SMe a el wheres EI = (eh,\n",
      "\n",
      "This is page 269\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-266.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CDB48>\n",
      "270 15. Multivariate Models\n",
      "15.1 Random Vectors\n",
      "\n",
      "Multivariate models involve a random vector X of the form\n",
      "\n",
      "XY\n",
      "X= :\n",
      "Xx\n",
      "The mean of a random vector X is defined by\n",
      "ma E(X1)\n",
      "w=|{: f= ‘ . (15.1)\n",
      "Me E(Xx)\n",
      "The covariance matrix © is defined to be\n",
      "V(X1) Cov(X1,X2) +++ Cov(X1, Xx)\n",
      "3=V(X) = Cov(X2, X1) V(%2) i Cov(Xa, Xe)\n",
      "Cov(X,,X1) Cov(X,,X2) +++ V(Xx)\n",
      "(15.2)\n",
      "\n",
      "This is also called the variance matrix or the variance-covariance\n",
      "matrix.\n",
      "\n",
      "Theorem 15.1 Leta be a vector of length k and let X be a ran-\n",
      "dom vector of the same length with mean yc and variance XS.\n",
      "Then E(a?X) = ap and V(aT X) = aTSa. If A is a matrix\n",
      "with k columns then E(AX) = Ay and V(AX) = ASAT.\n",
      "\n",
      "Now suppose we have a random sample of n vectors:\n",
      "\n",
      "Xu Xi Xin\n",
      "Xn Xn oo. Xm ; (15.3)\n",
      "Xri Xx Xin\n",
      "\n",
      "The sample mean X is a vector defined by\n",
      "\n",
      "x\n",
      "\n",
      "x :\n",
      "Xp\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-267.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C85C8>\n",
      "15.2 Estimating the Correlation 271\n",
      "\n",
      "ra jot Xig- The sample variance matrix, also\n",
      "\n",
      "called the covariance matrix or the variance-covariance matrix,\n",
      "is\n",
      "\n",
      "where X; = n\n",
      "\n",
      "Sii S12 *** Sik\n",
      "$12 822 *** San\n",
      "\n",
      "1 anne (15.4)\n",
      "Sik 82k *** Skk\n",
      "\n",
      "where\n",
      "\n",
      "1 “ a - >\n",
      "ja du — Xa)(Xvj — Xs).\n",
      "=\n",
      "It follows that E(X) = pt. and E(S) = ¥.\n",
      "\n",
      "15.2 Estimating the Correlation\n",
      "\n",
      "Consider n data points from a bivariate distribution:\n",
      "\n",
      "Xu Xi2 ee Xin\n",
      "Xo J 7 \\ X22 J? \\ Xan J”\n",
      "\n",
      "Recall that the correlation between X, and X is\n",
      "\n",
      "p= E(X ae (15.5)\n",
      "\n",
      "The sample correlation (the plug-in estimator) is\n",
      "\n",
      "i oh (a = 1) (Xai =X) (15.6)\n",
      "\n",
      "S189\n",
      "\n",
      " \n",
      "\n",
      "We can construct a confidence interval for p by applying the\n",
      "delta method as usual. However, it turns out that we get a more\n",
      "accurate confidence interval by first constructing a confidence\n",
      "interval for a function 9 = f(p) and then applying the inverse\n",
      "function p The method, due to Fisher, is as follows. Define\n",
      "\n",
      "f(r) = $(lestt +1) —log(1— r))\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-268.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8DC8>\n",
      "272\n",
      "\n",
      "1\n",
      "\n",
      "5. Multivariate Models\n",
      "\n",
      "akd let 9 = f(p). The inverse of r is\n",
      "\n",
      "Now do the following steps:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Approximate Confidence Interval for The Correlation\n",
      "\n",
      "1. Compute\n",
      "\n",
      "A= (8) = 5(log(1 +A) —log(t —7)).\n",
      "\n",
      ". Compute the approximate standard error of @ which can\n",
      "\n",
      "be shown to be\n",
      "\n",
      " \n",
      "\n",
      ". An approximate 1 — a confidence interval for 0 = f(p) is\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "(0) = (0- Ly ee ).\n",
      "yn—-3 yn —3\n",
      "\n",
      ". Apply the inverse transformation f~'(z) to get a confi-\n",
      "\n",
      "dence interval for p:\n",
      "\n",
      "C4 _1 eb _y\n",
      "e244 17 ce +1) °\n",
      "\n",
      " \n",
      "\n",
      "15.3 Multinomial\n",
      "\n",
      "Let us now review the Multinomial distribution. Consider\n",
      "drawing a ball from an urn which has balls with k different\n",
      "\n",
      "colors labeled color 1, color 2, ... , color k. Let p = (p1,.--, Pe)\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-269.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3508>\n",
      "15.3 Multinomial 273\n",
      "\n",
      "where p; > 0 and Sy pj =1 and suppose that p; is the prob-\n",
      "ability of drawing a ball of color 7. Draw n times (independent\n",
      "draws with replacement) and let X = (X,,...,X,) where X; is\n",
      "the number of times that color j appeared. Hence, n = Mis Xj.\n",
      "We say that X has a Multinomial (n,p) distribution. The prob-\n",
      "ability function is\n",
      "\n",
      "= = o Leen 7 Bid\n",
      "f(x; p) (\"Je Py\n",
      "\n",
      "n nt\n",
      ",...Lp ales)\n",
      "\n",
      "Theorem 15.2 Let X ~ Multinomial(n,p). Then the marginal\n",
      "\n",
      "where\n",
      "\n",
      "distribution of X; is X; ~ Binomial(n,p;). The mean and vari-\n",
      "ance of X are\n",
      "\n",
      "mp1\n",
      "EX)=|] :\n",
      "Pk\n",
      "and\n",
      "np, (1 — pi) —NPi pa aa —NP1PK\n",
      "Y= | “we ee) mee\n",
      "—\"PiPk —Np2Pr +++ mpe(1 — De)\n",
      "\n",
      "Proor. That X; ~ Binomial(n,p;) follows easily. Hence,\n",
      "E(.X;) = np; and V(Xj) = np;(1—p;). To compute Cov(.X;, X;)\n",
      "we proceed as follows. Notice that X;+X; ~ Binomial(n, p;+p,)\n",
      "and so V(X; + Xj) = n(p;+p;)(1 — p; — p;)- On the other hand,\n",
      "\n",
      "V(Xi+ Xj) = V(X) + V(Xj) + 2Cow( Xi, X;)\n",
      "npi(1 — pi) + np; (1 — pj) + 2Cov( Xj, X})-\n",
      "\n",
      "Equating this last expression with n(p;+p;)(1 —p; —p;) implies\n",
      "that Cov(X;,X;) = —npip;.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-270.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CDB48>\n",
      "274 15. Multivariate Models\n",
      "\n",
      "Theorem 15.3 The maximum likelihood estimator of p is\n",
      "\n",
      "g\\ (x\n",
      "~(\"\\) (*)\\ ix\n",
      "p= al @ l=%.\n",
      "\n",
      "Be Xe\n",
      "\n",
      "Proof. The log-likelihood (ignoring the constant) is\n",
      "\n",
      "k\n",
      "\n",
      "lp) = > Xjlogpy.-\n",
      "\n",
      "jal\n",
      "\n",
      "When we maximize ¢ we have to be careful since we must enforce\n",
      "the constraint that yj pj = 1. We use the method of Lagrange\n",
      "multipliers and instead maximize\n",
      "\n",
      " \n",
      "\n",
      "k\n",
      "A(p) = SX logp; + (Sop; - 1).\n",
      "j=l j\n",
      "Now .\n",
      "240) _ Xi,\n",
      "Op; Dy\n",
      "\n",
      "Setting “ = 0 yields p; = —X;/A. Since dj p; = 1 we see\n",
      "that \\= <n and hence p; = X;/n as claimed. 7\n",
      "\n",
      "Next we would like to know the variability of the MLE . We\n",
      "can either compute the variance matrix of p directly or we can\n",
      "approximate the variability of the mle by computing the Fisher\n",
      "information matrix. These two approaches give the same answer\n",
      "in this case. The direct approach is easy: V(p) = V(X/n) =\n",
      "n~?V(X) and so\n",
      "\n",
      "pa(l — pi) —PiP2 ae —P1PK\n",
      "—Pip2 — pa(L—pe) ++ —P2Pr\n",
      "\n",
      "vip) =>\n",
      "\n",
      "—PiPr pape ++ pe(L — px)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-271.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3508>\n",
      "15.4 Multivariate Normal 275\n",
      "\n",
      "15.4 Multivariate Normal\n",
      "\n",
      "Let us recall how the multivariate Normal distribution is de-\n",
      "fined. To begin, let\n",
      "XZ\n",
      "Z= |:\n",
      "Zi\n",
      "where Z,,...,Z, ~ N(0,1) are independent. The density of Z\n",
      "is\n",
      "\n",
      "1 ae 1 1\n",
      "fe) = Gane {3} =a {-5:\"=} ;\n",
      "\n",
      "(15.7)\n",
      "The variance matrix of Z is the identity matrix J. We write\n",
      "Z ~ N(0,1) where it is understood that 0 denotes a vector of\n",
      "\n",
      " \n",
      "\n",
      "k zeroes. We say that Z has a standard multivariate Normal\n",
      "distribution.\n",
      "\n",
      "More generally, a vector X has a multivariate Normal distri-\n",
      "bution, denoted by ¥ ~ N(,¥), if its density is\n",
      "\n",
      "fei 0.2) = pam { ge WTEe—1)}\n",
      "cae (27)*/2det(S)!/2 2\n",
      "(15.8)\n",
      "\n",
      "where det(-) denotes the determinant of a matrix, j1 is a vector\n",
      "of length k and ¥ isa k x k symmetric, positive definite matrix.\n",
      "Then E(X) = and V(X) =®. Setting p= 0 and © =I gives\n",
      "back the standard Normal.\n",
      "\n",
      "Theorem 15.4 The following properties hold:\n",
      "1. If Z~ N(0,1) and X=p+D'?Z then X ~ N(p, >).\n",
      "2. If X ~ N(u,=), then D-/?(X — p) ~ N(0, 1).\n",
      "\n",
      "3. If X ~ N(u,%) a is a vector of the same length as X,\n",
      "then a? X ~ N(a™ p,a7Sa).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-272.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1988>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276 15. Multivariate Models\n",
      "\n",
      "4. Let\n",
      "Va(X—p)DO'M(X — yp).\n",
      "\n",
      "Then V ~ xz.\n",
      "\n",
      "Suppose we partition a random Normal vector X into two\n",
      "parts X = (X,,X,) We can similarly partition the the mean\n",
      "t= (Ha; fp) and the variance\n",
      "\n",
      "Zaa Lab\n",
      "L= :\n",
      "(se Los )\n",
      "Theorem 15.5 Let X ~ N(u,¥). Then:\n",
      "(1) The marginal distribition of X, is Xq ~ N (pa; Naa):\n",
      "(2) The conditional distribition of X, given Xq = %q is\n",
      "\n",
      "Xp|Xa = Fa ~ N(Ui(a), Eta)\n",
      "\n",
      "where\n",
      "\n",
      "Hy + Eee (Ta — Ha) (15.9)\n",
      "Dap — Deeg Das- (15.10)\n",
      "\n",
      "Theorem 15.6 Given a random sample of size n from a N(p,™)\n",
      "\n",
      "the log-likelihood is (up to a constant not depending on ys or ©)\n",
      "is given by\n",
      "\n",
      "é(p, E) = -5(X- py So'(X—p)- str(S\"\"S) = ; log det(S).\n",
      "\n",
      "The MLE is\n",
      "\n",
      " \n",
      "\n",
      "fi=X and S= (—) 8. (15.11)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-273.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3508>\n",
      "15.5 Appendix 277\n",
      "15.5 Appendix\n",
      "\n",
      "Proof of Theorem 15.6. Denote the i‘ random vector by X'.\n",
      "The log-likelihood is\n",
      "\n",
      "as kn . n 1 ri\n",
      "Lu, D) = Dad (X*; pd) = ~ sy loa(27)—F log det (=) —5 SOx —}\n",
      "\n",
      "i\n",
      "\n",
      "Now,\n",
      "\n",
      "Sx —plToxXt-p) = SI (x! —X) + (X = p)PD1 (xt =\n",
      "\n",
      "i i\n",
      "\n",
      "iy Xt\n",
      "\n",
      "X)+(X\n",
      "\n",
      "r — p)]\n",
      "\n",
      " \n",
      "\n",
      "i\n",
      "\n",
      "since S7,(X' — X)\"-1(X — y) = 0. Also, notice that (X' —\n",
      "py)? S-1!(X' — 2) is a scalar, so\n",
      "\n",
      "Sox py ee -~ jp) = Le [(X? = p)7E1 (X= p)]\n",
      "- Dee “TX! = p(X\" = py)\"\n",
      "\n",
      "— dr [ xi =p) (Xe? = pe\n",
      "= ntr[=-'S|\n",
      "\n",
      "and the conclusion follows.\n",
      "15.6 Exercises\n",
      "\n",
      "1. Prove Theorem 15.1.\n",
      "\n",
      "2. Find the Fisher information matrix for the MLE of a Multi-\n",
      "nomial.\n",
      "\n",
      "3. Prove Theorem 15.5.\n",
      "\n",
      "4. (Computer Experiment. Write a function to generate nsim\n",
      "observations from a Multinomial(n, p) distribution.\n",
      "\n",
      "SO [Xt = XEN! = X)] + n(X-;\n",
      "\n",
      "Toa\n",
      "\n",
      "(X\n",
      "\n",
      "Lb).\n",
      "\n",
      "—h)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-274.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "278\n",
      "\n",
      "15. Multivariate Models\n",
      "\n",
      "5. (Computer Experiment. Write a function to generate nsim\n",
      "\n",
      "observations from a Multivariate normal with given mean\n",
      "} and covariance matrix D.\n",
      "\n",
      ". (Computer Experiment. Generate 1000 random vectors from\n",
      "\n",
      "a N(y,%) distribution where\n",
      "\n",
      "(2). 2(29)\n",
      "\n",
      "Plot the simulation as a scatterplot. Find the distribution\n",
      "of X2|X, = 2, using theorem 15.5. In particular, what is\n",
      "the formula for E(X2|X, = 2)? Plot E(X2|X, = 21) on\n",
      "your scatterplot. Find the correlation p between X; and\n",
      "X5. Compare this with the sample correlations from your\n",
      "simulation. Find a 95 per cent confidence interval for p.\n",
      "Estimate the covariance matrix DU.\n",
      "\n",
      "- Generate 100 random vectors from a multivariate Normal\n",
      "\n",
      "with mean (0, 2)\" and variance\n",
      "\n",
      "(31),\n",
      "\n",
      "Find a 95 per cent confidence interval for the correlation\n",
      "p. What is the true value of p?\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-275.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "16\n",
      "\n",
      "Inference about Independence\n",
      "\n",
      "In this chapter we address the following questions:\n",
      "\n",
      "(1) How do we test if two random variables are independent?\n",
      "(2) How do we estimate the strength of dependence between two\n",
      "random variables?\n",
      "\n",
      "When Y and Z are not independent, we say that they are\n",
      "dependent or associated or related. If Y and Z are associ-\n",
      "ated, it does not imply that Y causes Z or that Z causes Y. If\n",
      "Y does cause Z then changing Y will change the distribution of\n",
      "Z, otherwise it will not change the distribution of Z.\n",
      "\n",
      "For example, quitting smoking Y will reduce your probabil-\n",
      "ity of heart disease Z. In this case Y does cause Z. As another\n",
      "example, owning a TV Y is associated with having a lower in-\n",
      "cidence of starvation Z. This is because if you own a TV you\n",
      "are less likely to live in an impoverished nation. But giving a\n",
      "starving person will not cause them to stop being hungry. In\n",
      "this case, Y and Z are associated but the relationship is not\n",
      "causal.\n",
      "\n",
      "This is page 279\n",
      "Printer: Opaque this\n",
      "\n",
      "Recall that we write\n",
      "Y IZ to mean that\n",
      "Y and Z are inde-\n",
      "pendent.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-276.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1EC8>\n",
      "280 16. Inference about Independence\n",
      "\n",
      "We defer detailed discussion of the important question of cau-\n",
      "sation until a later chapter.\n",
      "\n",
      "16.1 Two Binary Variables\n",
      "\n",
      "Suppose that Y and Z are both binary. Consider a data set\n",
      "(%,Z,),---, (Yn, Zn). Represent the data as a two-by-two table:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "where the Xj; represent counts:\n",
      "\n",
      "Xj; = number of observations for which Y = 7 and Z = j.\n",
      "\n",
      "The dotted subscripts denote sums. For example, Xj. = My Xe\n",
      "This is a convention we use throughout the remainder of the\n",
      "book. Denote the corresponding probabilities by:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "where pj; P(Z = i,Y = j). Let X (Xoo, Xo1, X10, Xu1)\n",
      "denote the vector of counts. Then X ~ Multinomial(n, p) where\n",
      "P = (Poo, Po1, P10, Pi1)- It is now convenient to introduce two new\n",
      "parameters.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-277.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n",
      "16.1 Two Binary Variables 281\n",
      "\n",
      " \n",
      "\n",
      "Definition 16.1 The odds ratio is defined to be\n",
      "\n",
      "— PooPir\n",
      "\n",
      "w 8 16.1\n",
      "PoiPi0 ( )\n",
      "\n",
      "The log odds ratio is defined to be\n",
      "7 = log(w). (16.2)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 16.2 The following statements are equivalent:\n",
      "\n",
      "  \n",
      "\n",
      "& y=.\n",
      "4. For i,j € {0,1},\n",
      "\n",
      "Dij = Die -5- (16.3)\n",
      "Now consider testing\n",
      "\n",
      "Hy: YUZ versus H,: Y WZ.\n",
      "\n",
      "First we consider the likelihood ratio test. Under H,, X ~\n",
      "Multinomial(n, p) and the MLE is p= X/n. Under Ho, we again\n",
      "have that X ~ Multinomial(n,p) but p is subject to the con-\n",
      "straint\n",
      "\n",
      "Diy =PiP-j, J = 0,1.\n",
      "This leads to the following test.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 16.3 (Likelihood Ratio Test for Independence in a 2-by-2 table)\n",
      "Let\n",
      "dt XX\n",
      "_ - agX--\n",
      "r=2 » » Xj; log (==) : (16.4)\n",
      "\n",
      "Under Ho, T ~ x2. Thus, an approximate level a test is\n",
      "obtained by rejecting Ho then T > Xe\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-278.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1988>\n",
      "282 16. Inference about Independence\n",
      "\n",
      "Another popular test for independence is Pearson’s x? test.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 16.4 (Pearson’s y” test for Independence in a 2-by-2 table)\n",
      "\n",
      "Let i 4\n",
      "Xj; — Bij)?\n",
      "v=y ry Me Fl (16.5)\n",
      "i=0 j=0 ag\n",
      "where Bx\n",
      "By =\n",
      "n\n",
      "\n",
      "Under Ho, U ~ x7. Thus, an approximate level a test is\n",
      "obtained by rejecting Hy then U > a\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Here is the intuition for the Pearson test. Under Ho, pij =\n",
      "\n",
      "Pip.j, 80 the maximum likelihood estimator of p,; under Ho is\n",
      "a we, . MEM\n",
      "Pig = PicP Gj = “He\n",
      "\n",
      "Thus, the expected number of observations in the (i,j) cell is\n",
      "\n",
      "  \n",
      "\n",
      "Ey = np =\n",
      "\n",
      "n\n",
      "\n",
      "The statistic U compares the observed and expected counts.\n",
      "\n",
      "Example 16.5 The following data from Johnson and Johnson\n",
      "(1972) relate tonsillectomy and Hodgkins disease. (The data are\n",
      "actually from a case-control study; we postpone discussion of\n",
      "this point until the next section.)\n",
      "\n",
      "| Hodgkins Disease No Disease |\n",
      "\n",
      "Tonsillectomy 90 165 255\n",
      "No Tonsillectomy | 84 307 391\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Total 14 472 646\n",
      "\n",
      "We would like to know if tonsillectomy is related to Hodgkins\n",
      "disease. The likelihood ratio statistic is T = 14.75 and the p-\n",
      "value is P(x? > 14.75) = .0001. The x? statistic is U = 14.96\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-279.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "16.1 Two Binary Variables 283\n",
      "\n",
      "and the p-value is P(x{ > 14.96) = 0001. We reject the null\n",
      "hypothesis of independence and conclude that tonsillectomy is\n",
      "associated with Hodgkins disease. This does not mean that ton-\n",
      "sillectomies cause Hodgkins disease. Suppose, for example, that\n",
      "doctors gave tonsillectomies to the most seriosuly ill patients.\n",
      "Then the assocation between tonsillectomies and Hodgkins dis-\n",
      "ease may be due to the fact that those with tonsillectomies were\n",
      "the most ill patients and hence more likely to have a serious\n",
      "disease.\n",
      "\n",
      "We can also estimate the strength of dependence by estimat-\n",
      "ing the odds ratio 7 and the log-odds ratio y.\n",
      "Theorem 16.6 The MLE ’s of ) and y are\n",
      "XoXi ~ rT\n",
      "= ——, J=logy. 16.6\n",
      "XoXo a (166)\n",
      "\n",
      "The asymptotic stanrad errors (computed from the delta method)\n",
      "are\n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      "(16.7)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "(16.8\n",
      "\n",
      "Remark 16.7 For small sample sizes, o and ¥ can have a very\n",
      "large variance. In this case, we often use the modified estimator\n",
      "ga (Xoo 5) Sut 5)\n",
      "v=o (16.9\n",
      "\n",
      "(Xo +3) (Mio + 3)\n",
      "\n",
      "Yet another test for indepdnence is the Wald test for y = 0\n",
      "given by W = (7—0)/sé (7). A 1—a confidence interval for 7 is\n",
      "7+ 2a 286 (7). A 1—a confidence interval for y can be obtained\n",
      "in two ways. First, we could use ~ + 2/286 (w). Second, since\n",
      "w =e\" we could use\n",
      "\n",
      " \n",
      "\n",
      "exp {7 + zaj2se (7) }. (16.10\n",
      "\n",
      "This second method is usually more accurate.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-280.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CDB48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284 16. Inference about Independence\n",
      "\n",
      "Example 16.8 In the previous example,\n",
      "=~ 90 x 307\n",
      "jj ee a\n",
      "165 x 84 99\n",
      "and\n",
      "F =log(1.99) = .69.\n",
      "\n",
      "So tonsillectomy patients were twice as likely to have Hodgkins\n",
      "disease. The standard error of 7 is\n",
      "\n",
      " \n",
      "\n",
      "The Wald statistic is W = .69/.18 = 3.84 whose p-value is\n",
      "P(|Z| > 3.84) = .0001, the same as the other tests. A 95 per\n",
      "cent confidence interval for y is 7 + 2(.18) = (.33, 1.05). A 95\n",
      "per cent confidence interval for w is (e8, e!) = (1.39, 2.86).\n",
      "\n",
      "16.2 Interpreting The Odds Ratios\n",
      "\n",
      "Suppose event A as probability P(A). The odds of A are defined\n",
      "as\n",
      "\n",
      "odds(A) = TET\n",
      "\n",
      "It follows that ;\n",
      "P(A) = ' slag) ;\n",
      "+ odds(A)\n",
      "Let E be the event that someone is exposed to something\n",
      "(smoking, radiation, etc) and let D be the event that they get\n",
      "a disease. The odds of getting the disease given that you are\n",
      "\n",
      "exposed are\n",
      "PIE)\n",
      "1—-P(D|E)\n",
      "\n",
      "and the odds of getting the disease given that you are not ex-\n",
      "\n",
      "odds(D|E) =\n",
      "\n",
      "posed are\n",
      "PDE)\n",
      "\n",
      "odds(D|E*) = TPE)\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-281.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CD9C8>\n",
      "16.2 Interpreting The Odds Ratios 285\n",
      "\n",
      "The odds ratio is defined to be\n",
      "\n",
      "_ odds(D|E)\n",
      "oe odds(D|E*)”\n",
      "\n",
      "If = 1 then disease probability is the same for exposed and un-\n",
      "exposed. This implies that these events are independent. Recall\n",
      "that the log-odds ratio is defined as y = log(z). Independence\n",
      "corresponds to y = 0.\n",
      "\n",
      "Consider this table of probabilities:\n",
      "\n",
      "Denote the data by\n",
      "\n",
      " \n",
      "\n",
      "Ee\n",
      "B\n",
      "Now\n",
      "P(D|E)= Pu and P(D|E*) = Po.\n",
      "Po+Pu Poo + Por\n",
      "and so\n",
      "odds(D|E) = Pu and odds(D|E*) = Bow\n",
      "Pio Poo\n",
      "and therefore,\n",
      "Pi P11Po00\n",
      "PoiPio™\n",
      "\n",
      "To estimate the parameters, we have to first consider how the\n",
      "data were collected. There are three methods.\n",
      "\n",
      "MULTINOMIAL SAMPLING. We draw a sample from the pop-\n",
      "ulation and, for each person, record their exposure and disease\n",
      "status. In this case, ¥ = (Xoo, Xo1, X10, X11) ~ Multinomial(n, p).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-282.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907BAA88>\n",
      "286 16. Inference about Independence\n",
      "\n",
      "We then estimate the probabilities in the table by pj; = Xj;/n\n",
      "and\n",
      "i PirPoo as XuXoo\n",
      "Po1Pio Xoi X10\n",
      "PROSPECTIVE SAMPLING. (COHORT SAMPLING). We get\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "some exposed and unexposed people and count the number with\n",
      "disease in each group. Thus,\n",
      "\n",
      "Xo1 ~ Binomial(Xo., P(D|E*))\n",
      "Xi, ~ Binomial(X,., P(D|£)).\n",
      "\n",
      "We should really write xp. and x. instead of Xo. and Xj. since\n",
      "in this case, these are fixed not random but for notational sim-\n",
      "plicity I'll keep using capital letters. We can estimate P(D|E)\n",
      "and P(D|E°) but we cannot estimate all the probabilities in the\n",
      "table. Still, we can estimate ¢% since 7 is a function of P(D|E)\n",
      "and P(D|E*). Now\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Fs} Xu\n",
      "P(D|E) = X,.\n",
      "and x\n",
      "= cy _ Aor\n",
      "P(D|E) = Xe\n",
      "Thus,\n",
      "§= Xi Xoo\n",
      "Xo X10\n",
      "\n",
      "just as before.\n",
      "\n",
      "CASE-CONTROL (RETROSPECTIVE) SAMPLING. Here we get\n",
      "some diseased and non-diseased people and we observe how\n",
      "many are exposed. This is much more efficient if the disease\n",
      "is rare. Hence,\n",
      "\n",
      "Xio ~ Binomial(X., P(E|D*))\n",
      "Xi ~ Binomial(X.;, P(E|D)).\n",
      "\n",
      "From these data we can estimate P(E|D) and P(E|D*). Sur-\n",
      "prisingly, we can also still estimate 7. To understand why, note\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-283.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "16.2 Interpreting The Odds Ratios 287\n",
      "\n",
      "that\n",
      "\n",
      "Po odds(B|D) = 2.\n",
      "\n",
      "P(E|D) = —?4 = _,\n",
      "Poi + pit Poi\n",
      "\n",
      "~ por + pi’\n",
      "\n",
      "By a similar argument,\n",
      "\n",
      "1-P(E|D)\n",
      "\n",
      "odds(B|D*) = ay\n",
      "Poo\n",
      "\n",
      "Hence,\n",
      "odds(E|D) — pupoo _\n",
      "\n",
      "odds(E|D*) ~~ puPio\n",
      "From the data, we form the following estimates:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "a Xu i Xo => Xy — Xg\n",
      "P(B|D) = —, 1—P(E|D) = —, odds(E|D) = —, odds(E|D*°) = —.\n",
      "(ED) =, 1-Plz|D) = 5%, oddb(B|D) = F4, odas(e|D9) =\n",
      "Therefore,\n",
      "— XoXn\n",
      "XoXo\n",
      "\n",
      "So in all three data collection methods, the estimate of w turns\n",
      "out to be the same.\n",
      "\n",
      "It is tempting to try to estimate P(D|E) — P(D|E*). Ina\n",
      "case-control design, this quantity is not estimable. To see this,\n",
      "we apply Bayes’ theorem to get\n",
      "P(E|D)P(D) _ P(E*|D)P(D)\n",
      "\n",
      "P(DIE) - P(DIE) = FES\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Because of the way we obtained the data, P(D) is not estimable\n",
      "from the data. However, we can estimate € = P(D|E£)/P(D|E°),\n",
      "which is called the relative risk, under the rare disease as-\n",
      "sumption.\n",
      "Theorem 16.9 Let € = P(D|E)/P(D|E*). Then\n",
      "\n",
      "wv\n",
      "\n",
      "oe\n",
      "\n",
      "&\n",
      "\n",
      "as P(D) > 0.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-284.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1EC8>\n",
      "288 16. Inference about Independence\n",
      "\n",
      "Thus, under the rare disease assumption, the relative risk is\n",
      "approximately the same as the odds ratio and, as we have seen,\n",
      "we can estimate the ods ratio.\n",
      "\n",
      "In a randomized experiment, we can interpret a strong associ-\n",
      "ation, that is 7 # 1, as a causal relationship. In an observational\n",
      "(non-randomized) study, the association can be due to other\n",
      "unobserved confounding variables. We'll discuss causation in\n",
      "more detail later.\n",
      "\n",
      "16.3 Two Discrete Variables\n",
      "\n",
      "Now suppose that Y € {1,...,/} and Z € {1,..., J} are two\n",
      "discrete variables. The data can be represented as an I — by — J\n",
      "table of counts:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Z=i| Xa Xg ot My ct Xi\n",
      "Z=I| Xn Xp Xy eX :\n",
      "x <<: = X = Hy | 2\n",
      "\n",
      "Consider testing Hyp: Y IZ versus H,: Y WZ.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-285.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "16.3 Two Discrete Variables 289\n",
      "\n",
      " \n",
      "\n",
      "of i\n",
      "¥\n",
      "\n",
      " \n",
      "\n",
      "Theorem 16.10 Let\n",
      "\n",
      "The limiting distribution of T under the null hypothesis\n",
      "\n",
      "Asymptotically, under Ho, U has a x2, distribution where\n",
      "y=(I-1)(J—-1).\n",
      "\n",
      " \n",
      "\n",
      "rT J\n",
      "Xi X.,\n",
      "T=2 3 Xyloe (AE). 6.11\n",
      "\n",
      "i=1 j=l 4\n",
      "\n",
      "independence is x? where v = (I—1)(J—1). Pearson’s\n",
      "test statistic is\n",
      "\n",
      "U= yr ae a. (16.12)\n",
      "\n",
      "#=1 j=l\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 16.11 These data are from a study by Hancock et al\n",
      "(1979). Patients with Hodkins disease are classified by their re-\n",
      "sponse to treatment and by histological type.\n",
      "Type | Positive Response Partial Response No Response\n",
      "LP |% 18 12 104\n",
      "NS | 68 16 12 96\n",
      "MC | 154 54 58 266\n",
      "LD | 18 10 44 72\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The x? test statistic is 75.89 with 2x 3=6 degrees of freedom.\n",
      "The p-value is P(x3 > 75.89) % 0. The likelihood ratio test\n",
      "statistic is 68.30 with 2x 3=6 degrees of freedom. The p-value\n",
      "is P(x > 68.30) © 0. Thus there is strong evidence that reponse\n",
      "\n",
      "to trea\n",
      "\n",
      "tment and histological type are associated.\n",
      "\n",
      "There are a variety of ways to quantify the strength of depen-\n",
      "\n",
      "dence\n",
      "\n",
      "between two discrete variables Y and Z. Most of them\n",
      "\n",
      "are not very intuitive. The one we shall use is not standard but\n",
      "\n",
      "is more interpretable.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-286.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1BC8>\n",
      "290 16. Inference about Independence\n",
      "We define\n",
      "\n",
      "6(¥,Z) = max [Pyz(¥ € A,Z € B)—Py(¥ € A)~PA(Z € B)|\n",
      "\n",
      "(16.13)\n",
      "where the maximum is over all pairs of events A and B.\n",
      "\n",
      "Theorem 16.12 Properties of 6:\n",
      "L0<6(¥,2Z) <1.\n",
      "2. O(Y,Z) =0 if and only if Y UZ.\n",
      "\n",
      "3. The following identity holds:\n",
      "\n",
      "3(X,Y\n",
      "\n",
      " \n",
      "\n",
      "<P - Pip: (16.14)\n",
      "\n",
      "i Me\n",
      "\n",
      "4. The MLE of 6 is\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "tod\n",
      "Y=5 >) [a- 7.2, (16.15)\n",
      "i=l j=l\n",
      "where\n",
      "a My 2 Xp og My\n",
      "Di » Di. » By z\n",
      "n n n\n",
      "\n",
      "The interpretation of 6 is this: if one person makes probability\n",
      "statements assuming independence and another person makes\n",
      "probability statements without assuming independence, their\n",
      "probability statements may differ by as much as 0. Here is a\n",
      "suggested scale for interpreting 6:\n",
      "\n",
      "0<d<.01 negligible association\n",
      "\n",
      "01 <6 <.05 non-negligible association\n",
      "05 <6<.10 substantial association\n",
      "\n",
      "6 > .10 very strong association\n",
      "\n",
      "A confidence interval for 6 can be obtained by bootstrapping.\n",
      "The steps for bootstrapping are:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-287.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "16.4 Two Continuous Variables 291\n",
      "1. Draw X* ~ Multinomial(n, p);\n",
      "\n",
      ". Compute ij, Dj. Py.\n",
      "\n",
      "nN\n",
      "\n",
      "3. Compute 6*.\n",
      "\n",
      "4. Repeat.\n",
      "\n",
      "Now we use any of the methods we learned earlier for construct-\n",
      "ing bootstrap confidence intervals. However, we should not use\n",
      "a Wald interval in this case. The reason is that if Y Il Z then\n",
      "6 = 0 and we are on the boundary of the parameter space. In\n",
      "this case, the Wald method is not valid.\n",
      "\n",
      "Example 16.13 Returning to Example 16.11 we find that 6= 11.\n",
      "Using a pivotal bootstrap with 10,000 bootstrap samples, a 95\n",
      "per cent confidence interval for 6 is (.09,.22). Our conclusion\n",
      "is that the association between histological type and response is\n",
      "substantial.\n",
      "\n",
      "16.4 Two Continuous Variables\n",
      "\n",
      "Now suppose that Y and Z are both continuous. If we assume\n",
      "that the joint distribution of Y and Z is bivariate Normal, then\n",
      "we mesure the dependence between Y and Z by means of the\n",
      "correaltion coefficient p. Tests, estimates and confidence inter-\n",
      "vals for p in the Normal case are given in the previous chapter.\n",
      "If we do not assume Normality then we need a nonparametric\n",
      "method for assessing dependence.\n",
      "Recall that the correaltion is\n",
      "\n",
      "_ E((Xi — #4) (X2 — bi)\n",
      "\n",
      "102\n",
      "A nonparametric estimator of p is the plug-in estimator which\n",
      "is\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "pe Ea Xu FH) as - Ke)\n",
      "VDm Ou — HPD he - )?\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-288.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292 16. Inference about Independence\n",
      "\n",
      "which is just the sample correlation. A confidence interval can be\n",
      "constructed using the bootstrap. A test for p = 0 can be based\n",
      "on the Wald test using the bootstrap to estimate the standard\n",
      "error.\n",
      "\n",
      "The plug-in approach is useful for large samples. For small\n",
      "samples, we measure the correlation using the Spearman rank\n",
      "correaltion coefficient f;. We simply replace the data by their\n",
      "ranks — ranking each variable separately — then we compute the\n",
      "correaltion coefficient of the ranks. To test the null hypothesis\n",
      "that p\n",
      "pothesis. This can be obtained easily by simulation. We fix the\n",
      "ranks of the first variable as 1,2,...,n. The ranks of the second\n",
      "\n",
      "s = 0 we need the distribution of fg under the null hy-\n",
      "\n",
      "variable are chosen at random from the set of n! possible order-\n",
      "ings. Then we compute the correlation. This is repeated many\n",
      "times and the resulting distribution Po is the null distribution of\n",
      "fs. The p-value for the test is Po(|R| > |fs|) where R is drawn\n",
      "from the null distribution Po.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 16.14 The following data (Snedecor and Cochran, 1980,\n",
      "p. 191) are systolic blood pressure X, and are diastolic blood\n",
      "pressure Xo in millimeters:\n",
      "\n",
      "X,| 100 105 110 110 120 120 125 130 130 150 170 195\n",
      "\n",
      "X2.| 65 65 7 TW 7 80 1% 82 80 90\n",
      "\n",
      "The sample correlation is p = .88. The bootstrap standard\n",
      "error is .046 and the Wald test statistic is .88/.046 = 19.23.\n",
      "The p-value is near 0 giving strong evidence that the correlation\n",
      "is not 0. The 95 per cent pivotal bootstrap confidence interval is\n",
      "(.78,.94). Because the sample size is small, consider Spearman’s\n",
      "rank correlation. In ranking the data, we will use average ranks\n",
      "if there are ties. So if the third and fourth lowest numbers are\n",
      "the same, they each get rank 3.5. The ranks of the data are:\n",
      "\n",
      "95\n",
      "\n",
      "90\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-289.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x14891699B48>\n",
      "16.5 One Continuous Variable and One Discrete 293\n",
      "\n",
      "xX, it 23.5 8.5 5.5 5.5 7 85 85 10 I\n",
      "Xo] 25 15 4.5 8 6 15 4.5 9 7.5 10.5 12\n",
      "\n",
      "The rank correlation is pg = .94 and the p-value for testing the\n",
      "null hypothesis that there is no correaltion is Po(|R| > .94) +0\n",
      "which is was obtained by simulation.\n",
      "\n",
      "16.5 One Continuous Variable and One Discrete\n",
      "\n",
      "Suppose that Y € {1,...,I} is discrete and Z is continuous.\n",
      "Let F,(z) = P(Z < 2|Y = i) denote the CDF of Z conditional\n",
      "on Y =i.\n",
      "\n",
      "Theorem 16.15 When Y € {1,...,I} is discrete and Z is con-\n",
      "tinuous, then Y I Z if and only if Fy =-++-= Fy.\n",
      "\n",
      "It follows from the previous theorem that to test for indepen-\n",
      "dence, we need to test\n",
      "\n",
      "Ay: Fi =-:-=F;, versus H,: not Hp.\n",
      "\n",
      "For simplicity, we consider the case where J = 2. To test the\n",
      "null hypothesis that F; = F, we will use the two sample\n",
      "Kolmogorov-Smirnov test. Let n, denote the nimber of ob-\n",
      "servations for which Y; = 1 and let nz denote the nimber of\n",
      "observations for which Y; = 2. Let\n",
      "\n",
      "and\n",
      "\n",
      " \n",
      "\n",
      "denote the empirical distribution function of Z given Y = 1 and\n",
      "Y = 2 respectively. Define the test statistic\n",
      "\n",
      "D=sup|Fi(2) — F(x).\n",
      "\n",
      "12\n",
      "10.5\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-290.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C3C48>\n",
      "The data here are\n",
      "an approximate re-\n",
      "creation using the\n",
      "information in the\n",
      "article.\n",
      "\n",
      "294 16. Inference about Independence\n",
      "\n",
      "Theorem 16.16 Let\n",
      "\n",
      "Under the null hypthesis that Fy = F»,\n",
      "\n",
      "lim P ( /S™ p< ‘) = H(t).\n",
      "n—00 ny + ng\n",
      "\n",
      "It follows from the theorem that an approximate level a test\n",
      "is obtained by rejecting Hp when\n",
      "\n",
      ",/—_p > #\"(1-a).\n",
      "ny + Ng\n",
      "\n",
      "16.6 Bibliographic Remarks\n",
      "\n",
      "Johnson, $.K. and Johnson, R.E. (1972). New England Journal\n",
      "of Medicine. 287. 1122-1125.\n",
      "Hancock, B.W. (1979). Clinical Oncology, 5, 283-297.\n",
      "\n",
      "16.7 Exercises\n",
      "\n",
      "1. Prove Theorem 16.2.\n",
      "2. Prove Theorem 16.3.\n",
      "3. Prove Theorem 16.9.\n",
      "4. Prove equation (16.14).\n",
      "\n",
      "5. The New York Times (January 8, 2003, page A12) re-\n",
      "ported the following data on death sentencing and race,\n",
      "from a study in Maryland:\n",
      "\n",
      "Death Sentence No Death Sentence\n",
      "Black Victim 14 641\n",
      "White Victim 62 594\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-291.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1988>\n",
      "16.7 Exercises 295\n",
      "\n",
      "Analyze the data using the tools from this Chapter. In-\n",
      "terpret the results. Explain why, based only on this infor-\n",
      "mation, you can’t make causal conclusions. (The authors\n",
      "of the study did use much more information in their full\n",
      "report.\n",
      "\n",
      ". Analyze the data on the variables Age and Financial Sta-\n",
      "tus from:\n",
      "http://lib.stat.cmu.edu/DASL/Datafiles/montanadat.html\n",
      "\n",
      ". Estimate the correlation between temperature and lati-\n",
      "tude using the data from\n",
      "http://lib.stat.cmu.edu/DASL/Datafiles/USTemperatures.html\n",
      "Use the correlation coefficient and the Spearman rank cor-\n",
      "realtion. Provide estimates, tests and confidence intervals.\n",
      "\n",
      ". Test whether calcium intake and drop in blood pressure\n",
      "are associated. Use the data in\n",
      "\n",
      "http://lib.stat.cmu.edu/DASL/Datafiles/Calcium.html\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-292.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-293.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1988>\n",
      "17\n",
      "\n",
      "Undirected Graphs and Conditional\n",
      "Independence\n",
      "\n",
      "Graphical models are a class of multivariate statistical models\n",
      "that are useful for representing independence relations. They are\n",
      "also useful develop parsimonious models for multivariate data.\n",
      "\n",
      "To see why parsimonious models are useful in the multivariate\n",
      "setting, consider the problem of estimating the joint distribu-\n",
      "tion of several discrete random variables. Two binary variables\n",
      "Yj, and Y) can be represented as a two-by-two table which corre-\n",
      "sponds to a multinomial with four categories. Similarly, k binary\n",
      "variables Y,,..., Y, correspond to a multinomial with N = 2*\n",
      "categories. When k is even moderately large, N = 2\" will be\n",
      "huge. It can be shown in that case that the MLE is a poor es-\n",
      "timator. The reason is that the data are sparse: there are not\n",
      "enough data to estimate so many parameters. Graphical mod-\n",
      "els often require fewer parameters and may lead to estimators\n",
      "with smaller risk. There are two main types of graphical models:\n",
      "undirected and directed. Here, we introduce undirected graphs.\n",
      "We'll discuss directed graphs later.\n",
      "\n",
      "This is page 297\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-294.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "298 17. Undirected Graphs and Conditional Independence\n",
      "17.1 Conditional Independence\n",
      "\n",
      "Underlying graphical models is the concept of conditional inde-\n",
      "pendence.\n",
      "\n",
      " \n",
      "\n",
      "Definition 17.1 Let X, Y and Z be discrete random vari-\n",
      "ables. We say that X and Y are conditionally inde-\n",
      "pendent given Z, written X IY | Z, if\n",
      "\n",
      "P(X =2,Y =y|Z =z) = P(X =2|Z =2z)P(Y =y|Z = 2)\n",
      "(17.1)\n",
      "for alla,y,z.If X,Y and Z are continuous random vari-\n",
      "ables, we say that X and ¥Y are conditionally independent\n",
      "given Z if\n",
      "\n",
      "fxyviz(a, yl2) = fxiz(al2) fyiz(yl2)-\n",
      "\n",
      "for dll x, y andz.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Intuitively, this means that, once you know Z, Y provides no\n",
      "extra information about X.\n",
      "\n",
      "The conditional independence relation satisfies some basic\n",
      "properties.\n",
      "\n",
      "Theorem 17.2 The following implications hold:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "XILY|Z = YUX|Z\n",
      "\n",
      "XUY|Z and U=h(X) = UMY|Z\n",
      "XIY|Z and U=h(X) = XUY|(Z,U)\n",
      "NXUY |Z and XUW|(Y,Z) = XU(WY)|Z\n",
      "\n",
      "XUY|Z and XUZ|Y => XU(,2).\n",
      "\n",
      "The last property requires the assumption that all events have\n",
      "positive probability; the first four do not.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-295.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C85C8>\n",
      "17.2 Undirected Graphs 299\n",
      "\n",
      "xX Z\n",
      "\n",
      "FIGURE 17.1. A graph with vertices V = {X,Y, Z}. The edge set is\n",
      "B={(X,Y),(¥,Z)}.\n",
      "\n",
      "17.2 Undirected Graphs\n",
      "\n",
      "An undirected graph G = (V,£) has a finite set V of ver-\n",
      "tices (or nodes) and a set E of edges (or arcs) consisting of\n",
      "pairs of vertices. The vertices correspond to random variables\n",
      "X,Y,Z,... and edges are written as unordered pairs. For exam-\n",
      "ple, (X,Y) € E means that X and Y are joined by an edge.\n",
      "\n",
      "An example of a graph is in Figure 17.1.\n",
      "\n",
      "Two vertices are adjacent, written X ~ Y, if there is an edge\n",
      "between them. In Figure 17.1, X and Y are adjacent but X and\n",
      "Z are not adjacent. A sequence Xo,...,Xn is called a path if\n",
      "Xj-1 ~ X; for each 7. In Figure 17.1, X,Y, Z is a path. A graph\n",
      "is complete if there is an edge between every pair of vertices.\n",
      "A subset U C V of vertices together with their edges is called a\n",
      "subgraph.\n",
      "\n",
      "If A,B and C are three distinct subsets if V, we say that C\n",
      "separates A and B if every path from a variable in A toa\n",
      "variable in B intersects a variable in C. In Figure 17.2 {Y,W}\n",
      "and {Z} are separated by {X}. Also, W and Z are separated\n",
      "by {X,Y}.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-296.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 17. Undirected Graphs and Conditional Independence\n",
      "Ww x\n",
      "\n",
      "FIGURE 17.2. {Y,W} and {Z} are separated by {X}. Also, W and\n",
      "Z are separated by {X,Y}.\n",
      "\n",
      "y\n",
      "\n",
      "x Z\n",
      "\n",
      "FIGURE 17.3. X II Z|Y.\n",
      "\n",
      "17.3 Probability and Graphs\n",
      "\n",
      "Let V be a set of random variables with distribution P. Con-\n",
      "struct a graph with one vertex for each random variable in V.\n",
      "Suppose we omit the edge between a pair of variables if they are\n",
      "independent given the rest of the variables:\n",
      "\n",
      "no edge between X andY <=> X IY|rest\n",
      "\n",
      "where “rest” refers to all the other variables besides X and Y.\n",
      "This type of graph is called a pairwise Markov graph. Some\n",
      "examples are shown in Figures 17.3, 17.4 17.6 and 17.5.\n",
      "\n",
      "The graph encodes a set of pairwise conditional independence\n",
      "relations. These relations imply other conditional independence\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-297.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "17.3 Probability and Graphs 301\n",
      "\n",
      "xX Z\n",
      "\n",
      "FIGURE 17.4. No implied independence relations.\n",
      "\n",
      "FIGURE 17.5. X 1Z|{Y,W} and ¥ I W|{X, Z}.\n",
      "\n",
      "x ¥ Z Ww\n",
      "\n",
      "FIGURE 17.6. Pairwise independence implies that X II Z|{Y,W}.\n",
      "But is X I. Z|Y?\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-298.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1A48>\n",
      "302 17. Undirected Graphs and Conditional Independence\n",
      "\n",
      "relations. How can we figure out what they are? Fortunately, we\n",
      "can read these other conditional independence relations directly\n",
      "from the graph as well, as is explained in the next theorem.\n",
      "\n",
      "Theorem 17.3 Let G = (V,£) be a pairwise Markov graph for\n",
      "a distribution P. Let A,B and C be distinct subsets of V such\n",
      "that C separates A and B. Then AIL B\\C.\n",
      "\n",
      "Remark 17.4 If A and B are not connected (i.e. there is no path\n",
      "from A to B) then we may regard A and B as being separated\n",
      "by the empty set. Then Theorem 17.3 implies that AIL B.\n",
      "\n",
      "The independence condition in Theorem 17.3 is called the\n",
      "global Markov property. We thus see that the pairwise and\n",
      "global Markov properties are equivalent. Let us state this more\n",
      "precisely. Given a graph G, let Myair(G) be the set of distri-\n",
      "butions which satisfy the pairwise Markov property: thus P €\n",
      "Mpair(G) if, under P, X IL Y|rest if and only if there is no edge\n",
      "between X and Y. Let Mgiovai(G) be the set of distributions\n",
      "which satisfy the global Markov property: thus P € Mpair(G) if,\n",
      "under P, ALI BIC if and only if C separates A and B.\n",
      "\n",
      "Theorem 17.5 Let G be a graph. Then, Mpair(G) = Mgiovai(G).\n",
      "\n",
      "This theorem allows us to construct graphs using the simpler\n",
      "pairwise property and then we can deduce other independence\n",
      "relations using the global Markov property. Think how hard this\n",
      "would be to do algebraically. Returning to 17.6, we now see that\n",
      "XUZ|Y and YUW|Z.\n",
      "\n",
      " \n",
      "\n",
      "Example 17.6 Figure 17.7 implies that X UY, X IZ and X I\n",
      "(Y, Z).\n",
      "\n",
      "Example 17.7 Figure 17.8 implies that X ILW|(Y,Z) and X II\n",
      "ZY.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-299.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "17.3 Probability and Graphs\n",
      "\n",
      "Xe Z\n",
      "\n",
      "FIGURE 17.7. XY, XZ and XI(Y, Z).\n",
      "\n",
      "x Y\n",
      "\n",
      "Ww\n",
      "\n",
      "Z\n",
      "\n",
      "FIGURE 17.8. X IW|(Y,Z) and X 1Z|Y.\n",
      "\n",
      "303\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-300.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "304 17. Undirected Graphs and Conditional Independence\n",
      "17.4 Fitting Graphs to Data\n",
      "\n",
      "Given a data set, how do we find a graphical model that fits\n",
      "the data. Some authors have devoted whole books to this sub-\n",
      "ject. We will only treat the discrete case and we will consider a\n",
      "method based on log-linear models which are the subject of\n",
      "the next chapter.\n",
      "\n",
      "17.5 Bibliographic Remarks\n",
      "\n",
      "Thorough treatments of undirected graphs can be found in Whit-\n",
      "taker (1990) and Lauritzen (1996). Some of the exercises below\n",
      "are adapted from Whittaker (1990).\n",
      "\n",
      "17.6 Exercises\n",
      "\n",
      "1. Consider random variables (X,,X2,X3). In each of the\n",
      "following cases, draw a graph that has the given indepen-\n",
      "dence relations.\n",
      "\n",
      "(a) X; X53 | Xo.\n",
      "(b) Xy HE Xp | Xg and Xy IX | Xo.\n",
      "\n",
      "(c) X, IX, | X3 and X, UX; | Xz and X, UX; |X).\n",
      "\n",
      "2. Consider random variables (X,, X), X3, X,). In each of the\n",
      "following cases, draw a graph that has the given indepen-\n",
      "dence relations.\n",
      "\n",
      " \n",
      "\n",
      "(a) X; UX | Xo, X, and X, WX, | Xo, X3 and Xy IX, |\n",
      "X1,X3.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-301.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1F08>\n",
      "(b) Xi 1 Xs\n",
      "i Ki\n",
      "\n",
      "(c) X, IX\n",
      "\n",
      ". A conditiona\n",
      "\n",
      "17.6 Exercises 305\n",
      "\n",
      "xX, Xe xX\n",
      "X3\n",
      "FIGURE 17.9.\n",
      "XG X, X3 X4\n",
      "\n",
      "FIGURE 17.10.\n",
      "\n",
      "X3,.X4 and X, I X3 | Xo, X4 and Xj ILX; |\n",
      "\n",
      "Xy,X, and Xy WX, | X1,X3.\n",
      "\n",
      "independence between a pair of variables is\n",
      "\n",
      "minimal if it is not possible to use the Separation The-\n",
      "\n",
      "orem to elimi\n",
      "\n",
      "nate any variable from the conditioning set,\n",
      "\n",
      "i.e. from the right hand side of the bar (Whittaker 1990).\n",
      "\n",
      " \n",
      "\n",
      "Write down t\n",
      "\n",
      "he minimal conditional independencies from:\n",
      "\n",
      "(a) Figure 17.9; (b) Figure 17.10; (c) Figure 17.11; (d)\n",
      "\n",
      "Figure 17.12.\n",
      "\n",
      ". Here are breast cancer data on diagnostic center (X;),\n",
      "nuclear grade (X2), and survival (X53):\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-302.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8708>\n",
      "306 17. Undirected Graphs and Conditional Independence\n",
      "\n",
      "X Xs\n",
      "\n",
      "X, XxX\n",
      "\n",
      "FIGURE 17.11.\n",
      "\n",
      " \n",
      "\n",
      "x Xs Xs\n",
      "\n",
      "FIGURE 17.12.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-303.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1DC8>\n",
      "17.6 Exercises 307\n",
      "\n",
      " \n",
      "\n",
      "Xj malignant malignant benign benign\n",
      "\n",
      " \n",
      "\n",
      "X3 died survived died survived\n",
      "xX, Boston 35 59 AT 112\n",
      "Glamorgan 42 77 26 76\n",
      "\n",
      " \n",
      "\n",
      "(a) Treat this as a multinomial and find the maximum\n",
      "likelihood estimator.\n",
      "\n",
      "(b) If someone has a tumour classified as benign at the\n",
      "Glamorgan clinic, what is the estimated probability that\n",
      "they will die? Find the standard error for this estimate.\n",
      "\n",
      "(c) Test the following hypotheses:\n",
      "\n",
      "X, UW X|X3 versus X, WXy|X3\n",
      "X, 0 X3|X2 versus X, WX3|X2\n",
      "X_ I X3|X, versus =X» UX3|X,\n",
      "\n",
      "Based on the results of your tests, draw and interpret the\n",
      "resulting graph.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-304.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148918CD9C8>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-305.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1B08>\n",
      "18\n",
      "Loglinear Models\n",
      "\n",
      "In this chapter we study loglinear models which are useful for\n",
      "modelling multivariate discrete data. There is a strong connec-\n",
      "tion between loglinear models and undirected graphs. Parts of\n",
      "this Chapter draw on the material in Whittaker (1990).\n",
      "\n",
      "18.1 The Loglinear Model\n",
      "\n",
      "Let X = (Xj,...,Xm) be a random vector with probability\n",
      "function\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "f(x) = P(X =2) = P(X, = 2,...,Xm = Im)\n",
      "\n",
      "where t = (21,...,2m). Let r; be the number of values that\n",
      "X; takes. Without loss of generality, we can assume that X; €\n",
      "{Og iesenss f= 1}. Suppose now that we have n such random vec-\n",
      "tors. We can think of the data as a sample from a Multinomial\n",
      "with N=r, Xrg X-+++ X 1, categories. The data can be repre-\n",
      "sented as counts inary X72 X+++XTm table. Let p = (pi,---, Py)\n",
      "denote the multinomial parameter.\n",
      "\n",
      "This is page 309\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-306.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1F08>\n",
      "310 18. Loglinear Models\n",
      "\n",
      "Let S = {1,...,m}. Given a vector # = (21,...,2m) anda\n",
      "subset A CS, let #4 = (a;: j € A). For example, if A = {1,3}\n",
      "then x, = (21,23).\n",
      "\n",
      " \n",
      "\n",
      "Theorem 18.1 The joint probability function f(x) of a single\n",
      "\n",
      "random vector X = (X,,...,Xm) can be written as\n",
      "log f(2) = So vale) (18.1)\n",
      "ACS\n",
      "where the sum is over all subsets A of S = {1,...,m} and the\n",
      "\n",
      "w’s satisfy the following conditions:\n",
      "1. up(x) is a constant;\n",
      "2. For every A CS, w(x) is only a function of x4 and not\n",
      "the rest of the ris.\n",
      "3. Ift€ A and x; =0, then W4(x) =0.\n",
      "\n",
      "The formula in equation (18.1) is called the log-linear ex-\n",
      "pansion of f. Note that this is the probability function for a\n",
      "single draw. Each q4() will depend on some unknown parame-\n",
      "ters 84. Let 8 = (84: A CS) be the set of all these parameters.\n",
      "We will write f(x) = f(x; 8) when we want to estimate the de-\n",
      "\n",
      " \n",
      "\n",
      "pendence on the unknown parameters {.\n",
      "In terms of the multinomial, the parameter space is\n",
      "\n",
      "N\n",
      "P= {p= (p1,---pw) pj 0, Sop) = 1}.\n",
      "j=l\n",
      "\n",
      "This is an N — 1 dimensional space. In the log-linear represen-\n",
      "ation, the parameter space is\n",
      "\n",
      "O= {8 = (f:,.--Bn): 6 =lp),pe P}\n",
      "\n",
      "where {(p) is the set of 9 values associated with p. The set © is\n",
      "a N —1 dimensinal surface in RY. We can always go back and\n",
      "forth betwee the two parameterizations we can write 3 = 5(p)\n",
      "\n",
      "and p= p({).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-307.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907D1988>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.1 The Loglinear Model 311\n",
      "\n",
      "Example 18.2 Let X ~ Bernoulli(p) where 0 < p <1. We can\n",
      "write the probability mass function for X as\n",
      "\n",
      "f(a) =p*(L—p)'* =p ps\n",
      "for x=0,1, where p) =p and pp =1—p. Hence,\n",
      "\n",
      "log f (x) = up(x) + vi (x)\n",
      "where\n",
      "\n",
      "vol) = log(ps)\n",
      "wi(z) = «log (4).\n",
      "\n",
      "Notice that yg(a) is a constant (as a function of x) and y1(2)\n",
      "0 when « = 0. Thus the three conditions of the Theorem hold.\n",
      "\n",
      "  \n",
      "\n",
      "The loglinear parameters are\n",
      "\n",
      "f 6 Pp\n",
      "Bo =log(p2), 31 = log (2) .\n",
      "P2\n",
      "The original, multinomial parameter space is P = {(p,,p2) :\n",
      "pj > 0,pi+p2=1}. The log-linear parameter space is\n",
      "\n",
      "© = {(o, fi) ER’: et 4 6% = 1}\n",
      "\n",
      "Given (p,,p2) we can solve for (8, 5). Conversely, given (Bo, 31)\n",
      "we can solve for (p,,p2).\n",
      "\n",
      "Example 18.3 Let X = (X,,X2) where X, € {0,1} and Xz €\n",
      "{0,1,2}. The joint distribution of n such random vectors is a\n",
      "multinomial with 6 categories. The multinomial parameters can\n",
      "be written as a 2-by-3 table as follows:\n",
      "\n",
      " \n",
      "\n",
      "multinomial wy 0 1 2\n",
      "r, 0 Poo Por Po2\n",
      "\n",
      "1 Pio Pu Pi2\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-308.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907E92C8>\n",
      "312 18. Loglinear Models\n",
      "\n",
      "The n data vectors can be summarized as counts:\n",
      "data © 0 1 2:\n",
      "xr, 0 Coo Cor Coz\n",
      "1 Cio Cu Cr\n",
      "\n",
      "For x = (x1, %), the log-linear expansion takes the form\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "log f (x) = Wo (a) + vi (2) + ga(x) + Via(2)\n",
      "where\n",
      "\n",
      "u(x) = log poo\n",
      "\n",
      "tilt) = x log (22)\n",
      "ola) = I(x, =1) log (=) + I (29 = 2) log ()\n",
      "\n",
      "Poo Poo\n",
      "\n",
      "= I =1, 22 =1)log (a) + (2 = 1,2 = 2) log (a) ;\n",
      "PoiPi0 Po2Pi0\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Convince yourself that the three conditions on the w)’s of the\n",
      "theorem are satisfied. The six parameters of this model are:\n",
      "\n",
      " \n",
      "\n",
      "Bi = log poo Ba = log (=) Bs = log (2)\n",
      "\n",
      "6 =log (#2) 5 =log (Butte) Fg = log (BEE)\n",
      "\n",
      "The next Theorem gives an easy way to check for conditional\n",
      "independence in a loglinear model.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 18.4 Let (X,,X;,X.) be a partition of a vectors (X,...,Xm)-\n",
      "Then X,UX,|Xq if and only if all the y-terms in the log-linear\n",
      "expansion that have at least one coordinate in b and one coordi-\n",
      "\n",
      "nate inc are 0.\n",
      "\n",
      "To prove this Theorem, we will use the following Lemma\n",
      "whose proof follows easily from the definition of conditional in-\n",
      "dependence.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-309.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x148907C8708>\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path is given for for 64 bit installer\n",
    "pytesseract.pytesseract.tesseract_cmd = \"C:/Program Files/Tesseract-OCR/tesseract.exe\"\n",
    "\n",
    "f = []\n",
    "t = []\n",
    "input_dir = r'C:/Users/madarchod/Desktop/FreeL/yele/Images/'\n",
    "\n",
    "for root, dirs, filenames in os.walk(input_dir):\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            print(filename)\n",
    "            f.append(filename)\n",
    "            #print(f)\n",
    "            img = Image.open(input_dir + filename )\n",
    "            print(img)\n",
    "            text = pytesseract.image_to_string(img, lang = 'eng')\n",
    "            t.append(text)\n",
    "            print(text)\n",
    "            print('-='*20)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "df = pd.DataFrame(list(zip(f, t)),columns=['file_Name','Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install session-info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import session_info\n",
    "session_info.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
