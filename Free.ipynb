{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Downloading https://files.pythonhosted.org/packages/93/f3/4fec7dabe8802ebec46141345bf714cd1fc7d93cb74ddde917e4b6d97d88/pdfminer.six-20201018-py3-none-any.whl (5.6MB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six) (2.1.0)\n",
      "Requirement already satisfied: chardet; python_version > \"3.0\" in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six) (3.0.4)\n",
      "Requirement already satisfied: cryptography in c:\\programdata\\anaconda3\\lib\\site-packages (from pdfminer.six) (2.7)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography->pdfminer.six) (1.12.3)\n",
      "Requirement already satisfied: asn1crypto>=0.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography->pdfminer.six) (1.0.1)\n",
      "Requirement already satisfied: six>=1.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from cryptography->pdfminer.six) (1.12.0)\n",
      "Requirement already satisfied: pycparser in c:\\programdata\\anaconda3\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography->pdfminer.six) (2.19)\n",
      "Installing collected packages: pdfminer.six\n",
      "Successfully installed pdfminer.six-20201018\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/38/2f07f859344b26f7f21efc90544c64b1f001ecdb491f2246c766d0bfa940/PyMuPDF-1.18.16-cp37-cp37m-win_amd64.whl (5.4MB)\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.18.16\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdf2image in c:\\programdata\\anaconda3\\lib\\site-packages (1.16.0)\n",
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from pdf2image) (6.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##converting pdf to images\n",
    "\n",
    "import os, subprocess\n",
    "\n",
    "pdf_dir = r\"C:\\Users\\jkl\\Desktop\\FreeL\\yele\"\n",
    "os.chdir(pdf_dir)\n",
    "\n",
    "pdftoppm_path = r\"C:\\Program Files (x86)\\poppler-0.68.0\\bin\\pdftoppm.exe\"\n",
    "\n",
    "for pdf_file in os.listdir(pdf_dir):\n",
    "\n",
    "    if pdf_file.endswith(\".pdf\"):\n",
    "\n",
    "        subprocess.Popen('\"%s\" -jpeg %s out' % (pdftoppm_path, pdf_file), shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (6.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in c:\\programdata\\anaconda3\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from pytesseract) (6.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tesseract\n",
      "  Downloading https://files.pythonhosted.org/packages/8d/b7/c4fae9af5842f69d9c45bf1195a94aec090628535c102894552a7a7dbe6c/tesseract-0.1.3.tar.gz (45.6MB)\n",
      "Building wheels for collected packages: tesseract\n",
      "  Building wheel for tesseract (setup.py): started\n",
      "  Building wheel for tesseract (setup.py): finished with status 'done'\n",
      "  Created wheel for tesseract: filename=tesseract-0.1.3-cp37-none-any.whl size=45562576 sha256=54073a3793b64fbd68a60428e9645fee40e0a523b2ad226d3cc3e0c9db8a2051\n",
      "  Stored in directory: C:\\Users\\Madarchod\\AppData\\Local\\pip\\Cache\\wheels\\82\\1f\\d9\\24797b123379e4ea9511cf660835468b62dad609634cad2aba\n",
      "Successfully built tesseract\n",
      "Installing collected packages: tesseract\n",
      "Successfully installed tesseract-0.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (6.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytesseract in c:\\programdata\\anaconda3\\lib\\site-packages (0.3.8)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from pytesseract) (6.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tesseract in c:\\programdata\\anaconda3\\lib\\site-packages (0.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "out-001.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243B9470708>\n",
      "All of Statistics: A Concise Course in\n",
      "Statistical Inference\n",
      "\n",
      "Brief Contents\n",
      "\n",
      "1. INtrOdUCtON.....c..cccreeccnveaveaecavecnseneeraevenveasseneeaeceesevamenseens 11\n",
      "Part I Probability\n",
      "\n",
      "2, Probability...cccsssassssvssesesecevavsvevecvevsvveverevesesveseesevevesesoevees 21\n",
      "3s Randoin Variables scsieessssvsseesveveseveeuneecserveanseweserecswevsveswess 37\n",
      "A, Expectation sccssevecccssswenssercaeesescecesescaveresszaavesseseewssasxesust 69\n",
      "§, Equalities sssscsssvevecncassssacasacesnacssacceuaresexeste ciesereswxessesoaaees 85\n",
      "6. Convergence of Random Variables.............csssecscescecsseeceeeeesen 89\n",
      "\n",
      "Part II Statistical Inference\n",
      "7. Models, Statistical Inference and Learning...............ssecseceeees 105\n",
      "\n",
      "8. Estimating the CDF and Statistical Functionals\n",
      "\n",
      " \n",
      "\n",
      "9, The Bootstrap cccsssisvsssecvscevssevscsevesevsscesweaveesveeeveswresescwews 129\n",
      "10. Parametric Inference............sscessceccecsseeecceccssceceeseeceeceeoes 145\n",
      "11. Hypothesis Testing and p-values..............csececeesececseeeeeceeees 179\n",
      "12, Bayesian Inference sscsssvscssaszasersspesansansewsenravsavesncenenseen sans 205\n",
      "13. Statistical Decision Theory............cscccsecescesceeeeceecesceeseesees 227\n",
      "\n",
      "Part III Statistical Models and Methods\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-002.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243B75A2388>\n",
      "14.\n",
      "\n",
      "15.\n",
      "\n",
      "16.\n",
      "\n",
      "17.\n",
      "\n",
      "18.\n",
      "\n",
      "19.\n",
      "\n",
      "20.\n",
      "\n",
      "21.\n",
      "\n",
      "22.\n",
      "\n",
      "23.\n",
      "\n",
      "24,\n",
      "\n",
      "25,\n",
      "\n",
      "Linear Regressions: cicsscsssssesvesswesswscecsvensaysvenvevsoesvevewenese 245\n",
      "Multivariate. Model Sscssisvsecarssssssoesssceazesnesoosesssavenersevess 269\n",
      "Inference about Independence.............ssccseessececesceceseeseeee 279\n",
      "Undirected Graphs and Conditional Independence............... 297\n",
      "Loglinear Models.............csecsecsecseccecseceececeeceecenseeseeceees 309\n",
      "Causal Inference............ccsecsecsscseceecseccecsecescesceesseeeeeeees 327\n",
      "Directed. Graphs .secssessssssecovsssvanessseenecsssevsssesessevesveaeess 343\n",
      "\n",
      "Nonparametric curve Estimation..............sccscsessceeseeceeeees 359\n",
      "\n",
      "Smoothing Using Orthogonal Functions................cseceeseeeees 393\n",
      "ClaSsifi Cat Ol yesesesssrevssssnserexemmrerenmmnesmrueeseeereens 425\n",
      "\n",
      "SCOCHASTIC PROCESSES: savascssaasassserssnsaserserennvenssesanacnaaeaeeres 473\n",
      "Simulation Methods............csccsscsecceccececesccscceseeseeesseeees 505\n",
      "\n",
      "Appendix Fundamental Concepts in Inference\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-003.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243B9459AC8>\n",
      "Chapter 1\n",
      "\n",
      "Introduction\n",
      "\n",
      "The goal of this book is to provide a broad background in probability and\n",
      "statistics for students in statistics, computer science (especially data min-\n",
      "ing and machine learning), mathematics, and related disciplines. This book\n",
      "covers a much wider range of topics than a typical introductory text on\n",
      "mathematical statistics. It includes modern topics like nonparametric curve\n",
      "estimation, bootstrapping and classification, topics that are usually relegated\n",
      "to follow-up courses. The reader is assumed to know calculus and a little lin-\n",
      "ear algebra. No previous knowledge of probability and statistics is required.\n",
      "The text is suitable for advanced undergraduates and graduate students.\n",
      "\n",
      "Statistics, data mining and machine learning are all concerned with\n",
      "collecting and analyzing data. For some time, statistics research was con-\n",
      "ducted in statistics departments while data mining and machine learning re-\n",
      "search was conducted in computer science departments. Statisticians thought\n",
      "that computer scientists were reinventing the wheel. Computer scientists\n",
      "thought that statistical theory didn’t apply to their problems.\n",
      "\n",
      "Things are changing. Statisticians now recognize that computer scien-\n",
      "tists are making novel contributions while computer scientists now recognize\n",
      "the generality of statistical theory and methodology. Clever data mining al-\n",
      "gorithms are more scalable than statisticians ever though possible. Formal\n",
      "statistical theory is more pervasive than computer scientists had realized. All\n",
      "agree students who deal with the analysis of data should be well grounded\n",
      "in basic probability and mathematical statistics. Using fancy tools like neu-\n",
      "\n",
      "11\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-004.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80ED788>\n",
      "12\n",
      "\n",
      "CHAPTER 1. INTRODUCTION\n",
      "\n",
      "ral nets, boosting and support vector machines without understanding basic\n",
      "statistics is like doing brain surgery before knowing how to use a bandaid.\n",
      "But where can stu\n",
      "Nowhere. At least, that was my conclusion when my computer science col-\n",
      "leagues kept asking me:\n",
      "standing of modern statistics quickly?” The typical mathematical statistic\n",
      "course spends too muc\n",
      "methods, two dimensional integrals etc.) at the expense of covering modern\n",
      "\n",
      "ents learn basic probability and statistics quickly?\n",
      "“Where can I send my students to get a good a under-\n",
      "\n",
      "time on tedious and uninspiring topics (counting\n",
      "\n",
      "concepts (bootstrapping, curve estimation, graphical models etc.). So I set\n",
      "out to redesign our un\n",
      "matical statistics. This\n",
      "the main features of this book.\n",
      "\n",
      "1.\n",
      "\n",
      "lergraduate honors course on probability and mathe-\n",
      "book arose from that course. Here is a summary of\n",
      "\n",
      " \n",
      "\n",
      "The book is suital\n",
      "\n",
      "le for honors undergraduates in math, statistics and\n",
      "\n",
      "computer science as well as graduate students in computer science and\n",
      "other quantitative fields.\n",
      "\n",
      ". I cover advanced\n",
      "\n",
      "topics that are traditionally not taught in a first\n",
      "\n",
      "course. For example, nonparametric regression, bootstrapping, den-\n",
      "sity estimation and graphical models.\n",
      "\n",
      ". I have omitted topics in probability that do not play a central role\n",
      "\n",
      "in statistical inference. For example, counting methods are virtually\n",
      "\n",
      "absent.\n",
      "\n",
      ". In general, I try\n",
      "emphasizing concepts.\n",
      "\n",
      "to avoid belaboring tedious calculations in favor of\n",
      "\n",
      ". I cover nonparametric inference before parametric inference. This is the\n",
      "\n",
      "opposite of most statistics books but I believe it is the right way to do it.\n",
      "Parametric models are unrealistic and pedagogically unnatural. (How\n",
      "would we know the everything about the distribution except for one or\n",
      "\n",
      "two parameters?)\n",
      "\n",
      "I introduce statistical functionals and bootstrapping\n",
      "\n",
      " \n",
      "\n",
      "very early and students find this quite natural.\n",
      "\n",
      ". I abandon the usual “First Term = Probability” and “Second Term\n",
      "\n",
      "= Statistics” approach. Some students only take the first half and it\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-005.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80EDDC8>\n",
      "13\n",
      "\n",
      "would be a crime if they did not see any statistical theory. Furthermore,\n",
      "probability is more engaging when students can see it put to work in\n",
      "the context of statistics.\n",
      "\n",
      "7. The course moves very quickly and covers much material. My col-\n",
      "leagues joke that I cover all of statistics in this course and hence the\n",
      "title. The course is demanding but I have worked hard to make the ma-\n",
      "terial as intuitive as possible so that the material is very understandable\n",
      "despite the fast pace. Anyway, slow courses are boring.\n",
      "\n",
      "8. As Richard Feynman pointed out, rigor and clarity are not synony-\n",
      "mous. I have tried to strike a good balance. To avoid getting bogged\n",
      "down in uninteresting technical details, many results are stated without\n",
      "proof. The bibliographic references at the end of each chapter point\n",
      "the student to appropriate sources.\n",
      "\n",
      "9. On my website are files with R code which students can use for doing all\n",
      "the computing. However, the book is not tied to R and any computing\n",
      "language can be used.\n",
      "\n",
      "The first part of the text is concerned with probability theory, the formal\n",
      "language of uncertainty which is the basis of statistical inference. The basic\n",
      "problem that we study in probability is:\n",
      "\n",
      "Given a data generating process, what are the properties of the out-\n",
      "comes?\n",
      "\n",
      "The second part of the book is about statistical inference and its close cousins,\n",
      "data mining and machine learning. The basic problem of statistical inference\n",
      "is the inverse of probability:\n",
      "\n",
      "Given the outcomes, what can we say about the process that gener-\n",
      "ated the data?\n",
      "\n",
      "These ideas are illustrated in Figure 1.1. Prediction, classification, clus-\n",
      "tering and estimation are all special cases of statistical inference. Data anal-\n",
      "ysis, machine learning and data mining are various names given to the prac-\n",
      "tice of statistical inference, depending on the context. The second part of\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-006.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9888>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 CHAPTER 1. INTRODUCTION\n",
      "\n",
      "Probability\n",
      "\n",
      "a\n",
      "\n",
      " \n",
      " \n",
      "\n",
      "Observed data\n",
      "\n",
      "Data generating process\n",
      "\n",
      "   \n",
      "\n",
      "i ae\n",
      "\n",
      "Inference and Data Mining\n",
      "\n",
      "Figure 1.1: Probability and inference.\n",
      "\n",
      "the book contains one more chapter on probability that covers stochastic\n",
      "processes including Markov chains.\n",
      "\n",
      "I have drawn heavily on other books in many places. Most chapters\n",
      "contain a section called Bilbliographic Remarks which serves both to ac-\n",
      "knowledge my debt to other authors and to point readers to other useful\n",
      "references. I would especially like to mention the books by DeGroot and\n",
      "Schervish (2002) and Grimmett and Stirzaker (1982) from which I adapted\n",
      "many examples and excercises.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-007.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80ED788>\n",
      "Statistics/Data Mining Dictionary\n",
      "\n",
      "Statisticians and computer scientists often use different language for the\n",
      "same thing. Here is a dictionary that the reader may want to return to\n",
      "\n",
      "throughout the course.\n",
      "\n",
      " \n",
      "\n",
      "Statistics Computer Science Meaning\n",
      "estimation learning using data to estimate\n",
      "an unknown quantity\n",
      "classification supervised learning predicting a discrete Y from X € ¥\n",
      "clustering unsupervised learning putting data into groups\n",
      "data training sample (X1,Vi),---; (Xn, Xn)\n",
      "covariates features the X;,’s\n",
      "classifier hypothesis a map from covariates to outcomes\n",
      "hypothesis = subset of a parameter space O\n",
      "confidence interval — interval that contains unknown quantity\n",
      "with a prescribed frequency\n",
      "directed acyclic graph Bayes net multivariate distribution with\n",
      "\n",
      "Bayesian inference\n",
      "\n",
      "frequentist inference\n",
      "\n",
      "large deviation bounds\n",
      "\n",
      "Bayesian inference\n",
      "\n",
      "PAC learning\n",
      "\n",
      "specified conditional\n",
      "\n",
      "independence relations\n",
      "\n",
      "statistical methods for using data\n",
      "\n",
      "to update subjective beliefs\n",
      "\n",
      "statistical methods for producing\n",
      "\n",
      "point estimates and confidence intervals\n",
      "with guarantees on frequency behavior\n",
      "uniform bounds on probability of errors\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-008.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "16 CHAPTER 1. INTRODUCTION\n",
      "Notation\n",
      "Symbol Meaning\n",
      "R real numbers\n",
      "infrea f(x) infimum: the largest number y such that y < f(x) for allae A\n",
      "think of this as the minimum of f\n",
      "suPread (2) supremum: the smallest number y such that y > f(z) for all x € /\n",
      "think of this as the maximum of f\n",
      "n! 1) x (n—2)x+++x3x2x1\n",
      "(i) may\n",
      "T(a) Gamma function I ye te Udy\n",
      "w outcome\n",
      "Q sample space (set of outcomes)\n",
      "A event (subset of 2)\n",
      "I4(w) indicator function; 1 if w € A and 0 otherwise\n",
      "P(A) probability of event A\n",
      "|A number of points in set A\n",
      "Fx cumulative distribution function\n",
      "fx probability density (or mass) function\n",
      "XwF X has distribution F’\n",
      "Xof X has density f\n",
      "x4y X and Y have the same distribution\n",
      "X,.--5 Xn F sample of size n from F’\n",
      "a) standard Normal probability density\n",
      "od standard Normal distribution function\n",
      "Lee upper a quantile of N(0,1) ie. @-1(1— a)\n",
      "\n",
      "E(X) = frdF(z)\n",
      "E(r(X)) = fr(@)dF(x)\n",
      "V(X)\n",
      "\n",
      "Cov( X,Y)\n",
      "X1,.--)Xn\n",
      "n\n",
      "\n",
      "P:\n",
      "\n",
      "s.\n",
      "\n",
      "qm\n",
      "\n",
      "Xn & N(u,02)\n",
      "\n",
      "expected value (mean) of random variable X\n",
      "expected value (mean) of r(X)\n",
      "\n",
      "variance of random variable X\n",
      "\n",
      "covariance between X and Y\n",
      "\n",
      "data\n",
      "\n",
      "sample size\n",
      "\n",
      "convergence in probability\n",
      "\n",
      "convergence in distribution\n",
      "\n",
      "convergence in quadratic mean\n",
      "\n",
      "(X, — p)/on ~ N(0,1)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-009.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "Notation Continued\n",
      "\n",
      " \n",
      "\n",
      "Symbol Meaning\n",
      "\n",
      "g statistical model; a set of distribution functions,\n",
      "density functions or regression functions\n",
      "\n",
      "0 parameter\n",
      "\n",
      "a estimate of parameter\n",
      "\n",
      "T(F) statistical functional (the mean, for example)\n",
      "\n",
      "L£,(0) likelihood function\n",
      "\n",
      "In = 0(an) Zn/Gn 0\n",
      "\n",
      "In =O(Gn) —|an/an| is bounded for large n\n",
      "\n",
      "Xn =op(an)  Xn/an—+0\n",
      "\n",
      "Xn = Op(an)\n",
      "\n",
      "|Xn/an| is bounded in probability for large n\n",
      "\n",
      "17\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-010.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "18 CHAPTER 1. INTRODUCTION\n",
      "\n",
      "Useful Math Facts\n",
      "\n",
      "f= ye Baltat ht.\n",
      "\n",
      "UE, = GE ford<r<i\n",
      "\n",
      "limp soo (1+ 4)” = e*\n",
      "\n",
      "The Gamma function is is defined by (a) = f[~ y**eYdy for a > 0. If\n",
      "\n",
      "a > 1 then [(a) = (a—1)P(a—1). If n is an integer then I(n) = (n—1)!.\n",
      "Some special values are: (1) = 1 and (1/2) = V7.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-011.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "Part I\n",
      "\n",
      "Probability\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-012.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-013.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "Chapter 2\n",
      "Probability\n",
      "\n",
      "2.1 Introduction\n",
      "\n",
      "Probability is the mathematical language for quantifying uncertainty. We\n",
      "can apply probability theory to a diverse set of problems, from coin flipping to\n",
      "the analysis of computer algorithms. The starting point is to specify sample\n",
      "space, the set of possible outcomes.\n",
      "\n",
      "2.2 Sample Spaces and Events\n",
      "\n",
      "The sample space Q, is the set of possible outcomes of an experiment.\n",
      "Points w in Q are called sample outcomes or realizations. Events are\n",
      "subsets of Q.\n",
      "\n",
      "Example 2.1 If we toss a coin twice thenQ = {HH, HT,TH,TT}. The event\n",
      "that the first toss is heads is A= {HH, HT}. @\n",
      "\n",
      "Example 2.2 Let w be the outcome of a measurement of some physical quan-\n",
      "tity, for example, temperature. Then Q = R = (—00, 00). The event that the\n",
      "measurement is larger than 10 but less than or equal to 23 is A = (10, 23].\n",
      "\n",
      "Example 2.3 If we toss a coin forever then the sample space is the infinite set\n",
      "Q= {uw = (W1,W2,W3,---,), Wi E {H,T}}.\n",
      "\n",
      "Let E be the event that the first head appears on the third toss. Then\n",
      "\n",
      "E= { (1,42, 08,.--5) : wi = T,we = T,w3 = H, w; € {H,T} for i> 3}. a\n",
      "\n",
      "21\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-014.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "22 CHAPTER 2. PROBABILITY\n",
      "\n",
      "Given an event A, let Ac = {w € Q; w ¢ A} denote the complement\n",
      "of A. Informally, A° can be read as “not A.” The complement of 2 is the\n",
      "empty set (. The union of events A and B is defined AU B= {wEQ; we\n",
      "Aorw € Borw € both} which can be thought of as “A or B.” If Aj, Ao,..-\n",
      "is a sequence of sets then\n",
      "\n",
      "00\n",
      "UA= {w €2: we A; for at least one i}.\n",
      "i=1\n",
      "\n",
      "The intersection of A and B is Af] B = {w € Q; w € A and w € B} read\n",
      "“A and B.” Sometimes we write A().B as AB. If Aj, Ao,... is a sequence\n",
      "of sets then\n",
      "\n",
      "eS\n",
      "N= (wee: w € A; for all i}.\n",
      "‘et\n",
      "\n",
      "Let A— B= {w: we A,w ¢ B}. If every element of A is also contained in\n",
      "B we write A C B or, equivalently, B D A. If A is a finite set, let |A| denote\n",
      "the number of elements in A. See Table 1 for a summary.\n",
      "\n",
      " \n",
      "\n",
      "Table 1. Sample space and events.\n",
      "2 sample space\n",
      "w outcome\n",
      "A event (subset of 2)\n",
      "|A| number of points in A (if A is finite)\n",
      "AC complement of A (not A)\n",
      "AUB union (A or B)\n",
      "Af)B or AB intersection(A and B)\n",
      "A-B set difference (points in A that are not in B)\n",
      "ACB set inclusion (A is a subset of or equal to B)\n",
      "0 null event (always false)\n",
      "2 true event (always true)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "We say that A;, Ao, ... are disjoint or are mutually exclusive if A;(] Aj =\n",
      "@ whenever i 4 j. For example, A; = [0,1), Ay = [1,2), A3 = [2,3),... are\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-015.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "2.3. PROBABILITY 23\n",
      "disjoint. A partition of Q is a sequence of disjoint sets A,,Ao,... such that\n",
      "US, Ai = &. Given an event A, define the indicator function of A by\n",
      "\n",
      "1 ifweA\n",
      "\n",
      "ale) = He ea) = { ifwé A.\n",
      "\n",
      "A sequence of sets Aj, Ao,... is monotone increasing if A; C A) C\n",
      "\n",
      "- and we define limp oo An = U2, Ai- A sequence of sets Aj, Ao,... i\n",
      "\n",
      "monotone decreasing if A; D A) > --- and then we define lim,-,., An\n",
      "(2, Ai. In either case, we will write A, — A.\n",
      "\n",
      "Example 2.4 Let Q = R and let A; = [0,1/i) fori =1,2,.... Then U2, Ai =\n",
      "(0,1) and (\\2, Ai = {0}. If instead we define A; = (0,1/i) then UX, Ai =\n",
      "(0,1) and 2, A: = 0.\n",
      "\n",
      "2.3 Probability\n",
      "\n",
      "We want to assign a real number P(A) to every event A, called the prob-\n",
      "ability of A. We also call P a probability distribution or a probability\n",
      "measure. To qualify as a probability, P has to satisfy three axioms:\n",
      "\n",
      " \n",
      "\n",
      "Definition 2.5 A function P that assigns a real number P(A) to each\n",
      "event A is a probability distribution or a probability measure if\n",
      "it satisfies the following three axioms:\n",
      "\n",
      "Axiom 1: P(A) > 0 for every A\n",
      "\n",
      "Axiom 2: P(Q) = 1\n",
      "\n",
      "Axiom 3: [f Aj, Ao,...\n",
      "\n",
      "P (U 4) = 5 *P(A).\n",
      "\n",
      "are disjoint then\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "There are many interpretations of P(A). The two common interpretations\n",
      "are frequencies and degrees of beliefs. In frequency interpretation, P(A) is\n",
      "\n",
      "It is not always pos-\n",
      "sible to assign a\n",
      "probability to every\n",
      "event A if the sam-\n",
      "ple space is large,\n",
      "such as the whole\n",
      "real line. Instead,\n",
      "we assign probabil-\n",
      "ities to a limited\n",
      "class of set called\n",
      "a o-field. See the\n",
      "technical appendix\n",
      "for details.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-016.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 CHAPTER 2. PROBABILITY\n",
      "\n",
      "the long run proportion of times that A is true in repetitions. For example,\n",
      "if we say that the probability of heads is 1/2, when mean that if we flip the\n",
      "coin many times then the proportion of times we get heads tends to 1/2 as\n",
      "the number of tosses increases. An infinitely long, unpredictable sequence of\n",
      "tosses whose limiting proportion tends to a constant is an idealization, much\n",
      "like the idea of a straight line in geometry. The degree-of-belief interpreta-\n",
      "tion is that P(A) measures an observer’s strength of belief that A is true.\n",
      "In either interpretation, we require that Axioms 1 to 3 hold. The difference\n",
      "in interpretation will not matter much until we deal with statistical infer-\n",
      "ence. There, the differing interpretations lead to two schools of inference:\n",
      "the frequentist and the Bayesian schools. We defer discussion until later.\n",
      "One can derive many properties of P from the axioms. Here are a few:\n",
      "\n",
      "PO) = 0\n",
      "ACB = P(A)<P(B)\n",
      "o< P(A) <1\n",
      "P(A°) = 1-P(A)\n",
      "AQ\\B=0 = P(AUB) =P(4) +P). (2.1)\n",
      "\n",
      "A less obvious property is given in the following Lemma.\n",
      "\n",
      "Lemma 2.6 For any events A and B, P(A\\ B) = P(A) + P(B) — P(AB).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "PROOF. Write AUB = (AB‘)U(AB)U(4°B) and note that these\n",
      "events are disjoint. Hence, making repeated use of the fact that P is ad-\n",
      "ditive for disjoint events, we see that\n",
      "\n",
      "P (4U2) P ((4B») Ute) '2) ))\n",
      ")\n",
      ")\n",
      "\n",
      "= P(AB) + sci + ee\n",
      "= P(AB*) + P(AB) + P(A°B) + P(AB) — P(AB)\n",
      "\n",
      "= P((4B) (4B) +P ((4°B) (4B) — P(AB)\n",
      "= P(A)+P(B)—P(AB). &\n",
      "\n",
      "Example 2.7 Two coin tosses. Let H, be the event that heads occurs on toss 1\n",
      "and let H» be the event that heads occurs on toss 2. If all outcomes are equally\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-017.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "2.4. PROBABILITY ON FINITE SAMPLE SPACES 25\n",
      "\n",
      "likely, that is, P({H1, H2}) = P({Hi,T}) = P({Ti, He}) = P({N, B}) =\n",
      "1/4, then P(H, U Hz) = P(Ai) + P(e) — P(A Aa) = + 5-4 = 3/4.\n",
      "\n",
      "Theorem 2.8 (Continuity of Probabilities.) If 4, + A then P(A,) — P(A) as\n",
      "n— oo.\n",
      "\n",
      "PROOF. Suppose that A, is monotone increasing so that Ay C Ay C ---.\n",
      "Let A = limp An = U2, 4i- Define B, = Ai, Bo = {fw EQ: we\n",
      "Az,w ¢€ Ai}, Bs = {w EQ: w € Az,w ¢ Ao,w ¢ Ai},... It can be\n",
      "shown that B,, Bo,... are disjoint, A, = UL, Ai = UL, B; for each n and\n",
      "Ue, Bi = UE; Ai. (See excercise 1.) From Axiom 3,\n",
      "\n",
      "P(A,) =P (U a) =S PB)\n",
      "\n",
      "and hence, using Axiom 3 again,\n",
      "\n",
      "oo\n",
      "\n",
      "Jim P(A,) = Jim So P(B) = )OP(B) =P (U a.) =P(4). &\n",
      "\n",
      "i=1\n",
      "\n",
      "2.4 Probability on Finite Sample Spaces\n",
      "\n",
      "Suppose that the sample space 2 = {w1,...,Wn} is finite. For example,\n",
      "if we toss a die twice, then has 36 elements: 2 = {(i, 7); i,7 € {1,...6}}.\n",
      "If each outcome is equally likely, then P(A) = |A|/36 where |A| denotes the\n",
      "number of elements in A. The probability that the sum of the dice is 11 is\n",
      "2/36 since there are two outcomes that correspond to this event.\n",
      "\n",
      "In general, if Q is finite and if each outcome is equally likely, then\n",
      "\n",
      "_l\n",
      "\n",
      "Pa)= 5.\n",
      "\n",
      "which is called the uniform probability distribution. To compute prob-\n",
      "abilities, we need to count the number of points in an event A. Methods\n",
      "for counting points are called combinatorial methods. We needn’t delve into\n",
      "these in any great detail. We will, however, need a few facts from counting\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-018.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "26 CHAPTER 2. PROBABILITY\n",
      "\n",
      "theory that will be useful later. Given n objects, the number of ways of\n",
      "ordering these objects is n! = n(n —1)(n — 2)---3+2-1. For convenience, we\n",
      "define 0! = 1. We also define\n",
      "\n",
      "(:) 7 ae (2.2)\n",
      "\n",
      "read “n choose k”, which is the number of distinct ways of choosing k objects\n",
      "from n. For example, if we have a class of 20 people and we want to select a\n",
      "committee of 3 students, then there are\n",
      "\n",
      "!\n",
      "20 _ 20! _ 20 x 19 x 18 1140\n",
      "3 3!17! 3x2x1\n",
      "\n",
      "possible committees. We note the following properties:\n",
      "n n n n\n",
      "(c)- (2-2. (@)= (02):\n",
      "2.5 Independent Events\n",
      "\n",
      "If we flip a fair coin twice, then the probability of two heads is 3 x 3. We\n",
      "multiply the probabilities because we regard the two tosses as independent.\n",
      "The formal definition of independence is as follows.\n",
      "\n",
      " \n",
      "\n",
      "Definition 2.9 Two events A and B are independent if\n",
      "P(AB) = P(A)P(B) (2.3)\n",
      "and we write AIL B. A set of events {A;: i € I} is independent if\n",
      "\n",
      "P (0 4) =[[P(4)\n",
      "\n",
      "ier ies\n",
      "\n",
      "for every finite subset J of I.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Independence can arise in two distinct ways. Sometimes, we explicitly\n",
      "assume that two events are independent. For example, in tossing a coin\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-019.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "2.5. INDEPENDENT EVENTS 27\n",
      "\n",
      "twice, we usually assume the tosses are independent which reflects the fact\n",
      "that the coin has no memory of the first toss. In other instances, we derive\n",
      "independence by verifying that P(AB) = P(A)P(B) holds. For example, in\n",
      "tossing a fair die, let A = {2,4,6} and let B = {1,2,3,4}. Then, A) B=\n",
      "{2,4}, P(AB) = 2/6 = P(A)P(B) = (1/2) x (2/3) and so A and B are\n",
      "independent. In this case, we didn’t assume that A and B are independent\n",
      "it just turned out that they were.\n",
      "\n",
      "Suppose that A and B are disjoint events, each with positive probabil-\n",
      "ity. Can they be independent? No. This follows since P(A)P(B) > 0 yet\n",
      "P(AB) = P(#) = 0. Except in this special case, there is no way to judge\n",
      "independence by looking a the sets in a Venn diagram.\n",
      "\n",
      "Example 2.10 Toss a fair coin 10 times. Let A = “at least one Head.” Let T;\n",
      "be the event that tails occurs on the j\" toss. Then\n",
      "\n",
      "P(A) = 1-P(A)\n",
      "= 1-—P(all tails)\n",
      "= 1-P(T:--T)\n",
      "= 1—P(%)P(2)---P(Tio) using independence\n",
      "\n",
      "1 10\n",
      "= 1-(=) ~.999. &\n",
      "(2)\n",
      "\n",
      "Example 2.11 Two people take turns trying to sink a basketball into a net.\n",
      "Person 1 succeeds with probability 1/3 while person 2 succeeds with probability\n",
      "1/4. What is the probability that person 1 succeeds before person 2? Let EB\n",
      "denote the event of interest. Let Aj be the event that the first success is\n",
      "by person 1 and that it occurs on trial number j. Note that A,, Ao,... are\n",
      "disjoint and that E = U7, Aj. Hence,\n",
      "\n",
      "Now, P(A;) = 1/3. Ay occurs if we have the sequence 1 misses, 2 misses, 1\n",
      "succeeds. This has probability P(A2) = (2/3)(3/4)(1/3) = (1/2)(1/3). Fol-\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-020.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "28 CHAPTER 2. PROBABILITY\n",
      "\n",
      "lowing this logic we see that P(A;) = (1/2))~1(1/3). Hence,\n",
      "Salvi” Tea7Tr\"* 8\n",
      "\n",
      "Ppe)=y-=(=) =2N°(=) =2.\n",
      "@=¥3(3) -32G) =3\n",
      "\n",
      "Here we used that fact that, if 0 <r <1 then ee ri=r*/(1—r). ©\n",
      "\n",
      " \n",
      "\n",
      "Summary of Independence\n",
      "1. Aand B are independent if P(AB) = P(A)P(B).\n",
      "2. Independence is sometimes assumed and sometimes derived.\n",
      "\n",
      "3. Disjoint events with positive probability are not independent.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "2.6 Conditional Probability\n",
      "\n",
      "Assuming that P(B) > 0, we define the conditional probability of A given\n",
      "that B has occurred as follows.\n",
      "\n",
      " \n",
      "\n",
      "Definition 2.12 If P(B) > 0 then the conditional probability of A\n",
      "\n",
      "given B is\n",
      "P(AB)\n",
      "\n",
      "P(AIB) = Som\n",
      "\n",
      " \n",
      "\n",
      ". (2.4)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Think of P(A|B) as the fraction of times A occurs among those in which\n",
      "B occurs. Here are some facts about conditional probabilities. For any fixed\n",
      "B such that P(B) > 0, P(-|B) is a probability i.e. it satisfies the three axioms\n",
      "of probability. In particular, P(A|B) > 0, P(Q|B) = 1 and if Aj, Ao,... are\n",
      "disjoint then P(U, Ai|B) = OX, P(Ai|B). But it is in general not true that\n",
      "P(A|BUC) = P(A|B) + P(A|C). The rules of probability apply to events\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-021.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "2.6. CONDITIONAL PROBABILITY 29\n",
      "\n",
      "on the left of the bar. In general it is not the case that P(A|B) = P(B|A).\n",
      "People get this confused all the time. For example, the probability of spots\n",
      "given you have measles is 1 but the probability that you have measles given\n",
      "that you have spots is not 1. In this case, the difference between P(A|B) and\n",
      "P(B|A) is obvious but there are cases where it is less obvious. This mistake is\n",
      "made often enough in legal cases that it is sometimes called the prosecutor’s\n",
      "fallacy.\n",
      "\n",
      "Example 2.13 A medical test for a disease D has outcomes + and —. The\n",
      "probabilities are:\n",
      "\n",
      " \n",
      "\n",
      "+} .0081 .0900\n",
      "— | .0090 .9010\n",
      "\n",
      "From the definition of conditional probability, P(+|D) = P(+,D)/P(D) =\n",
      ".0081 /(.0081 + .0009) = 9 and P(—|D°) = P(—, D*)/P(D*) = .9010/(.9010 +\n",
      "0900) = .9. Apparently, the test is fairly accurate. Sick people yield a\n",
      "positive 90 percent of the time and healthy people yield a negative about 90\n",
      "percent of the time. Suppose you go for a test and get a positive. What is the\n",
      "probability you have the disease? Most people answer .90. The correct answer\n",
      "is P(D|+) = P(+, D)/P(+) = .0081/(.0081 + .0900) = .08. The lesson here is\n",
      "that you need to compute the answer numerically. Don’t trust your intuition.\n",
      "|\n",
      "\n",
      "If A and B are independent events then\n",
      "\n",
      "P(AB) _ P(A)P(B)\n",
      "P(A|B) = 777) 17) P(A).\n",
      "So another interpretation of independence is that knowing B doesn’t change\n",
      "the probability of A.\n",
      "From the definition of conditional probability we can write P(AB) =\n",
      "P(A|B)P(B) and also P(AB) = P(B|A)P(A). Often, these formulae give us\n",
      "a convenient way to compute P(AB) when A and B are not independent.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-022.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 CHAPTER 2. PROBABILITY\n",
      "\n",
      "Example 2.14 Draw two cards from a deck, without replacement. Let A be the\n",
      "event that the first draw is Ace of Clubs and let B be the event that the second\n",
      "draw is Queen of Diamonds. Then P(A, B) = P(A)P(B|A) = (1/52)x (1/51).\n",
      "|\n",
      "\n",
      " \n",
      "\n",
      "Summary of Conditional Probability\n",
      "\n",
      "1. If P(B) > 0 then\n",
      "P(AIB) = =\n",
      "\n",
      "2. P(-|B) satisfies the axioms of probability, for fixed B. In general, P(A|-)\n",
      "does not satisfy the axioms of probability, for fixed A.\n",
      "\n",
      "3. In general, P(A|B) # P(B|A).\n",
      "\n",
      "4. Aand B are independent if and only if P(A|B) = P(B).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "2.7 Bayes’ Theorem\n",
      "\n",
      "Bayes’ theorem is a useful result that is the basis of “expert systems” and\n",
      "“Bayes’ nets.” First, we need a preliminary result.\n",
      "\n",
      "Theorem 2.15 (The Law of Total Probability.) Let A,,...,A, be a partition\n",
      "of Q. Then, for any event B, P(B) = xe, P(BIA)P(A,)-\n",
      "\n",
      "Proor. Define C; = BA; and note that C,,...,C; are disjoint and that\n",
      "B= Ula Cj. Hence,\n",
      "\n",
      " \n",
      "\n",
      "P(B) = S7P(C;) = PBA) = PBIA)P(A))\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-023.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "2.8. BIBLIOGRAPHIC REMARKS 31\n",
      "\n",
      "since P(BA;) = P(B|A;)P(A;) from the definition of conditional probability.\n",
      "|\n",
      "\n",
      "Theorem 2.16 (Bayes’ Theorem.) Let Ai,..., Ax be a partition of Q such that\n",
      "P(A;) > 0 for each i. If P(B) > 0 then, for eachi=1,...,k,\n",
      "\n",
      " \n",
      "\n",
      "P(B\\A,)P(Ai) z\n",
      "P(Ai|B) = PPA) (2.5)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Remark 2.17 We call P(A,) the prior probability of A and P(A,|B) the\n",
      "posterior probability of A.\n",
      "\n",
      "PrRooF. We apply the definition of conditional probability twice, followed\n",
      "by the law of total probability:\n",
      "\n",
      "PUB) = PAB) _PBIAIP(A) __ PUBLA)P(A)\n",
      "\n",
      "PCB) PB) ‘3S PUBIA)PUG)\n",
      "\n",
      " \n",
      "\n",
      "Example 2.18 I divide my email into three categories: A, = “spam,” Ay =\n",
      "‘low priority” and A; = “high priority.” From previous experience I find that\n",
      "P(A) = .7, P(A2) = .2 and P(A3) = .1. Of course, .7+.2+.1=1. Let B be\n",
      "the event that the email contains the word “free.” From previous experience,\n",
      "P(B|A;) = 9, P(B\\Ag) = 01, P(B|A,) = 01. (Note: .9+.01+ 0141.) I\n",
      "receive an email with the word “free.” What is the probability that it is spam?\n",
      "Bayes’ theorem yields,\n",
      "9x7\n",
      "\n",
      "P(Ai|B) = (ox aFWix 24x >\n",
      "\n",
      " \n",
      "\n",
      "2.8 Bibliographic Remarks\n",
      "\n",
      "The material in this chapter is standard. Details can be found in any\n",
      "number of books. At the introductory level, there is DeGroot and Schervish\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-024.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "32 CHAPTER 2. PROBABILITY\n",
      "\n",
      "(2002), at the intermediate level, Grimmett and Stirzaker (1982) and Karr\n",
      "(1993), and at the advanced level, Billingsley (1979) and Breiman (1968). I\n",
      "adapted many examples and problems from DeGroot and Schervish (2002)\n",
      "and Grimmett and Stirzaker (1982).\n",
      "\n",
      "2.9 Technical Appendix\n",
      "\n",
      "Generally, it is not feasible to assign probabilities to all subsets of a sample\n",
      "space 2. Instead, one restricts attention to a set of events called a c-algebra\n",
      "or a o-field which is a class A that satisfies:\n",
      "\n",
      "(i) DEA,\n",
      "\n",
      "(ii) if 41, Ap,...,€ A then UZ, A; € A and\n",
      "\n",
      "(iii) A € A implies that Ac € A.\n",
      "\n",
      "The sets in A are said to be measurable. We call (2,A) a measurable\n",
      "space. If P is a probability measure defined on A then (Q, A, P) is called a\n",
      "probability space. When 2 is the real line, we take A to be the smallest\n",
      "o-field that contains all the open subsets, which is called the Borel o-field.\n",
      "\n",
      "2.10 Excercises\n",
      "\n",
      "1. Fill in the details of the proof of Theorem 2.8. Also, prove the monotone\n",
      "decreasing case.\n",
      "\n",
      "2. Prove the statements in equation (2.1).\n",
      "\n",
      "3. Let Q be a sample space and let A,, Ao,..., be events. Define B, =\n",
      "US, Ai and C, = (2, Ai.\n",
      "(a) Show that B, D By, D +++ and that C; C By C++.\n",
      "\n",
      "(b) Show that w € ()°2, B, if and only if w belongs to an infinite\n",
      "number of the events Aj, A»,....\n",
      "\n",
      "(c) Show that w € U%, C, if and only if w belongs to all the events\n",
      "Aj, Ag,... except possibly a finite number of those events.\n",
      "\n",
      "4. Let {A; : i € I} be a collection of events where J is an arbitrary index\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-025.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "2.10.\n",
      "\n",
      "10.\n",
      "\n",
      "EXCERCISES 33\n",
      "\n",
      "set. Show that\n",
      "\n",
      "(Us) - (4s and (q+) - U4\n",
      "\n",
      "ier ier ier ier\n",
      "\n",
      "Hint: First prove this for J = {1,...,n}.\n",
      "\n",
      ". Suppose we toss a fair coin until we get exactly two heads. Describe\n",
      "\n",
      "the sample space S. What is the probability that exactly k tosses are\n",
      "required?\n",
      "\n",
      ". Let 2 = {0,1,..., }. Prove that there does not exist a uniform distri-\n",
      "\n",
      "bution on Q ie. if P(A) = P(B) whenever |A| = |B| then P cannot\n",
      "satisfy the axioms of probability.\n",
      "\n",
      ". Let Aj, Ao,... be events. Show that\n",
      "\n",
      "P (U 4) < STP (An).\n",
      "\n",
      "Hint: Define B, = A, — U2] Ai. Then show that the B, are disjoint\n",
      "\n",
      "i=\n",
      "\n",
      "and that Up. An = Un2a Bn-\n",
      "\n",
      ". Suppose that P(A;) = 1 for each i. Prove that\n",
      "\n",
      "P (A+) a1\n",
      "\n",
      ". For fixed B such that P(B) > 0, show that P(-|B) satisfies the axioms\n",
      "\n",
      "of probability.\n",
      "\n",
      "‘You have probably heard it before. Now you can solve it rigorously.\n",
      "It is called the “Monty Hall Problem.” A prize is placed at random\n",
      "between one of three doors. You pick a door. To be concrete, let’s\n",
      "suppose you always pick door 1. Now Monty Hall chooses one of the\n",
      "other two doors, opens it and shows you that it is empty. He then gives\n",
      "you the opportunity to keep your door or switch to the other unopened\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-026.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "34\n",
      "\n",
      "11.\n",
      "\n",
      "12.\n",
      "\n",
      "13.\n",
      "\n",
      "14.\n",
      "\n",
      "16.\n",
      "\n",
      "CHAPTER 2. PROBABILITY\n",
      "\n",
      "door. Should you stay or switch? Intuition suggests it doesn’t matter.\n",
      "The correct answer is that you should switch. Prove it. It will help to\n",
      "specify the sample space and the relevant events carefully. Thus write\n",
      "Q = {(wi, we): w; € {1,2,3}} where w; is where the prize is and wy is\n",
      "the door Monty opens.\n",
      "\n",
      "Suppose that A and B are independent events. Show that A° and B¢\n",
      "are independent events.\n",
      "\n",
      "There are three cards. The first is green on both sides, the second is\n",
      "red on both sides and the third is green on one side and red on the\n",
      "other. We choose a card at random and we see one side (also chosen\n",
      "at random). If the side we see is green, what is the probability that\n",
      "the other side is also green? Many people intuitively answer 1/2. Show\n",
      "that the correct answer is 2/3.\n",
      "\n",
      " \n",
      "\n",
      "Suppose that a fair coin is tossed repeatedly until both a head and tail\n",
      "have appeared at least once.\n",
      "\n",
      "(a) Describe the sample space 2.\n",
      "(b) What is the probability that three tosses will be required?\n",
      "Show that if P(A) = 0 or P(A) = 1 then A is independent of every\n",
      "\n",
      "other event. Show that if A is independent of itself then P(A) is either\n",
      "0 or 1.\n",
      "\n",
      ". The probability that a child has blue eyes is 1/4. Assume independence\n",
      "\n",
      "between children. Consider a family with 5 children.\n",
      "\n",
      "(a) If it is known that at least one child has blue eyes, what is the\n",
      "probability that at least three children have blue eyes?\n",
      "\n",
      "(b) If it is known that the youngest child has blue eyes, what is the\n",
      "probability that at least three children have blue eyes?\n",
      "\n",
      " \n",
      "\n",
      "Show that\n",
      "\n",
      " \n",
      "\n",
      "P(ABC) = P(A|BC)P(B|C)P(C).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-027.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "2.10. EXCERCISES 35\n",
      "\n",
      "17.\n",
      "\n",
      "18.\n",
      "\n",
      "19.\n",
      "\n",
      "20.\n",
      "\n",
      "21.\n",
      "\n",
      "Suppose k events form a partition of the sample space 2, i.e. they\n",
      "are disjoint and U_, A; = 2. Assume that P(B) > 0. Prove that if\n",
      "P(A,|B) < P(A;) then P(A;|B) > P(A;) for some i = 2,...,k.\n",
      "\n",
      "Suppose that 30 percent of computer owners use a Macintosh, 50 use\n",
      "Windows and 20 percent use Linux. Suppose that 65 percent of the Mac\n",
      "users have succumbed to a computer virus, 82 percent of the Windows\n",
      "users get the virus and 50 percent of the Linux users get the virus. We\n",
      "select a person at random and learn that her system was infected with\n",
      "the virus. What is the probability that she is a Windows user?\n",
      "\n",
      "A box contains 5 coins and each has a different probability of showing\n",
      "heads. Let pi,...,p5 denote the probability of heads on each coin.\n",
      "Suppose that\n",
      "\n",
      "Pr =0, po = 1/4, py = 1/2, pa = 3/4 and ps5 = 1.\n",
      "\n",
      "Let H denote “heads is obtained” and let C; denote the event that coin\n",
      "7 is selected.\n",
      "\n",
      "(a) Select a coin at random and toss it. Suppose a head is obtained.\n",
      "What is the posterior probability that coin i was selected (i = 1,...,5)?\n",
      "In other words, find P(C;|H) fori =1,...,5.\n",
      "\n",
      "(b) Toss the coin again. What is the probability of another head? In\n",
      "other words find P(H2|H,) where H; = “heads on toss j.”\n",
      "\n",
      "Now suppose that the experiment was carried out as follows. We select\n",
      "a coin at random and toss it until a head is obtained.\n",
      "\n",
      "(c) Find P(C;,|B,) where B, = “first head is obtained on toss 4.”\n",
      "\n",
      "(Computer Experiment.) Suppose a coin has probability p of falling\n",
      "heads. If we flip the coin many times, we would expect the proportion\n",
      "of heads to be near p. We will make this formal later. Take p= .3 and\n",
      "n = 1000 and simulate n coin flips. Plot the proportion of heads as a\n",
      "function of n. Repeat for p= .03.\n",
      "\n",
      "(Computer Experiment.) Suppose we flip a coin n times and let p denote\n",
      "the probability of heads. Let X be the number of heads. We call X\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-028.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "\n",
      "22.\n",
      "\n",
      "CHAPTER 2. PROBABILITY\n",
      "\n",
      "a binomial random variable which is discussed in the next chapter.\n",
      "Intuition suggests that X will be close to np. To see if this is true,\n",
      "we can repeat this experiment many times and average the X values.\n",
      "Carry out a simulation and compare the average of the X’s to np. Try\n",
      "this for p= .3 and n = 10, 100, 1000.\n",
      "\n",
      "(Computer Experiment.) Here we will get some experience simulating\n",
      "conditional probabilities. Consider tossing a fair die. Let A = {2, 4,6}\n",
      "and B = {1,2,3,4}. Then, P(A) = 1/2, P(B) = 2/3 and P(AB) = 1/3.\n",
      "Since P(AB) = P(A)P(B), the events A and B are independent. Simu-\n",
      "late draws from the sample space and verify that P(AB) = P(A)P(B)\n",
      "where P(A) is the proportion of times A occurred in the simulation\n",
      "and similarly for P(AB) and P(B). Now find two events A and B that\n",
      "are not independent. Compute P(A), P(B) and P(AB). Compare the\n",
      "calculated values to their theoretical values. Report your results and\n",
      "interpret.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-029.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "Chapter 3\n",
      "\n",
      "Random Variables\n",
      "\n",
      "3.1 Introduction\n",
      "\n",
      "Statistics and data mining are concerned with data. How do we link\n",
      "sample spaces and events to data? The link is provided by the concept of a\n",
      "random variable.\n",
      "\n",
      " \n",
      "\n",
      "Definition 3.1 A random variable is a mapping X : Q — R that\n",
      "assigns a real number X(w) to each outcome w.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Technically, a ran-\n",
      "At acertain point in most probability courses, the sample space is rarely dom variable must\n",
      "mentioned and we work directly with random variables. But you should keep be measurable. See\n",
      "\n",
      "in mind that the sample space is really there, lurking in the background. the _technical _ AP\n",
      "pendix for details.\n",
      "\n",
      "Example 3.2 Flip a coin ten times. Let X(w) be the number of heads in the\n",
      "sequence w. For ecample, ifw = HHTHHTHHTT then X(w)=6.\n",
      "\n",
      "Example 3.3 Let Q = {(x,y); a?+y? < 1} be the unit disc. Consider drawing\n",
      "a point “at random” from Q. (We will make this idea more precise later.) A\n",
      "typical outcome is of the form w = (a, y). Some examples of random variables\n",
      "are X(w) = 2, Yw)=y, Zw) =a+y, Ww) = Vo? +y?.\n",
      "\n",
      "Given a random variable X and asubset A of the real line, define X~'(A) =\n",
      "{w €Q: X(w) € A} and let\n",
      "\n",
      "37\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-030.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "38 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      " \n",
      "\n",
      "P(X71(A)) = P({w €Q; X(w) € A})\n",
      "P(X\"\"(2)) = P({w € 2; X(w) =2}).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "X denotes the random variable and x denotes a possible value of X.\n",
      "\n",
      " \n",
      "\n",
      "Example 3.4 Flip a coin twice and let X be the number of heads. Then,\n",
      "P(X = 0) = P({TT}) = 1/4, P(X = 1) = P({AT,TH}) = 1/2 and P(X =\n",
      "2) = P({HH}) = 1/4. The random variable and its distribution can be\n",
      "summarized as follows:\n",
      "\n",
      " \n",
      "\n",
      "w_ P((w)) |X) 1868 an)\n",
      "g O} 1/4\n",
      "1 1|1p\n",
      "> 2|1/4\n",
      "\n",
      " \n",
      "\n",
      "Try generalizing this to n flips. Ml\n",
      "\n",
      "3.2 Distribution Functions and Probability Func-\n",
      "tions\n",
      "\n",
      "Given a random variable X, we define an important function called the\n",
      "cumulative distribution function (or distribution function) in the following\n",
      "way.\n",
      "\n",
      " \n",
      "\n",
      "Definition 3.5 The cumulative distribution function cbr Fy : R—\n",
      "[0, 1] of a random variable X is defined by\n",
      "\n",
      "Fx (x) = P(X <2).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-031.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "3.2. DISTRIBUTION FUNCTIONS AND PROBABILITY FUNCTIONS39\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "——\n",
      "—_—_.\n",
      "2 x\n",
      "\n",
      "Figure 3.1: cpr for flipping a coin twice (Example 3.6.)\n",
      "\n",
      "‘You might wonder why we bother to define the CbF . You will see later\n",
      "that the CpF is a useful function: it effectively contains all the information\n",
      "about the random variable.\n",
      "\n",
      "Example 3.6 Flip a fair coin twice and let X be the number of heads. Then\n",
      "P(X =0) = P(X = 2) =1/4 and P(X =1)=1/2. The distribution function\n",
      "}S\n",
      "\n",
      "‘ 0 a <0\n",
      "\n",
      "1/4 0<a<1\n",
      "\n",
      "3/4 1<a<2\n",
      "\n",
      "1 @>2.\n",
      "\n",
      "Fx (az) =\n",
      "\n",
      "The cpr is shown in Figure 3.1. Although this example is simple, study\n",
      "it carefully. CDF ’s can be very confusing. Notice that the function is right\n",
      "continuous, non-decreasing and that it is defined for all x even though the\n",
      "random variable only takes values 0,1 and 2. Do you see why F(1.4) = .75?\n",
      "|\n",
      "\n",
      "The following result, which we do not prove, shows that the CDF com-\n",
      "pletely determines the distribution of a random variable.\n",
      "\n",
      "Theorem 3.7 Let X have cpr F and let Y have cor G. If F(x) = G(a)\n",
      "for alla then P(X € A) =P(Y € A) for all A.\n",
      "\n",
      "Theorem 3.8 A function F mapping the real line to [0,1] is a CDF for some\n",
      "probability measure P if and only if it satisfies the following three conditions:\n",
      "\n",
      "Technically, we only\n",
      "have that P(X €\n",
      "A) = P(Y € A)\n",
      "for every measurable\n",
      "event A.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-032.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "A set is countable\n",
      "if it is finite or\n",
      "it can be put in\n",
      "a one-to-one corre-\n",
      "spondence with the\n",
      "integers. The even\n",
      "numbers, the odd\n",
      "numbers and the ra-\n",
      "tionals are count-\n",
      "able; the set of real\n",
      "numbers between 0\n",
      "and 1 is not count-\n",
      "able.\n",
      "\n",
      "40 CHAPTER 3. RANDOM VARIABLES\n",
      "(i) F is non-decreasing i.e. £1 < x2 implies that F(a) < F(a2).\n",
      "(ii) F is normalized: lim,,-.~ F(x) = 0 and lim, F(x) = 1.\n",
      "(iii) F is right-continuous, i.e. F(x) = F(a*) for all x, where\n",
      "8\n",
      "F(a\") = limy2F'(y).\n",
      "Proor. Suppose that F is a CDF .\n",
      "be a real number and let y1, y2,... be a sequence of real numbers such that\n",
      "Yi > yo >--- and lim; y; = 2. Let A; = (—00, y;] and let A = (—0o,z]. Note\n",
      "that A = Me: A; and also note that A; > A, D ---. Because the events are\n",
      "monotone, lim; P(A;) = P((); Ai). Thus,\n",
      "\n",
      "Let us show that (iii) holds. Let x\n",
      "\n",
      "F(x) = 9 =?(N a) = = limP(A)) = lim F(yi) = F(a\").\n",
      "\n",
      "i\n",
      "Showing (i) and (ii) is similar. Proving the other direction namely, that if\n",
      "F satisfies (i), (ii) and (iii) then it is a CbF for some random variable, uses\n",
      "some deep tools in analysis. ll\n",
      "\n",
      " \n",
      "\n",
      "Definition 3.9 X is discrete if it takes countably many values\n",
      "\n",
      "{1, U2,-- }.\n",
      "\n",
      "We define the probability function or probability mass function\n",
      "for X by\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Thus, fx(x) > 0 for all a € R and 30; fx(a;) = 1. The cpr of X is\n",
      "telated to fx by\n",
      "DY ie\n",
      "\n",
      "a<r\n",
      "\n",
      "Fy@)=PX ez)=\n",
      "\n",
      "Sometimes we write fy and Fy simply as f and F.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-033.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "3.2. DISTRIBUTION FUNCTIONS AND PROBABILITY FUNCTIONS41\n",
      "\n",
      "fx(z)\n",
      "\n",
      "ar\n",
      "\n",
      "io\n",
      "oe\n",
      "°\n",
      "\n",
      " \n",
      "\n",
      "—\n",
      "\n",
      "0 1 2 x\n",
      "Figure 3.2: Probability function for flipping a coin twice (Example 3.6.)\n",
      "\n",
      " \n",
      "\n",
      "Example 3.10 The probability function for Example 3.6 is\n",
      "\n",
      "1/4 «=0\n",
      "fee)=4 aig baa\n",
      "0 2x ¢ {0,1,2}.\n",
      "\n",
      "See Figure 3.2. @\n",
      "\n",
      " \n",
      "\n",
      "Definition 3.11 A random variable X is continuous if there exists a\n",
      "function fx such that fx(a) > 0 for all x, [°. fx(x)dx = 1 and for\n",
      "every a <b,\n",
      "\n",
      "Pa<X <b)= [ seoee. (3.1)\n",
      "\n",
      "The function fx is called the probability density function (PDF ).\n",
      "We have that\n",
      "\n",
      "Fx (2) = [ fx (dt\n",
      "\n",
      "and fx(x) = FX.(a) at all points « at which Fx is differentiable.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Sometimes we shall write f f(a)dz or simply f f to mean f°. f(«)de.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-034.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "42 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "0 1 x\n",
      "Figure 3.3: Cpr for Uniform (0,1).\n",
      "\n",
      "Example 3.12 Suppose that X has PDF\n",
      "\n",
      "1 for0O<a<1\n",
      "0 otherwise.\n",
      "\n",
      "x(a) = {\n",
      "\n",
      "Clearly, fx(x) > 0 and Jf fx(x)dx = 1. A random variable with this density\n",
      "is said to have a Uniform (0,1) distribution. The CpF is given by\n",
      "\n",
      "0 «2<0\n",
      "Fy(t)={ « 0<a<l\n",
      "L ¢S 1.\n",
      "\n",
      "See Figure 3.3. @\n",
      "\n",
      "Example 3.13 Suppose that X has PDF\n",
      "\n",
      "fa)={? forx <0\n",
      "\n",
      "aa otherwise.\n",
      "Since f f(x)dx =1, this is a well-defined por .\n",
      "\n",
      "Warning! Continuous random variables can lead to confusion. First,\n",
      "note that if X is continuous then P(X = a) = 0 for every x! Don’t try to\n",
      "think of f(x) as P(X = x). This only holds for discrete random variables.\n",
      "We get probabilities from a ppF by integrating. A PDF can be bigger than\n",
      "1 (unlike a mass function). For example, if f(x) = 5 for x € [0,1/5] and 0\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-035.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "3.2. DISTRIBUTION FUNCTIONS AND PROBABILITY FUNCTIONS43\n",
      "\n",
      "otherwise, then f(x) > 0 and f f(x)dx = 1 so this is a well-defined por\n",
      "even though f(z) = 5 in some places. In fact, a PDF can be unbounded.\n",
      "For example, if f(z) = (2/3)x~!8 for 0 < « < 1 and f(z) = 0 otherwise,\n",
      "then f f(x)dx = 1 even though f is not bounded.\n",
      "\n",
      "Example 3.14 Let\n",
      "\n",
      "0 fora <0\n",
      "f@)= +— otherwise\n",
      "ay .\n",
      "\n",
      "This is not appr since f f(x)dx = f>° dx/(1+ 2) = JP du/u=log(oo) =\n",
      "co.\n",
      "\n",
      "Lemma 3.15 Let F be the cbF for a random variable X. Then:\n",
      "(i) P(X = 2) = F(a) — F(a-) where F(a~) = limyze F(y),\n",
      "(ii) P(e < X <y) = Fly) - F(a),\n",
      "\n",
      "(iti) P(X > 2) =1- F(a),\n",
      "(iv) If X is continuous then\n",
      "\n",
      "P(a< X <b) =P(a< X <b) =Pa<X <b) =P(a<X <0).\n",
      "\n",
      "It is also useful to define the inverse CDF (or quantile function).\n",
      "\n",
      " \n",
      "\n",
      "Definition 3.16 Let X be a random variable with cor F. The inverse\n",
      "CDF or quantile function is defined by\n",
      "\n",
      "FQ = inf {2 : F(a) < a}\n",
      "\n",
      "for q € [0,1]. If F is strictly increasing and continuous then F-\\(q) is\n",
      "the unique real number x such that F(x) = q.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "If you are unfamil-\n",
      "We call F-1(1/4) the first quartile, F-1(1/2) the median (or second quar- iar with “inf”, just\n",
      "\n",
      "tile) and F-'(3/4) the third quartile. think of it as the\n",
      "minimum.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-036.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80ED608>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "Two random variables X and Y are equal in distribution - written\n",
      "X £Y — if Fy() = Fy() for all x. This does not mean that X and Y are\n",
      "equal. Rather, it means that all probability statements about X and Y will\n",
      "be the same.\n",
      "\n",
      "3.3. Some Important Discrete Random Vari-\n",
      "ables\n",
      "\n",
      "Warning About Notation! It is traditional to write X ~ F to indicate\n",
      "that X has distribution F. This is unfortunate notation since the symbol ~\n",
      "is also used to denote an approximation. The notation X ~ F is so pervasive\n",
      "that we are stuck with it. Read X ~ F as “X has distribution F” not as X\n",
      "is approximately F.\n",
      "\n",
      "Tue Point Mass DistriBuTION. X has a point mass distribution at\n",
      "a, written X ~ 6a, if P(X = a) = 1 in which case\n",
      "0 uw<a\n",
      "ra={9 756\n",
      "\n",
      "The probability function is f(x) = 1 for « =a and 0 otherwise.\n",
      "\n",
      "THE DISCRETE UNIFORM DISTRIBUTION. Let k > 1 be a given integer.\n",
      "Suppose that X has probability mass function given by\n",
      "\n",
      "fa) = U/k forc=1,...,k\n",
      "\n",
      "0 otherwise.\n",
      "\n",
      "We say that X has a uniform distribution on {1,..., k}.\n",
      "\n",
      "THE BERNOULLI DISTRIBUTION. Let X represent a coin flip. Then\n",
      "P(X = 1) =pand P(X = 0) =1—> for some p € (0, 1]. We say that X has\n",
      "a Bernoulli distribution written X ~ Bernoulli(p). The probability function\n",
      "is f(z) = p*(1—p)'~* for a € {0,1}.\n",
      "\n",
      "THE BINOMIAL DISTRIBUTION. Suppose we have a coin which falls\n",
      "heads with probability p for some 0 < p< 1. Flip the coin n times and let\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-037.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "3.3. SOME IMPORTANT DISCRETE RANDOM VARIABLES 45\n",
      "\n",
      "X be the number of heads. Assume that the tosses are independent. Let\n",
      "f(x) = P(X =2) be the mass function. It can be shown that\n",
      "\n",
      "fe)= { (p= p)* fora =0,....n\n",
      "\n",
      "0 otherwise.\n",
      "\n",
      "A random variable with the mass function is called a Binomial random\n",
      "variable and we write X ~ Binomial(n,p). If X, ~ Binomial(n,p,) and\n",
      "X_ ~ Binomial(n, pz) then X; + X2 ~ Binomial(n, p, + p2).\n",
      "\n",
      "Warning! Let us take this opportunity to prevent some confusion. X\n",
      "is a random variable; z denotes a particular value of the random variable;\n",
      "n and p are parameters, that is, fixed real numbers. The parameter p is\n",
      "usually unknown and must be estimated from data; that’s what statistical\n",
      "inference is all about. In most statistical models, there are random variables\n",
      "and parameters: don’t confuse them.\n",
      "\n",
      "Tue GEOMETRIC DisTRIBUTION. X has a geometric distribution with\n",
      "parameter p € (0,1), written X ~ Geom(p), if\n",
      "\n",
      "P(X =k)=p—p)s, ke.\n",
      "\n",
      "We have that\n",
      "\n",
      "ES ES Pe\n",
      "DPX =k) = PP (0-9) =T=G2p 7!\n",
      "\n",
      "Think of X as the number of flips needed until the first heads when flipping\n",
      "a coin.\n",
      "\n",
      "THE Polsson DISTRIBUTION. X has a Poisson distribution with pa-\n",
      "rameter A, written X ~ Poisson() if\n",
      "\n",
      "Note that\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-038.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "46 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "The Poisson is often used as a model for counts of rare events like radioactive\n",
      "decay and traffic accidents. If X, ~ Poisson(n, A) and X2 ~ Poisson(n, A2)\n",
      "then X, + X2 ~ Poisson(n, A; + A2).\n",
      "\n",
      "Warning! We defined random variables to be mappings from a sample\n",
      "space 2 to R but we did not mention the sample space in any of the distri-\n",
      "butions above. As I mentioned earlier, the sample space often “disappears”\n",
      "but it is really there in the background. Let’s construct a sample space ex-\n",
      "plicitly for a Bernoulli random variable. Let 2 = [0,1] and define P to satisfy\n",
      "P((a, 6]) = b—a for0 <a <b< 1. Fix pe [0,1] and define\n",
      "\n",
      "xw)={\n",
      "\n",
      "Then P(X = 1) = PW < p) = P((0,p]) = p and P(X = 0) =1-p. Thus,\n",
      "X ~ Bernoulli(p). We could do this for all the distributions defined above.\n",
      "In practice, we think of a random variable like a random number but formally\n",
      "it is a mapping defined on some sample space.\n",
      "\n",
      "3.4 Some Important Continuous Random Vari-\n",
      "ables\n",
      "\n",
      "lw<p\n",
      "0 w>p.\n",
      "\n",
      "THE UNIFORM DIsTRIBUTION. X has a Uniform(a, 6) distribution, writ-\n",
      "ten X ~ Uniform(a, 8), if\n",
      "\n",
      "—, fors €[a,)]\n",
      "—) Fa ’\n",
      "f(a) = { 0 otherwise\n",
      "\n",
      "where a < b. The distribution function is\n",
      "\n",
      "0 L<G\n",
      "F@)=4 = wela,b]\n",
      "1 wi.b.\n",
      "\n",
      "NORMAL (GaussIAN). X has a Normal (or Gaussian) distribution with\n",
      "parameters js and a, denoted by X ~ N(y,07), if\n",
      "\n",
      "f(x) = on {-sa(e-1), ceER\n",
      "\n",
      " \n",
      "\n",
      "oV2n\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-039.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "3.4. SOME IMPORTANT CONTINUOUS RANDOM VARIABLES AT\n",
      "\n",
      "where  € R and o > 0. Later we shall see that ys is the “center” (or\n",
      "mean) of the distribution and o is the “spread” (or standard deviation) of\n",
      "the distribution. The Normal plays an important role in probability and\n",
      "statistics. Many phenomena in nature have approximately Normal distribu-\n",
      "tions. Later, we shall see that the distribution of a sum of random variables\n",
      "can be approximated by a Normal distribution (the central limit theorem).\n",
      "\n",
      "We say that X has astandard Normal distribution if = 0 anda = 1.\n",
      "Tradition dictates that a standard Normal random variable is denoted by Z.\n",
      "The ppF and Cpr of a standard Normal are denoted by ¢(z) and ®(z).\n",
      "The ppr is plotted in Figure 3.4. There is no closed-form expression for ®.\n",
      "Here are some useful facts:\n",
      "\n",
      "(i) If X ~ N(p,07) then Z = (X - p)/o ~ N(0,1).\n",
      "(ii) If Z ~ N(0,1) then X = w+ oZ ~ N(p, 07).\n",
      "\n",
      "(iii) If X; ~ N(y;,0?), i= 1,-..,n are independent then\n",
      "wx ~y (Dm De).\n",
      "a1 i=1 i=1\n",
      "\n",
      "It follows from (i) that if X ~ N(y, 0?) then\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Thus we can compute any probabilities we want as long as we can compute\n",
      "the cpr &(z) of a standard Normal. All statistical computing packages will\n",
      "compute ©(z) and @-!(z). All statistics texts, including this one, have a\n",
      "table of values of ®(z).\n",
      "\n",
      "Example 3.17 Suppose that X ~ N(3,5). Find P(X > 1). The solution is\n",
      "\n",
      " \n",
      "\n",
      "1-3\n",
      "V5\n",
      "\n",
      " \n",
      "\n",
      "P(X > 1)=1-P(x <1)=1-P(Z< ) = 1= 8(-0.8048) = 1.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-040.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "48 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "—2 -l1 0 1 2 =\n",
      "\n",
      "Figure 3.4: Density of a standard Normal.\n",
      "\n",
      "Now find q such that P(X < q) =.2. In other words, find q = ®-'(.2). We\n",
      "solve this by writing\n",
      "\n",
      "2=P(X<q)=P(2< 44) -9 (24),\n",
      "\n",
      "o o\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "From the Normal table, &(—.8416) = .2. Therefore,\n",
      "\n",
      "- 3\n",
      "—s4ig- 20d?\n",
      "oO\n",
      "\n",
      "  \n",
      "\n",
      "and hence q = 3 — 8416/5 = 1.1181.\n",
      "\n",
      "EXPONENTIAL DISTRIBUTION. X has an Exponential distribution with\n",
      "parameter 3, denoted by X ~ Exp(@), if\n",
      "\n",
      "fle) = sei, 2>0\n",
      "\n",
      "where 3 > 0. The exponential distribution is used to model the lifetimes of\n",
      "electronic components and the waiting times between rare events.\n",
      "\n",
      "GAMMA DISTRIBUTION. For a > 0, the Gamma function is defined\n",
      "by ['(a) = fo? y**e¥dy. X has a Gamma distribution with parameters a\n",
      "and 8, denoted by X ~ Gamma(a, §), if\n",
      "\n",
      "1\n",
      "12) = Bara)\n",
      "\n",
      "ge te tl 4 >0\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-041.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "3.4. SOME IMPORTANT CONTINUOUS RANDOM VARIABLES 49\n",
      "\n",
      "where a, 3 > 0. The exponential distribution is just a Gamma(1, 9) distribu-\n",
      "\n",
      "tion. If X; ~ Gamma(a,, 8) are independent, then )>\"_, X; ~ Gamma(}>” 0%, B)\n",
      "i=1 % P)-\n",
      "\n",
      "THE BETA DISTRIBUTION. X has an Beta distribution with parameters\n",
      "a>0and 6 > 0, denoted by X ~ Beta(a, 8), if\n",
      "ce (1—2)P!, O<a<l.\n",
      "\n",
      "f(a) = Pahl\n",
      "\n",
      "¢t AND Caucuy DistTrRiBuTION. X has a t distribution with v degrees of\n",
      "freedom — written X ~ t, —if\n",
      "\n",
      "The ¢ distribution is similar to a Normal but it has thicker tails. In fact, the\n",
      "Normal corresponds to a ¢ with vy = oo. The Cauchy distribution is a special\n",
      "case of the ¢ distribution corresponding to v = 1. The density is\n",
      "\n",
      "1\n",
      "f= aw\n",
      "\n",
      "To see that this is indeed a density, let’s do the integral:\n",
      "\n",
      "8 1? de 1 f® dtan\n",
      "[teu TI olte TJ &\n",
      "\n",
      " \n",
      "\n",
      "= Feo -eartooo) = FR (-B)) =\n",
      "\n",
      "THE y? DISTRIBUTION. X has a x? distribution with p degrees of freedom\n",
      "~ written X ~ xp - if\n",
      "\n",
      "1\n",
      "= (p/2)—1e-a/2\n",
      "f@) Tea” ees 0\n",
      "If Z,..., Z are independent standard Normal random variables then )7?_, Z? ~\n",
      "\n",
      "Xp\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-042.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "50 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "3.5 Bivariate Distributions\n",
      "\n",
      "Given a pair of discrete random variables X and Y, define the joint\n",
      "mass function by f(x,y) = P(X =a and Y = y). From now on, we write\n",
      "P(X =a and Y =y)asP(X =2,Y =y). We write f as fx, when we want\n",
      "to be more explicit.\n",
      "\n",
      "Example 3.18 Here is a bivariate distribution for two random variables X\n",
      "and Y each taking values 0 or 1:\n",
      "\n",
      "Y= Y =I\n",
      "X=0/1/9 2/9 [173\n",
      "X=1| 2/9 4/9 | 1/3\n",
      "73 173 [1\n",
      "\n",
      "Thus, P(X =1,Y =1)=f(1,1)=4/9. @\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Definition 3.19 In the continuous case, we call a function f(x,y) a pdf\n",
      "for the random variables (X,Y) if (i) f(x,y) > 0 for all (x,y), (ii)\n",
      "is {ef een = 1 and, for any set AC RXR, P((X,Y) €\n",
      "\n",
      "=f a a, y)dxdy. In the discrete or continuous case we define the\n",
      "ot cpF as Fy y(z,y)=P(X <2,Y <y).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 3.20 Let (X,Y) be uniform on the unit square. Then,\n",
      "\n",
      "1 if0<2<1,0<y<l\n",
      "0 otherwise.\n",
      "\n",
      "s,0)= {\n",
      "\n",
      "FindP(X <1/2,Y < 1/2). The event A= {X <1/2,Y <1/2} corresponds\n",
      "to a subset of the unit square. Integrating f over this subset corresponds, in\n",
      "this case, to computing the area of the set A which is 1/4. So, P(X <\n",
      "1/2,Y <1/2)=1/4.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-043.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5. BIVARIATE DISTRIBUTIONS 51\n",
      "\n",
      "Example 3.21 Let (X,Y) have density\n",
      "\n",
      "ab if0<a<1,0<y<1\n",
      "f(x,y) { y a5 y\n",
      "Then\n",
      "\n",
      "0 otherwise.\n",
      "1 1\n",
      "/ | (x + y)dxdy\n",
      "0 0\n",
      "\n",
      "1 1\n",
      "LUfee|of [fy rele\n",
      "0 0\n",
      "which verifies that this is a PDF. ll\n",
      "\n",
      "eS\n",
      "\n",
      "1\n",
      "[ xv+ ydy= 5+\n",
      "\n",
      "Example 3.22 If the distribution is defined over a non-rectangular region, then\n",
      "the calcuations are a bit more complicated. Here is an nice erample which I\n",
      "borrowed from DeGroot and Schervish (2002). Let (X,Y) have density\n",
      "ca’y ifa?<y<l\n",
      "Feu) = { 0 otherwise.\n",
      "Note first that -1 < «<1. Now let us find the value of c. The trick here\n",
      "is to be careful about the range of integration. We pick one variable, x say,\n",
      "and let it range over its values. Then, for each fixed value of x, we let y vary\n",
      "over its range which is 22 < y <1. It may help if you look at figure 3.5.\n",
      "\n",
      "Thus,\n",
      "1 1\n",
      "| [ senduar =e f | xy dy daz\n",
      "-1Ja?\n",
      ": ' jl-at 4c\n",
      "2 _ 2 _ de\n",
      "cf 2 [[ve]armefs 5 da =\n",
      "\n",
      "Hence, c = 21/4. Now let us compute P(X > Y). This corresponds to the\n",
      "set A = {(z,y);0 < a < 1,2? < y < a}. (You can see this by drawing a\n",
      "diagram.) So,\n",
      "\n",
      "PX SY) = tf ferme 3 [= lf vay] az\n",
      "1 9a? :\n",
      "\n",
      "at\n",
      "-ife ote= 5\n",
      "\n",
      "ll\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-044.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "52 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "xz\n",
      "\n",
      "Figure 3.5: The light shaded region is 2? < y < 1. The density is positive\n",
      "over this region. The hatched region is the event X > Y intersected with\n",
      "GPS pred,\n",
      "\n",
      "3.6 Marginal Distributions\n",
      "\n",
      " \n",
      "\n",
      "Definition 3.23 If (X,Y) have joint distribution with mass function fxy,\n",
      "then the marginal mass function for X is defined by\n",
      "\n",
      "fx(a) =P(X =2) = SOP(X=2,Y=9)=Sof@y) (3.3)\n",
      "\n",
      "and the marginal mass function for Y is defined by\n",
      "\n",
      "fry) =PW =9) = OPK =2,Y=y) = Posey). 34)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 3.24 Suppose that fx,y is given in the table that follows. The marginal\n",
      "distribution for X corresponds to the row totals and the marginal distribution\n",
      "for Y corresponds to the columns totals.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-045.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "3.6. MARGINAL DISTRIBUTIONS 53\n",
      "\n",
      "Y=0 Ysi\n",
      "0| 1/10 2/10 | 3/10\n",
      "1| 3/10 4/10 | 7/10\n",
      "4/10 6/10 |4\n",
      "For eample, fx(0) = 3/10 and fx(1) = 7/10.\n",
      "\n",
      " \n",
      "\n",
      "X=\n",
      "X=\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Definition 3.25 For continuous random variables, the marginal densities\n",
      "are\n",
      "\n",
      "ix) =f Hew), amd foy)= [ Hewde. 5)\n",
      "\n",
      "The corresponding marginal distribution functions are denoted by Fx\n",
      "and Fy.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 3.26 Suppose that\n",
      "\n",
      "fxy(a,y) =e @)\n",
      "\n",
      "for x,y >0. Then fx(x) = e~* fo° eYdy = e~*.\n",
      "Example 3.27 Suppose that\n",
      "\n",
      "_faty f0<¢<l1,0<y<l1\n",
      "F(a,9) = { 0 otherwise.\n",
      "\n",
      "Then 1 1 1\n",
      "1\n",
      "fw) = [ @+sae= f vac + f ydy= 5+. a\n",
      "0 0 0\n",
      "\n",
      "Example 3.28 Let (X,Y) have density\n",
      "\n",
      "a 8. - 2\n",
      "_ f a’y ifa?<y<l\n",
      "F(e.u)= { 0 otherwise.\n",
      "Thus,\n",
      "\n",
      "21 21\n",
      "fete) =f fleway= 2? [yay = 2020-2\")\n",
      "for -1<a<1 and fx(x) = 0 otherwise.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-046.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "54 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "3.7 Independent Random Variables\n",
      "\n",
      " \n",
      "\n",
      "Definition 3.29 Two random variables X and Y are independent jf,\n",
      "for every A and B,\n",
      "\n",
      "P(X € A,Y € B) = P(X € A)P(Y € B). (3.6)\n",
      "\n",
      "We write X WY.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "In principle, to check whether X and Y are independent we need to check\n",
      "equation (3.6) for all subsets A and B. Fortunately, we have the following\n",
      "result which we state for continuous random variables though it is true for\n",
      "discrete random variables too.\n",
      "\n",
      "Theorem 3.30 Let X and Y have joint pdf fxy. Then X UY if and only if\n",
      "The statement is fxy(z,y) = fx(z)fy(y) for all values x and y.\n",
      "not rigorous be-\n",
      "cause the density is\n",
      "defined only up to\n",
      "\n",
      "sets of measure 0. Example 3.31 Let X and Y have the following distribution:\n",
      "\n",
      "Ye 0. Y = 1\n",
      "X-0|177 1/7 |i?\n",
      "X=1/1f4 1744 |172\n",
      "\n",
      "172 172 [a\n",
      "\n",
      "Then, fx(0) = fx(1) = 1/2 and fy(0) = fy(1) = 1/2. X and Y are inde-\n",
      "pendent because fx(0)fy(0) = f(0,0), fx fr) = £0), fe) fr(0) =\n",
      "f(1,0), fxQ)fy(@) = f(,1). Suppose instead that X and Y have the fol-\n",
      "lowing distribution:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Y=0 Y=1\n",
      "\n",
      "X=0[1/72 0 12\n",
      "\n",
      "X=1|0 1/72 | 1/2\n",
      "i 172 [4\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-047.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "3.7. INDEPENDENT RANDOM VARIABLES 55\n",
      "\n",
      "These are not independent because fx(0)fy(1) = (1/2)(1/2) = 1/4 yet\n",
      "f(0,1)=0. @\n",
      "\n",
      "Example 3.32 Suppose that X and Y are independent and both have the same\n",
      "density\n",
      "\n",
      "Qa if0<ar<l\n",
      "f@)= { 0 otherwise.\n",
      "\n",
      "Let us find P(X +Y <1). Using independence, the joint density is\n",
      "_ _ f 4ay if0<a<1, O<y<l\n",
      "F(t.9) = fx@)fv) = { 0 otherwise.\n",
      "Now,\n",
      "\n",
      "P(X+Y<1) = If F(a, y)dydx\n",
      "\n",
      "aty<1\n",
      "1\n",
      "\n",
      "- fell\n",
      "\n",
      "1 2\n",
      "_ (1 - a) _l\n",
      "= afi 5) dx =>. wl\n",
      "\n",
      " \n",
      "\n",
      "The following result is helpful for verifying independence.\n",
      "\n",
      "Theorem 3.33 Suppose that the range of X and Y is a (possibly infinite)\n",
      "rectangle. If f(x,y) = g(x)h(y) for some functions g and h (not necessarily\n",
      "probability density functions) then X and Y are independent.\n",
      "\n",
      "Example 3.34 Let X and Y have density\n",
      "\n",
      "Qe“) if a >0 and y>0\n",
      "f(#,y) = { 0 otherwise.\n",
      "\n",
      "The range of X andY is the rectangle (0,00) x(0, 00). We can write f (x,y) =\n",
      "g(x)h(y) where g(x) = 2e-* and h(y) =e\"¥. Thus, XUY. @\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-048.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243B75A2388>\n",
      "56 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "3.8 Conditional Distributions\n",
      "\n",
      "If X and Y are discrete, then we can compute the conditional distribution\n",
      "of X given that we have observed Y = y. Specifically, P(X = a|Y = y) =\n",
      "P(X = 2,Y = y)/P(Y = y). This leads us to define the conditional mass\n",
      "function as follows.\n",
      "\n",
      " \n",
      "\n",
      "Definition 3.35 The conditional probability mass function is\n",
      "\n",
      "fxyv(aly) = P(X =2|¥ = y) = — y) _ farlea)\n",
      "\n",
      " \n",
      "\n",
      "if fy(y) > 0.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "We are treading in For continuous distributions we use the same definitions. The interpre-\n",
      "deep water here. tation differs: in the discrete case, fx\\y(z|y) is P(X = 2|Y = y) but in the\n",
      "When we compute continuous case, we must integrate to get a probability.\n",
      "\n",
      "P(X € A|Y = y)\n",
      "in the continuous\n",
      "case we are condi-\n",
      "tioning on the event\n",
      "{Y = y} which has\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Definition 3.36 For continuous random variables, the conditional prob\n",
      "ability density function is\n",
      "\n",
      "probability 0. We £53\n",
      "avoid this problem Ixy@ly) = fee\n",
      "by defining things\n",
      "\n",
      "in terms of the assuming that fy(y) > 0. Then,\n",
      "\n",
      "PDF . The fact\n",
      "\n",
      "that this leads to a P(XeA\n",
      "\n",
      "well-defined theory\n",
      "\n",
      "is proved in more\n",
      "\n",
      "advanced courses.\n",
      "\n",
      "We simply take it\n",
      "\n",
      "as a definition. Example 3.37 Let X and Y have a uniform distribution on the unit square.\n",
      "Verify that fxjy (aly) = 1 for0 <a <1 and 0 otherwise. Thus, given Y = y,\n",
      "X is Uniform (0,1). We can write this as X|Y = y ~ Unif(0,1).\n",
      "\n",
      " \n",
      "\n",
      "Y=y)= [ few(olniae.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "From the definition of the conditional density, we see that fxy(x,y) =\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-049.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "3.8. CONDITIONAL DISTRIBUTIONS 57\n",
      "\n",
      "fxiy (aly) fy) = fyix(y|x)fx(). This can sometimes be useful as in the\n",
      "next example.\n",
      "\n",
      "Example 3.38 Let\n",
      "\n",
      "_faty if0<2<1,0<y<1\n",
      "f(a,y) = { 0 otherwise.\n",
      "\n",
      "Let us find P(X < 1/4[Y = 1/3). In example 3.27 we saw that fy(y) =\n",
      "y + (1/2). Hence,\n",
      "\n",
      "So,\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 3.39 Suppose that X ~ Unif(0,1). After obtaining a value of X we\n",
      "generate Y|X =a ~ Uniform(a,1). What is the marginal distribution of Y ?\n",
      "First note that,\n",
      "\n",
      "1 if0<a¢<1\n",
      "fx(z) = { 0 otherwise\n",
      "and 1 if\n",
      "_ fre if0<r<y<l\n",
      "frix(ylz) = { 0 otherwise.\n",
      "So,\n",
      "\n",
      "a if0<2r<y<1\n",
      "= . = Ts\n",
      "fx(@.y) = frxtule) fx) { 0 ° otherwise.\n",
      "\n",
      "The marginal for Y is\n",
      "\n",
      "-\n",
      "fw) = [' tevewar= f= - [=~ 1080-0)\n",
      "\n",
      "1-2\n",
      "\n",
      " \n",
      "\n",
      "forO<y<1.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-050.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80ED788>\n",
      "58 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "Example 3.40 Consider the density in Example 3.28. Let’s find fy\\x(y|\n",
      "When X = 2, y must satisfy x? < y < 1. Earlier, we saw that fx(z)\n",
      "(21/8)a°(1—a*). Hence, for x? <y <1,\n",
      "\n",
      "f(y) Bay 2y\n",
      "\n",
      "fyx(y\\a) = Fx(@) = ELT 7) “Tog\n",
      "\n",
      " \n",
      "\n",
      "Now let us compute P(Y > 3/4|X = 1/2). This can be computed by first\n",
      "noting that fy|x(y|1/2) = 32y/15. Thus,\n",
      "\n",
      ". 1 1 32y 7\n",
      "P(Y > 3/4|X =1/2)= | f(yl1/2)dy = v= ®\n",
      "3/4 3/4 19 a\n",
      "\n",
      "3.9 Multivariate Distributions and IID Samples\n",
      "\n",
      "Let X = (X1,...,X,) where X,,...,X,, are random variables. We call\n",
      "X a random vector. Let f(21,..-,%n) denote the pdf. It is possible to define\n",
      "their marginals, conditionals etc. much the same way as in the bivariate case.\n",
      "We say that X),...,X,, are independent if, for every Ai,...,An,\n",
      "\n",
      "n\n",
      "P(X, € Ai,..-,Xn © An) = | [ P(X € Ai)- (3.7)\n",
      "ot\n",
      "n\n",
      "It suffices to check that f(@1,..., 2%) = Tr fr,(ae). If X1,...,Xp are inde-\n",
      "pendent and each has the same marginal distribution with density f, we say\n",
      "that X,,...,X,, are 1p (independent and identically distributed). We shall\n",
      "write this as X,,...X, ~ f or, in terms of the cpr , X,,...X, ~ F. This\n",
      "means that X,,...,X, are independent draws from the same distribution.\n",
      "We also call X,,...,X, a random sample from F’.\n",
      "\n",
      "Much of statistical theory and practice begins with [Ip observations and\n",
      "\n",
      "we shall study this case in detail when we discuss statistics.\n",
      "\n",
      "3.10 Two Important Multivariate Distribu-\n",
      "tions\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-051.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.10. TWO IMPORTANT MULTIVARIATE DISTRIBUTIONS 59\n",
      "\n",
      "MULTINOMIAL. The multivariate version of a Binomial is called a Multi-\n",
      "nomial. Consider drawing a ball from an urn which has balls with k different\n",
      "colors labeled color 1, color 2, ..., color k. Let p = (pi,...,px) where\n",
      "pj = 0 and Dye Pi = 1 and suppose that p; is the probability of drawing\n",
      "a ball of color j. Draw n times (independent draws with replacement) and\n",
      "let X = (Xj,--., Xx) where X; is the number of times that color j appears.\n",
      "Hence, n = Ya Xt: We say that X has a Multinomial (n,p) distribution\n",
      "written X ~ Multinomial(n,p). The probability function is\n",
      "\n",
      "fe)=(,.\" ,, )et oat (38)\n",
      "\n",
      "n _ n!\n",
      "@--.Up ay! -- ay!\n",
      "\n",
      "Lemma 3.41 Suppose that X ~ Multinomial(n,p) where X = (X1,...,X«)\n",
      "and p= (pi,---, Px). The marginal distribution of X; is Binomial (n,p;).\n",
      "\n",
      "where\n",
      "\n",
      "MULTIVARIATE NORMAL. The univariate Normal had two parameters,\n",
      "and o. In the multivariate version, js is a vector and a is replaced by a\n",
      "matrix 4. To begin, let\n",
      "\n",
      "“4\n",
      "Z=| 3\n",
      "Zk\n",
      "where Z,,...,Z, ~ N(0,1) are independent. The density of Z is If a and 6 are vec-\n",
      "\n",
      "‘ tors then a7b =\n",
      "\n",
      "1 1S 1 1 fs agbi.\n",
      "1e)= TL) = pene {223} = pepo {ae}\n",
      "We say that Z has a standard multivariate Normal distribution written Z ~\n",
      "N(0,I) where it is understood that 0 represents a vector of k zeroes and I\n",
      "is the k x k identity matrix.\n",
      "\n",
      "More generally, a vector X has a multivariate Normal distribution, de-\n",
      "noted by X ~ N(, 5), if it has density ZT js the inverse of\n",
      "\n",
      "the matrix D.\n",
      "\n",
      "f(a} 4%) = aya? {-5 — po (a — w} (3.9)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-052.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "A matrix © is pos-\n",
      "itive definite if, for\n",
      "all non-zero vectors\n",
      "x, 27xX > 0.\n",
      "\n",
      "60 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "where det(-) denotes the determinant of a matrix, pz is a vector of length k\n",
      "and » isa k x k symmetric, positive definite matrix. Setting 4 = 0 and\n",
      "X= I gives back the standard Normal.\n",
      "\n",
      "Since & is symmetric and positive definite, it can be shown that there\n",
      "exists a matrix D'/? — called the square root of © — with the following\n",
      "properties: (i) D1 is symmetric, (ii) © = S'/?E!/? and (iii) D?N-V/? =\n",
      "E-M2y 1/2 — J where DW? = (D1/)-1,\n",
      "\n",
      "Theorem 3.42 If Z ~ N(O,J) and X = w+D\"?Z then X ~ N(u,).\n",
      "Conversely, if X ~ N(p,S), then S-/?(X — pw) ~ N(0,1).\n",
      "\n",
      "Suppose we partition a random Normal vector X as X = (Xq,Xs) We\n",
      "can similarly partition ps = (a, Wy) and\n",
      "\n",
      "Yaa\n",
      "ye aa Mab)\n",
      "( Xa Lop )\n",
      "Theorem 3.43 Let X ~ N(u,¥). Then:\n",
      "\n",
      "(1) The marginal distribition of Xq is Xq ~~ N (fa, Naa)-\n",
      "(2) The conditional distribition of X, given X, = tq is\n",
      "\n",
      "Xp|Xa = ta ~ N ( pe + StaZaa (a — Ha): Sov — VsaUaa Lab ) -\n",
      "\n",
      "(3) If a is a vector then aX ~ N(a™p, a7 Sa).\n",
      "(4) V = (X— wPEX — H) ~ XG.\n",
      "\n",
      "3.11 Transformations of Random Variables\n",
      "\n",
      "Suppose that X is a random variable with ppF fx and cpr Fy. Let\n",
      "Y =r(X) be a function of X, for example, Y = X? or Y = e*. We call\n",
      "Y =r(X) a transformation of X. How do we compute the ppF and CDF of\n",
      "Y? In the discrete case, the answer is easy. The mass function of Y is given\n",
      "by\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-053.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "3.11. TRANSFORMATIONS OF RANDOM VARIABLES 61\n",
      "\n",
      " \n",
      "\n",
      "fry) = P(Y = y) = P(r(X) = 9) = P({a5 r(@) = y}) = P(X €r\"(y));\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 3.44 Suppose that P(X = —1) = P(X =1) =1/4 and P(X = 0) =\n",
      "1/2. Let Y = X*. Then, P(Y = 0) = P(X = 0) = 1/2 and P(Y = 1) =\n",
      "P(X =1)+ P(X = -1) = 1/2. Summarizing:\n",
      "x é\n",
      "ae y_ fy)\n",
      "0 172\n",
      "0 1/2 1 1/2\n",
      "1 If\n",
      "Y takes fewer values than X because the transformation is not one-to-one.\n",
      "\n",
      "The continuous case is harder. There are three steps for finding fy:\n",
      "\n",
      " \n",
      "\n",
      "Three steps for transformations\n",
      "1. For each y, find the set Ay = {x: r(x) < y}.\n",
      "2. Find the cbF\n",
      "PY <y)=P(r(X) <y)\n",
      "P({x; r(x) < y}) =| fx (x)dx (3.10)\n",
      "\n",
      "Ay\n",
      "\n",
      "Fy(y)\n",
      "\n",
      "3. The por is fy(y) = Fy-(y).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 3.45 Let fx(x) = e* for x > 0. Then Fx(x) = Io fx(s)ds =\n",
      "l—e*. Let Y =r(X)=logX. Then Ay = {x: x < e%} and\n",
      "\n",
      "Fy(y) =P(Y < y) = P(logX < y) =P(X < e”) = Fx(e’) =1- e.\n",
      "\n",
      "Therefore, fy(y) = ee\" foryeR. wf\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-054.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "62 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      "Example 3.46 Let X ~ Unif(-1,3). Find the pdf of Y = X?. The density of\n",
      "X is\n",
      "\n",
      "_f 1/4 if -l<a<3\n",
      "f(a) = { 0 otherwise.\n",
      "\n",
      "Y can only take values in (0,9). Consider two case: (i) 0 < y < 1 and\n",
      "(ti) VS y <9. For case (i), Ay = [—/U./9] and Fy(y) = fy, fx(@)dx =\n",
      "\n",
      "(1/2),/¥. For case (ii), Ay = [-1, /y] and Fy(y) = Sa, fx(a)dx = (1/4)(\\/7+\n",
      "1). Differentiating F we get\n",
      "\n",
      "if0<y<1\n",
      "fyry= ifl<y<9\n",
      "\n",
      "8\n",
      "0 otherwise. MI\n",
      "\n",
      "is\n",
      "\n",
      "a\n",
      "\n",
      "When r is strictly monotone increasing or strictly monotone decreasing\n",
      "then r has an inverse s = r~' and in this case one can show that\n",
      "\n",
      "f(y) = fx(s))\n",
      "\n",
      " \n",
      "\n",
      "(3.11)\n",
      "\n",
      " \n",
      "\n",
      "ao ;\n",
      "dy\n",
      "\n",
      "3.12 Transformations of Several Random Vari-\n",
      "ables\n",
      "\n",
      "In some cases we are interested in transformation of several random vari-\n",
      "ables. For example, if X and Y are given random variables, we might want\n",
      "to know the distribution of X/Y, X + Y, max{X,Y} or min{X,Y}. Let\n",
      "Z = 1r(X,Y) be the function of interest. The steps for finding fz are the\n",
      "same as before:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-055.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "3.12. TRANSFORMATIONS OF SEVERAL RANDOM VARIABLES 63\n",
      "\n",
      " \n",
      "\n",
      "1. For each z, find the set A, = {(2,y) + r(z,y) <2}.\n",
      "2. Find the cpF\n",
      "F(z) = P(Z <2) =P(r(X,Y) <2)\n",
      "P(e): raw) <2) = ff txv(ew)aedy.\n",
      "\n",
      "Az\n",
      "\n",
      "3. Then fz(z) = F(z).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 3.47 Let X,,X2 ~ Unif(0,1) be independent. Find the density of\n",
      "Y = X,+ X2. The joint density of (X1, X2) is\n",
      "\n",
      "f(#1,%2) = {\n",
      "\n",
      "Let r(#1,%2) = 41 +22. Now,\n",
      "\n",
      "Fy) = PY <y) =P(r(%, Xe) <0)\n",
      "= P({(ai,92) + r(01,02) < y}) = | I flr, 22)derdry.\n",
      "\n",
      "Now comes the hard part: finding Ay. First suppose that 0 < y <1. Then\n",
      "Ay is the triangle with vertices (0,0), (y,0) and (0,y). See Figure 3.6. In\n",
      "this case, Sta f (a1, £2)dxiday is the area of this triangle which y?/2. If\n",
      "1<y <2 then A, is everything in the unit square except the triangle with\n",
      "vertices (1,y — 1), (1,1), (y—1,1). This set has area 1 — y?/2. Therefore,\n",
      "\n",
      "1 0<a<1,0<a%,<1\n",
      "0 otherwise.\n",
      "\n",
      "0 y<0\n",
      "jz\n",
      "Fy(y) = . P : : * :\n",
      "-E isy<2\n",
      "1 y > 2.\n",
      "By differentiation, the PDF is\n",
      "y O<sy<l\n",
      "fry)=4 l-y lsys2\n",
      "\n",
      "0 otherwise. Mi\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-056.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "64 CHAPTER 3. RANDOM VARIABLES\n",
      "\n",
      " \n",
      "\n",
      "ae\n",
      "r i (y )\n",
      "(0,y) (l,y-1)\n",
      "0 0\n",
      "0 (y,0) 4 0 1\n",
      "This is the case 0 <y <1. This is the case 1 < y <2.\n",
      "\n",
      "Figure 3.6: The set A, for example 3.47.\n",
      "\n",
      "3.13 Technical Appendix\n",
      "\n",
      "Recall that a probability measure P is defined on a o-field A of a sam-\n",
      "ple space 2. A random variable X is a measurable map X :2 > R.\n",
      "Measurable means that, for every x, {w: X(w) <a} eA.\n",
      "\n",
      "3.14 Excercises\n",
      "\n",
      "1. Show that\n",
      "\n",
      "and\n",
      "F (a2) — F(a1) = P(X < a2) —P(X < 21).\n",
      "2. Let X be such that P(X = 2) = P(X = 3) = 1/10 and P(X =\n",
      "5) = 8/10. Plot the cor F. Use F to find P(2 < X < 4.8) and\n",
      "P2< X < 4.8).\n",
      "\n",
      "3. Prove Lemma 3.15.\n",
      "\n",
      "4, Let X have probability density function\n",
      "\n",
      "1/4 0<a<l\n",
      "fx(z) = 4 3/8 3<a<5\n",
      "(0 otherwise.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-057.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "3.14.\n",
      "\n",
      "10.\n",
      "\n",
      "11.\n",
      "\n",
      "12.\n",
      "\n",
      "EXCERCISES 65\n",
      "\n",
      "(a) Find the cumulative distribution function of X.\n",
      "\n",
      "(b) Let Y = 1/X. Find the probability density function fy(y) for Y.\n",
      "Hint: Consider three cases: } <y <$,}<y<landy>1.\n",
      "\n",
      ". Let X and Y be discrete random variables. Show that X and Y are\n",
      "\n",
      "independent if and only if fxy(#,y) = fx(x)fy(y) for all x and y.\n",
      "\n",
      ". Let X have distribution F' and density function f and let A be a subset\n",
      "\n",
      "of the real line. Let I4(x) be the indicator function for A:\n",
      "\n",
      "1aeA\n",
      "\n",
      "m= { 0 tg 4\n",
      "\n",
      "Let Y = J4(X). Find an expression for the cumulative distribution of\n",
      "Y. (Hint: first find the probability mass function for Y.)\n",
      "\n",
      ". Let X and Y be independent and suppose that each has a Uniform(0, 1)\n",
      "\n",
      "distribution. Let Z = min{X, Y}. Find the density fz(z) for Z. Hint:\n",
      "It might be easier to first find P(Z > z).\n",
      "\n",
      ". Let X have cdf F’. Find the cdf of Xt = max{0, X}.\n",
      "\n",
      ". Let X ~ Exp(f). Find F(x) and F-1(q).\n",
      "\n",
      "Let X and Y be independent. Show that g(X) is independent of h(Y)\n",
      "where g and h are functions.\n",
      "\n",
      "Suppose we toss a coin once and let p be the probability of heads. Let\n",
      "X denote the number of heads and let Y denote the number of tails.\n",
      "(a) Prove that X and Y are dependent.\n",
      "\n",
      "(b) Let N ~ Poisson(\\) and suppose we toss a coin N times. Let X\n",
      "and Y be the number of heads and tails. Show that X and Y are\n",
      "independent.\n",
      "\n",
      "Prove Theorem 3.33.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-058.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n",
      "\n",
      "13.\n",
      "\n",
      "14,\n",
      "\n",
      "16.\n",
      "\n",
      "I.\n",
      "\n",
      "18.\n",
      "\n",
      "CHAPTER 3. RANDOM VARIABLES\n",
      "Let X ~ N(0,1) and let Y = e*.\n",
      "(a) Find the pdf for Y. Plot it.\n",
      "\n",
      "(b) (Computer Experiment.) Generate a vector « = (21,-.--, 210,000)\n",
      "consisting of 10,000 random standard Normals. Let y = (y1,---,¥10,000)\n",
      "where y; = e*'. Draw a histogram of y and compare it to the PDF you\n",
      "found in part (a).\n",
      "\n",
      "Let (X,Y) be uniformly distributed on the unit dise {(x,y): a?+y? <\n",
      "1}. Let R= /X?+Y?. Find the cdf and pdf of R.\n",
      "\n",
      ". (A universal random number generator.) Let X have a continuous,\n",
      "\n",
      "strictly increasing cor F’. Let Y = F(X). Find the density of Y. This\n",
      "is called the probability integral transform. Now let U ~ Uniform(0, 1)\n",
      "and let X = F-1(U). Show that X ~ F. Now write a program that\n",
      "takes Uniform (0,1) random variables and generates random variables\n",
      "from an Exponential (3) distribution.\n",
      "\n",
      "Let X ~ Poisson(\\) and Y ~ Poisson(y) and assume that X and Y are\n",
      "independent. Show that the distribution of X given that X +Y =n\n",
      "is Binomial(n, 7) where 7 = A/(A + p).\n",
      "\n",
      "Hint 1: You may use the following fact: If X ~ Poisson(A) and Y ~\n",
      "Poisson(y), and X and Y are independent, then X + Y ~ Poisson(y+\n",
      "d).\n",
      "\n",
      "Hint 2: Note that {X =a, X+Y=n}={X =2, Y=n-a}.\n",
      "\n",
      "Let\n",
      "_ f caty’) 0<a<land0<y<1\n",
      "fav @9) = { 0 otherwise.\n",
      "Find P(X <4|¥Y =$).\n",
      "Let X ~ N(3,16). Solve the following using the Normal table and\n",
      "using a computer package.\n",
      "\n",
      "(a) Find P(X <7).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-059.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "3.14. EXCERCISES 67\n",
      "\n",
      "(b) Find P(X > —2).\n",
      "(c) Find g such that P(X > x) = .05.\n",
      "(d) Find P(O< X <4).\n",
      "(e) Find g such that P(|X| > |z|) = .05.\n",
      "19. Prove formula (3.11).\n",
      "20. Let X,Y ~ Unif(0,1) be independent. Find the ppr for X — Y and\n",
      "X/Y.\n",
      "21. Let X1,...,X, ~ Exp(8) be tp . Let Y = max{X,,...,X,}. Find\n",
      "the ppr of Y. Hint: Y < y if and only if X; < y fori =1,...,n.\n",
      "22. Let X and Y be random variables. Suppose that E(Y|X) = X. Show\n",
      "that Cov(X,Y) = V(X).\n",
      "\n",
      "23. Let X ~ Uniform(0,1). Let 0<a<b<1. Let\n",
      "\n",
      "1 0<a<b\n",
      "Y= { 0 otherwise\n",
      "\n",
      "and let\n",
      "\n",
      "Zea la<a<l\n",
      "~ [0 otherwise\n",
      "\n",
      "(a) Are Y and Z independent? Why/Why not?\n",
      "\n",
      "(b) (10 points) Find E(Y|Z). Hint: What values z can Z take? Now\n",
      "find E(Y|Z = z).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-060.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-061.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "Chapter 4\n",
      "\n",
      "Expectation\n",
      "\n",
      "4.1 Expectation of a Random Variable\n",
      "\n",
      "The expectation (or mean) of a random variable X is the average value\n",
      "of X. The formal definition is as follows.\n",
      "\n",
      " \n",
      "\n",
      "Definition 4.1 The expected value, or mean, or first moment, of\n",
      "X is defined to be\n",
      "\n",
      "_ _ tf (x) if X is discrete\n",
      "K(X) = [earc) ~ { {af(c)dx if X is continuous (4)\n",
      "\n",
      "assuming that the sum (or integral) is well-defined. We use the follow-\n",
      "ing notation to denote the expected value of X:\n",
      "\n",
      "E(X) = EX = [« dF(é)= w= te: (4.2)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The expectation is a one-number summary of the distribution. Think of\n",
      "E(X) as the average value you would obtain if you computed the numerical\n",
      "average n=! 7\", X; of a large number of 1D draws X,,...,X,. The fact\n",
      "that E(X) + n-! 72, X; is actually more than a heuristic: it is a theorem\n",
      "called the law of large numbers that we will discuss later. The notation\n",
      "\n",
      "69\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-062.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "70 CHAPTER 4. EXPECTATION\n",
      "\n",
      "[ cdF(x) deserves some comment. We use it merely as a convenient unifying\n",
      "notation so we don’t have to write >, f(x) for discrete random variables\n",
      "and f xf(z)dz for continuous random variables but you should be aware that\n",
      "J cdF(z) has a precise meaning that is discussed in real anlysis courses.\n",
      "\n",
      "To ensure that E(X) is well defined, we say that E(X) exists if [, |x|dFx(x) <\n",
      "co. Otherwise we say that the expectation does not exist.\n",
      "\n",
      "Example 4.2 Let X ~ Bernoulli(p). Then E(X) = ye af(z) = (0x -\n",
      "p))+(.x p)=p.\n",
      "\n",
      "Example 4.3 Flip a fair coin two times. Let X be the number of heads. Then,\n",
      "E(X) = fadFx(e) = 0, efx(2) = (0 x f(0)) + Ax £G)) + 2x £2) =\n",
      "(0 x (1/4)) + (1 x (1/2)) + (2x (1/4)) =1.\n",
      "\n",
      "Example 4.4 Let X ~ Unif(—1,3). Then, E(X) = f adF x(x) = f xfx(x)dt =\n",
      "LPiadv=1. a\n",
      "\n",
      "Example 4.5 Recall that a random variable has a Cauchy distribution if it has\n",
      "density fx(x) = {1(1 + .27)}-1. Using integration by parts, (set u = x and\n",
      "v= tans),\n",
      "\n",
      "/ ja|dF (x) = =[ A [z tao] f tan! x da = 00\n",
      "0\n",
      "\n",
      " \n",
      "\n",
      "T 1+ a?\n",
      "\n",
      "so the mean does not exist. If you simulate a Cauchy distribution many\n",
      "times and take the average, you will see that the average never settles down.\n",
      "This is because the Cauchy has thick tails and hence extreme observations\n",
      "are common.\n",
      "\n",
      "From now on, whenever we discuss expectations, we implicitly assume\n",
      "that they exist.\n",
      "\n",
      "Let Y = r(X). How do we compute E(Y)? One way is to find fy(y) and\n",
      "then compute E(Y) = f yfy(y)dy. But there is an easier way.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-063.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "4.1. EXPECTATION OF A RANDOM VARIABLE 71\n",
      "\n",
      " \n",
      "\n",
      "Theorem 4.6 (The rule of the lazy statistician.) Let Y = r(X). Then\n",
      "\n",
      "EY) =B((x)) = f rare. (4.3)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "This result makes intuitive sense. Think of playing a game where we\n",
      "draw X at random and then I pay you Y = r(X). Your average income is\n",
      "r(x) times the chance that X = x, summed (or integrated) over all values\n",
      "of x. Here is a special case. Let A be an event and let r(x) = I4(x) where\n",
      "I4(z) =1ifae € A and I4(x) = 0 if ¢ A. Then\n",
      "\n",
      "E(14(X)) = [ taw@ix(aae = | fula)de = P(X € A).\n",
      "A\n",
      "In other words, probability is a special case of expectation.\n",
      "\n",
      "Example 4.7 Let X ~ Unif(0,1). Let Y =r(X) =e*. Then,\n",
      "\n",
      "1 1\n",
      "E(Y)= | e*f(a)dr = | @dc =e—1.\n",
      "\n",
      "Alternatively, you could find fy(y) which turns out to be fy(y) = 1/y for\n",
      "l<y<e. Then, EY) = ffyfy)dy=e—1. w\n",
      "\n",
      "Example 4.8 Take a stick of unit length and break it at random. Let Y be the\n",
      "length of the longer piece. What is the mean of Y? If X is the break point\n",
      "then X ~ Unif(0,1) and Y = r(X) = max{X,1— X}. Thus, r(z) =1—a\n",
      "when 0 <a <1/2 andr(x) =a when1/2< 2 <1. Hence,\n",
      "\n",
      "a0) = frware= [\"0-aars f eae = - 1\n",
      "\n",
      "Functions of several variables are handled in a similar way. If Z = r(X,Y)\n",
      "then\n",
      "\n",
      "BZ) =B(r(x,¥)) = ff re,wate,9)- (44)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-064.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "72 CHAPTER 4. EXPECTATION\n",
      "\n",
      "Example 4.9 Let (X,Y) have a jointly uniform distribution on the unit square.\n",
      "Let Z=r(X,Y) =X?+Y?. Then,\n",
      "\n",
      "BZ)= ff rowareaa= fo [dra = [oars [vr av= : Lt\n",
      "\n",
      "The k'* moment of X is defined to be E(X*) assuming that E(|X|*) <\n",
      "oo. We shall rarely make much use of moments beyond k = 2.\n",
      "\n",
      "4.2 Properties of Expectations\n",
      "\n",
      "Theorem 4.10 Jf X1,...,X, are random variables and ai,...,Gn are con-\n",
      "stants, then\n",
      "\n",
      "E (= oa =o aE(%). (4.5)\n",
      "\n",
      "Example 4.11 Let X ~ Binomial(n,p). What is the mean of X? We could\n",
      "try to appeal to the definition:\n",
      "\n",
      "Bx) = f edFee) =D 2fs(a) = 02(\"\\ra—p\"\n",
      "\n",
      "a «=0\n",
      "\n",
      "but this is not an easy sum to evaluate. Instead, note that X = SY, X;\n",
      "where X; = 1 if the i* toss is heads and X; = 0 otherwise. Then E(X;) =\n",
      "\n",
      "(p x 1) + ((L—p) x 0) = p and E(X) = E(X), Xi) = DEX) = np.\n",
      "\n",
      "Theorem 4.12 Let X,,...,X, be independent random variables. Then,\n",
      "\n",
      "E (11 x) =| [EX). (4.6)\n",
      "\n",
      "Notice that the summation rule does not require independence but the\n",
      "multiplication rule does.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-065.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "can’t use E(X —\n",
      "as a measure of\n",
      "sad since E(X —\n",
      "= E(X)-p=\n",
      "-p = 0. We\n",
      "and sometimes\n",
      "use E|X — pu| as\n",
      "leasure of spread\n",
      "more often we\n",
      "the variance.\n",
      "\n",
      "4.3. VARIANCE AND COVARIANCE 73\n",
      "\n",
      "4.3. Variance and Covariance\n",
      "\n",
      "The variance measures the “spread” of a distribution.\n",
      "\n",
      " \n",
      "\n",
      "Definition 4.13 Let X be a random variable with mean yp. The variance\n",
      "of X — denoted by 0? or 0% or V(X) or V(X) or VX — is defined by\n",
      "\n",
      "=B(X — p= f (@- are) (47)\n",
      "\n",
      "assuming this expectation exists. The standard deviation is sd(X) =\n",
      "V(X) and is also denoted by o and ox.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 4.14 Assuming the variance is well defined, it has the following\n",
      "properties:\n",
      "\n",
      "1. V(X) = E(X?) =p’.\n",
      "2. Ifa and b are constants then V(aX + b) = a? V(X).\n",
      "\n",
      "3. If Xy,...,Xq are independent and a1,..., Gn are constants, then\n",
      "\n",
      "Vv (= oat = yraV(X). (4.8)\n",
      "\n",
      "Example 4.15 Let X ~ Binomial(n,p). We write X = >, X; where X; = 1\n",
      "if toss i is heads and X; = 0 otherwise. Then X = S°, X; and the random\n",
      "variables are independent. Also, P(X; = 1) = p and P(X; = 0) = 1—p.\n",
      "Recall that\n",
      "\n",
      "E(X;) = [px 1] +[(1—p) x ]=p\n",
      "Now,\n",
      "\n",
      "E(X?) = [px ?]+[G —p) xO] =p.\n",
      "Therefore, V(X;) = E(X?) — p = p—p = p(l—p). Finally, V(X) =\n",
      "V(O; Xi) = 0; VOG) = Op. — p) = np. — p). Notice that V(X) = 0 if\n",
      "\n",
      "p=1orp=0. Make sure you see why this makes intuitive sense.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-066.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 CHAPTER 4. EXPECTATION\n",
      "\n",
      "If X,,...,X, are random variables then we define the sample mean to\n",
      "be\n",
      "an bie\n",
      "X= 7 OX (49)\n",
      "t=1\n",
      "\n",
      "and the sample variance to be\n",
      "\n",
      "Sis Se i-®). (4.10)\n",
      "a=1\n",
      "\n",
      " \n",
      "\n",
      "Theorem 4.16 Let X1,...,X, be UD and let p= E(X;), 0? = V(X;). Then\n",
      "\n",
      "2\n",
      "B(X,) =, VO%n)=— and E(S{) =o.\n",
      "\n",
      "If X and Y are random variables, then the covariance and correlation\n",
      "between X and Y measure how strong the linear relationship is between X\n",
      "\n",
      "and Y.\n",
      "\n",
      " \n",
      "\n",
      "Definition 4.17 Let X and Y be random variables with means x and\n",
      "by and standard deviations ox and oy. Define the covariance between\n",
      "\n",
      "X andY by\n",
      "Cov(X,Y) = E[(X — px)(¥ — py) (4.11)\n",
      "and the correlation by\n",
      "“3 Cov(X, Y\n",
      "P= pxy =AX,Y) = Co) (4.12)\n",
      "Ox0y\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 4.18 The covariance satisfies:\n",
      "Cov(X,Y) = E(XY) — E(X)E(¥).\n",
      "\n",
      "The correlation satisfies:\n",
      "\n",
      "-1< (X,Y) <1\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-067.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "4.4, EXPECTATION AND VARIANCE OF IMPORTANT RANDOM VARIABLES75\n",
      "\n",
      "If Y =a+bX for some constants a and b then p(X,Y) = 1 ifb > 0 and\n",
      "p(X, Y) =—-1ifb <0. IfX andY are independent, then Cov(X,Y) = p=0.\n",
      "The converse is not true in general.\n",
      "\n",
      "Theorem 4.19 V(X + Y) = V(X) + V(Y) + 2Cov(X,Y) and V(X —Y) =\n",
      "V(X)+V(Y)—2Cov(X, Y). More generally, for random variables X,,...,Xn,\n",
      "\n",
      "Vv (= oa = Ss a?V(X;) +2 S S aja; Cov(X;, Xj).\n",
      "\n",
      "i<j\n",
      "\n",
      "4.4 Expectation and Variance of Important\n",
      "Random Variables\n",
      "\n",
      "Here we record the expectation of some important random variables.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Distribution Mean Variance\n",
      "\n",
      "Point mass at a a 0\n",
      "\n",
      "Bernoulli (p) p p(1-p)\n",
      "\n",
      "Binomial (n,p) p n p (1-p)\n",
      "Geometric (p) 1/p (l—p)/r\n",
      "\n",
      "Poisson (A) a a\n",
      "\n",
      "Uniform (a,b) (a+b) /2 (b—a)?/12\n",
      "\n",
      "Normal (1,07) Lb o\n",
      "\n",
      "Exponential (() B fom\n",
      "\n",
      "Gamma (a, 3) aB ap?\n",
      "\n",
      "Beta (a, 8) a/(a+ 8) aB/((a+ 8)(a+8+1))\n",
      "ty O(ify >1) v/(v—2) (ifv > 2)\n",
      "Pe P 2p\n",
      "\n",
      "Multinomial (n, p) np see below\n",
      "Multivariate Normal (1,2) ps x\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "We derived E(X) and V(X) for the binomial in the previous section. The\n",
      "calculations for some of the others are in the excercises.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-068.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "76 CHAPTER 4. EXPECTATION\n",
      "\n",
      "The last two entries in the table are multivariate models which involve a\n",
      "random vector X of the form\n",
      "\n",
      "X\n",
      "X=| :\n",
      "Xk\n",
      "The mean of a random vector X is defined by\n",
      "Ma E(X1)\n",
      "w=| i f=]:\n",
      "Mk E(Xx)\n",
      "The variance-covariance matrix » is defined to be\n",
      "V(Xi) Cov(X1,X2) +++ Cov(X1, Xx)\n",
      "v(x) = Conk) nial : . Cov(X2, Xe)\n",
      "Cov(Xx,X1) Cov( Xz, X2) +++ V(Xx)\n",
      "If X ~ Multinomial(n, p) then E(X) = np = n(pi,-.-., px) and\n",
      "npi(l— pi) —npipe +++ NPL\n",
      "v(x) = ~npeps nas — pe) ” ~nPeP\n",
      "—NPKP1 —NDKP2 +++ mpr(1 — pr)\n",
      "\n",
      "To see this, note that the marginal distribution of any one component of\n",
      "the vector is binomial, that is X; ~ Binomial(n,p;). Thus, E(X;) = np;\n",
      "and V(X;) = np;(1— pi). Note that X; + X; ~ Binomial(n,p; + p;). Thus,\n",
      "V(X; + X;) = n(p; + pj) — [p; + pj])- On the other hand, using the formula\n",
      "for the variance of a sum, we have that V(X; + X;) = V(X;) + V(X;) +\n",
      "2Cov(X;, X;) = npi(1 — pi) + npj(1 — pj) + 2Cov(X;, X;). If you equate this\n",
      "formula with n(p; + p;)(pi + pj) and solve, one gets Cov(X;, X;) = —npip;.\n",
      "\n",
      "Finally, here is a lemma that can be useful for finding means and variances\n",
      "of linear combinations of multivariate random vectors.\n",
      "\n",
      "Lemma 4.20 Jf a is a vector and X is a random vector with mean js and\n",
      "variance © then E(a?X) = aT and V(a? X) = a7Xa. If A is a matrix then\n",
      "E(AX) = Ay and V(AX) = ASAT.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-069.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "4.5. CONDITIONAL EXPECTATION 77\n",
      "\n",
      "4.5 Conditional Expectation\n",
      "\n",
      "Suppose that X and Y are random variables. What is the mean of X\n",
      "among those times when Y = y? The answer is that we compute the mean\n",
      "of X as before but we substitute fxjy(x|y) for fx(x) in the definition of\n",
      "expectation.\n",
      "\n",
      " \n",
      "\n",
      "Definition 4.21 The conditional expectation of X given Y = y is\n",
      "\n",
      "Ye fxiy(ely) dx discrete case\n",
      "\n",
      "4.13\n",
      "[2 fxjy(zly) dx continuous case. (4.13)\n",
      "\n",
      "nar=0={\n",
      "\n",
      "If r(x, y) is a function of x and y then\n",
      "\n",
      "Sr(a,y) fxjy (aly) da discrete case\n",
      "\n",
      "r(@y) fxiy(aly) dx continuous case.\n",
      "(4.14)\n",
      "\n",
      "Biri yi = 0) = {\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Whereas, E(X) is a number, E(X|Y = y) is a function of y. Before we\n",
      "observe Y, we don’t know the value of E(X|Y = y) so it is a random variable\n",
      "which we denote E(X|Y). In other words, E(X|Y) is the random variable\n",
      "whose value is E(X|Y = y) when y = y. Similarly, E(r(X,Y)|Y) is the\n",
      "random variable whose value is E(r(X,Y)|Y = y) when y = y. This isa\n",
      "very confusing point so let us look at an example.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 4.22 Suppose we draw X ~ Unif(0,1). After we observe X = z,\n",
      "we draw Y|\\X = x ~ Unif(z,1). Intuitively, we expect that E(Y|X = x) =\n",
      "(1+ .2)/2. In fact, fy\\x(y|x) =1/(l-«) fora <y <1 and\n",
      "\n",
      " \n",
      "\n",
      "1 1 1 1 +2\n",
      "E(Y|X = 2) -[ y Fyix (yla)dy = —/ ydy =\n",
      "© 1-gJ, 2\n",
      "as expected. Thus, E(Y|X) = (1+X)/2. Notice that E(Y|X) = (1+ X)/2 is\n",
      "a random variable whose value is the number E(Y|X = x) = (1+ 2)/2 once\n",
      "X =x is observed. @\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-070.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "78 CHAPTER 4. EXPECTATION\n",
      "\n",
      "Theorem 4.23 (The rule of iterated expectations.) For random variables X and\n",
      "Y, assuming the expectations exist, we have that\n",
      "\n",
      "E[E(Y|X)]=E(Y) and E[E(X|Y)] = E(X). (4.15)\n",
      "More generally, for any function r(x, y) we have\n",
      "\n",
      "E([E(r(X,Y)|X)] =E(r(X,Y)) and E[B(r(X,Y)|X)] = E(r(X,Y)).\n",
      "(4.16)\n",
      "\n",
      "ProoF. We'll prove the first equation. Using the definition of conditional\n",
      "expectation and the fact that f(,y) = f(x)f(y|z),\n",
      "\n",
      "E(E(Y|X)]\n",
      "\n",
      "[eix=aisteyae= ff veculaaysteyar\n",
      "| [vtolore@aca = [ [vile naedy = 80. 7\n",
      "\n",
      "Example 4.24 Consider example 4.22. How can we compute E(Y)? One\n",
      "method is to find the joint density f(x,y) and then compute E(Y) = f fy f(z, y)dxdy.\n",
      "An easier way is to do this in two steps. First, we already know that E(Y|X) =\n",
      "\n",
      "(1+ X)/2. Thus,\n",
      "\n",
      " \n",
      "\n",
      "1 RB Hd (es 0) = ORO) 40/2) ayy\n",
      "\n",
      "Definition 4.25 The conditional variance is defined as\n",
      "\n",
      "Vorix =a) = fw ne))*70la)ay (417)\n",
      "where u(x) = E(Y|X = 2).\n",
      "Theorem 4.26 For random variables X and Y,\n",
      "\n",
      "V(Y) = EV(Y|X) + VE(Y|X).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-071.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "4.6. TECHNICAL APPENDIX 79\n",
      "\n",
      "Example 4.27 Draw a county at random from the United States. Then draw\n",
      "n people at random from the county. Let X be the number of those people\n",
      "who have a certain disease. If Q denotes the proportion of of people in that\n",
      "county with the disease then Q is also a random variable since it varies from\n",
      "county to county. Given Q = q, we have that X ~ Binomial(n,q). Thus,\n",
      "E(X|Q = q) = ng and V(X|Q = q) = ng(1—q). Suppose that the random\n",
      "variable P has a Uniform (0,1) distribution. Then, E(X) = EE(X|Q) =\n",
      "E(nQ) = nE(Q) = n/2. Let us compute the variance of X. Now, V(X) =\n",
      "EV(X|Q) + VE(X|Q). Let’s compute these two terms. First, EV(X|Q) =\n",
      "E[nQ(— Q)] = nE(Q(1—Q)) =n f alt —a) fa)da =n fi q(d —g)dq = 1/6.\n",
      "Nett, VE(X|Q) = V(nQ) = n?V(Q) = n? f (q— (1/2))?dq = n?/12. Hence,\n",
      "V(X) = (n/6) + (n?/12).\n",
      "\n",
      "4.6 Technical Appendix\n",
      "\n",
      "4.6.1 Expectation as an Integral\n",
      "\n",
      "The integral of a measurable function r(x) is defined as follows. First sup-\n",
      "pose that r is simple, meaning that it takes finitely many values ay,..., ax\n",
      "over a partition Ai,...,Ag- Then fr(z)dF(«) = WW, aP(r(X) € Aj).\n",
      "The integral of a positive measurable function r is defined by f r()dF (a) =\n",
      "lim, f r;(x)dF(a) where r; is a sequence of simple functions such that r;(x) <\n",
      "r(z) and ri(z) > r(x) as i + co. This does not depend on the partic-\n",
      "ular sequence. The integral of a measurable function r is defined to be\n",
      "[r(@)dF(x) = frt(@)dF(z) — [r-(z)dF (x) assuming both integrals are\n",
      "finite, where r*(x) = max{r(x),0} and r-(z) = — min{r(z), O}.\n",
      "\n",
      "4.6.2 Moment Generating Functions\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-072.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "80 CHAPTER 4. EXPECTATION\n",
      "\n",
      " \n",
      "\n",
      "Definition 4.28 The moment generating function (mgf), or Laplace|\n",
      "transform, of X is defined by\n",
      "\n",
      "x(t) = E(e*) = [exarc)\n",
      "\n",
      "where t varies over the real numbers.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "In what follows, we assume that the megf is well defined for all ¢ in small\n",
      "neighborhood of 0. A related function is the characteristic function, defined\n",
      "by E(e*) where i = /—I. This function is always well defined for all t. The\n",
      "ingf is useful for several reasons. First, it helps us compute the moments of\n",
      "a distribution. Second, it helps us find the distribution of sums of random\n",
      "variables. Third, it is used to prove the central limit theorem which we\n",
      "discuss later.\n",
      "\n",
      " \n",
      "\n",
      "When the mef is well defined, it can be shown that we change interchange\n",
      "the operations of differentiation and “taking expectation.” This leads to\n",
      "d\n",
      "\n",
      "d\n",
      "y'(0) = ae\" | =E ae] =E[Xe*],_, =E(X).\n",
      "dt t=0 dt t=0 Jiao\n",
      "\n",
      " \n",
      "\n",
      "By taking for derivatives we conclude that 7“) (0) = E(X*). This gives us a\n",
      "method for computing the moments of a distribution.\n",
      "\n",
      "Example 4.29 Let X ~ Exp(1). For anyt <1,\n",
      "\n",
      "20 0\n",
      "x(t) = Eé* = | edz = [ eV dz = —_.\n",
      "0 0 1—t\n",
      "\n",
      "The integral is divergent if t >1. So, wx(t)=1/(1—t) for allt < 1. Now,\n",
      "w'(0) = 1 and w\"(0) = 2. Hence, E(X) = 1 and V(X) = E(X?) — 2 =\n",
      "21 1.\n",
      "\n",
      "Lemma 4.30 Properties of the mgf.\n",
      "\n",
      "(1) IfY = aX +6 then dy (t) = eux (at).\n",
      "\n",
      "(2) If X1,...,Xn are independent and Y = SY, X; then py (t) = TI, vi(t)\n",
      "where w; is the maf of X;.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-073.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7. EXCERCISES 81\n",
      "\n",
      "Example 4.31 Let X ~ Binomial(n,p). As before we know that X = 0; X;\n",
      "where P(X; = 1) = p and P(X; = 0) =1—p. Now y(t) = Ee*\" = (px e')+\n",
      "((1—p)) = pe'+q where q=1-—p. Thus, x(t) = [], v(t) = (pe +g)”.\n",
      "Theorem 4.32 Let X and Y be random variables. If yx (t) = Wy(t) for all t\n",
      "in an open interval around 0, then X 4y.\n",
      "\n",
      "Example 4.33 Let X ~ Binomial(n1,p) and X ~ Binomial(ng, p) be indepen-\n",
      "dent. Let Y = X, +X. Now\n",
      "\n",
      "by (t) = Yi (t)do() = (pel + 4) (pel +g)” = (pel +g)\"\n",
      "\n",
      "and we recognize the latter as the mgf of a Binomial(n; + n2,p) distribution.\n",
      "Since the mgf characterizes the distribution (i.e. there can’t be another ran-\n",
      "dom variable which has the same mgf) we conclude that Y ~ Bin(ni+nz, p)-\n",
      "\n",
      "Moment Generating Function for Some Common Distributions\n",
      "\n",
      "Distribution mgf\n",
      "Bernoulli (p) pe’ + (1— p)\n",
      "Binomial (n,p) (pe’ + (1 — p))”\n",
      "\n",
      "Poisson (A) exe)\n",
      "\n",
      " \n",
      "\n",
      "242\n",
      "\n",
      "Normal (4,0) exp {ut + sh\n",
      "\n",
      "Gamma (a,,3) (4)* fort < B\n",
      "\n",
      "4.7 Excercises\n",
      "\n",
      "1. Suppose we play a game where we start with c dollars. On each play of\n",
      "the game you either double or half your money, with equal probability.\n",
      "What is your expected fortune after n trials?\n",
      "\n",
      "2. Show that V(X) = 0 if and only if there is a constant c such that\n",
      "P(X =c)=1.\n",
      "\n",
      "3. Let Xi,...,X, ~ Uniform(0,1) and let Y, = max{X),...,X,}. Find\n",
      "E(Y,).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-074.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "82\n",
      "\n",
      "10.\n",
      "\n",
      "11.\n",
      "\n",
      "CHAPTER 4. EXPECTATION\n",
      "\n",
      ". A particle starts at the origin of the real line and moves along the\n",
      "\n",
      "line in jumps of one unit. For each jump the probability is p that the\n",
      "particle will jump one unit to the left and the probability is 1 — p that\n",
      "the particle will jump one unit to the right. Let X,, be the position of\n",
      "the particle after n units. Find E(X,,) and V(X,,). (This is known as\n",
      "a random walk.)\n",
      "\n",
      ". A fair coin is tossed until a head is obtained. What is the expected\n",
      "\n",
      "number of tosses that will be required?\n",
      "\n",
      ". Prove Theorem 4.6 for discrete random variables.\n",
      "\n",
      ". Let X be a continuous random variable with cDF F. Suppose that\n",
      "\n",
      "P(X > 0) = 1 and that E(X) exists. Show that E(X) = f>°P(X >\n",
      "a)dz.\n",
      "\n",
      "Hint: Consider integrating by parts. The following fact is helpful: if\n",
      "E(X) exists then limy_,.. a[1 — F(2)] = 0.\n",
      "\n",
      ". Prove Theorem 4.16.\n",
      "\n",
      ". (Computer Experiment.) Let X1,X2,...,X, be N(0,1) random vari-\n",
      "\n",
      "ables and let X, = n7+ Wh Xi. Plot Xp versus n for n = 1,..., 10,000.\n",
      "Repeat for X1,.Xo,...,X, ~ Cauchy. Explain why there is such a dif-\n",
      "ference.\n",
      "\n",
      "Let X ~ N(0,1) and let Y = e*. Find E(Y) and V(Y).\n",
      "\n",
      "(Computer Experiment: Simulating the Stock Market.) Let Yi, Yo,... be\n",
      "independent random variables such that P(Y; = 1) = P(Y¥; = -1) =\n",
      "1/2. Let X, = 0, Yi. Think of Y; = 1 as “the stock price increased\n",
      "by one dollar”, Y; = —1 as “the stock price decreased by one dollar”\n",
      "and X,, as the value of the stock on day n.\n",
      "\n",
      "(a) Find E(X,) and V(X,,).\n",
      "(b) Simulate X,, and plot X,, versus n for n = 1,2,...,10,000. Repeat\n",
      "\n",
      "the whole simulation several times. Notice two things. First, it’s easy\n",
      "to “see” patterns in the sequence even though it is random. Second,\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-075.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "4.7.\n",
      "\n",
      "12.\n",
      "\n",
      "13.\n",
      "\n",
      "14.\n",
      "\n",
      "16.\n",
      "\n",
      "EXCERCISES 83\n",
      "\n",
      "you will find that the four runs look very different even though they\n",
      "were generated the same way. How do the calculations in (a) explain\n",
      "the second observation?\n",
      "\n",
      "Prove the formulas given in the table at the beginning of Section 4.4\n",
      "for the Bernoulli, Poisson, Uniform, Exponential, Gamma and Beta.\n",
      "Here are some hints. For the mean of the Poisson, use the fact that\n",
      "e* = 2, a*/a!. To compute the variance, first compute E(X (X —1)).\n",
      "For the mean of the Gamma, it will help to multiply and divide by\n",
      "T(a+1)/8°*' and use the fact that a Gamma density integrates to 1.\n",
      "For the Beta, multiply and divide by ['(a + 1)I'(8)/T(a+ 8+ 1).\n",
      "\n",
      "Suppose we generate a random variable X in the following way. First\n",
      "we flip a fair coin. If the coin is heads, take X to have a Unif(0,1)\n",
      "distribution. If the coin is tails, take X to have a Unif(3,4) distribution.\n",
      "\n",
      "(a) Find the mean of X.\n",
      "(b) Find the standard deviation of X.\n",
      "\n",
      "Let X1,...,Xm and Yi,...,¥Y, be random variables and let a1,...,@m\n",
      "and b;,...,6, be constants. Show that\n",
      "\n",
      "Cov (Soo, Ysx;) = 323 aibCov(X:,¥)).\n",
      "ro j=l\n",
      "\n",
      "i=1 j=l\n",
      "\n",
      " \n",
      "\n",
      ". Let\n",
      "\n",
      "(uty) 0OSa@<10<y<2\n",
      "otherwise.\n",
      "\n",
      "Ixy(a,y) = { 3\n",
      "Find V(2X — 3Y +8).\n",
      "Let r(x) be a function of x and let s(y) be a function of y. Show that\n",
      "E(r(X)s(¥)|X) = r(X)E(s()|X).\n",
      "\n",
      "Also, show that E(r(X)|X) = r(X).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-076.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "84\n",
      "\n",
      "17.\n",
      "\n",
      "18.\n",
      "\n",
      "19.\n",
      "\n",
      "20.\n",
      "\n",
      "CHAPTER 4. EXPECTATION\n",
      "\n",
      "Prove that\n",
      "\n",
      "V(Y) =EV(Y | X)+ VE(Y | X).\n",
      "Hint: Let m = E(Y) and let b(x) = E(Y|X = x). Note that E(b(X)) =\n",
      "EE(Y |X) = E(Y) = m. Bear in mind that b is a function of x. Now\n",
      "write V(Y) = E(Y — m)? = E((Y — b(X)) + (b(X) — m))?. Expand the\n",
      "square and take the expectation. You then have to take the expectation\n",
      "\n",
      " \n",
      "\n",
      "of three terms. In each case, use the rule of the iterated expectation:\n",
      "ie. E(stuff) = E(E(stuff|X)).\n",
      "\n",
      "Show that if E(X\n",
      "uncorrelated.\n",
      "\n",
      " \n",
      "\n",
      "Y = y) =c for some constant c then X and Y are\n",
      "\n",
      "This question is to help you understand the idea of a sampling dis-\n",
      "tribution. Let X,,...,X, be IID with mean jp and variance 0. Let\n",
      "X, =n\" dyX;. Then X,, is a statistic, that is, a function of the\n",
      "data. Since X,, is a random variable, it has a distribution. This distri-\n",
      "bution is called the sampling distribution of the statistic. Recall from\n",
      "Theorem 4.16 that E(X,,) = and V(X,,) = 0?/n. Don’t confuse the\n",
      "distribution of the data fx and the distribution of the statistic FR, To\n",
      "make this clear, let X,,...,X, ~ Uniform(0,1). Let fx be the density\n",
      "of the Uniform(0, 1). Plot fy. Now let X, = n7! 7, Xj. Find E(X,,)\n",
      "and V(X,,). Plot them as a function of n. Comment. Now simulate the\n",
      "distribution of X,, for n = 1,5, 25, 100. Check that the simulated values\n",
      "of E(X,,) and V(X,,) agree with your theoretical calculations. What\n",
      "do you notice about the sampling distribution of X,, as n increases?\n",
      "\n",
      "Prove Lemma 4.20.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-077.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "Chapter 5\n",
      "\n",
      "Inequalities\n",
      "\n",
      "5.1 Markov and Chebychev Inequalities\n",
      "\n",
      "Inequalities are useful for bounding quantities that might otherwise be\n",
      "hard to compute. They will also be used in the theory of convergence which\n",
      "is discussed in the next chapter. Our first inequality is Markov’s inequality.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 5.1 (Markov’s Inequality.) Let X be a non-negative random\n",
      "variable and suppose that E(X) exists. For any t > 0,\n",
      "\n",
      "P(X >2)< ae (5.1)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Proor. E(X) = f° af(a)dx = fi af(x)de+f xf (a)dx > f° xf (x)de >\n",
      "th f(a)de = 1P(X >t).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-078.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "86 CHAPTER 5. INEQUALITIES\n",
      "\n",
      " \n",
      "\n",
      "Theorem 5.2 (Chebyshev’s inequality.) Let p= E(X) and o? = V(X).\n",
      "Then,\n",
      "\n",
      "a 1 7\n",
      "z and P(|Z|>k)< Rp (5.2)\n",
      "where Z = (X — p)/a. In particular, P(|Z| > 2) < 1/4 and P(|Z| >\n",
      "3) < 1/9.\n",
      "\n",
      "P(X -p)>4) <\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "PRooF. We use Markov’s inequality to conclude that\n",
      "\n",
      "E(X — p)? o\n",
      "P(X — pl > 0) =P(X— pl? > 8) < PAW oe\n",
      "\n",
      "The second part follows by setting t= ko.\n",
      "\n",
      "Example 5.3 Suppose we test a prediction method, a neural net for example,\n",
      "on a set of n new test cases. Let X; = 1 if the predictor is wrong and X; =0\n",
      "if the predictor is right. Then X, = n-* SLi Xi is the observed error rate.\n",
      "Each X; may be regarded as a Bernoulli with unknown mean p. We would\n",
      "like to know the true, but unknown error rate p. Intuitively, we expect that\n",
      "X, should be close to p. How likely is X,, to not be within € of p? We have\n",
      "that V(X,) = V(X1)/n? = p(1— p)/n and\n",
      "\n",
      "ba V(Xn) _ pp) 1\n",
      "P(|X >e< <\n",
      "(| P| 6) = e ne — Ane?\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "since p(l—p) < + for all p. For € = .2 andn = 100 the bound is .0625.\n",
      "\n",
      "5.2 Hoeffding’s Inequality\n",
      "\n",
      "Hoeffding’s inequality is similar in spirit to Markov’s inequality but it is\n",
      "a sharper inequality. We present the result here in two parts. The proofs are\n",
      "in the technical appendix.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-079.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2. HOEFFDING’S INEQUALITY 87\n",
      "\n",
      " \n",
      "\n",
      "Theorem 5.4 Let ¥i,...,Y, be independent observations such that\n",
      "E(¥%j) =0 anda; < ¥; < b;. Lete > 0. Then, for any t > 0,\n",
      "\n",
      "P (= ¥i> ‘ Se Tae, (5.3)\n",
      "1, i=1\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 5.5 Let X,,...,X, ~ Bernoulli(p). Then, for any € > 0,\n",
      "P ([X,—p| > 6) $26\" (5.4)\n",
      "\n",
      "where X, =n oy, Xj.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 5.6 Let X,,...,X, ~ Bernoulli(p). Let n = 100 ande = .2. We\n",
      "saw that Chebyshev’s yielded\n",
      "\n",
      "P(|X, —p| > €) < .0625.\n",
      "According to Hoeffding’s inequality,\n",
      "P([X — p| > .2) < 26-7” = 00067\n",
      "which is much smaller than .0625.\n",
      "\n",
      "Hoeffding’s inequality gives us a simple way to create a confidence in-\n",
      "terval for a binomial parameter p. We will discuss confidence intervals later\n",
      "but here is the basic idea. Fix a > 0 and let\n",
      "\n",
      "_ 1 7 2 1/2\n",
      "én = 1 5 los | 3\n",
      "By Hoeffding’s inequality,\n",
      "\n",
      "P(X, — pl > &n) < 26°\" =a.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-080.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243B75A2388>\n",
      "88 CHAPTER 5. INEQUALITIES\n",
      "\n",
      "Let C = (X,, —¢,X, +). Then, P(C ¢ p) = P(|X, —p| > ©) < a. Hence,\n",
      "P(p € C) > 1—a, that is, the random interval C traps the true parameter\n",
      "value p with probability 1 — a; we call C a 1 — a confidence interval. More\n",
      "on this later.\n",
      "\n",
      "5.3 Cauchy-Schwarz and Jensen Inequalities\n",
      "\n",
      "This section contains two inequalities on expected values that are often\n",
      "useful.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 5.7 (Cauchy-Schwarz inequality.) If X and Y have finite vari-\n",
      "\n",
      "ances then\n",
      "E|XY| < /EXX)EQ”). (5.5)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Recall that a function g is convex if for each x, y and each a € [0,1],\n",
      "glax + (1— a)y) < ag(x) + (1 — a)g(y)-\n",
      "\n",
      "If g is twice differentiable, then convexity reduces to checking that g\"(x) > 0\n",
      "for all x. It can be shown that if g is convex then it lies above any line that\n",
      "touches g at some point, called a tangent line. A function g is concave if\n",
      "—g is convex. Examples of convex functions are g(z) = 2° and g(x) = e.\n",
      "Examples of concave functions are g(x) = —2” and g(x) = logz.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 5.8 (Jensen’s Inequality.) If g is convex then\n",
      "Eg(X) > g(EX). (56)\n",
      "\n",
      "If g is concave then\n",
      "Eq(X) < g(BX). (6.7)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Proor. Let L(x) = a+ bz be a line, tangent to g(x) at the point E(X).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-081.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "5.4. TECHNICAL APPENDIX: PROOF OF HOEFFDING’S INEQUALITY89\n",
      "Since g is convex, it lies above the line L(x). So,\n",
      "Eg(X) > EL(X) = E(a+ bX) =a + bE(X) = L(E(X)) = g(EX).\n",
      "\n",
      "From Jensen’s inequality we see that EX? > (EX)? and E(1/X) >\n",
      "1/E(X). Since log is concave, E(log X) < logE(X). For example, suppose\n",
      "that X ~ N(3,1). Then E(1/X) > 1/3.\n",
      "\n",
      "5.4 Technical Appendix: Proof of Hoeffding’s\n",
      "Inequality\n",
      "We will make use of the exact form of Taylor’s theorem: if g is a smooth\n",
      "\n",
      "function, then there is a number € € (0, u) such that g(u) = g(0) + ug'(0) +\n",
      "eon\n",
      "+9 (é)-\n",
      "\n",
      "PRrooF of Theorem 5.4. For any t > 0, we have, from Markov’s inequality,\n",
      "that\n",
      "\n",
      "S)\n",
      "\n",
      "é=1\n",
      "\n",
      "P (ox > “) =P Cone > \")\n",
      "a1\n",
      "\n",
      "e*E (¢=m%) =e“ Ee). (5.8)\n",
      "\n",
      "IA\n",
      "\n",
      "Since a; < Y; < bi, we can write Y; as a convex combination of a; and bj,\n",
      "namely, Y; = ab;+(1—a)a; where a = (Y;—a;)/(b;-a). So, by the convexity\n",
      "of e” we have\n",
      "\n",
      "Y= Gi othe b= Yi tas,\n",
      "\n",
      "bj -— a; bi — ay\n",
      "\n",
      " \n",
      "\n",
      "Take expectations of both sides and use the fact that E(Y;) =0 to get\n",
      "\n",
      "ee a et = es) (5.9)\n",
      "\n",
      "Eé¥ < -\n",
      "i — Bi by — a;\n",
      "\n",
      "where u = t(b; — aj), g(u) = —yu+ log(1 — y+ ye\") and y = —a;/(b; — ai).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-082.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "90 CHAPTER 5. INEQUALITIES\n",
      "\n",
      "Note that g(0) = g'(0) = 0. Also, g (u) < 1/4 for all u > 0. By Taylor’s\n",
      "theorem, there is a € € (0, u) such that\n",
      "\n",
      "g(u) = g(0)+ug'(0) +\n",
      "\n",
      "Hence,\n",
      "\n",
      "The result follows from (5.8). ll\n",
      "\n",
      "Proor of Theorem 5.5. Let Y¥; = (1/n)(X; — p). Then E(Y;) = 0 and\n",
      "a < Y¥; < b where a = —p/n and b = (1—p)/n. Also, (b- a)? = 1/n?.\n",
      "Applying the last Theorem we get\n",
      "\n",
      "P(X, -p>o= »(¥ > ‘) meet NS)\n",
      "i\n",
      "\n",
      "The above holds for any t > 0. In particular, take t = 4ne and we get\n",
      "P(X, -p>e< e?\" By a similar argument we can show that P(X, -p<\n",
      "—c) <e-*\"©. Putting these together we get P ([X,-pl>e < 26-2?\n",
      "\n",
      "5.5 Bibliographic Remarks\n",
      "An excellent reference on probability inequalities and their use in statistics\n",
      "\n",
      "and pattern recognition is Devroye, Gy6rfi and Lugosi (1996). The proof of\n",
      "Hoeffding’s inequality is from that text.\n",
      "\n",
      "5.6 Excercises\n",
      "\n",
      "1. Let X ~ Exponential(@). Find P(|X —yx| > kox) for k > 1. Compare\n",
      "this to the bound you get from Chebyshev’s inequality.\n",
      "\n",
      "2. Let X ~ Poisson(A). Use Chebyshev’s inequality to show that P(X >\n",
      "2) <1/A.\n",
      "\n",
      "3. Let X1,...,X, ~ Bernoulli(p) and X,, = n-! iL, X;. Bound P(|Xn—-\n",
      "p| > ©) using Chebyshev’s inequality and using Hoeffding’s inequality.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-083.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "5.6. EXCERCISES 91\n",
      "\n",
      "Show that, when n is large, the bound from Hoeffding’s inequality is\n",
      "smaller than the bound from Chebyshev’s inequality.\n",
      "\n",
      "4. Let X,,...,Xn ~ Bernoulli(p).\n",
      "(a) Let a > 0 be fixed and define\n",
      "\n",
      "= fio (2\n",
      "&u = 4/5, log ( )-\n",
      "\n",
      "Let p, = n7* 77, Xi. Define C, = (Pa — €n, Bn + €n). Use Hoeffding’s\n",
      "inequality to show that\n",
      "\n",
      "P(C,, contains p) > 1— a.\n",
      "\n",
      "We call C,, a 1 — a confidence interval for p. In practice, we truncate\n",
      "the interval so it does not go below 0 or above 1.\n",
      "\n",
      "(b) (Computer Experiment.) Let’s examine the properties of this confi-\n",
      "dence interval. Let a = 0.05 and p = 0.4. Conduct a simulation study\n",
      "to see how often the interval contains p (called the coverage). Do this\n",
      "for various values of n between 1 and 10000. Plot the coverage versus\n",
      "n.\n",
      "\n",
      "(c) Plot the length of the interval versus n. Suppose we want the length\n",
      "of the interval to be no more than .05. How large should n be?\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-084.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-085.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "4.\n",
      "6\n",
      "\n",
      "Convergence of Random Variables\n",
      "\n",
      "6.1 Introduction\n",
      "\n",
      "The most important aspect of probability theory concerns\n",
      "the behavior of sequences of random variables. This part of\n",
      "probability is called large sample theory or limit theory\n",
      "or asymptotic theory. This material is extremely important\n",
      "for statistical inference. The basic question is this: what can we\n",
      "say about the limiting behavior of a sequence of random vari-\n",
      "ables X1, X2,X3,...? Since statistics and data mining are all\n",
      "about gathering data, we will naturally be interested in what\n",
      "happens as we gather more and more data.\n",
      "\n",
      "In calculus we say that a sequence of real numbers x, con-\n",
      "verges to a limit x if, for every « > 0, |v, — 2| < € for all\n",
      "large n. In probability, convergence is more subtle. Going back\n",
      "to calculus for a moment, suppose that 2, = 2 for all n. Then,\n",
      "trivially, lim, x, = x. Consider a probabilistic version of this\n",
      "example. Suppose that X,, X2,... is a sequence of random vari-\n",
      "ables which are independent and suppose each has a N(0,1)\n",
      "distribution. Since these all have the same distribution, we are\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "This is page 89\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-086.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "90 6. Convergence of Random Variables\n",
      "\n",
      "tempted to say that X, “converges” to X ~ N(0,1). But this\n",
      "\n",
      "can’t quite be right since P(X,, = Z) = 0 for all n. (Two con-\n",
      "\n",
      "tinuous random variables are equal with probability zero.)\n",
      "Here is another example. Consider X,,X2,... where X; ~\n",
      "\n",
      "N(0,1/n). Intuitively, X;, is very concentrated around 0 for large\n",
      "\n",
      "n. But P(X, = 0) = 0 for all n. This chapter develops appro-\n",
      "\n",
      "priate methods of discussing convergence of random variables.\n",
      "There are two main ideas in this chapter:\n",
      "\n",
      "1. The law of large numbers says that sample average\n",
      "X, =n! So, X; converges in probability to the ex-\n",
      "pectation pp = E(X).\n",
      "\n",
      "2. The central limit theorem says that sample average\n",
      "has approximately a Normal distribution for large n. More\n",
      "precisely, /n(Xn — 2) converges in distribution to a\n",
      "Normal(0, 0”) distribution, where 0? = V(X).\n",
      "\n",
      "6.2 Types of Convergence\n",
      "\n",
      "The two main types of convergence are defined as follows.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-087.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.2 Types of Convergence\n",
      "\n",
      "91\n",
      "\n",
      " \n",
      "\n",
      "Definition 6.1 Let X,,Xo,... be a sequence of random vari-\n",
      "ables and let X be another random variable. Let F,, de-\n",
      "note the CDF of X, and let F denote the CDF of X.\n",
      "\n",
      "if, for every € > 0,\n",
      "P(\\X, —X|>e) 0\n",
      "asn— oo.\n",
      "\n",
      "if,\n",
      "lim F,(t) = F(t)\n",
      "\n",
      "n—=00\n",
      "\n",
      "at allt for which F is continuous.\n",
      "\n",
      "1. X,;, converges to X in probability, written X23 X%,\n",
      "\n",
      "(6.1)\n",
      "\n",
      "2. X, converges to X in distribution, written X, ~~ X,\n",
      "\n",
      "(6.2)\n",
      "\n",
      " \n",
      "\n",
      "There is another type of convergence which we introduce\n",
      "\n",
      "mainly because it is useful for proving convergence in proba-\n",
      "\n",
      "bility.\n",
      "\n",
      " \n",
      "\n",
      "Definition 6.2 X,, converges to X in quadratic mean\n",
      "(also called convergence in Lz), written XS X, if,\n",
      "\n",
      "E(X, — X)? +0 (6.3)\n",
      "\n",
      "asn— oo.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "qm\n",
      "\n",
      "f X is a point mass at c — that is P(X = c) = 1 — we write\n",
      "\n",
      "am. a . > P\n",
      "X,— ¢ instead of X—> X. Similarly, we write X,—+ c and\n",
      "\n",
      "Xe.\n",
      "\n",
      "Example 6.3 Let X,, ~ N(0,1/n). Intuitively, X,, is concentrat-\n",
      "ing at 0 so we would like to say that X,, ~+ 0. Let’s see if this\n",
      "is true. Let F be the distribution function for a point mass at\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-088.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "92 6. Convergence of Random Variables\n",
      "\n",
      "0. Note that /nXn ~ N(0,1). Let Z denote a standard normal\n",
      "random variable. For t < 0, F,(t) = P(X, < t) = P(/nxX, <\n",
      "Jnt) = P(Z < Wnt) — 0 since nt — —oo. Fort > 0,\n",
      "F,(t) = P(X, < t) = P( nx, < Jnt) = P(Z < nt) 41\n",
      "since ,/nt — oo. Hence, F,(t) > F(t) for allt 4 0 and so\n",
      "X, ~ 0. But notice that F,(0) = 1/2 4 F(1/2) = 1 so con-\n",
      "vergence fails at t = 0. But that doesn’t matter because t = 0 is\n",
      "not a continuity point of F and the definition of convergence\n",
      "distribution only requires convergence at continuity points.\n",
      "\n",
      "in\n",
      "\n",
      " \n",
      "\n",
      "The next theorem gives the relationship between the types of\n",
      "convergence. The results are summarized in Figure 6.1.\n",
      "\n",
      "Theorem 6.4 The following relationships hold:\n",
      "\n",
      "(a) pea ® implies that os Kes\n",
      "\n",
      "(b) X,—+X implies that X,, + X.\n",
      "\n",
      "(c) If X, ~~ X and if P(X =c) =1 for some real number c,\n",
      "then Xp—2> X.\n",
      "\n",
      "In general, none of the reverse implications hold except the\n",
      "special case in (c).\n",
      "\n",
      "PROOF. We start by proving (a). Suppose that X, “5 X. Fix\n",
      "€ > 0. Then, using Chebyshev’s inequality,\n",
      "\n",
      "E|X, — X[?\n",
      "aie AL\n",
      "\n",
      "P(|Xn— X| > 6) = P([Xn— XP > 2) $5 0.\n",
      "\n",
      "€\n",
      "Proof of (b). This proof is a little more complicated. You may\n",
      "skip if it you wish. Fix € > 0 and let a be a continuity point of\n",
      "F. Then\n",
      "F(t) = P(X, <2) =P(X, <2,X <at+e)+P(X, <2,X >r+6)\n",
      "P(X <2+e6)+P(|X,-X|>6)\n",
      "F(x +6) +P(|X, — X| >).\n",
      "\n",
      "IA\n",
      "\n",
      "ll\n",
      "\n",
      "Also.\n",
      "\n",
      "F(@-6€) = P(X <a-e) =P(X <a—-e€,X, <2) + P(X <4+6,X, >2)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-089.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "6.2 Types of Convergence 93\n",
      "< F(z) +P(|Xn — X| > ©).\n",
      "Hence,\n",
      "F(z—€)—P(|Xn—X| > €) S Fa(z) < F(a+e)+P(|Xn—X| > ©).\n",
      "Take the limit as n — oo to conclude that\n",
      "\n",
      "F(a —«) < liminf F,(x) < limsup F(x) < F(a +).\n",
      "\n",
      "n—00 n—00\n",
      "\n",
      "This holds for all ¢ > 0. Take the limit as € > 0 and use the fact\n",
      "that F is continuous at « and conclude that lim, F(a) = F(a).\n",
      "Proof of (c). Fix € > 0. Then,\n",
      "\n",
      "P(\\Xn-cl>e) = P(X, <c—e)+P(X, >c+6)\n",
      "\n",
      "< P(X, <e-6) + P(X > ete)\n",
      "= F,(c-69)+1-F,(c+e¢\n",
      "> F(e-6é)+1-F(e+e\n",
      "\n",
      "= 0+1-0=0.\n",
      "\n",
      "Let us now show that the reverse implications do not hold.\n",
      "\n",
      "CONVERGENCE IN PROBABILITY DOES NOT IMPLY CON-\n",
      "VERGENCE IN QUADRATIC MEAN. Let U ~ Unif(0,1) and let\n",
      "Xn = VnToa/n)(U)- Then P(|Xn| > €) = P(VnIo1/m)(U) >\n",
      "6) = BPO <U <1/n) = 1/n = 0. Hence, Then X,,++0. But\n",
      "E(X2) = nfo\" du = 1 for all n so X,, does not converge in\n",
      "quadratic mean.\n",
      "\n",
      "CONVERGENCE IN DISTRIBUTION DOES NOT IMPLY CON-\n",
      "VERGENCE IN PROBABILITY. Let X ~ N(0,1). Let X, = —X\n",
      "for n = 1,2,3,...; hence X, ~ N(0,1). X;, has the same distri-\n",
      "bution function as X for all n so, trivially, lim, F(x) = F(x)\n",
      "for all x. Therefore, X,,  X. But P(|X, —X| > €) = P(|2X| >\n",
      "©) = P(|X| > €/2) £0. So X,, does not tend to X in probability.\n",
      "a\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-090.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "94 6. Convergence of Random Variables\n",
      "\n",
      " \n",
      "\n",
      "waar\n",
      "quadratic mean ————— = probability — distribution\n",
      "\n",
      "FIGURE 6.1. Relationship between types of convergence.\n",
      "\n",
      "Warning! One might conjecture that if Kys b then E(X,,) >\n",
      "We can conclude b. This is not true. Let X,, be a random variable defined by\n",
      "that E(X,) —> b P(X, =n?) =1/n and P(X, =0) = 1 -(1/n). Now, P(|Xn| <\n",
      "if Xn is uniformly e) = P(X, = 0) = 1- (1/n) - 1. Hence, Z*+0. However,\n",
      "integrable. See the Bex. ) = In? x (1/n)]+[0x (1—-(1/n))] =n. Thus, E(X,,) — 00.\n",
      "technical appendix.\n",
      "\n",
      "Summary. Stare at Figure 6.1.\n",
      "\n",
      "Some convergence properties are preserved under transforma-\n",
      "\n",
      "tions.\n",
      "\n",
      "Theorem 6.5 Let X,,X,Yn,Y be random variables. Let g be a\n",
      "continuous function.\n",
      "\n",
      "(a) If X,—+X and ¥,—>Y, then X,»+Y¥n2>X+Y.\n",
      "\n",
      "(b) If Xn X and ¥,22Y, then X,+V¥neX+Y.\n",
      "\n",
      "(c) If X, ~~» X and Y, ~ ¢, then X,+Yn~ X +e.\n",
      "\n",
      "(d) If Xp—7>X and Y¥,—>Y, then X,Yn—> XY.\n",
      "\n",
      "(e) If Xn ~» X and Y, ~ ¢, then XnYn ~> cX.\n",
      "\n",
      "(f) If Xp2+X then g(Xp)+ 9(X).\n",
      "\n",
      "(9) If X, ~» X then g(X,) ~ g({X).\n",
      "\n",
      "6.3 The Law of Large Numbers\n",
      "\n",
      "Now we come to a crowning achievement in probability, the law\n",
      "of large numbers. This theorem says that the mean of a large\n",
      "sample is close to the mean of the distribution. For example,\n",
      "the proportion of heads of a large number of tosses is expected\n",
      "to be close to 1/2. We now make this more precise.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-091.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "6.3 The Law of Large Numbers 95\n",
      "\n",
      "Let X1,X2,..., be an 1D sample and let yp = E(X,) and\n",
      "o? = V(X;). Recall that the sample mean is defined as X, =\n",
      "n-! SYj- Xj and that E(X,,) = and V(X,,) = 0?/n.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 6.6 (The Weak Law of Large Numbers (WLLN).)\n",
      "If X1,...,Xn are UD , then pomeagy\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Interpretation of WLLN: The distribution of X,, be-\n",
      "comes more concentrated around ju as n gets large.\n",
      "\n",
      "PRoor. Assume that ¢ < oo. This is not necessary but it\n",
      "simplifies the proof. Using Chebyshev’s inequality,\n",
      "V(Xn) _\n",
      "\n",
      "P(|X,-vl>e) <\n",
      "\n",
      " \n",
      "\n",
      "e ne\n",
      "\n",
      "which tends to 0 as n — oo.\n",
      "\n",
      "Example 6.7 Consider flipping a coin for which the probability\n",
      "of heads is p. Let X; denote the outcome of a single toss (0\n",
      "or 1). Hence, p = P(X; = 1) = E(X;j). The fraction of heads\n",
      "after n tosses is X,. According to the law of large numbers,\n",
      "X,, converges to p in probability. This does not mean that X,\n",
      "will numerically equal p. It means that, when n is large, the\n",
      "distribution of X,, is tightly concentrated around p. Suppose that\n",
      "p= 1/2. How large should n be so that P(.4< Xn < .6) > .7?\n",
      "\n",
      "First, E(X,,) = p = 1/2 and V(X,,) = 0?/n = p(l—p)/n =\n",
      "1/(4n). From Chebyshev’s inequality,\n",
      "\n",
      "P(4<X, <6) = P(X,-p| <1)\n",
      "1-—P(|Xn — pl] > 1)\n",
      "oo Lo t- 25\n",
      "~ 4n(.1)? n-\n",
      "\n",
      "ll\n",
      "\n",
      "ll\n",
      "\n",
      "The last expression will be larger than .7 ifn = 84.\n",
      "\n",
      "Note that po =\n",
      "E(X;) is the same\n",
      "for all i so we can\n",
      "define yp = E(X;)\n",
      "for any i. By con-\n",
      "vention, we often\n",
      "write js = E(X,).\n",
      "\n",
      "There is a stronger\n",
      "theorem in the ap-\n",
      "pendix called the\n",
      "strong law of large\n",
      "numbers.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-092.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "96 6. Convergence of Random Variables\n",
      "6.4 The Central Limit Theorem\n",
      "\n",
      "Suppose that X,,...,X, are iid with mean yz and variance o.\n",
      "The central limit theorem (CLT) says that X, = n-1 >>, X;\n",
      "has a distribution which is approximately Normal with mean pu\n",
      "and variance o?/n. This is remarkable since nothing is assumed\n",
      "about the distribution of X;, except the existence of the mean\n",
      "and variance.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 6.8 (The Central Limit Theorem (CLT).) Let X1].\n",
      "be ID with mean ps and variance o?. Let X, =n! oP, X}.\n",
      "\n",
      "Then _\n",
      "7, = VIX) og\n",
      "o\n",
      "\n",
      "where Z ~ N(0,1). In other words,\n",
      "\n",
      "1 2\n",
      "= 27/24\n",
      "€ it\n",
      "Van\n",
      "\n",
      "n—00\n",
      "\n",
      "lim P(Z, < z) = O(z) = [\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Interpretation: Probability statements about X, can\n",
      "be approximated using a Normal distribution. It’s the\n",
      "probability statements that we are approximating, not\n",
      "the random variable itself.\n",
      "\n",
      "In addition to Z, ~+ N(0,1), there are several forms of nota-\n",
      "tion to denote the fact that the distribution of Z,, is converging\n",
      "to a Normal. They all mean the same thing. Here they are:\n",
      "\n",
      "Z, ~ N(01)\n",
      "Zz\n",
      "\n",
      "x & N(u “)\n",
      "n\n",
      "\n",
      "(05)\n",
      "\n",
      "n\n",
      "\n",
      "Xn-p ® N\n",
      "N (0, 02)\n",
      "\n",
      "4\n",
      "al\n",
      "|\n",
      "gv\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-093.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.4 The Central Limit Theorem 97\n",
      "\n",
      "Vi(Xn — #)\n",
      "\n",
      "x= N(0,1).\n",
      "\n",
      "Example 6.9 Suppose that the number of errors per computer\n",
      "program has a Poisson distribution with mean 5. We get 125 pro-\n",
      "grams. Let Xy,..., Xj25 be the number of errors in the programs.\n",
      "We want to approximate P(X < 5.5). Let w= E(X1) =A =5\n",
      "and 0? = V(X,) =A=5. Then,\n",
      "\n",
      "_ Ky — i(5.5 — ju\n",
      "P(X <5.5) =P ( = H) sa “) = P(Z < 2.5) = .9938.\n",
      "\n",
      " \n",
      "\n",
      "The central limit theorem tells us that Z, = //n(X — p)/o\n",
      "is approximately N(0,1). However, we rarely know o. We can\n",
      "estimate o? from X,,...,X, by\n",
      "\n",
      "1 n _\n",
      "\n",
      "Le =).\n",
      "\n",
      "Wo\n",
      "\n",
      "This raises the following question: if we replace o with S', is the\n",
      "central limit theorem still true? The answer is yes.\n",
      "\n",
      "Theorem 6.10 Assume the same conditions as the CLT. Then,\n",
      "\n",
      "Vil(Xn = 1)\n",
      "\n",
      "o> NOD).\n",
      "\n",
      "You might wonder, how accurate the normal approximation\n",
      "is. The answer is given in the Berry-Essten theorem.\n",
      "\n",
      "Theorem 6.11 (Berry-Esséen.) Suppose that E|.X,|* < oo. Then\n",
      "\n",
      "(6.4)\n",
      "\n",
      " \n",
      "\n",
      "There is also a multivariate version of the central limit theo-\n",
      "rem.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-094.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "98 6. Convergence of Random Variables\n",
      "\n",
      "Theorem 6.12 (Multivariate central limit theorem) Let X;\n",
      "be UD random vectors where\n",
      "\n",
      "Xi\n",
      "X= as\n",
      "Xki\n",
      "with mean\n",
      "MM E(Xii)\n",
      "He E(X2;)\n",
      "L= = :\n",
      "Mi E(Xii)\n",
      "and variance matria X. Let\n",
      "X,\n",
      "x-|\n",
      "Xi\n",
      "\n",
      "where X; =n! 0\", Xy;. Then,\n",
      "\n",
      "Vn(X — 1) ~» N(0,5).\n",
      "\n",
      "6.5 The Delta Method\n",
      "\n",
      "If Y,, has a limiting Normal distribution then the delta method\n",
      "allows us to find the limiting distribution of g(Y,,) where g is\n",
      "any smooth function.\n",
      "\n",
      "Theorem 6.13 (The Delta Method) Suppose that\n",
      "Ven =), (0,1)\n",
      "o\n",
      "and that g is a differentiable function such that g'(w) £0. Then\n",
      "\n",
      "ve NOD\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-095.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "6.5 The Delta Method 99\n",
      "\n",
      "In other words,\n",
      "2\n",
      "\n",
      "me »(m *) = 9%) © x(a wre).\n",
      "\n",
      "Example 6.14 Let X,,...,Xn be 1D with finite mean ju and fi-\n",
      "nite variance o?. By the central limit theorem, \\/n(Xn)/o ~>\n",
      "N(0,1). Let W, = eX, Thus, W, = g(Xn) where g(s) = e°.\n",
      "Since g'(s) = e°, the delta method implies that W, ~ N(e\",e?\"0?/n).\n",
      "a\n",
      "\n",
      "There is also a multivariate version of the delta method.\n",
      "Theorem 6.15 (The Multivariate Delta Method) Suppose that\n",
      "Yn = (Yni,---,Ynk) is a sequence of random vectors such that\n",
      "\n",
      "Vail — pn)» N(0,3).\n",
      "\n",
      "Let g: R* > R ane let\n",
      "dg\n",
      "\n",
      "Oy\n",
      "\n",
      "Voy)= |:\n",
      "ag.\n",
      "Oi\n",
      "\n",
      "Let V,, denote Vg(y) evaluated at y = js and assume that the\n",
      "elements of V,, are non-zero. Then\n",
      "\n",
      "Vn(g(¥n) ~ 9()) > N (0, Vi=V,) -\n",
      "Example 6.16 Let\n",
      "Xu X12 Xin\n",
      "Xo J? \\ Xe J? \\ Xan\n",
      "\n",
      "be 1D random vectors with mean f= (}t1, 2)? and variance D.\n",
      "\n",
      "Let h h\n",
      "X= nt X= 7\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-096.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "100 6. Convergence of Random Variables\n",
      "\n",
      "and define Y, = X1X2. Thus, Yn = g(X1, X») where g(s1, 82) =\n",
      "$189. By the central limit theorem,\n",
      "\n",
      "X2— be\n",
      "og\n",
      "3 -(2)\n",
      "og\n",
      "Os2 a\n",
      "\n",
      "VIDV. = (Mo be ou 13 Be) x Boy + 2p p2012 + peor.\n",
      "pov Gin my (2 09 I M2011 1 44 2012 TF Hy O22\n",
      "\n",
      "vii( Xi i ) ~» N(0,5).\n",
      "\n",
      "Now\n",
      "Va(s)\n",
      "\n",
      "Il\n",
      "\n",
      "and so\n",
      "\n",
      "Therefore,\n",
      "\n",
      "Vn(X Xo — [i fl2) N (0,130 + Qu pooi + pon). i\n",
      "\n",
      "6.6 Bibliographic Remarks\n",
      "\n",
      "Convergence plays a central role in modern probability the-\n",
      "ory. For more details, see Grimmet and Stirzaker (1982), Karr\n",
      "(1993) and Billingsley (1979). Advanced convergence theory is\n",
      "explained in great detail in van der Vaart and Wellner (1996)\n",
      "and van der Vaart (1998).\n",
      "\n",
      "6.7 Technical Appendix\n",
      "\n",
      "6.7.1 Almost Sure and L, Convergence\n",
      "\n",
      "We say that X, converges almost surely to X, written\n",
      "X,— > X, if\n",
      "P({s: Xn(s) + X(s)}) =1.\n",
      "\n",
      "We say that X,, converges in L; to X, written aE, X, if\n",
      "\n",
      "E|X, —X|—0\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-097.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "6.7 Technical Appendix 101\n",
      "\n",
      "asn— oo.\n",
      "Theorem 6.17 Let X,, and X be random vaiables. Then:\n",
      "(a) X,—+ X implies that Xue x\n",
      "(b) X,“4.X implies that Nig\n",
      "(c) X,25 xX implies that Xe XxX.\n",
      "\n",
      "The weak law of large numbers says that X,, converges to\n",
      "EX; in probability. The strong law asserts that this is also true\n",
      "almost surely.\n",
      "\n",
      "Theorem 6.18 (The strong law of large numbers.) Let X,,Xo,...\n",
      "be tid. If 1p = E|.X1| < oo then se\n",
      "\n",
      "A sequence X,, is asymptotically uniformly integrable if\n",
      "\n",
      "Jim lim sup E (|X,,|Z(|X,,| > M)) = 0.\n",
      "M=00\n",
      "\n",
      "n—00\n",
      "\n",
      "If X,—> b and X,, is asymptotically uniformly integrable, then\n",
      "E(X;) = b.\n",
      "\n",
      "6.7.2 Proof of the Central Limit Theorem\n",
      "\n",
      "If X is a random variable, define its moment generating func-\n",
      "\n",
      "tion (mgf) by Wx(t) = Ee'*. Assume in what follows that the\n",
      "mef is finite in a neighborhood around t = 0.\n",
      "Lemma 6.19 Let Z,,Z,... be a sequence of random variables.\n",
      "Let Ww, the mgf of Z,. Let Z be another random variable and\n",
      "denote its mgf by w. If Un(t) + w(t) for all t in some open\n",
      "interval around 0, then Z, ~+ Z.\n",
      "\n",
      "PROOF OF THE CENTRAL LIMIT THEOREM. Let Y; = (X; —\n",
      "p)/o. Then, Z, = n-¥/? do Vi. Let v(t) be the mef of Y;. The\n",
      "megf of >, Y; is (u(t))” and mef of Z,, is [v(t//n)]”\" = &.(t).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-098.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "102 6. Convergence of Random Variables\n",
      "\n",
      "Now w'(0) = E(¥i) = 0, w\"(0) = E(Y?) = Var(Yi) = 1. So,\n",
      "\n",
      "2 3\n",
      "w(t) = (0) +4v\"(0) + su\") + 7\") fives\n",
      "\n",
      "a.\n",
      "\n",
      "= 1404042 wo\n",
      "= ta 3 (0) +-++\n",
      "\n",
      "Pb\n",
      "= lt yt qQva+\n",
      "\n",
      "2\n",
      "\n",
      "Now,\n",
      "\n",
      " \n",
      "\n",
      "which is the mgf of a N(0,1). The result follows from the previous\n",
      "Theorem. In the last step we used the fact that, if a, — a then\n",
      "\n",
      "Gn\\”\n",
      "(1 + =) ee.\n",
      "n\n",
      "\n",
      "6.8 Excercises\n",
      "\n",
      "1. Let X4,..., X,, be iid with finite mean pp = E(X,) and\n",
      "finite variance 0? = V(X,). Let X,, be the sample mean\n",
      "and let S? be the sample variance.\n",
      "\n",
      "(a) Show that E(S?) = 0?.\n",
      "\n",
      "(b) Show that se, o?. Hint: Show that $2 = can~! 0\", X?—\n",
      "dX, where c, — 1 and d, — 1. Apply the law of large\n",
      "numbers to n~! )77_, X? and to X,,. Then use part (e) of\n",
      "Theorem 6.5.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-099.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      ". Let X1,...\n",
      "\n",
      "6.8 Excercises 103\n",
      "\n",
      ". Let X1,X2,... be a sequence of random variables. Show\n",
      "\n",
      "that X,,“ b if and only if\n",
      "\n",
      "lim E(X,) =6 and lim V(X,) =0.\n",
      "\n",
      "n—00 n—00\n",
      "\n",
      ". Let X4,..., X,, be iid and let pp = E(X,). Suppose that\n",
      "\n",
      "the variance is finite. Show that X,,\"> pu.\n",
      "\n",
      ". Let X1,X2,... be a sequence of random variables such\n",
      "that\n",
      "1 1 1\n",
      "P(x = *) =1- 72 and P(X, =n)= ae\n",
      "\n",
      "Does X,, converge in probability? Does X,, converge in\n",
      "quadratic mean?\n",
      "\n",
      "X, ~ Bernoulli(p). Prove that\n",
      "\n",
      "n\n",
      "\n",
      "ig 1 im\n",
      "~S7 x? Foy and —S>X?Sp.\n",
      "n f=1 n\n",
      "\n",
      "i=l\n",
      "\n",
      ". Suppose that the height of men has mean 68 inches and\n",
      "\n",
      "standard deviation 4 inches. We draw 100 men at ran-\n",
      "dom. Find (approximately) the probability that the aver-\n",
      "age height of men in our sample will be at least 68 inches.\n",
      "\n",
      ". Let A, = 1/n for n = 1,2,.... Let X,, ~ Poisson(A,).\n",
      "\n",
      "(a) Show that Xe s0.\n",
      "(b) Let Y;, = nX,,. Show that Y,—> 0.\n",
      "\n",
      ". Suppose we have a computer program consisting of n =\n",
      "\n",
      "100 pages of code. Let X; be the number of errors on the i”\n",
      "page of code. Suppose that the X/s are Poisson with mean\n",
      "1 and that they are independent. Let Y = 7, X; be the\n",
      "total number of errors. Use the central limit theorem to\n",
      "approximate P(Y < 90).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-100.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "104\n",
      "\n",
      "9:\n",
      "\n",
      "10.\n",
      "\n",
      "11.\n",
      "\n",
      "12.\n",
      "\n",
      "6. Convergence of Random Variables\n",
      "\n",
      "Suppose that P(X = 1) = P(X = —1) = 1/2. Define\n",
      "\n",
      "n\n",
      "\n",
      "with probability 2.\n",
      "\n",
      "n\n",
      "\n",
      "X= { X~ with probability 1— +\n",
      "\n",
      "e\n",
      "\n",
      "Does X,, converge to X in probability? Does X,, converge\n",
      "to X in distribution? Does E(X — X,,)? converge to 0?\n",
      "\n",
      "Let Z~ N(0,1). Let t > 0.\n",
      "(a) Show that, for any k > 0,\n",
      "\n",
      "E|Z|*\n",
      "\n",
      "P(Z|>#) <=.\n",
      "\n",
      " \n",
      "\n",
      "(b) (Mill's inequality.) Show that\n",
      "9)?\n",
      "P(|Z|>t)<4-\n",
      "(a>n< {zh\n",
      "\n",
      "Hint. Note that P(|Z| > t) = 2P(Z > t). Now write out\n",
      "what P(Z > t) means and note that x/t > 1 whenever\n",
      "z>t.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Suppose that X, ~ N(0,1/n) and let X be a random\n",
      "variable with distribution F(x) = 0 if a <0 and F(x) =1\n",
      "if a > 0. Does X, converge to X in probability? (Prove or\n",
      "disprove). Does X,, converge to X in distribution? (Prove\n",
      "or disprove).\n",
      "\n",
      "Let X,X), X2, X3,... be random variables that are posi-\n",
      "tive and integer valued. Show that X,, ~» X if and only\n",
      "if\n",
      "\n",
      "lim P(X, =k) = P(X =k)\n",
      "\n",
      "noo\n",
      "\n",
      "for every integer k.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-101.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.\n",
      "\n",
      "14.\n",
      "\n",
      "6.8 Excercises 105\n",
      "\n",
      "Let Z, Z2,... be ii.d., random variables with density f.\n",
      "Suppose that P(Z; > 0) = 1 and that \\ = lim, \\9 f(x) > 0.\n",
      "Let\n",
      "\n",
      "X, =nmin{Z,...,Z,}.\n",
      "\n",
      "Show that X,, ~+ Z where Z has an exponential distribu-\n",
      "tion with mean 1/A.\n",
      "\n",
      "Let X,,...,X,, ~ Uniform(0, 1). Let Y, = X?. Find the\n",
      "limiting distribution of Y,,.\n",
      "\n",
      "Xu X12 Xin\n",
      "Xa J? \\ X22 J \\ Xen\n",
      "be iid random vectors with mean js = (/41, /2) and variance\n",
      "x. Let\n",
      "i 1\n",
      "X= 5% Ras Xa\n",
      "\n",
      "and define Y,, = X1/X». Find the limiting distribution of\n",
      "Yn\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-102.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80ED608>\n",
      "Part II\n",
      "\n",
      "Statistical Inference\n",
      "\n",
      "103\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-103.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-104.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-105.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "7\n",
      "\n",
      "Models, Statistical Inference and\n",
      "Learning\n",
      "\n",
      "7.1 Introduction\n",
      "\n",
      "Statistical inference, or “learning” as it is called in computer\n",
      "science, is the process of using data to infer the distribution\n",
      "that generated the data. The basic statistical inference problem\n",
      "is this:\n",
      "\n",
      "We observe X1,...,X, ~ F. We want to infer (or\n",
      "estimate orlearn) F orsome feature of F' such as its\n",
      "\n",
      "mean.\n",
      "\n",
      "7.2. Parametric and Nonparametric Models\n",
      "\n",
      "A statistical model is a set of distributions (or a set of\n",
      "densities) §. A parametric model is a set § that can be pa-\n",
      "rameterized bya finite nunber of parameters. For example, if\n",
      "we assume that the data come from a Normal distribution then\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-106.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "The distinction\n",
      "between parametric\n",
      "and nonparametric\n",
      "is more subtle than\n",
      "this but we don’t\n",
      "\n",
      "need a_ rigorous\n",
      "definition for our\n",
      "purposes.\n",
      "\n",
      "106 7. Models, Statistical Inference and Learning\n",
      "\n",
      "the model is\n",
      "\n",
      "=\n",
      "\n",
      "{fleima) =e\n",
      "\n",
      "ex {sale wt », HER o> of :\n",
      "\n",
      "(7.1)\n",
      "This is a two-parameter model. We have written the density\n",
      "as f(2;p,0) to show that x is a value of the random variable\n",
      "whereas ji and o are parameters. In general, a parametric model\n",
      "takes the form\n",
      "\n",
      "B= { se): eco} (7.2)\n",
      "where @ is an unknown parameter (or vector of parameters) that\n",
      "can take values in the parameter space 0. If @ is a vector but\n",
      "we are only interested in one component of @, we call the re-\n",
      "maining parameters nuisance parameters. A nonparamet-\n",
      "ric model is a set § that cannot be parameterized by a finite\n",
      "number of parameters. For example, $a, = {all CDF 's} is non-\n",
      "parametric.\n",
      "\n",
      "Example 7.1 (One-dimensional Parametric Estimation.) Let Xj,...\n",
      "be independent Bernoulli(p) observations. The problem is to es-\n",
      "timate the parameter p. fl\n",
      "\n",
      "Example 7.2 (Two-dimensional Parametric Estimation.) Suppose that\n",
      "X1,...,Xn~ F and we assume that the PDF f € § where § is\n",
      "given in (7.1). In this case there are two parameters, jt and o.\n",
      "The goal is to estimate the parameters from the data. If we are\n",
      "only interested in estimating p then jt is the parameter of inter-\n",
      "est and o is a nuisance parameter. Wl\n",
      "\n",
      "Example 7.3 (Nonparametric estimation of the cdf.) Let X,,...,Xp\n",
      "be independent observations from a cdf F. The problem is to es-\n",
      "timate F assuming only that F € §ay, = {all cpr 's}.\n",
      "\n",
      "Example 7.4 (Nonparametric density estimation.) Let X,,...,Xn\n",
      "\n",
      "be independent observations from a cdf F and let f = F\"' be the\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-107.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80ED788>\n",
      "7.2 Parametric and Nonparametric Models 107\n",
      "\n",
      "PDF . Suppose we want to estimate the PDF f. It is not pos-\n",
      "sible to estimate f assuming only that F € Sau. We need to\n",
      "assume some smoothness on f. For example, we might assume\n",
      "that f € § = ®pens() Ssow where Fpens is the set of all proba-\n",
      "bility density functions and\n",
      "\n",
      "son = {s : fu\"opar< oo}.\n",
      "\n",
      "The class §sop is called a Sobolev space; it is the set of func-\n",
      "tions that are not “too wiggly.” 1\n",
      "\n",
      "Example 7.5 (Nonparametric estimation of functionals.) Let Xj,...\n",
      "F. Suppose we want to estimate pp = E(Xi) = f rdF (x) as-\n",
      "\n",
      "suming only that 4s exists. The mean js may be thought of as a\n",
      "\n",
      "function of F: we can write y = T(F) = f «dF (x). In general,\n",
      "\n",
      "any function of F is called a statistical functional. Other\n",
      "\n",
      "examples of functions are the variance T(F) = f 2°dF(x) —\n",
      "\n",
      "(f adF(zx))” and the median T(F) = F-‘/?,\n",
      "\n",
      "Example 7.6 (Regression, prediction and classification.) Suppose we\n",
      "observe pairs of data (X,,Y;),---(Xn,Yn). Perhaps X; is the\n",
      "blood pressure of subject i and Y; is how long they live. X is\n",
      "called a predictor or regressor or feature or independent\n",
      "variable. Y is called the outcome or the response variable\n",
      "or the dependent variable. We call r(x) = E(Y|X = 2) the\n",
      "regression function. If we assume that f € § where § is fi-\n",
      "nite dimensional — the set of straight lines for example — then\n",
      "\n",
      " \n",
      "\n",
      "we have a parametric regression model. If we assume that\n",
      "f € where § is not finite dimensional then we have a para-\n",
      "metric regression model. The goal of predicting Y for a new\n",
      "patient based on their X value is called prediction. If Y is dis-\n",
      "crete (for example, live or die) then prediction is instead called\n",
      "classification. If our goal is to estimate the functin f, then we\n",
      "call this regression or curve estimation. Regression models\n",
      "\n",
      "Xn~\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-108.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "108 7. Models, Statistical Inference and Learning\n",
      "are sometimes written as\n",
      "Y =f(X) +e (7.3)\n",
      "\n",
      "where E(e) = 0. We can always rewrite a regression model this\n",
      "way. To see this, define e = Y — f(X) and hence Y = Y +\n",
      "f(X)—f(X) = f(X)+e. Moreover, E(e) = EE(e|X) = E(E(Y—\n",
      "F(X)IX) = EEX) — f(X)) = E(F(X) — F(X) =0.\n",
      "\n",
      "Wuat’s NEXT? It is traditional in most introductory courses\n",
      "\n",
      " \n",
      "\n",
      "to start with parametric inference. Instead, we will start with\n",
      "nonparametric inference and then we will cover parametric in-\n",
      "ference. In some respects, nonparametric inference is easier to\n",
      "understand and is more useful than parametric inference.\n",
      "\n",
      "FREQUENTISTS AND BAYESIANS. There are many approaches\n",
      "to statistical inference. The two dominant approaches are called\n",
      "frequentist inference and Bayesian inference. We'll cover\n",
      "both but we will start with frequentist inference. We'll postpone\n",
      "a discussion of the pro’s and con’s of these two until later.\n",
      "\n",
      "Some Noration. If § = {f(x; 0) : @ € O} is a parametric\n",
      "model, we write Po(X € A) = f, f(x; @)dx and Eg(r(X)) =\n",
      "[r(2)f(2; 9)dx. The subscript @ indicates that the probability\n",
      "or expectation is with respect to f(z; @); it does not mean we\n",
      "are averaging over @. Similarly, we write Vy for the variance.\n",
      "\n",
      " \n",
      "\n",
      "7.3 Fundamental Concepts in Inference\n",
      "\n",
      "Many inferential problems can be identified as being one of\n",
      "three types: estimation, confidence sets or hypothesis testing.\n",
      "We will treat all of these problems in detail in the rest of the\n",
      "book. Here, we give a brief introduction to the ideas.\n",
      "\n",
      "7.8.1 Point Estimation\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-109.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "7.3 Fundamental Concepts in Inference 109\n",
      "\n",
      "Point estimation refers to providing a single “best guess”\n",
      "of some quantity of interest. The quantity of interest could be a\n",
      "parameter in a parametric model, aCDF F, a probability density\n",
      "function f, a regression function r, or a prediction for a future\n",
      "value Y of some random variable.\n",
      "\n",
      "By convention, we denote a point estimate of @ by 0.\n",
      "Remember that @ is a fixed, unknown quantity. The es-\n",
      "timate 0 depends on the data so @ is a random variable.\n",
      "\n",
      "More formally, let X;,...,X, be n 1D data point from some\n",
      "distribution F. A point estimator 6, of a parameter @ is some\n",
      "function of X,,...,X,:\n",
      "\n",
      "6, =9(&,..\n",
      "\n",
      " \n",
      "\n",
      "_X,).\n",
      "\n",
      "We define\n",
      "bias (8,,) = E9(0,) — 0 (7.4)\n",
      "\n",
      "to be the bias of G,,. We say that 6, is unbiased if E(0,) =.\n",
      "Unbiasedness used to receive much attention but these days it is\n",
      "not considered very important; many of the estimators we will\n",
      "use are biased. A point estimator 8, of a parameter 0 is con-\n",
      "sistent if On—+ @. Consistency is a reasonable requirement for\n",
      "estimators. The distribution of 6, is called the sampling dis-\n",
      "tribution. The standard deviation of 6, is called the standard\n",
      "error, denoted by se:\n",
      "\n",
      "se =se (,) = 1/V(8,)- (7.5)\n",
      "\n",
      "Often, it is not possible to compute the standard error but usu-\n",
      "ally we can estimate the standard error. The estimated standard\n",
      "error is denoted by sé.\n",
      "\n",
      "Example 7.7 Let X;,...,X, ~ Bernoulli(p) and let p,, =n! 7,Xj.\n",
      "Then E(pn) =n7! 3), E(X;) =p so py is unbiased. The standard\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-110.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 7. Models, Statistical Inference and Learning\n",
      "\n",
      "error is se = \\/V(pn) = /p(1—p)/n. The estimated standard\n",
      "error is Sé = \\/p(1—p)/n.\n",
      "\n",
      "The quality of a point estimate is sometimes assessed by the\n",
      "mean squared error, or MSE, defined by\n",
      "\n",
      "MSE = E,(0,, — 0)?.\n",
      "\n",
      "Recall that E,(-) refers to expectation with respect to the dis-\n",
      "tribution\n",
      "\n",
      "F(21,---52n3 0) =] Foes 8)\n",
      "i=l\n",
      "\n",
      "that generated the data. It does not mean we are averaging over\n",
      "\n",
      " \n",
      "\n",
      "a density for 0.\n",
      "\n",
      "Theorem 7.8 The MSE can be written as\n",
      "\n",
      " \n",
      "\n",
      "MSE = bias (6,,)? + Vo(On).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "PrRoor. Let 9, = Ey(0n). Then\n",
      "Eo(@n — 9)? = E(u —On + On — 0)”\n",
      "E,(0, — 9)? + 2(0n — 9)\"Eo(8, — 9) +E gn — 0)?\n",
      "(8, — 0)? +E9(On — On)?\n",
      "bias? + V(6,). i\n",
      "\n",
      "Theorem 7.9 If bias — 0 and se — 0 as n —+ 00 then 0, is\n",
      "consistent, that is, 6,2 0.\n",
      "\n",
      "ProorF. If bias — 0 and se —+ 0 then, by Theorem 7.8,\n",
      "MSE -> 0. It follows that 0,4. (Recall definition 6.3.) The\n",
      "result follows from part (b) of Theorem 6.4. ll\n",
      "\n",
      "Example 7.10 Returning to the coin flipping example, we have\n",
      "\n",
      "that E,(p,) =p so that bias = p—p = 0 andse = /p(1 — p)/n >\n",
      "\n",
      "0. Hence, Pn—>D, that is, Py is a consistent estimator.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-111.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "7.3 Fundamental Concepts in Inference 111\n",
      "\n",
      "Many of the estimators we will encounter will turn out to\n",
      "have, approximately, a Normal distribution.\n",
      "\n",
      " \n",
      "\n",
      "Definition 7.11 An estimator is asymptotically Nor-\n",
      "mal if\n",
      "\n",
      "6, —\n",
      "\n",
      "- 8. N(O,1). (7.7)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "7.8.2 Confidence Sets\n",
      "\n",
      "A 1—a confidence interval for a parameter 6 is an interval\n",
      "C,, = (a,b) where a = a(X1,...,X,) and b = b(X,,...,X,) are\n",
      "functions of the data such that.\n",
      "\n",
      "P,(@EC,) >1—a, for all@€O. (7.8)\n",
      "\n",
      "In words, (a,b) traps 9 with probability 1 — a. We call 1 — a\n",
      "the coverage of the confidence interval. Commonly, people use\n",
      "95 per cent confidence intervals which corresponds to choosing\n",
      "a =0.05. Note: C,, is random and @ is fixed! If 6 is a vector\n",
      "then we use a confidence set (such a sphere or an ellipse) instead\n",
      "of an interval.\n",
      "\n",
      "Warning! There is much confusion about how to interpret\n",
      "a confidence interval. A confidence interval is not a probability\n",
      "statement about @ since @ is a fixed quantity, not a random\n",
      "variable. Some texts interpret confidence intervals as follows: if\n",
      "I repeat the experiment over and over, the interval will contain\n",
      "the parameter 95 per cent of the time. This is correct but useless\n",
      "since we rarely repeat the same experiment over and over. A\n",
      "better interpretation is this:\n",
      "\n",
      "On day 1, you collect data and construct a 95 per\n",
      "cent confidence interval for a parameter 6,. On day\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-112.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "112 7. Models, Statistical Inference and Learning\n",
      "\n",
      "2, you collect new data and construct a 95 per cent\n",
      "confidence interval for an unrelated parameter 02. On\n",
      "day 3, you collect new data and construct a 95 per\n",
      "cent confidence interval for an unrelated parameter 63.\n",
      "You continue this way constructing confidence inter-\n",
      "vals for a sequence of unrelated parameters 6}, 02,...\n",
      "Then 95 per cent your intervals will trap the true pa-\n",
      "rameter value. There is no need to introduce the idea\n",
      "of repeating the same experiment over and over.\n",
      "\n",
      "Example 7.12 Every day, newspapers report opinion polls. For\n",
      "example, they might say that “83 per cent of the population fa-\n",
      "vor arming pilots with guns.” Usually, you will see a statement\n",
      "like “this poll is accurate to within 4 points 95 per cent of the\n",
      "time.” They are saying that 83 +4 is a 95 per cent confidence\n",
      "interval for the true but unknown proportion p of people who\n",
      "favor arming pilots with guns. If you form a confidence inter-\n",
      "val this way everyday for the rest of your life, 95 per cent of\n",
      "your intervals will contain the true parameter. This is true even\n",
      "though you are estimating a different quantity (a different poll\n",
      "question) every day.\n",
      "\n",
      "Later, we will discuss Bayesian methods in which we treat\n",
      "@ as if it were a random variable and we do make probability\n",
      "statements about 0. In particular, we will make statements like\n",
      "“the probability that @ is C,, given the data, is 95 per cent.”\n",
      "However, these Bayesian intervals refer to degree-of-belief prob-\n",
      "abilities. These Bayesian intervals will not, in general, trap the\n",
      "parameter 95 per cent of the time.\n",
      "Example 7.13 In the coin flipping setting, let Cn = (Pn—€n, Dnt\n",
      "€n) where €2 = log(2/a)/(2n). From Hoeffding’s inequality (5.4)\n",
      "it follows that\n",
      "\n",
      "P(pEC,) >1—a\n",
      "\n",
      "for every p. Hence, Cn is al —a confidence interval.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-113.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "7.3 Fundamental Concepts in Inference 113\n",
      "\n",
      "As mentioned earlier, point estimators often have a limiting\n",
      "Normal distribution, meaning that equation (7.7) holds, that\n",
      "is, 0, & N(0,”). In this case we can construct (approximate)\n",
      "confidence intervals as follows.\n",
      "\n",
      "Theorem 7.14 (Normal-based Confidence Interval.) Suppose that\n",
      "0, ~ N(0,s7). Let ® be the CDF of a standard Normal and\n",
      "let 2a/2 = ®-'(1 — (a/2)), that is, P(Z > 2aj2) = a/2 and\n",
      "P(—2a)2 < Z < 2a/2) =1—a where Z ~ N(0,1). Let\n",
      "Qe (On — 20/2, On + 2/28). (7.9)\n",
      "Then\n",
      "P(@EC,) 3 1—a. (7.10)\n",
      "\n",
      "Proor. Let Z, = (0, —0)/S. By assumption Z, ~+ Z where\n",
      "Z ~ N(0,1). Hence,\n",
      "\n",
      "Po(O€ Cn) = Po (On zap ® <0 <b, + xap8)\n",
      "\n",
      "6,—9\n",
      "= Pyl- ” z\n",
      "»(-2a< a <=]\n",
      "\n",
      "+ P(-2za2 < Z < zap)\n",
      "= l-a. @\n",
      "\n",
      " \n",
      "\n",
      "For 95 per cent confidence intervals, a = 0.05 and zy). =\n",
      "1.96 = 2 leading to the approximate 95 per cent confidence\n",
      "interval 6, +28€. We will discuss the construction of confidence\n",
      "intervals in more generality in the rest of the book.\n",
      "\n",
      "Example 7.15 Let X,,...,X, ~ Bernoulli(p) and let p, =n-! Sy, Xi.\n",
      "Then V(f,) =n\", V(X) = 2-2 pll—p) = nap —\n",
      "\n",
      "p) =p(1—p)/n. Hence, se = \\/p(1— p)/n and & = \\/p, (1 —p,)/n.\n",
      "By the Central Limit Theorem, fy © N(p,&°). Therefore, an\n",
      "approximate 1— a confidence interval is\n",
      "\n",
      " \n",
      "\n",
      "PH 2p =P+ rap\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-114.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "114 7. Models, Statistical Inference and Learning\n",
      "\n",
      "Compare this with the confidence interval in the previous exam-\n",
      "ple. The Normal-based interval is shorter but it only has approx-\n",
      "imately (large sample) correct coverage. Mi\n",
      "\n",
      "7.8.8 Hypothesis Testing\n",
      "\n",
      "In hypothesis testing, we start with some default theory\n",
      "— called a null hypothesis — and we ask if the data provide\n",
      "sufficient evidence to reject the theory. If not we retain the null\n",
      "The term “retaining hypothesis.\n",
      "the null hypothesis” Example 7.16 (Testing if a Coin is Fair) Suppose X,,...,X, ~ Bernoulli(p)\n",
      "is due to Chris Gen- denote n independent coin flips. Suppose we want to test if the\n",
      "\n",
      "owese: Other termi- coin is fair. Let Hy denote the hypothesis that the coin is fair\n",
      "nology is “accepting\n",
      "\n",
      "the null” or “failing\n",
      "to reject the null.”\n",
      "\n",
      "and let H, denote the hypothesis that the coin is not fair. Ho\n",
      "is called the null hypothesis and H, is called the alternative\n",
      "hypothesis. We can write the hypotheses as\n",
      "\n",
      "Hy:p=1/2 versus H,:p#1/2.\n",
      "\n",
      "It seems reasonable to reject Hy if T = |p, — (1/2)| is large.\n",
      "When we discuss hypothesis testing in detail, we will be more\n",
      "precise about how large T should be to reject Ho.\n",
      "\n",
      "7.4 Bibliographic Remarks\n",
      "\n",
      "Statistical inference is covered in many texts. Elementary\n",
      "texts include DeGroot and Schervish (2001) and Larsen and\n",
      "Marx (1986). At the intermediate level I recommend Casella\n",
      "and Berger (2002) and Bickel and Doksum (2001). At the ad-\n",
      "vanced level, Lehmann and Casella (1998), Lehmann (1986) and\n",
      "van der Vaart (1998).\n",
      "\n",
      "7.5 Technical Appendix\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-115.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "7.5 Technical Appendix 115\n",
      "\n",
      "Our definition of confidence interval requires that P(@ €\n",
      "Cn) > 1— a for all 6 € ©. An pointwise asymptotic con-\n",
      "fidence interval requires that lim inf, ,.. Pg(@ € C,,) > 1—a for\n",
      "all 9 € ©. An uniform asymptotic confidence interval requres\n",
      "that lim inf, 4. infyee Po(@ € Cn) > 1— a. The approximate\n",
      "Normal-based interval is a pointwise asymptotic confidence in-\n",
      "terval. In general, it might not be a uniform asymptotic confi-\n",
      "dence interval.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-116.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 7. Models, Statistical Inference and Learning\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-117.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "8\n",
      "\n",
      "Estimating the CDF and Statistical\n",
      "Functionals\n",
      "\n",
      "The first inference problem we will consider is nonparametric\n",
      "\n",
      "estimationthh cpF F and functions of the CDF.\n",
      "\n",
      "8.1 The Empirical Distribution Function\n",
      "\n",
      "Let X1,...,X, ~ F be 1D where F is a distribution function\n",
      "\n",
      "on the real line. We will estimate F with the empirical distribu-\n",
      "\n",
      "tion function, which is defined as follo ws.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Definition 8.1 The empirical distribution function F,\n",
      "is the ODF that puts mass 1/n at e ach data pint X;. For-\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "mally,\n",
      "\n",
      "di nl Xy <\n",
      "\n",
      "Fic) = Die (Xi < 2)\n",
      "\n",
      "n\n",
      "nunber of observations less than or equal BE\n",
      "—\n",
      "n\n",
      "where\n",
      "\n",
      ". 1 ifX;<2\n",
      "1x<a)={ 4 if Xj >\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-118.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "Actually,\n",
      "\n",
      "sup, |F,(x) — F(x)\n",
      "\n",
      "converges to\n",
      "almost surely.\n",
      "\n",
      "0\n",
      "\n",
      "118 8. Estimating the CDF and Statistical Functionals\n",
      "\n",
      "Example 8.2 (Nerve Data.) Cor and Lewis (1966) reported 799\n",
      "waiting times between successive pulses along a nerve fibre. The\n",
      "first plot in Figure 8.1 shows the a “toothpick plot” where each\n",
      "toothpick shows the location of one data point. The second plot\n",
      "shows that empirical CDF F,. Suppose we want to estimate the\n",
      "fraction of waiting times between .4 and .6 seconds. The estimate\n",
      "is F,(.6) — F,(.4) =.93— .84 =.09.\n",
      "\n",
      "The following theorems give some properties of F',(z7).\n",
      "\n",
      "Theorem 8.3 At any fixed value of x,\n",
      "\n",
      "E(Fi(z)) = F(o) and V (Fala) =\n",
      "Thus,\n",
      "\n",
      "F(a) = Fle),\n",
      "\n",
      "F(z)(1- F(a)\n",
      "\n",
      "MSE = > 0\n",
      "\n",
      "and hence, F,,(%)——> F (2).\n",
      "\n",
      "Theorem 8.4 (Glivenko-Cantelli Theorem.) Let X,,...,X,~ F.\n",
      "Then\n",
      "sup |F,(x) — F(x)|—+0.\n",
      "\n",
      "8.2 Statistical Functionals\n",
      "\n",
      "A statistical functional T(F) is any function of F. Examples\n",
      "are the mean p. = f rdF (x), the variance o? = f(a — 1)*dF (x\n",
      "and the median m = F-!(1/2).\n",
      "\n",
      "a\n",
      "\n",
      " \n",
      "\n",
      "Definition 8.5 The plug-in estimator of 0 = T(F) is\n",
      "defined by\n",
      "\n",
      "6, =T(F,).\n",
      "\n",
      "In other words, just plug in F, for the unknown F.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-119.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "8.2 Statistical Functionals 119\n",
      "\n",
      " \n",
      "\n",
      "Reteseesaccuucasesceagencccuovaseesneaengvcssreaeenay(eau yCereeiteetUU ne)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "0.0 0.2 0.4 0.6 0.8 1.0 1.2 1.4\n",
      "\n",
      "time\n",
      "\n",
      " \n",
      "\n",
      "cdf\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "0.0 04 08\n",
      "\n",
      " \n",
      "\n",
      "time\n",
      "\n",
      "FIGURE 8.1. Nerve data. The solid line in the middle is the empirical\n",
      "distribution function. The lines above and below the middle line are\n",
      "a 95 per cent confidence band. The confidence band is explained in\n",
      "the appendix.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-120.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "120 8. Estimating the CDF and Statistical Functionals\n",
      "\n",
      "A functional of the form f r(«)dF(2) is called a linear func-\n",
      "tional. Recall that fr(x)dF(x) is defined to be [r(2) f(x)dx\n",
      "in the continuous case and }),r(2;)f(x;) in the discrete. The\n",
      "empirical cdf F,(x) is discrete, putting mass 1/n at each X;.\n",
      "Hence, if T(F) = fr(x)dF(2) is a linear functional then we\n",
      "have:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The plug-in estimator for linear functional T(F) = fr(2)dF (x) is:\n",
      "\n",
      "(8.2)\n",
      "\n",
      " \n",
      "\n",
      "Sometimes we can find the estimated standard error s@ of\n",
      "T(E,) by doing some calculations. However, in other cases it\n",
      "is not obvious how to estimate the standard error. In the next\n",
      "chapter, we will discuss a general method for finding sé. For\n",
      "now, let us just assume that somehow we can find sé. In many\n",
      "\n",
      "cases, it turns out that\n",
      "T(F,) © N(T(F), &).\n",
      "\n",
      "By equation (7.10), an approximate 1 — a confidence interval\n",
      "for T(F) is then\n",
      "\n",
      " \n",
      "\n",
      "T (Fy) £ 20/2 %.\n",
      "\n",
      " \n",
      "\n",
      "(8.3)\n",
      "\n",
      " \n",
      "\n",
      "We will call this the Normal-based interval. For a 95 per cent\n",
      "confidence interval, 24/2 = 2.05/2 = 1.96 © 2 so the interval is\n",
      "\n",
      "T(F,) £28.\n",
      "\n",
      "Example 8.6 (The mean.) Let = T(F) = f cdF (2). The plug-\n",
      "in estimator is fi = [vdF,(z) = X,. The standard error is\n",
      "\n",
      "se = \\/V(Xn) = o/Yn. If & denotes an estimate of o, then\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-121.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "8.2 Statistical Functionals 121\n",
      "\n",
      "the estimated standard error is G/\\/n. (In the next example, we\n",
      "shall see how to estimate 0.) A Normal-based confidence interval\n",
      "forpisX,+ Za/2 se’.\n",
      "\n",
      "Example 8.7 (The Variance) Leto? = T(F) = V(X) = f a?dF(x)—\n",
      "(f adF(z))”. The plug-in estimator is\n",
      "\n",
      "2 [ec a (fsa)\n",
      "EGE)\n",
      "= 1 4-%y\n",
      "\n",
      "Another reasoable estimator of 0? is the sample variance\n",
      "\n",
      "1 “ _\n",
      "Xo)\n",
      "Seno\n",
      "\n",
      "In practice, there is little difference between 6? and S? and you\n",
      "\n",
      "oe\n",
      "\n",
      "n\n",
      "\n",
      " \n",
      "\n",
      "can use either one. Returning the our last example, we now see\n",
      "that the estimated standard error of the estimate of the mean is\n",
      "\n",
      "®@=3/Vn.\n",
      "\n",
      "Example 8.8 (The Skewness) Let js and o? denote the mean and\n",
      "variance of a random variable X. The skewness is defined to be\n",
      "\n",
      "E(X — p)3 [(e— w)?dF (2)\n",
      "\n",
      "o {[(@— dF (2) ye\n",
      "The skewness measure the lack of symmetry of a distribution.\n",
      "To find the plug-in estimate, first recall that ji = n7 yO, Xi and\n",
      "@ =n1D)(X;— ji)’. The plug-in estimate of « is\n",
      "\n",
      "fle w'MFa) 206-0)\n",
      "as 3/2 oe .\n",
      "{ f(x — PdFa(a)}\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "k=\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-122.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "122 8. Estimating the CDF and Statistical Functionals\n",
      "\n",
      "Example 8.9 (Correlation.) Let Z = (X,Y) and let p=T(F) =\n",
      "E(X—px)(¥—py)/(720y) denote the correlation between X and\n",
      "\n",
      "Y, where F(x, y) is bivariate. We can write T(F) = a(T; (F), To(F), T3(F), Ti(F), Ts(F))\n",
      "where\n",
      "\n",
      "TF) =fadF(z2) T(F)=fydF(2) T(F) = f cydF (2)\n",
      "Ti(F) = f a2dF(z) Ts te =f ydF(z)\n",
      "and bh hb\n",
      "Replace F with F, in T,(F), ..., Ts(F), and take\n",
      "p= a(T,(F,),T2(F,), Ts(Fa), Ti(Fr), Ts(F,))-\n",
      "We get\n",
      "\n",
      "Dili — Xn)(¥i — Yn)\n",
      "asi X-¥,2/OH-Y.)\n",
      "\n",
      "which is called the sample correlation. Hi\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 8.10 (Quantiles.) Let F be strictly increasing with den-\n",
      "sity f. The T(F) = F7'(p) be the p quantile. The estimate\n",
      "if T(F) is F>'\\(p). We have to be a bit careful since F, is\n",
      "not invertible. To avoid ambiguity we define F>'(p) = inf{a :\n",
      "F(x) > Pp}. We call F-'(p) the p\"\" sample quantile. ll\n",
      "\n",
      "Only in the first example did we compute a standard error or\n",
      "a confidence interval. How shall we handle the other examples.\n",
      "When we discuss parametric methods, we will develop formulae\n",
      "for standard errors and confidence intervals. But in our non-\n",
      "parametric setting we need something else. In the next chapter,\n",
      "we will introduce two methods — the jackknife and the bootstrap\n",
      "— for getting standard errors and confidence intervals.\n",
      "\n",
      "Example 8.11 (Plasma Cholesterol.) Figure 8.2 shows histograms\n",
      "for plasma cholesterol (in mg/dl) for 371 patients with chest\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-123.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "8.2 Statistical Functionals 123\n",
      "\n",
      "pain (Scott et al. 1978). The histograms show the percentage of\n",
      "patients in 10 bins. The first histogram is for 51 patients who\n",
      "had no evidence of heart disease while the second histogram is\n",
      "for 820 patients who had narrowing of the arteries. Is the mean\n",
      "cholesterol different in the two groups? Let us regard these data\n",
      "as samples from two distributions F, and F9. Let ju, = f adF (2)\n",
      "and [2 = [ vdF2(2) denote the means of the two populations.\n",
      "The plug-in estimates are fi, = frdF,,(«) = Xna = 195.27\n",
      "and fiz = f tdF,2(2) = Xn2 = 216.19. Recall that the standard\n",
      "error of the sample mean fi = 437\", X; is\n",
      "\n",
      "(f= v(t>%) = vx) (ees\n",
      "\n",
      "which we estimate by\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "where\n",
      "\n",
      " \n",
      "\n",
      "For the two groups this yields sé(ji;) = 5.0 and sé (fiz) = 2.4.\n",
      "Approximate 95 per cent confidence intervals for j, and 12 are\n",
      "fi; + 28 (fix) = (185, 205) and fin + 288 (fig) = (211, 221).\n",
      "\n",
      "Now, consider the functional @ = T(F,)—T(F,) whose plug-in\n",
      "estimate is 0 = Ji2— fi, = 216.19 — 195.27 = 20.92. The standard\n",
      "error of 0 is\n",
      "\n",
      " \n",
      "\n",
      "se = VV (fiz — fis) = VV (fa) + V(ii) = V/(se (fix)? + (se (fia)?\n",
      "\n",
      "and we estimate this by\n",
      "\n",
      "sé = \\/ (se (f71))? + (sé (fiz)? = 5.55.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-124.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "124 8. Estimating the CDF and Statistical Functionals\n",
      "\n",
      "An approximate 95 per cent confidence interval for 0 is 0428 =\n",
      "(9.8, 32.0). This suggests that cholesterol is higher among those\n",
      "with narrowed arteries. We should not jump to the conclusion\n",
      "(from these data) that cholesterol causes heart disease. The leap\n",
      "from statistical evidence to causation is very subtle and is dis-\n",
      "cussed later in this text. 1\n",
      "\n",
      "8.3 Technical Appendix\n",
      "\n",
      "In this appendix we explain how to construct a confidence\n",
      "band for the CDF .\n",
      "Theorem 8.12 (Dvoretzky-Kiefer-Wolfowitz (DKW) inequality.) Let\n",
      "X,...,X, be iid from F. Then, for any € > 0,\n",
      "\n",
      "(sup Fla) Fla)|>e) <2, (8\n",
      "\n",
      "From the DKW inequality, we can construct a confidence set.\n",
      "Let €? = log(2/a)/(2n), L(x) = max{F;,(x)—€n, 0} and U(x) =\n",
      "min{F,,(2) + €,, 1}. It follows from (8.4) that for any F,\n",
      "\n",
      "P(FEC,) >1-a.\n",
      "\n",
      "Thus, C,, is a nonparametric 1—a confidence set for F’. A better\n",
      "name for C’, is a confidence band. To summarize:\n",
      "\n",
      " \n",
      "\n",
      "A 1—a nonparametric confidence band for F is (L(x), U(a)) where\n",
      "\n",
      "L(x) = max{F,(x) — €,, 0}\n",
      "U(r) = ait oie 1}\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-125.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.3 Technical Appendix 125\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "[ T T T T T I\n",
      "100 150 200 250 300 350 400\n",
      "\n",
      "plasma cholesterol for patients without heart disease\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "4\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "[ T T T T T 1\n",
      "100 150 200 250 300 350 400\n",
      "\n",
      " \n",
      "\n",
      "plasma cholesterol for patients with heart disease\n",
      "\n",
      "FIGURE 8.2. Plasma cholesterol for 51 patients with no heart disease\n",
      "and 320 patients with narrowing of the arteries.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-126.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "126 8. Estimating the CDF and Statistical Functionals\n",
      "\n",
      "Example 8.13 The dashed lines in Figure 8.1 give a 95 per cent\n",
      "\n",
      "confidence band using €n =\n",
      "\n",
      " \n",
      "\n",
      "8.4 Bibliographic Remarks\n",
      "\n",
      "The Glivenko-Cantelli theorem is the tip of the iceberg. The\n",
      "theory of distribution functions is a special case of what are\n",
      "called empirical processes which underlie much of modern statis-\n",
      "tical theory. Some references on empirical processes are Shorack\n",
      "and Wellner (1986) and van der Vaart and Wellner (1996).\n",
      "\n",
      "8.5 Exercises\n",
      "\n",
      "1. Prove Theorem 8.3.\n",
      "\n",
      "2. Let X),...,X, ~ Bernoulli(p) and let Y¥1,.--,¥n ~ Bernoulli(q).\n",
      "Find the plug-in estimator and estimated standard error\n",
      "for p. Find an approximate 90 per cent confidence interval\n",
      "for p. Find the plug-in estimator and estimated standard\n",
      "error for p—q. Find an approximate 90 per cent confidence\n",
      "interval for p— q.\n",
      "\n",
      "3. (Computer Experiment.) Generate 100 observations from\n",
      "a N(0,1) distribution. Compute a 95 per cent confidence\n",
      "band for the CDF F’. Repeat this 1000 times and see how\n",
      "often the confidence band contains the true distribution\n",
      "function. Repeat using data from a Cauchy distribution.\n",
      "\n",
      "4. Let X),...,X, ~ F and let F,,(x) be the empirical distri-\n",
      "bution function. For a fixed x, use the central limit theo-\n",
      "rem to find the limiting distribution of F,,(2).\n",
      "\n",
      " \n",
      "\n",
      "5. Let « and y be two distinct points. Find Cov(Fi,(«), Fa(y))-\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-127.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "10.\n",
      "\n",
      "8.5 Exercises 127\n",
      "\n",
      ". Let X,,...,X, ~ F and let F be the empirical distri-\n",
      "\n",
      "bution function. Let a < b be fixed numbers and define\n",
      "0 =T(F) =F(b)— F(a). Let 0=T(F,) = F,(b) — F(a).\n",
      "\n",
      "Find the estimated standard error of 6. Find an expression\n",
      "for an approximate 1 — a confidence interval for 0.\n",
      "\n",
      ". Data on the magnitudes of earthquakes near Fiji are avail-\n",
      "\n",
      "able on the course website. Estimate the cdf F(a). Com-\n",
      "pute and plot a 95 per cent confidence envelope for F’.\n",
      "Find an approximate 95 per cent confidence interval for\n",
      "F(4.9) — F(4.3).\n",
      "\n",
      ". Get the data on eruption times and waiting times between\n",
      "\n",
      "eruptions of the old faithful geyser from the course web-\n",
      "site. Estimate the mean waiting time and give a standard\n",
      "error for the estimate. Also, give a 90 per cent confidence\n",
      "interval for the mean waiting time. Now estimate the me\n",
      "dian waiting time. In the next chapter we will see how to\n",
      "get the standard error for the median.\n",
      "\n",
      ". 100 people are given a standard antibiotic to treat an in-\n",
      "\n",
      "fection and another 100 are given a new antibiotic. In the\n",
      "first group, 90 people recover; in the second group, 85 peo-\n",
      "ple recover. Let p; be the probability of recovery under\n",
      "the standard treatment and let py be the probability of\n",
      "recovery under the new treatment. We are interested in\n",
      "estimating @ = p, — po. Provide an estimate, standard er-\n",
      "ror, an 80 per cent confidence interval and a 95 per cent\n",
      "confidence interval for 0.\n",
      "\n",
      "In 1975, an experiment was conducted to see if cloud seed-\n",
      "ing produced rainfall. 26 clouds were seeded with silver\n",
      "nitrate and 26 were not. The decision to seed or not was\n",
      "made at random. Get the data from\n",
      "\n",
      "http://lib.stat.cmu.edu/DASL/Stories/CloudSeeding.html\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-128.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "128 8. Estimating the CDF and Statistical Functionals\n",
      "\n",
      "Let @ be the difference in the median precipitation from\n",
      "the two groups. Estimate @. Estimate the standard error of\n",
      "the estimate and produce a 95 per cent confidence interval.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-129.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "9\n",
      "The Bootstrap\n",
      "\n",
      "The bootstrap is a nonparametric method for estimating stan-\n",
      "dard errors and computing confidence intervals. Let\n",
      "\n",
      "Th =Wg (Kis ong Xp)\n",
      "\n",
      "be a statistic, that is, any function of the data. Suppose we\n",
      "want to know V;(T;,), the variance of T,,. We have written V»\n",
      "to emphasize that the variance usually depends on the unknown\n",
      "distribution function F. For example, if T,, =n! iL, X; then\n",
      "Vp(T,) = 0?/n where o? = [(2— p)?dF (x) and p= f rdF (x).\n",
      "The bootstrap idea has two parts. First we estimate V;-(T,,) with\n",
      "Va,(Tn)- Thus, for T,, = n7! SO\", X; we have Vg (Tn) = G?/n\n",
      "where 6? = n7! So\", (X; — X,,). However, in moremplicated\n",
      "cases we cannot write down a simple formula for Vj, (T,,). This\n",
      "leads us to the second step which is to approximate Vet)\n",
      "using simulation. Before proceeding, let us discuss the idea of\n",
      "simulation.\n",
      "\n",
      "9.1 Simulation\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-130.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "130 9. The Bootstrap\n",
      "\n",
      "Suppose we draw an IID sample Yj,..., Y, from a distribution\n",
      "G. By the law of large numbers,\n",
      "\n",
      "B\n",
      "eZ [vac =20)\n",
      "j=l\n",
      "\n",
      "as B — oo. So if we draw a large sample from G,, we can use the\n",
      "sample mean Y,, to approximate E(Y). In a simulation, we can\n",
      "make B as large as we like in which case, the difference between\n",
      "Y,, and E(Y) is negligible. More generally, if h is any function\n",
      "with finite mean then\n",
      "\n",
      "Few [rac = E(hA(Y))\n",
      "as B — ov. In particular,\n",
      "Lorry 3 3 3 - (3 oy) / yPdF (y)— (/ wlF ad) =v).\n",
      "\n",
      "Hence, we can use the sample variance of the simulated values\n",
      "\n",
      " \n",
      "\n",
      "to approximate V(Y).\n",
      "9.2 Bootstrap Variance Estimation\n",
      "\n",
      "According to what we just learned, we can approximate Vp (Th)\n",
      "by simulation. Now Vella) means “the variance of T), if the\n",
      "distribution of the data is Fv How can we simulate from the\n",
      "distribution of T,, when the data are assumed to have distribu-\n",
      "tion F.? The answer is to simulate X7,..., X% from F, and then\n",
      "compute T* = g(Xj,...,X*). This constitutes one draw from\n",
      "the distribution of T,,. The idea is illustrated in the following\n",
      "diagram:\n",
      "\n",
      "Real world Fo = > X,...,Xn => Th=9(X1,---;Xn)\n",
      "Bootstrap world F, => Xj,...,.Xi => Te =g(X},...,X%)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-131.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "9.2 Bootstrap Variance Estimation 131\n",
      "\n",
      "How do we simulate X7,..., X% from F,,? Notice that F, puts\n",
      "mass 1/n at each data point X;,...,X,. Therefore, drawing an\n",
      "observation from F,, is equivalent to drawing one point\n",
      "at random from the original data set. Thu, to simulate\n",
      "DF gccong RE ES F, it suffices to draw n observations with re-\n",
      "placement from X,,...,X,. Here is a summary:\n",
      "\n",
      " \n",
      "\n",
      "Boostrap Variance Estimation\n",
      "1. Draw X},...,X*~ F,.\n",
      "2. Compute T* = g(Xf,..., X})-\n",
      "3. Repeat steps 1 and 2, B times, to get Tras . The\n",
      "\n",
      "4. Let\n",
      "\n",
      "B 2\n",
      "1 * 1 =e\n",
      "Upoot = B > (x a B yn.) m (9.1)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 9.1 The following pseudo-code shows how to use the\n",
      "bootstrap to estimate the standard of the median.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-132.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "132 9. The Bootstrap\n",
      "\n",
      " \n",
      "\n",
      "Bootstrap for The Median\n",
      "\n",
      "Given data X = (X(1), ..., X(m)):\n",
      "\n",
      "T <- median(X)\n",
      "Tboot <- vector of length B\n",
      "for(i in 1:i){\n",
      "\n",
      "Tboot [i] <- median(Xstar)\n",
      "}\n",
      "\n",
      "se <- sqrt (variance(Tboot) )\n",
      "\n",
      " \n",
      "\n",
      "Xstar <- sample of size n from X (with replacement)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The following schematic diagram will remind you that we are\n",
      "using two approximations:\n",
      "\n",
      "not so small small\n",
      "\n",
      "Vi(In) 73> Vig (Tn) Vboot-\n",
      "\n",
      "Example 9.2 Consider the nerve data. Let 0 = T(F) = f(a—-\n",
      "y)dF(x)/o> be the skewness. The skewness is a measure of\n",
      "asymmetry. A Normal distribution, for example, has skewness\n",
      "0. The plug-in estimate of the skewness is\n",
      "\n",
      "15nd OnE\n",
      "\n",
      "el = 1.76.\n",
      "oO\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "To estimate the standard error with the bootstrap we follow the\n",
      "same steps as with the medin example except we compute the\n",
      "skewness from each bootstrap sample. When applied to the nerve\n",
      "data, the bootstrap, based on B = 1000 replications, yields a\n",
      "standard error for the estimated skewness of .16.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-133.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.3 Bootstrap Confidence Intervals 133\n",
      "9.3 Bootstrap Confidence Intervals\n",
      "\n",
      "There are several ways to construct bootstrap confidence in-\n",
      "tervals. Here we discuss three methods.\n",
      "\n",
      "Normal Interval. The simplest is the Normal interval\n",
      "\n",
      "Ty + 2a/2 B hoot (9.2)\n",
      "\n",
      " \n",
      "\n",
      "where S€}oo is the bootstrap estimate of the standard error.\n",
      "This interval is not accurate unless the distribution of T,, is\n",
      "close to Normal.\n",
      "\n",
      "Pivotal Intervals. Let @ = T(F) and 0, = T(F,) and define the\n",
      "\n",
      "pivot R, = On — 6. Let 0, cess Os denote bootstrap replica-\n",
      "tions of 6,,. Let H(r) denote the CDF of the pivot:\n",
      "H(r) =Pp(Rn <1). (9.3)\n",
      "\n",
      "Define C* = (a,b) where\n",
      "\n",
      " \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "a\n",
      "\n",
      " \n",
      "\n",
      "8, —H™ (1-5) and 6, —H-' (£). (9.4)\n",
      "\n",
      "Tt follows that\n",
      "P(a<0<b) = Paa-#,<\n",
      "= P@,-b\n",
      "\n",
      "ll\n",
      "=\n",
      "=\n",
      "I\n",
      "= 1A IA\n",
      "Es)\n",
      "g\n",
      "IA\n",
      "§\n",
      ";\n",
      "&\n",
      "\n",
      "Il\n",
      "za\n",
      "i\n",
      "y\n",
      "nw] s\n",
      "=e\n",
      "|\n",
      "=\n",
      "c\n",
      "as\n",
      "\n",
      "|\n",
      "a\n",
      "\n",
      "|\n",
      "\n",
      "mls\n",
      "\n",
      "Il\n",
      "Hw\n",
      "\n",
      "|\n",
      "2\n",
      "\n",
      "Hence, C* is an exact 1 — a confidence interval for 6. Unfortu-\n",
      "nately, a and b depend on the unknown distribution H but we\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-134.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "134 9. The Bootstrap\n",
      "\n",
      "can form a bootstrap estimate of H:\n",
      "\n",
      "B\n",
      "=5 <r) (9.5)\n",
      "b=1\n",
      "\n",
      "where Ri, = 0, —6,. Let rj denote the 8 sample qauntile\n",
      "Of (Ri yguos Rt) and let 0% denote the 6 sample qauntile of\n",
      "(5 1,-+-,93,n)- Note that rh = 05 - @,,. It follows that an ap-\n",
      "\n",
      "proximate 1 — a confidence interval is C, = (@,) where\n",
      "\n",
      "a 6, — (1-$) =O, — Thuja = 2, — Of aps\n",
      "\n",
      "b= bh\n",
      "\n",
      "Il\n",
      "|\n",
      ")\n",
      "b\n",
      "I\n",
      "ee\n",
      "|\n",
      "=>\n",
      "|\n",
      "=\n",
      "=\n",
      "iS\n",
      "|\n",
      "w\n",
      "S\n",
      "5)\n",
      ">\n",
      "e)\n",
      "3\n",
      "\n",
      "In summary:\n",
      "\n",
      " \n",
      "\n",
      "The 1 — a bootstrap pivotal confidence interval is\n",
      "\n",
      "Cn = (28, — Baja, 28n — 8p) -\n",
      "\n",
      " \n",
      "\n",
      "Typically, this is a pointwise, asymptotic confidence interval.\n",
      "\n",
      "(9.6)\n",
      "\n",
      " \n",
      "\n",
      "Theorem 9.3 Under weak conditions onT(F), Pe(T(F) € Cr) 4\n",
      "\n",
      "1—aasn- oo, where C,, is given in (9.6).\n",
      "\n",
      "Percentile Intervals. The bootstrap percentile interval is\n",
      "defined by\n",
      "\n",
      "Cr = (0% aja» Oi a/2) +\n",
      "\n",
      "The justification for this interval is given in the appendix.\n",
      "\n",
      "Example 9.4 For estimating the skewness of the nerve data, here\n",
      "are the various confidence intervals.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-135.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "9.3 Bootstrap Confidence Intervals 135\n",
      "\n",
      "Method 95% Interval\n",
      "Normal (1.44, 2.09)\n",
      "Percentile (1.42, 2.03)\n",
      "Pivotal (1.48, 2.11)\n",
      "\n",
      "All these confidence intervals are approximate. The probabil-\n",
      "ity that T(F) is in the interval is not exactly 1— a. All three\n",
      "intervals have the same level of accuracy. There are more accu-\n",
      "rate bootstrap confidence intervals but they are more compli-\n",
      "cated and we will not discuss them here.\n",
      "\n",
      "Example 9.5 (The Plasma Cholesterol Data.) Let us return to the\n",
      "cholesterol data. Suppose we are interested in the difference of\n",
      "the medians. Pseudo-code for the bootstrap analysis is as follows:\n",
      "\n",
      "xi <- first sample\n",
      "\n",
      "x2 <- second sample\n",
      "\n",
      "ni <- length(x1)\n",
      "\n",
      "n2 <- length(x2)\n",
      "\n",
      "th.hat <- median(x2) - median(x1)\n",
      "\n",
      "B <- 1000\n",
      "\n",
      "Tboot <- vector of length B\n",
      "\n",
      "for(i in 1:B){\n",
      "xxi <- sample of size ni with replacement from x1\n",
      "xx2 <- sample of size n2 with replacement from x2\n",
      "Tboot [i] <- median(xx2) - median(xx1)\n",
      "3\n",
      "\n",
      "se <- sqrt (variance (Tboot))\n",
      "\n",
      "Normal <- (th.hat - 2*xse, th.hat + 2*se)\n",
      "\n",
      "percentile <- (quantile(Tboot,.025), quantile(Tboot, .975))\n",
      "\n",
      "pivotal <- ( 2*th.hat-quantile(Tboot, .975), 2*th.hat-quantile(Tboot, .025)\n",
      "\n",
      "The point estimate is 18.5, the bootstrap standard error is 7.42\n",
      "and the resulting approzimate 95 per cent confidence intervals\n",
      "are as follows:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-136.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "136 9. The Bootstrap\n",
      "\n",
      "Method 95% Interval\n",
      "\n",
      "Percentile (5.0, 33.3)\n",
      "\n",
      "Pivotal (5.0, 34.0)\n",
      "Since these intervals exclude 0, it appears that the second group\n",
      "has higher cholesterol although there is considerable uncertainty\n",
      "about how much higher as reflected in the width of the intervals.\n",
      "\n",
      "The next two examples are based on small sample sizes. In\n",
      "practice, statistical methods based on very small sample sizes\n",
      "might not be reliable. We include the examples for their peda-\n",
      "gogical value but we do want to sound a note of caution about\n",
      "interpreting the results with some skepticism.\n",
      "\n",
      "Example 9.6 Here is an example that was one of the first used\n",
      "to illustrate the bootstrap by Bradley Efron, the inventor of the\n",
      "bootstrap. The data are LSAT scores (for entrance to law school)\n",
      "and GPA.\n",
      "\n",
      "LSAT 576 635 558 578 666 580 555 661\n",
      "651 605 653. 575 545 572 594\n",
      "\n",
      "GPA 3.39 3.30 2.81 3.03 3.44 3.07 3.00 3.43\n",
      "3.36 3.13 3.12 2.74 2.76 2.88 3.96\n",
      "\n",
      "Each data point is of the form X; = (Yj, Z;) where Y; = LSAT;\n",
      "and Z; = GPA;. The law school is interested in the correlation\n",
      "\n",
      "ga [fy -— uy) — wz) dF ly, 2) ;\n",
      "Vu = ay)24P (y) fe = nzPaF (2)\n",
      "The plug-in estimate is the sample correlation\n",
      "<. BM-Wez\n",
      "YDM- YP DZ\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-137.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "9.3 Bootstrap Confidence Intervals 137\n",
      "\n",
      "The estimated correlation is 0 = .776. The bootstrap based on\n",
      "B =1000 gives sé = .137. Figure 9.1 shows the data and a his-\n",
      "togram of the bootstrap replications 6%, cess 0%. This histogram is\n",
      "an approximation to the sampling distribution of 0. The Normal-\n",
      "based 95 per cent confidence interval is .78 + 2se = (.51,1.00)\n",
      "while the percentile interval is (.46,.96). In large samples, the\n",
      "\n",
      "two methods will show closer agreement.\n",
      "\n",
      "Example 9.7 This example is borrowed from An Introduction to\n",
      "the Bootstrap by B. Efron and R. Tibshirani. When drug com-\n",
      "panies introduce new medications, they are sometimes requires\n",
      "to show bioequivalence. This means that the new drug is not\n",
      "substantially different than the current treatment. Here are data\n",
      "on eight subjects who used medical patches to infuse a hormone\n",
      "into the blood. Each subject received three treatments: placebo,\n",
      "old-patch, new-patch.\n",
      "\n",
      "subject placebo old new old-placebo new-old\n",
      "fi 9243 17649 16449 8406 -1200\n",
      "2 9671 12013 14614 2342 2601\n",
      "3 11792 19979 17274 8187 2705\n",
      "4 13357 21816 23798 8459 1982\n",
      "5 9055 13850 12560 4795 -1290\n",
      "6 6290 9806 10157 3516 351\n",
      "Z4 12412 17208 16570 4796 -638\n",
      "8 18806 29044 26325 10238 -2719\n",
      "\n",
      "Let Z = old — placebo and Y = new — old. The Food and\n",
      "Drug Administration (FDA) requirement for bioequivalence is\n",
      "that |0| < .20 where\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-138.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "138 9. The Bootstrap\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "=~ J 7\n",
      "oo . *\n",
      "™ J\n",
      "2\n",
      "x= = .\n",
      "oO .\n",
      "° .\n",
      "2 | ‘\n",
      "cd .\n",
      "o | ‘\n",
      "“N .\n",
      "560 580 600 620 640 660\n",
      "LSAT\n",
      "B\n",
      "\n",
      "100\n",
      "\n",
      "50\n",
      "\n",
      " \n",
      "\n",
      "0.2 0.4 0.6 0.8 1.0\n",
      "\n",
      "Bootstrap Samples\n",
      "\n",
      "FIGURE 9.1. Law school data.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-139.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "9.4 Bibliographic Remarks 139\n",
      "The estimate of 0 is\n",
      "\n",
      "Y 452.3\n",
      "\n",
      "Z 6342\n",
      "\n",
      ">)\n",
      "\n",
      "= = —.0713.\n",
      "\n",
      "The bootstrap standard error is 5 = .105. To answer the bioe-\n",
      "quivalence question, we compute a confidence interval. From\n",
      "B = 1000 bootstrap replications we get the 95 per cent inter-\n",
      "val is (-.24,.15). This is not quite contained in [-.20,.20] so at\n",
      "the 95 per cent level we have not demonstrated bioequivalence.\n",
      "Figure 9.2 shows the histogram of the bootstrap values.\n",
      "\n",
      "9.4 Bibliographic Remarks\n",
      "\n",
      "The boostrap was invented by Efron (1979). There are several\n",
      "books on these topics including Efron and Tibshirani (1993),\n",
      "Davison and Hinkley (1997), Hall (1992), and Shao and Tu\n",
      "(1995). Also, see section 3.6 of van der Vaart and Wellner (1996).\n",
      "9.5 Technical Appendix\n",
      "\n",
      "9.5.1 The Jackknife\n",
      "\n",
      "There is another method for computing standard errors called\n",
      "the jackknife, due to Quenouille (1949). It is less computa-\n",
      "tionally expensive than the bootstrap but is less general. Let\n",
      "T, =T(X,...,Xn) be a statistic and T_; denote the statistic\n",
      "with the i** observation removed. Let T,, =n! 77, Tia. The\n",
      "jackknife estimate of var(T,,) is\n",
      "\n",
      " \n",
      "\n",
      "n\n",
      "\n",
      "n-1 tN\n",
      "Vjack = n Cre) _ Gy\n",
      "\n",
      "i=l\n",
      "\n",
      " \n",
      "\n",
      "and the jackknife estimate of the standard error is S€ jack =\n",
      "/Tjack- Under suitable conditions on T, it can be shown that\n",
      "\n",
      "Ujack Consistently estimates var(T;,) in the sense that vyack/var(Tn) 7,\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-140.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140 9. The Bootstrap\n",
      "\n",
      "ih,\n",
      "T T T T T 1\n",
      "\n",
      "-0.3 -0.2 -0.1 0.0 0.1 0.2\n",
      "\n",
      "60\n",
      "|\n",
      "\n",
      "40\n",
      "|\n",
      "\n",
      "20\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Bootstrap Samples\n",
      "\n",
      "FIGURE 9.2. Patch data.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-141.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "9.6 Excercises 141\n",
      "\n",
      "1. However, unlike the bootstrap, the jackknife does not produce\n",
      "consistent estimates of the standard error of sample quantiles.\n",
      "\n",
      "9.5.2 Justification For The Percentile Interval\n",
      "\n",
      "Suppose there exists a monotone transformation U = m(T)\n",
      "such that U ~ N(@,c*) where ¢ = m(0). We do not suppose we\n",
      "know the transformation, only that one exist. Let Uf = m(6%,,)-\n",
      "Let u%Z be the 6 sample quantile of the U;’s. Since a mono-\n",
      "tone transformation preserves quantiles, we have that Usp =\n",
      "m(O%)- Also, since U ~ N(¢,c?), the a/2 quantile of U is\n",
      "@— Za/2c. Hence Uap = o- 2a/2¢. Similarly, Ula/a = 0+ Za/2€.\n",
      "Therefore,\n",
      "\n",
      "PO. SOS Oop) =\n",
      "\n",
      " \n",
      "\n",
      "An exact normalizing transformation will rarely exist but there\n",
      "may exist approximate normalizing transformations.\n",
      "9.6 Excercises\n",
      "\n",
      "1. Consider the data in Example 9.6. Find the plug-in esti-\n",
      "mate of the correlation coefficient. Estimate the standard\n",
      "error using the bootstrap. Find a 95 per cent confidence\n",
      "inerval using all three methods.\n",
      "\n",
      "2. (Computer Experiment.) Conduct a simulation to compare\n",
      "the four bootstrap confidence interval methods. Let n =\n",
      "50 and let T(F) = f(a — p)*dF(x)/o* be the skewness.\n",
      "Draw Y\\,..-,Xn ~ N(0,1) and set X; =e%, i=1,...,n.\n",
      "Construct the four types of bootstrap 95 per cent intervals\n",
      "for T(F) from the data X,,...,X,. Repeat this whole\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-142.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "142\n",
      "\n",
      "9. The Bootstrap\n",
      "\n",
      "thing many times and estimate the true coverage of the\n",
      "four intervals.\n",
      "\n",
      "Let\n",
      "Xi,..-,Xn~ ts\n",
      "\n",
      "where n = 25. Let 0 = T(F) = (q75 — ¢.25)/1.34 where q,\n",
      "denotes the p“” quantile. Do a simulation to compare the\n",
      "coverage and length of the following confidence intervals\n",
      "for 8: (i) Normal interval with standard error from the\n",
      "bootstrap, (ii) bootstrap percentile interval.\n",
      "\n",
      "Remark: The jackknife does not give a consistent estima-\n",
      "tor of the variance of a quantile.\n",
      "\n",
      ". Let X\\,...,X, be distinct observations (no ties). Show\n",
      "\n",
      "that there are\n",
      "2n-1\n",
      "n\n",
      "distinct bootstrap samples.\n",
      "\n",
      "Hint: Imagine putting n balls into n buckets.\n",
      "\n",
      "denote a bootstrap sample and let Ke = wT XP\n",
      "Find: E(X;|¥1,.--, Xn), V(X4|X1,.--,Xn), E(X,) and\n",
      "V(X,)-\n",
      "\n",
      "Computer Experiment.) Let X1,...,.X;, Normal(j,1). Let\n",
      "0 =! and let 0 = e* be the mle. Create a data set (using\n",
      "jt = 5) consisting of n=100 observations.\n",
      "\n",
      " \n",
      "\n",
      "a) Use the bootstrap to get the seand 95 percent confi-\n",
      "dence interval for 0.\n",
      "\n",
      "(b) Plot a histogram of the bootstrap replications for the\n",
      "parametric and nonparametric bootstraps. These are es-\n",
      "timates of the distribution of 0. Compare this to the true\n",
      "sampling distribution of 6.\n",
      "\n",
      "Let X,,..., Xn be distinct observations (no ties). Let Xj,..., X*\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-143.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "9.6 Excercises 143\n",
      "\n",
      "7. Let Xj, ...,X, Unif(0, 0). The mle is @= Ximar = max{X,..., Xp}.\n",
      "Generate a data set of size 50 with @ = 1.\n",
      "\n",
      "(a) Find the distribution of 0. Compare the true distri-\n",
      "bution of 6 to the histograms from the parametric and\n",
      "nonparametric bootstraps.\n",
      "\n",
      "(b) This is a case where the nonparametric bootstrap does\n",
      "very poorly. In fact, we can prove that this is the case.\n",
      "Show that, for the parametric bootstrap P(6* = 0) = 0\n",
      "but for the nonparametric bootstrap P(0* = 0) ~ .632.\n",
      "Hint: show that, P(@* = 0) = 1 — (1 —(1/n))” then take\n",
      "\n",
      "the limit as n gets large.\n",
      "\n",
      "8. Let Th = x, w= E(X), ax = f |e — w|*dF (2) and\n",
      "& = 4 (i - X,,|*. Show that\n",
      "\n",
      "=~ | ane\n",
      "4X Go 4 4X03 4\n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Vboot =\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-144.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "144 9. The Bootstrap\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-145.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "10\n",
      "\n",
      "Parametric Inference\n",
      "\n",
      "We now turn our attention to parametric models, that is,\n",
      "models of the form\n",
      "\n",
      "t= { F(239) :0€ o} (10.1)\n",
      "\n",
      "where the © C R* is the parameter space and 6 = (61,..., 0)\n",
      "is the parameter. The problem of inference then reduces to the\n",
      "problem of estimating the parameter 0.\n",
      "\n",
      "Students learning statistics often ask: how would we ever\n",
      "know that the distribution that generated the data is in some\n",
      "parametric model? This is an excellent question. Indeed, we\n",
      "would rarely havesuch knowledge which is why nonparametric\n",
      "methods are preferable. Still, studying methods for parametric\n",
      "models is useful for two reasons. First, there are some cases\n",
      "where background knowledge suggests that a parametric model\n",
      "provides a reasonable approximation. F orexample, counts of\n",
      "traffic accidents are known from prior experience to follow ap-\n",
      "proximately a P oissonmodel. Second, the inferential concepts\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-146.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "146 10. Parametric Inference\n",
      "\n",
      "for parametric models provide background for understanding\n",
      "certain nonparametric methods.\n",
      "\n",
      "We will begin with a brief dicussion about parameters of in-\n",
      "terest and nuisance parameters in the next section, then we will\n",
      "discuss two methods for estimating 6, the method of moments\n",
      "and the method of maximum likelihood.\n",
      "\n",
      "10.1 Parameter of Interest\n",
      "\n",
      "Often, we are only interested in some function T(0). For ex-\n",
      "\n",
      " \n",
      "\n",
      "ample, if X ~ N(y,07) then the parameter is @ = (1,0). If our\n",
      "goal is to estimate ys then pp = T(@) is called the parameter\n",
      "of interest and ¢ is called a nuisance parameter. The pa-\n",
      "rameter of interest can be a complicated function of 6 as in the\n",
      "following example.\n",
      "\n",
      "Example 10.1 Let X),...,X, ~ Normal(j,0?). The parameter\n",
      "is0=(p,0) is O={(p,0): w ER o> O}. Suppose that X;\n",
      "is the outcome of a blood test and suppose we are interested in\n",
      "T, the fraction of the population whose test score is larger than\n",
      "1. Let Z denote a standard Normal random variable. Then\n",
      "\n",
      " \n",
      "\n",
      "X-po1-p\n",
      "T = P(X >1)=1-P(x <1) =1-P( H< 1H)\n",
      "oO oO\n",
      "\n",
      "1-n l-n\n",
      "1-P(z< t) -1-9(2< “).\n",
      "oO oO\n",
      "\n",
      "The parameter of interest is T =T(p,0) =1—((1—)/c).\n",
      "\n",
      " \n",
      "\n",
      "Example 10.2 Recall that X has a Gamma(a, 8) distribution if\n",
      "\n",
      "1\n",
      "\n",
      " \n",
      "\n",
      "x e-t/6 a > 0\n",
      "\n",
      "f(x; 4,8) =\n",
      "\n",
      "BT (a)\n",
      "\n",
      "where a, 3 >0 and\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-147.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "10.2 The Method of Moments 147\n",
      "\n",
      "is the Gamma function. The parameter is 0 = (a, 3). The Gamma\n",
      "distribution is sometimes used to model lifetimes of people, an-\n",
      "imals, and electronic equipment. Suppose we want to estimate\n",
      "the mean lifetime. Then T(a, 8) = Eg(X1) =f.\n",
      "\n",
      "10.2 The Method of Moments\n",
      "\n",
      "The first method for generating parametric estimators that\n",
      "we will study is called the method of moments. We will see\n",
      "that these estimators are not optimal but they are often easy to\n",
      "compute. They are are also useful as starting values for other\n",
      "methods that require iterative numerical routines.\n",
      "\n",
      "Suppose that the parameter 6 = (0),...,0,) has k compo-\n",
      "nents. For 1 <j <k, define the j** moment\n",
      "\n",
      "0; = 0,(0) =Ey(X4) = / aldF a(x) (10.2)\n",
      "and the j*\" sample moment\n",
      "\n",
      "~ Igy ‘\n",
      "aj = ioe (10.3)\n",
      "\n",
      " \n",
      "\n",
      "Definition 10.3 The method of moments estimator 6,\n",
      "is defined to be the value of 0 such that\n",
      "\n",
      " \n",
      "\n",
      "ay (6,) = a\n",
      "a2(O,) = @\n",
      "ax (On) = a. (10.4)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Formula (10.4) defines a system of k equations with k un-\n",
      "\n",
      "knowns.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-148.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "148 10. Parametric Inference\n",
      "\n",
      "Example 10.4 Let X,,\n",
      "pand@,=n-\"!Syr XxX\n",
      "\n",
      "~ 1,\n",
      "P= Xe\n",
      "\n",
      "Example 10.5 Let X1,...,Xn ~ Normal(j, 07). Then, ay = Eg(X1) =\n",
      "Recall that V(X) = ys and ay = Eg(X?) = Vo(X1) + (Eo(X1))? = 0? +7. We need\n",
      "E(X?) — (E (X))?. to solve the equations\n",
      "Hence, E(X?\n",
      "\n",
      ") »\n",
      "V(X) + (a) a - 10x,\n",
      "\n",
      "X,, ~ Bernoulli(p). Thena, = E,(X) =\n",
      "i. By equating these we get the estimator\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      ": 1g\n",
      "72\n",
      "o+h = nee\n",
      "This is a system of 2 equations with 2 unknowns. The solution\n",
      "is\n",
      "i= X,\n",
      "P= Foe ;-X,)?.\n",
      "Theorem 10.6 Let 0, denote the method of moments estimator.\n",
      "\n",
      "Under the the conditions given in the appendix, the following\n",
      "statements hold:\n",
      "\n",
      "1. The estimate 0, exists with probability tending to 1.\n",
      "2. The estimate is consistent: 0,5 0.\n",
      "3. The estimate is asymptotically Normal:\n",
      "\n",
      "n(n — 8) ~» N(0,5)\n",
      "\n",
      "where\n",
      "D = gEe(YY\")g\"\n",
      "\n",
      "Y = (X,X7,..., X97, 9 = (gi... -. 9x) and g; = da; '(0)/00.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-149.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.3 Maximum Likelihood 149\n",
      "\n",
      "The last statement in the Theorem above can be used to find\n",
      "standard errors and confidence intervals. However, there is an\n",
      "easier way: the bootstrap. We defer discussion of this until the\n",
      "end of the chapter.\n",
      "\n",
      "10.3. Maximum Likelihood\n",
      "\n",
      "The most common method for estimating parameters in a\n",
      "parametric model is the maximum likelihood method. Let\n",
      "X\\,.--,Xn be UD with PDF f(zx;6).\n",
      "\n",
      " \n",
      "\n",
      "Definition 10.7 The likelihood function is defined by\n",
      "\n",
      "n\n",
      "\n",
      "Lo(0) = TP £58). (10.5)\n",
      "\n",
      "i=l\n",
      "\n",
      "The log-likelihood function is defined by ¢,,(0) = log Ln{@).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The likelihood function is just the joint density of the data,\n",
      "except that we treat it is a function of the parameter 6.\n",
      "Thus £, : © > [0,00). The likelihood function is not a density\n",
      "function: in general, it is not true that £,,(@) integrates to 1.\n",
      "\n",
      " \n",
      "\n",
      "Definition 10.8 The maximum likelihood estimator\n",
      "MLE , denoted by 6,, is the value of 0 that maximizes\n",
      "\n",
      "Lal(6).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The maximum of ¢,,(@) occurs at the same place as the max-\n",
      "\n",
      " \n",
      "\n",
      "imum of £,,(9), so maximizing the log-likelihood leads to the\n",
      "same answer as maximizing the likelihood. Often, it is easier to\n",
      "work with the log-likelihood.\n",
      "\n",
      "Remark 10.9 If we multiply £,(0) by any positive constant c (not\n",
      "depending on 0) then this will not change the MLE . Hence, we\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-150.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "150 10. Parametric Inference\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "S 3\n",
      "5\n",
      "Q #1\n",
      "34 |\n",
      "\n",
      " \n",
      "\n",
      "00 02 04 06 08 10\n",
      "Pp\n",
      "\n",
      "FIGURE 10.1. Likelihood function for Bernoulli with n = 20 and\n",
      "\n",
      "yi\", Xj =12. The Mux is p, = 12/20 = 0.6.\n",
      "\n",
      "shall often be sloppy about dropping constants in the likelihood\n",
      "function.\n",
      "\n",
      "Example 10.10 Suppose that X\\,...,X, ~ Bernoulli(p). The prob-\n",
      "ability function is f(x;p) = p*(1— p)'~* for x =0,1. The un-\n",
      "known parameter is p. Then,\n",
      "\n",
      "L,(p) = [] xe p= TT“ —p) S=p*(1—p)\n",
      "\n",
      "where S = \\), Xj. Hence,\n",
      "\n",
      "ln(p) = Slogp + (n — S) log(1 — p).\n",
      "\n",
      "Take the derivative of ¢,(p), set it equal to 0 to find that the\n",
      "MLE is py, = S/n. See Figure 10.1. Wl\n",
      "\n",
      "Example 10.11 Let X,,...,Xn ~ N(,07). The parameter is\n",
      "6 = (1,0) and the likelihood function is (ignoring some con-\n",
      "stants)\n",
      "\n",
      "1\n",
      "\n",
      "Ln(p,o) = Tse {zai 0}\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-151.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "10.3 Maximum Likelihood 151\n",
      "<n 1 - 2\n",
      "ania 2 a wee — 1)\n",
      "\n",
      "_n nS? n(X — p)?\n",
      "o \"exp ) —>y f EXP) —— a\n",
      "\n",
      "where X =n-! D>, X; is the sample mean and §? = n7! 0,(Xj—\n",
      "X)?. The last equality above follows from the fact that V(X -\n",
      "pH)? =nS? +n(X—p)? which can be verified by writing (X;—\n",
      "bw)? = (Xi; -X +X —)? and then expanding the square. The\n",
      "log-likelihood is\n",
      "\n",
      "ns? n(X — p)?\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "&(u,0) =—nlogo —\n",
      "\n",
      "20? 20?\n",
      "Solving the equations\n",
      "Ol( pL, 7) O( 1,0)\n",
      "EAP q Skee\n",
      "Ou 0 an aa 0\n",
      "\n",
      "we conclude that ji =X and@ = S. It can be verified that these\n",
      "are indeed global maxima of the likelihood.\n",
      "\n",
      "Example 10.12 (A Hard Example.) Here is the example that con-\n",
      "fuses everyone. Let X,,...,X, ~ Unif (0,0). Recall that\n",
      "4 0<a<o\n",
      "\n",
      "Has) ={3\n",
      "\n",
      "0 otherwise.\n",
      "\n",
      "Consider a fied value of 0. Suppose @ < X; for some i. Then,\n",
      "f(Xi;0) = 0 and hence Ln (0) = |]; f (Xi; 0) = 0. It follows that\n",
      "L,(0) = 0 if any X; > 0. Therefore, £L,(0) = 0 if 0 < Xn)\n",
      "where X(n) = max{X,,..-,X,}. Now consider any 0 > Xn).\n",
      "For every X; we then have that f(X;;0) =1/0 so that £,(0) =\n",
      "TI, f(X% 0) = 0. In conclusion,\n",
      "(4)\" 90> X\n",
      "= 7) = h(n)\n",
      "\n",
      "£n(@) { 0 0< Xn).\n",
      "See Figure 10.2. Now L£,(0) is strictly decreasing over the inter-\n",
      "val [X (mn), 00). Hence, 0, = X(n). Ml\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-152.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "152 10. Parametric Inference\n",
      "\n",
      " \n",
      "\n",
      "18\n",
      "15\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "FIGURE 10.2. Likelihood function for Uniform (0,0). The\n",
      "vertical lines show the observed data. The first three\n",
      "plots show f(«;0) for three different values of 6. When\n",
      "9 < Xn) = max{X1,...,Xp}, as in the first plot, f(X(,);0) = 0\n",
      "and hence £,(0) = []j_; f(Xi;0) = 0. Otherwise f(X;; 0) = 1/0\n",
      "for each i and hence £,(0) = []j-1 f(Xi; 4) = (1/0)”. The last plot\n",
      "shows the likelihood function.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-153.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "10.4 Properties of Maximum Likelihood Estimators. 153\n",
      "\n",
      "10.4 Properties of Maximum Likelihood\n",
      "Estimators.\n",
      "\n",
      "Under certain conditions on the model, the maximum likeli-\n",
      "hood estimator 6, possesses many properties that make it an ap-\n",
      "pealing choice of estimator. The main properties of the MLE are:\n",
      "(1) It is consistent: 6,7 0, where 6, denotes the true value\n",
      "of the parameter 0;\n",
      "\n",
      "(2) It is equivariant: if 6, is the MLE of @ then g(@,) is the\n",
      "MLE of g(9);\n",
      "\n",
      "(3) It is asymptotically Normal: \\/n(6— 6,)/ ~» N(0,1)\n",
      "where sé can be computed analytically;\n",
      "\n",
      "(4) It is asymptotically optimal or efficient: roughly, this\n",
      "means that among all well behaved estimators, the MLE has the\n",
      "\n",
      " \n",
      "\n",
      "smallest variance, at least for large samples.\n",
      "(5) The mle is approximately the Bayes estimator. (To be\n",
      "explained later.)\n",
      "\n",
      "We will spend some time explaining what these properties\n",
      "mean and why they are good things. In sufficiently complicated\n",
      "problems, these properties will no longer hold and the MLE will\n",
      "no longer be a good estimator. For now we focus on the simpler\n",
      "situations where the MLE works well. The properties we discuss\n",
      "only hold if the model satisfies certain regularity conditions.\n",
      "These are essentially smoothness conditions on f(2;0). Unless\n",
      "otherwise stated, we shall tacitly assume that these con-\n",
      "ditions hold.\n",
      "\n",
      "10.5 Consistency of Maximum Likelihood\n",
      "Estimators.\n",
      "\n",
      "Consistency means that the MLE converges in probability to\n",
      "the true value. To proceed, we need a definition. If f and g are\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-154.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "This is not a dis-\n",
      "tance in the formal\n",
      "sense.\n",
      "\n",
      "154 10. Parametric Inference\n",
      "\n",
      "PDF ’s, define the Kullback-Leibler distance between f and\n",
      "\n",
      "g to be\n",
      "D(f,9) = [re log (42) da. (10.6)\n",
      "\n",
      "It can be shown that D(f,g) > 0 and D(f,f) = 0. For any\n",
      "6,u € © write D(@,w) to mean D(f (2; @), f(z; w)). We will\n",
      "assume that 0 # w implies that D(@,w) > 0.\n",
      "\n",
      "Let @, denote the true value of 9. Maximizing ¢,,(0) is equiv-\n",
      "alent to maximizing\n",
      "\n",
      "M,,( =7 Lhe: a ood\n",
      "By the law of large numbers, M,,() converges to\n",
      "(Xi 8) ) / (7 J (238) ?\n",
      "Ey, { lo = lo. x; 0,)dx\n",
      "wf °F) 8 FG 2) £ (5 44)\n",
      "\n",
      "= —D(6,,0).\n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Hence, M,,(0) & —D(@,, 0) which is maximized at 0, since —D(0,,0,) =\n",
      "Oand —D(0,,0.) <0 for 0 4 6,. Hence, we expect that the max-\n",
      "imizer will tend to 6,. To prove this formally, we need more than\n",
      "M,,(0)—3 — D(6,,0). We need this convergence to be uniform\n",
      "\n",
      "over 9. We also have to make sure that the function D(0,,0) is\n",
      "\n",
      "well behaved. Here are the formal details.\n",
      "\n",
      "Theorem 10.13 Let 0, denote the true value of 0. Define\n",
      "u (Xz 9)\n",
      "M,(@) =— > lo =\n",
      "(@) ye SFX)\n",
      "and M(0) = —D(0,,0). Suppose that\n",
      "\n",
      "sup |M,,(@) — M(6)|+30 (10.7)\n",
      "\n",
      "060\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-155.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "10.6 Equivariance of the MLE 155\n",
      "\n",
      "and that, for every € > 0,\n",
      "\n",
      "sup M(0) < M(6,). (10.8)\n",
      "0:|0-0.|>€\n",
      "\n",
      "Let 0, denote the mle. Then O23 Ops\n",
      "\n",
      "PROOF. See appendix. ll\n",
      "\n",
      "10.6 Equivariance of the MLE\n",
      "\n",
      "Theorem 10.14 Let 7 = g(0) be a one-to-one function of 0. Let\n",
      "6, be the MLE of 0. Then 7, = g(On) is the MLE of rT.\n",
      "\n",
      "ProoF. Let h = g~! denote the inverse of g. Then , =h(Tn)-\n",
      "For any 7, L(r) = [], f(wish(7)) = T], f(@is0) = £(@) where\n",
      "\n",
      "6 =h(r). Hence, for ana 7, £,(7) = £(0) < £(0) = £,(7).\n",
      "\n",
      "Example 10.15 Let X),...,X, ~N(0,1). The mle for 0 is 0, =\n",
      "Xn. Let t =e\". Then, the mle fort is? =e’ =e*. Hl\n",
      "\n",
      "10.7. Asymptotic Normality\n",
      "\n",
      "It turns out that 8, is approximately Normal and we can\n",
      "compute its variance analytically. To explore this, we first need\n",
      "a few definitions.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-156.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "156 10. Parametric Inference\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Definition 10.16 The score function is defined to be\n",
      "s(X;0) = =a fy (10.9)\n",
      "The Fisher information is defined to be\n",
      "(0) = Wo e)\n",
      "i=l\n",
      "= Sve (Xi3)) (10.10)\n",
      "For n = 1 we will sometimes write [() instead of I, (6).\n",
      "\n",
      "It can be shown that E,(s(X;0@)) = 0. It then follows that\n",
      "Vo(s(X;0)) = Eo(s?(X;0)). In fact a further simplification of\n",
      "I,,(@) is given in the next result.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 10.17 I,,(0) = nJ(@) and\n",
      "\n",
      "10) = -n, (Pees 0)\n",
      "\n",
      "_ -|[ (EE tte Ade. (10.11)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-157.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.7 Asymptotic Normality 157\n",
      "\n",
      " \n",
      "\n",
      "Theorem 10.18 (Asymptotic Normality of the MLE .) Under appr\n",
      "priate regularity conditions, the following hold:\n",
      "\n",
      "1. Let se = \\/1/I,(0). Then,\n",
      "\n",
      "(On = 9)\n",
      "\n",
      "a+ N(0,1). (10.12)\n",
      "2. Let & = 4/1/In(On). Then,\n",
      "\n",
      "es ~ N(0,1). (10.13)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The proof is in the appendix. The first statement says that\n",
      "0, ~ N(0,se) where the standard error of 0, is se = \\/1/T,,(0).\n",
      "The second statement says that this is still true even if we re-\n",
      "\n",
      " \n",
      "\n",
      "place the standard error by its estimated standard error sé =\n",
      "\n",
      "Informally, the theorem says that the distribution of the MLE can\n",
      "be approximated with N (0, se), From this fact we can construct\n",
      "an (asymptotic) confidence interval.\n",
      "\n",
      "Theorem 10.19 Let\n",
      "C, = (6, — zp 8, by, + 20/28).\n",
      "Then, P9(9 € Cn) + 1—a asn oo.\n",
      "\n",
      "Proor. Let Z denote a standard normal random variable.\n",
      "Then,\n",
      "\n",
      "Po(9€Cn) = Py (6, = 0/288 <9 <6, +228)\n",
      "\n",
      "6,—8\n",
      "= Pyl—zan<— <2,\n",
      "( af2S—~Z_ Ss »2)\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-158.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "158 10. Parametric Inference\n",
      "> P(-2ajp<Z<2ay2)=1l-a. Of\n",
      "\n",
      "For a = .05, Za/2 = 1.96 & 2, so:\n",
      "\n",
      " \n",
      "\n",
      "0, $28\n",
      "\n",
      "is an approximate 95 per cent confidence interval.\n",
      "\n",
      " \n",
      "\n",
      "(10.14)\n",
      "\n",
      " \n",
      "\n",
      "When you read an opinion poll in the newspaper, you often\n",
      "see a statement like: the poll is accurate to within one point,\n",
      "95 per cent of the time. They are simply giving a 95 per cent\n",
      "confidence interval of the form 6, + 286.\n",
      "\n",
      "Example 10.20 Let X,,...,X, ~ Bernoulli(p). The MLE is p, =\n",
      "Yo; Xi/n and f (x;p) = p*(1—p)'™*, log f(x; p) = xlogp + (1—\n",
      "1) log(1—p), s(X;p) = (x/p)—(1—2)/(1—p), and —s'(X;p) =\n",
      "(x/p?) + (1 x)/(L—p)?. Thus,\n",
      "\n",
      " \n",
      "\n",
      "Hence,\n",
      "\n",
      " \n",
      "\n",
      "~_ 1 1 sf p= 3)\n",
      "8 = eas em { m }\n",
      "\n",
      "An approximate 95 per cent confidence interval is\n",
      "\n",
      "oy 1/2\n",
      "as tL\n",
      "j+2 {HOD\n",
      "n\n",
      "Compare this with the Hoeffding interval.\n",
      "\n",
      "Example 10.21 Let X,,...,X, ~ N(@, 10\") where o? is known.\n",
      "The score function is s(X;0) = (X 9)/o? and s'(X;0) =\n",
      "—1/o? so that I,(0) = 1/02. The MLE is 6, = X,. According\n",
      "to Theorem 10.18, Xn &% N(0,0?/n). In fact, in this case, the\n",
      "distribution is exact.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-159.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "10.8 Optimality 159\n",
      "\n",
      "Example 10.22 Let X,,...,X, ~ Poisson(\\). Then a =X,\n",
      "and some calculations show that I,(A) = 1/2, so\n",
      "\n",
      " \n",
      "\n",
      "Therefore, an approximate 1 — a confidence interval for X is\n",
      "\n",
      "Xe t 2a) n/n. a\n",
      "\n",
      " \n",
      "\n",
      "10.8 Optimality\n",
      "\n",
      "Suppose that X,,...,X, ~ N(@,07). The MLB is 6, =X,.\n",
      "Another reasonable estimator is the sample median Oy. The\n",
      "MLE satisfies\n",
      "\n",
      "Vnl6, — 0) + N(0,0°).\n",
      "\n",
      "It can be proved that the median satisfies\n",
      "Vin, — 0) ~~ N (0.0?) .\n",
      "\n",
      "This means that the median converges to the right value but\n",
      "has a larger variance than the MLE .\n",
      "More generally, consider two estimators T,, and U,, and sup-\n",
      "\n",
      "pose that\n",
      "\n",
      "Vn(Tp, — 0) ~» N(0, t?)\n",
      "and that\n",
      "\n",
      "Jn(U, — 0) ~ N(0,u?).\n",
      "We define the asymptotic relative efficiency of U to T by ARE(U,\n",
      "(?/u2. In the Normal example, ARE(6,,6,) = 2/7 = .63. The\n",
      "interpretation is that if you use the median, you are only using\n",
      "about 63 per cent of the available data.\n",
      "Theorem 10.23 If 6, is the MLE and 6, is any other estimator\n",
      "then\n",
      "\n",
      "Ts\n",
      "\n",
      "The result is ac-\n",
      "tually more subtle\n",
      "than this but we\n",
      "needn't worry about\n",
      "the details.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-160.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "160 10. Parametric Inference\n",
      "ARE(,,0,) <1.\n",
      "\n",
      "Thus, the MLE has the smallest (asymptotic) variance and we\n",
      "say that MLE is efficient or asymptotically optimal.\n",
      "\n",
      "This result is predicated upon the assumed model being cor-\n",
      "rect. If the model is wrong, the MLE may no longer be optimal.\n",
      "We will discuss optimality in more generality when we discuss\n",
      "decision theory.\n",
      "\n",
      "10.9 The Delta Method.\n",
      "\n",
      "Let rt = g(@) where g is a smooth function. The maximum\n",
      "\n",
      "likelihood estimator of 7 is F = g(@). Now we address the fol-\n",
      "lowing question: what is the distribution of T?\n",
      "\n",
      " \n",
      "\n",
      "Theorem 10.24 (The Delta Method) Jf rt = g(0) where g is differ-\n",
      "entiable and g'(0) #0 then\n",
      "\n",
      "viii, = 7)\n",
      "\n",
      "Ba NOD (10.15)\n",
      "where F = (On) and\n",
      "% (F,) = |9'()| 8 (6) (10.16)\n",
      "Hence, if\n",
      "On = (F — 2/28 (Fn), Tn + 20/28 ()) (10.17)\n",
      "\n",
      "then P(t € Cy) + 1—a asn—- oo.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 10.25 Let X,,...,X, ~ Bernoulli(p) and let y = g(p) =\n",
      "log(p/(1—p)). The Fisher information function is I(p) = 1/(p(—\n",
      "p)) 80 the standard error of the MLE Py isse = {Pn(1 — pn)/n}'”.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-161.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "10.9 The Delta Method. 161\n",
      "\n",
      " \n",
      "\n",
      "The MLE of w is yy = logp/(1—p). Since, g(p) =1/(p/(1—p)),\n",
      "according to the delta method\n",
      "ae ae ee SS L\n",
      "$6 (Un) = |9' (Bn) |S (Px) = —————.\n",
      "\"Pn (1 — Pn)\n",
      "\n",
      "An approximate 95 per cent confidence interval is\n",
      "\n",
      " \n",
      "\n",
      "Example 10.26 Let X),...,X, ~ N(j,07). Suppose that ps is\n",
      "known, 0 is unknown and that we want to estimate w = logo.\n",
      "The log-likelihood is €(o) = —nlogo — sy 0 ,(a; — 1)?. Differ-\n",
      "entiate and set equal to 0 and conclude that\n",
      "\n",
      "= {2 oa\n",
      "\n",
      "C=\n",
      "n\n",
      "\n",
      "To get the standard error we need the Fisher information. First,\n",
      "\n",
      "log f (X;0) =— logo — om\n",
      "with second derivative\n",
      "1 3(xX- hy?\n",
      "o ot\n",
      "and hence i. 34 9\n",
      "i a\n",
      "\n",
      "Hence, & = G,/V2n. Let = g(o) = log(a). Then, by, =\n",
      "logG,. Since, g' = 1/o,\n",
      "\n",
      "6 1\n",
      "$e (Wn) =\n",
      "\n",
      "van Vn\n",
      "\n",
      "and an approximate 95 per cent confidence interval is On +\n",
      "\n",
      "2/\\/2n.\n",
      "\n",
      "iy) Rad\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-162.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "162 10. Parametric Inference\n",
      "10.10 Multiparameter Models\n",
      "\n",
      "These ideas can directly be extended to models with several\n",
      "parameters. Let 9 = (0,,...,;) and let @ = (01,...,;) be the\n",
      "MLE . Let £, = )5j_, log f (Xs 0),\n",
      "\n",
      "Oe, ln\n",
      "\n",
      "H, and Ae = 3950,\n",
      "0},\n",
      "\n",
      "= OBE\n",
      "J\n",
      "\n",
      "Define the Fisher Information Matrix by\n",
      "\n",
      "Eo(Hi1) Eg(Hiz) +++ Eg(ix)\n",
      "1,0) =- Beit) mt) a “ Malta (tos)\n",
      "Ey(Hi1) Eo(Hy2) +++ Eo(Hex)\n",
      "\n",
      "Let J,(0) = 17'(@) be the inverse of I,,.\n",
      "Theorem 10.27 Under appropriate regularity conditions,\n",
      "\n",
      "Vnl0 — 0) = N(0, Jn).\n",
      "\n",
      "Also, if 6; is the j'\" component of 0, then\n",
      "\n",
      "6, = 9j)\n",
      "\n",
      " \n",
      "\n",
      "where &; = Jn(j,j) is the j** diagonal element of Jn. The ap-\n",
      "protimate covariance of 0; and 0, is Cov(0;, Ox) x In( j,k).\n",
      "\n",
      "There is also a multiparameter delta method. Let rT = g(01,---, 9%)\n",
      "be a function and let\n",
      "cm\n",
      "00\n",
      "Vg= ‘\n",
      "oo\n",
      "00;.\n",
      "\n",
      "be the gradient of g.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-163.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "10.11 The Parametric Bootstrap 163\n",
      "\n",
      "Theorem 10.28 (Multiparameter delta method) Suppose that\n",
      "Vg evaluated at 0 is not 0. Let 7 = g(@). Then\n",
      "\n",
      " \n",
      "\n",
      "where\n",
      "\n",
      "& (7) = V (Vg) FAV 9), (10.19)\n",
      "Tn = In (On) and Vo is Vg evaluated at 0 = 0.\n",
      "\n",
      "Example 10.29 Let X1,...,Xn ~ N(p,07). Let rT = g(p,o) =\n",
      "a/p. In homework question 8 you will show that\n",
      "\n",
      "Hence,\n",
      "\n",
      " \n",
      "\n",
      "Thus,\n",
      "\n",
      " \n",
      "\n",
      "10.11 The Parametric Bootstrap\n",
      "\n",
      "For parametric models, standard errors and confidence in-\n",
      "tervals may also be estimated using the bootstrap. There is\n",
      "only one change. In the nonparametric bootstrap, we sampled\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-164.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "164 10. Parametric Inference\n",
      "\n",
      "X{,...,X; from the empirical distribution F,. In the paramet-\n",
      "ric bootstrap we sample instead from f (2; 0,,). Here, 6, could\n",
      "be the MLE or the method of moments estimator.\n",
      "\n",
      "Example 10.30 Consider example 10.29. To get to bootstrap stan-\n",
      "dard error, simulate X1,...,Xz ~ N(fi,6?), compute ji* =\n",
      "n-1 30, X7 and 6* = n7' 30,(X} — 7*)?. Then compute 7 =\n",
      "g(t, 6*) = 6*/j*. Repeating this B times yields bootstrap repli-\n",
      "cations\n",
      "\n",
      "oo a\n",
      "Ty yess TB\n",
      "\n",
      "and the estimated standard error is\n",
      "\n",
      "2\n",
      "$€ boot = B _\n",
      "\n",
      "The bootstrap is much easier than the delta method. On the\n",
      "other hand, the delta method has the advantage that it gives a\n",
      "closed form expression for the standard error.\n",
      "\n",
      "10.12 Technical Appendix\n",
      "\n",
      "10.12.1 Proofs\n",
      "\n",
      "PROOF OF THEOREM 10.13. Since 6, maximizes M,,(@), we\n",
      "\n",
      "have M,,(0,,) > M,,(6,). Hence,\n",
      "\n",
      "M(6.) — M(@,) M,(0,) — M(6,) + M(6,) — Mn(6.)\n",
      "\n",
      "M,(8) — M(0n) + M(0.) — Mn (0x)\n",
      "sup |M,,(0) — M(0)| + M(0.) — Mn(0.)\n",
      "0\n",
      "\n",
      "[PAA\n",
      "\n",
      "0\n",
      "\n",
      "where the last line follows from (10.7). It follows that, for any\n",
      "5>0,\n",
      "P (1G) <M) - d) 30.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-165.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.12 Technical Appendix 165\n",
      "\n",
      "Pick any € > 0. By (10.8), there exists 6 > 0 such that |—0,| > €\n",
      "implies that W(@) < M(0,) — 6. Hence,\n",
      "\n",
      "P(|0, —%| >) <P (1@.) < M(0,) - °) 30. ff\n",
      "Next we want to prove Theorem 10.18. First we need a Lemma.\n",
      "Lemma 10.31 The score function satisfies\n",
      "Ep [s(X;4)] =0.\n",
      "\n",
      "Proor. Note that 1 = f f(2;0)dx. Differentiate both sides\n",
      "of this equation to conclude that\n",
      "\n",
      " \n",
      "\n",
      "0 = wf seme f Zresoar\n",
      "ae a Dog (29) 6 1\n",
      "Ca al ( O)dz = ——y (2 O)dx\n",
      "\n",
      "[ s(osoyrtes0)ae = E,ys(X;0).\n",
      "\n",
      "PROOF OF THEOREM 10.18. Let (0) = log £(0). Then,\n",
      "0=£(0) = C0) + (-9)0\"(0).\n",
      "\n",
      "Rearrange the above equation to get @— 6 = —0'(0)/0\"(8) or, in\n",
      "other words,\n",
      "\n",
      "Fal) TOP\n",
      "Va _\n",
      "G9) ae OE\n",
      "Wale—8) —17\"(@) ~ BOTTOM\n",
      "\n",
      "Let Y; = 0 log f (X;;0)/00. Recall that E(Y;) =0 from the pre-\n",
      "vious Lemma and also V(Y;) = 1(@). Hence,\n",
      "\n",
      "TOP =n7'? S°Y; = VnY = Vn(¥ -0) ~ W~ NOD)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-166.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "166 10. Parametric Inference\n",
      "\n",
      "by the central limit theorem. Let A; = —0? log f (Xj;0)/00?.\n",
      "Then E(A;) =1(@) and\n",
      "\n",
      "BOTTOM = A—> (6)\n",
      "\n",
      "by the law of large numbers. Apply Theorem 6.5 part (e), to\n",
      "conclude that\n",
      "\n",
      "~ Ww 1\n",
      "Vn(0 — 0) “THAN (0.75):\n",
      "\n",
      "Assuming that [(@) is a continuous function of @, it follows that\n",
      "1(6,) 4 1(8). Now\n",
      "\n",
      " \n",
      "\n",
      "—— = val'?(On)(On — 9)\n",
      "\n",
      "~. 1/2\n",
      "= {vare@@, 0} te} ;\n",
      "\n",
      "The first terms tends in distribution to N(0,1). The second term\n",
      "tends in probability to 1. The result follows from Theorem 6.5\n",
      "part (e).\n",
      "\n",
      "OUTLINE OF PROOF OF THEOREM 10.24. Write,\n",
      "\n",
      " \n",
      "\n",
      "F = (8) = (0) + (0 —8)9'(9) =7 + 0 — A)g'(0).\n",
      "\n",
      "Thus, .\n",
      "Vil? — 7) = V/nl(8 — 0) 9'(0)\n",
      "and hence\n",
      "nl(O)(F—T) - ~\n",
      "Fe) z I(0)(0 — 0).\n",
      "\n",
      "Theorem 10.18 tells us that the right hand side tends in distri-\n",
      "bution to a N(0,1). Hence,\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-167.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "10.12 Technical Appendix 167\n",
      "\n",
      "or, in other words,\n",
      "\n",
      "where Woy\n",
      "a2\\)_ VY\n",
      "se7(7,) = “nl @)\n",
      "The result remains true if we substitute 0 for 0 by Theorem 6.5\n",
      "part (ec).\n",
      "\n",
      "10.12.2 Sufficiency\n",
      "\n",
      "A statistic is a function T(X\") of the data. A sufficient statis-\n",
      "tic is a statistic that contains all the information in the data.\n",
      "To make this more formal, we need some defintions.\n",
      "\n",
      " \n",
      "\n",
      "Definition 10.32 Write 2” © y” if f(x\";0) = c f(y\";@)\n",
      "for some constant c that might depend on x\" and y” but\n",
      "not 6. A statistic T(a”) is sufficient if T(x”) «+ T(y\")\n",
      "implies that 2” <> y\".\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Notice that if x” < y” then the likelihood function based on\n",
      "\n",
      "x” has the same shape as the likelihood function based on y\".\n",
      "\n",
      "Roughly speaking, a statistic is sufficient if we can calculate the\n",
      "likelihood function knowing only T(X\").\n",
      "\n",
      "Example 10.33 Let X),...,X, ~ Bernoulli(p). Then L(p)\n",
      "ps(1—p)\"-* where S = yo, X; so S is sufficient.\n",
      "\n",
      "Example 10.34 Let X1,...,X;, ~ N(,0) and let T = (X,S).\n",
      "Then,\n",
      "\n",
      "f(X\"sp,0) = []f4in0\n",
      "\n",
      "II o 7 oP {-s des - wh\n",
      "\n",
      "_ 1 _ ns? a _n(X = p)?\n",
      "= (se) PP) age GF\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-168.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "168 10. Parametric Inference\n",
      "\n",
      "The last expression depends on the data only through T and\n",
      "therefore, T = (X,S) is a sufficient statistic. Note that U =\n",
      "(17X,S) is also a sufficient statistic. If I tell you the value of U\n",
      "then you can easily figure out T and then compute the likelihood.\n",
      "Sufficient statistics are far from unique. Consider the following\n",
      "statistics for the N(,0?) model:\n",
      "\n",
      "TX\") = (X1,---, Xn)\n",
      "T,(X\") = (X,S)\n",
      "T;(X\") x\n",
      "\n",
      "Ti(X\") = (X,S, Xs).\n",
      "\n",
      "The first statistic is just the whole data set. This is sufficient.\n",
      "The second is also sufficient as we proved above. The third is not\n",
      "sufficient: you can’t compute L(p,0) if I only tell you X. The\n",
      "fourth statistic T, is sufficient. The statistics T; and T, are suffi-\n",
      "cient but they contain redundant information. Intuitively, there\n",
      "is a sense in which Ty is a “more concise” sufficient statistic\n",
      "than either T; or Ty. We can express this formally by noting\n",
      "that Tz is a function of T, and similarly, T> is a function of Ty.\n",
      "For example, T, = g(T;) where g(a,,@2,a3) = (a), a).\n",
      "\n",
      " \n",
      "\n",
      "Definition 10.35 A statistic T is minimally sufficient if\n",
      "(i) it is sufficient and (ii) it is a function of every other\n",
      "sufficient statistic.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 10.36 T is minimal sufficient if T(a\") =T(y\") if and\n",
      "only if a\" 4 y”.\n",
      "\n",
      "A statistic induces a partition on the set if outcomes. We can\n",
      "think of sufficiency in terms of these partitions.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-169.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "10.12 Technical Appendix 169\n",
      "\n",
      "Example 10.37 Let X,,X2,X3 ~ Bernoulli(@). Let V = X,\n",
      "T = 30,X; and U = (T,X). Here is the set of outcomes and\n",
      "the statistics:\n",
      "\n",
      "Xi X\n",
      "\n",
      "’ T U\n",
      "\n",
      " \n",
      "\n",
      "Then V is not sufficient but T and U are sufficient. T is min-\n",
      "imal sufficient; U is not minimal since if x2” = (1,0,1) and\n",
      "y” = (0,1,1), then 2” 4 y” yet U(x\") # U(y\"). The statistic\n",
      "W =17T generates the same partition as T. It is also minimal\n",
      "sufficient. il\n",
      "\n",
      "Example 10.38 For a N(j,07) model, T = (X,S) is a minimally\n",
      "sufficient statistic. For the Bernoulli model, T = 3); X; is a\n",
      "minimally sufficient statistic. For the Poisson model, T = )), Xj\n",
      "is a minimally sufficient statistic. Check that T = (30; X;,X1)\n",
      "is sufficient but not minimal sufficient. Check that T = X, is\n",
      "not sufficient. Hi\n",
      "\n",
      "I did not give the usual definition of sufficiency. The usual\n",
      "definition is this: T is sufficient if the distribution of X” given\n",
      "T(X\") =t does not depend on 0.\n",
      "\n",
      "Example 10.39 Two coin flips. Let X = (X,, Xz) ~ Bernoulli(p).\n",
      "Then T = X, + Xq is sufficient. To see this, we need the dis-\n",
      "tribution of (X,,X2) given T = t. Since T can take 3 possible\n",
      "values, there are 3 conditional distributions to check. They are:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-170.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "170 10. Parametric Inference\n",
      "(i) the distribution of (X1, Xz) given T = 0:\n",
      "\n",
      "P(X; =0,X2.=\n",
      "\n",
      "  \n",
      "\n",
      "t= 0) =1, P(X, =0,Xy = 1|t=0) =0,\n",
      "\n",
      "P(X, =1,Xy =0|t =0) =0, P(X, =1,.X, = 1t=0) =0\n",
      "(ii) the distribution of (X,,X2) given T =1:\n",
      "\n",
      "1\n",
      "\n",
      "P(X; =0, Xz =0lf = 1) = 0, P(X =0, Xx == 1) =5,\n",
      "\n",
      "1\n",
      "\n",
      "P(X, = 1, X2 = 0|t=1) =5,P(X1 = 1, Xp = 1 = 1) =0\n",
      "\n",
      "(iii) the distribution of (X,,X2) given T = 2:\n",
      "\n",
      "P(X, = 0, X_ = Olt = 2) = 0, P(X, = 0, X» = 1\\t = 2) =0,\n",
      "\n",
      " \n",
      "\n",
      "P(X; =1, Xz = 0|t = 2) =0, P(X =1, Xe = 1|t = 2) = 1.\n",
      "None of these depend on the parameter p. Thus, the distribution\n",
      "\n",
      "of X,,X2|T does not depend on @ so T is sufficient. Wl\n",
      "\n",
      "Theorem 10.40 (Factorization Theorem) T is sufficient if and\n",
      "only if there are functions g(t,0) and h(x) such that f(2\";0) =\n",
      "\n",
      "g(t(2\"), @)h(2\").\n",
      "Example 10.41 Return to the two coin flips. Let t = a, 4+ £9.\n",
      "Then\n",
      "\n",
      "F (1130) f (v2; 9)\n",
      "Mao eo\n",
      "= g(t, A)h(x1, x2)\n",
      "\n",
      "where g(t,0) = 0'(1 —0)?-' and h(x1,22) =1. Therefore, T =\n",
      "X, +X is sufficient. HH\n",
      "\n",
      "Ff (a1, #238)\n",
      "\n",
      "Now we discuss an implication of sufficiency in point estima-\n",
      "tion. Let 0 be an estimator of 9. The Rao-Blackwell theorem says\n",
      "that an estimator should only depend on the sufficient statistic,\n",
      "\n",
      "otherwise it can be improved. Let R(0,0) = E,[( — @)?] denote\n",
      "the MSE of the estimator.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-171.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "10.12 Technical Appendix 171\n",
      "\n",
      "Theorem 10.42 (Rao-Blackwell) Let @ be an estimator and\n",
      "let T be a sufficient statistic. Define a new estimator by\n",
      "\n",
      "6 =E(6|T).\n",
      "Then, for every 0, R(0,0) < R(6,@).\n",
      "Example 10.43 Consider flipping a coin twice. Let d= X. This\n",
      "is a well defined (and unbiased) estimator. But it is not a func-\n",
      "tion of the sufficient statistic T = X, + Xz. However, note\n",
      "that 0 = E(X,|T) = (X, + X2)/2. By the Rao-Blackwell Theo-\n",
      "rem, 0 has MSE at least as small as @ = X,. The same applies\n",
      "with n coin flips. Again define @ = X, and T = \\>,X;. Then\n",
      "6 =E(X,|T) =n! So, X; has improved vse . Wl\n",
      "\n",
      "10.12.8 Exponential Families\n",
      "\n",
      "Most of the parametric models we have studied so far are spe-\n",
      "cial cases of a general class of models called exponential families.\n",
      "We say that {f(2;0;0 € QO} is a one-parameter exponential\n",
      "family if there are functions 7(0), B(@), T(x) and h(x) such\n",
      "that:\n",
      "\n",
      "f(a30) = h(x)en@T@)-BO |\n",
      "\n",
      "It is easy to see that T(X) is sufficient. We call T the natural\n",
      "sufficient statistic.\n",
      "Example 10.44 Let X ~ Poisson(@). Then\n",
      "\n",
      " \n",
      "\n",
      "Oe? 1c togo-0\n",
      "f (230) = a = ae e\n",
      "\n",
      " \n",
      "\n",
      "and hence, this is an exponential family with n(0) = log 0, B(@) =\n",
      "6, T(x) =2, h(x) =1/c!.\n",
      "\n",
      "Example 10.45 Let X ~ Bin(n,@). Then\n",
      "\n",
      "(230) = (“Jera—oy = (\") exp {tog (4) + nlog(1 — a} .\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-172.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172 10. Parametric Inference\n",
      "\n",
      "In this case,\n",
      "\n",
      "and\n",
      "\n",
      "a\n",
      "We can rewrite an exponential family as\n",
      "flesn) = h(e)e\"o-a\n",
      "\n",
      "where 7 = 7(9) is called the natural parameter and\n",
      "A(n) = tog f h(a)et™ar.\n",
      "\n",
      "For example a Poisson can be written as f(2;7) = e™—°\"/z!\n",
      "where the natural parameter is 7 = log@.\n",
      "\n",
      "Let X\\,...,X,, be iid froma exponential family. Then f(x”; 4)\n",
      "is an exponential family:\n",
      "\n",
      "f(s\" 0) =h, (a? i, a ee\n",
      "\n",
      "where h,,(2\") = J],;h(#), Tr(w\") = 30,7 (2;) and B,(@) =\n",
      "nB(@). This implies that }>,T(X;) is sufficient.\n",
      "\n",
      "Example 10.46 Let X),...,X, ~ Uniform(0,0). Then\n",
      "sf 1\n",
      "f@5O)= gel (Xn) <0)\n",
      "\n",
      "where I is 1 if the term inside the brackets is true and 0 other-\n",
      "wise, and x(n) = max{x,,...,%n}. Thus T(X\") = mar{X,,...,X,}\n",
      "is sufficient. But since T(X\") # 3),T (Xj), this cannot be an ex-\n",
      "ponential family.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-173.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "10.12 Technical Appendix 173\n",
      "\n",
      "Theorem 10.47 Let X have density in an exponential family.\n",
      "Then,\n",
      "E(T(X)) = A'(n), V(T(X)) = A\"(n)-\n",
      "\n",
      "If 0 = (,...,0;) is a vector, then we say that f(2;@) has\n",
      "exponential family form if\n",
      "\n",
      "k\n",
      "f (230) = h(x) exp {= nj(0)T; (2) — ovo) :\n",
      "jel\n",
      "Again, T = (T),.-..,T7),) is sufficient and n iid samples also has\n",
      "exponential form with sufficient statistic ()>; Ti (Xj), -.., 0; Tk(X))-\n",
      "\n",
      "Example 10.48 Consider the normal family with 0 = (1,0). Now,\n",
      "\n",
      "(230) = ef He - a -2 (4 + lox( na?) } .\n",
      "\n",
      "202 2 \\o?\n",
      "\n",
      "This is exponential with\n",
      "\n",
      "m(0)= 4, Til) =«\n",
      "m0) = ~9g2? T(x) = 2°\n",
      "B(d) = B (5 + lo(2x0?)) , A(z) =1.\n",
      "\n",
      "Hence, with n iid samples, (37;Xi, >, X?) is sufficient. i\n",
      "\n",
      "As before we can write an exponential family as\n",
      "\n",
      "f(a3n) = h(x) exp {17 (a)n — A(m)}\n",
      "\n",
      "where A(y) = f h(z)e™” \"dz. It can be shown that\n",
      "\n",
      "where the first expression is the vector of partial derivatives and\n",
      "the second is the matrix of second derivatives.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-174.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "174\n",
      "\n",
      "10. Parametric Inference\n",
      "\n",
      "10.13 Exercises\n",
      "\n",
      "1.\n",
      "\n",
      "Let X),...,X, ~ Gamma(a, 3). Find the method of mo-\n",
      "ments estimator for a and .\n",
      "\n",
      ". Let X\\,...,X, ~ Uniform(a,b) where a and 6 are un-\n",
      "\n",
      "known parameters and a < b.\n",
      "\n",
      "(a) Find the method of moments estimators for a and b.\n",
      "(b) Find the MLE @ and 6.\n",
      "\n",
      "(c) Let r = f xdF (x). Find the MLB of 7.\n",
      "\n",
      "(d) Let 7 be the MLE from (1bc). Let 7 be the nonpara-\n",
      "metric plug-in estimator of 7 = f dF (x). Suppose that\n",
      "a =1,b=3 and n= 10. Find the MsE of 7 by simulation.\n",
      "Find the MSE of 7 analytically. Compare.\n",
      "\n",
      ". Let X1,...,Xn ~ N(p,07). Let 7 be the .95 percentile,\n",
      "\n",
      "ie. P(X < 7) =.95.\n",
      "(a) Find the MLE of r.\n",
      "\n",
      "(b) Find an expression for an approximate 1—a confidence\n",
      "interval for T.\n",
      "\n",
      "(c) Suppose the data are:\n",
      "\n",
      "3.23 -2.50 1.88 -0.68 4.43 0.17\n",
      "1.03 -0.07 -0.01 0.76 1.76 3.18\n",
      "0.33 -0.31 0.30 -0.61 1.52 5.43\n",
      "1.54 2.28 0.42 2.33 -1.03 4.00\n",
      "0.39\n",
      "\n",
      "Find the mle 7. Find the standard error using the delta\n",
      "method. Find the standard error using the parametric\n",
      "bootstrap.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-175.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "10.13 Exercises 175\n",
      "\n",
      "4. Let X,,...,X, ~ Uniform(0,0). Show that the MLE is\n",
      "\n",
      "consistent. Hint: Let Y = max{Xj,...,X,}.. For any c,\n",
      "PY <c) = P(X, < ¢,X) < ¢...,X, <c) = P(X <\n",
      "c)P(X2 < c)...P(X, < c).\n",
      "\n",
      "- Let X,,...,X, ~ Poisson(\\). Find the method of mo-\n",
      "\n",
      "ments estimator, the maximum likelihood estimator and\n",
      "the Fisher information I().\n",
      "\n",
      ". Let Xy,...,X, ~ N(0,1). Define\n",
      "\n",
      "yall XxX >0\n",
      "‘= L0 if X) <0.\n",
      "\n",
      "Let  =P(% =1).\n",
      "\n",
      "a) Find the maximum likelihood estimate bof v.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "(b) Find an approximate 95 per cent confidence interval\n",
      "for w.\n",
      "\n",
      "(c) Define = (1/n) >, ¥i- Show that ® is a consistent\n",
      "estimator of y.\n",
      "\n",
      "(d) Compute the asymptotic relative efficiency of wb tov.\n",
      "Hint: Use the delta method to get the standard error of the\n",
      "MLE . Then compute the standard error (i.e. the standard\n",
      "deviation) of 1.\n",
      "\n",
      "(ec ) Suppose that the data are not really normal. Show that\n",
      "w is not consistent. What, if anything, does o converge to?\n",
      "\n",
      ". (Comparing two treatments.) n, people are given treat-\n",
      "ment 1 and nz people are given treatment 2. Let X; be\n",
      "the number of people on treatment 1 who respond fa-\n",
      "yorably to the treatment and let Xj be the number of\n",
      "people on treatment 2 who respond favorably. Assume\n",
      "that X, ~ Binomial(n;,p,) X2 ~ Binomial(ng, pz). Let\n",
      "w= Ppi — pe-\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-176.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "176 10. Parametric Inference\n",
      "\n",
      "(a) Find the MLE for w.\n",
      "\n",
      "(b) Find the Fisher information matrix I(P,, p2).\n",
      "(\n",
      "\n",
      "t\n",
      "\n",
      "c) Use the multiparameter delta method to find the asymp-\n",
      "otic standard error of W.\n",
      "\n",
      "(d) Suppose that nj = ny = 200, X; = 160 and Xy = 148.\n",
      "\n",
      "Find w. Find an approximate 90 percent confidence inter-\n",
      "\n",
      "val for u) using (i) the delta method and (ii) the parametric\n",
      "\n",
      "bootstrap.\n",
      "\n",
      "8. Find the Fisher information matrix for Example 10.29.\n",
      "\n",
      " \n",
      "\n",
      "9. Let Xj,...,X, Normal(j,1). Let 6 = e” and let 0\n",
      "be the mle. Create a data set (using 4 = 5) consisting of\n",
      "n=100 observations.\n",
      "\n",
      "(a) Use the delta method to get sé and 95 percent confi-\n",
      "dence interval for 6. Use the parametric bootstrap to get\n",
      "sé and 95 percent confidence interval for 0. Use the non-\n",
      "parametric bootstrap to get sé and 95 percent confidence\n",
      "interval for 6. Compare your answers.\n",
      "\n",
      "(b) Plot a histogram of the bootstrap replications for the\n",
      "parametric and nonparametric bootstraps. These are esti-\n",
      "mates of the distribution of @. The delta method also gives\n",
      "an approximation to this distribution namely, Normal (9, se”).\n",
      "Compare these to the true sampling distribution of 6 (whihe\n",
      "you can get by simulation). Which approximation, para-\n",
      "metric bootstrap, bootstrap, or delta method is closer to\n",
      "the true distribution?\n",
      "\n",
      "10. Let X),...,X, Unif(0, 0). The MLE is? = X(q = max{Xi,..., Xn}.\n",
      "Generate a data set. of size 50 with @=1.\n",
      "\n",
      "(a) Find the distribution of @ analytically. Compare the\n",
      "true distribution of @ to the histograms from the paramet-\n",
      "ric and nonparametric bootstraps.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-177.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "10.13 Exercises 177\n",
      "\n",
      "(b) This is a case where the nonparametric bootstrap\n",
      "does very poorly. Show that, for the parametric bootstrap\n",
      "P(6* = 0) =0 but for the nonparametric bootstrap P(O* =\n",
      "0) ~ .632. Hint: show that, P(@* = 0) =1 - (1—(1/n))\"\n",
      "then take the limit as n gets large. What is the implication\n",
      "of this?\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-178.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "178 10. Parametric Inference\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-179.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "11\n",
      "Hypothesis Testing and p-values\n",
      "\n",
      "Suppose we want to know if a certain chemical causes cancer.\n",
      "We take some rats and randomly divide them into two groups.\n",
      "We expose one group to the chemical and then we compare\n",
      "the cancer rate in the two groups. Consider the following two\n",
      "hypotheses:\n",
      "\n",
      "The Null Hypothesis: The cancer rate is the same\n",
      "in the two groups\n",
      "\n",
      "The Alternative Hypothesis: The cancer rate is\n",
      "not the same in the two groups.\n",
      "\n",
      "If the exposed group has a much higher rate of cancer than the\n",
      "unexposed group then we will reject the null hypothesis and\n",
      "conclude that the evidence favors the alternative h ypothesis;\n",
      "in other words we will conclude that there is evidence that the\n",
      "chemical causes cancer. This is an example of hypothesis testing.\n",
      "\n",
      "More formally, suppose that we partition the parameter space\n",
      "Q into two disjoint sets O9 and ©, and that we wish to test\n",
      "\n",
      "Ho:0€@o versus H,:0€O4. (11.1)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-180.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "180 11. Hypothesis Testing and p-values\n",
      "\n",
      "We call Hp the null hypothesis and H, the alternative hy-\n",
      "pothesis.\n",
      "\n",
      "Let X be a random variable and let Y be the range of X. We\n",
      "test a hypothesis by finding an appropriate subset. of outcomes\n",
      "RC # called the rejection region. If X € R we reject the\n",
      "null hypothesis, otherwise, we do not reject the null hypothesis:\n",
      "\n",
      "XeER => reject Ho\n",
      "X ¢R => retain (do not reject) Ho\n",
      "\n",
      "Usually the rejection region R is of the form\n",
      "\n",
      "Rafer T(x) > e}\n",
      "\n",
      "where T is a test statistic and c is a critical value. The prob-\n",
      "lem in hypothesis testing is to find an appropriate test statistic\n",
      "T and an appropriate cutoff value c.\n",
      "\n",
      "Warning! There is a tendency to use hypthesis testing meth-\n",
      "ods even when they are not appropriate. Often, estimation and\n",
      "confidence intervals are better tools. Use hypothesis testing only\n",
      "when you want to test a well defined hypothesis.\n",
      "\n",
      "Hypothesis testing is like a legal trial. We asume someone is\n",
      "innocent unless the evidence strongly suggests that he is guilty.\n",
      "Similarly, we retain Ho unless there is strong evidence to reject.\n",
      "Hy. There are two types of errors when can make. Rejecting Ho\n",
      "when Hp is true is called a type I error. Rejecting H; when\n",
      "#1, is true is called a type II error. The possible outcomes for\n",
      "hypothesis testing are summarized in the Table below:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-181.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11. Hypothesis Testing and p-values 181\n",
      "\n",
      "Retain Null Reject Null\n",
      "Ho true | / type I error\n",
      "H, true | type ILerror /\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Summary of outcomes of hypothesis testing.\n",
      "\n",
      "Definition 11.1 The power function of a test with rejection re-\n",
      "gion R is defined by\n",
      "\n",
      "B(0) = Py(X € R). (11.2)\n",
      "The size of a test is defined to be\n",
      "\n",
      "a= sup (8). (11.3)\n",
      "\n",
      "0E@0\n",
      "\n",
      "A test is said to have level a if its size is less than or equal to\n",
      "Q.\n",
      "\n",
      "A hypothesis of the form 0 = @p is called a simple hypoth-\n",
      "esis. A hypothesis of the form @ > 69 or @ < @ is called a\n",
      "composite hypothesis. A test of the form\n",
      "\n",
      "Ao:0=0605 versus H,:04 6\n",
      "is called a two-sided test. A test of the form\n",
      "AHyo:0<0) versus H,;:0> 4\n",
      "\n",
      "or\n",
      "Ho :0>6) versus H,:0< 4%\n",
      "\n",
      "is called a one-sided test. The most common tests are two-\n",
      "sided.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-182.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "182 11. Hypothesis Testing and p-values\n",
      "\n",
      "Example 11.2 Let X),...,X, ~ N(y,0) where o is known. We\n",
      "want to test Hy: <0 versus H, : p> 0. Hence, Og = (—ov, 0]\n",
      "and ©, = (0,00). Consider the test:\n",
      "\n",
      "reject Hy if T >c\n",
      "\n",
      "where T =X. The rejection region is R= {x 2 (GP) &. a}.\n",
      "Let Z denote a standard normal random variable. The power\n",
      "function is\n",
      "\n",
      "Bu) = Pr (X>e)\n",
      "~ 7, (A=, view)\n",
      "\n",
      "o o\n",
      "\n",
      "_ P(z> a)\n",
      "_ 1-0(4),\n",
      "\n",
      "o\n",
      "\n",
      "This function is increasing in w. Hence\n",
      "\n",
      " \n",
      "\n",
      "sive sup (i) = 60) =1-0( ° ).\n",
      "\n",
      "u<d\n",
      "To get a size a test, set this equal to a and solve for c to get\n",
      "\n",
      "od\"(L—a)\n",
      "\n",
      "—\n",
      "\n",
      " \n",
      "\n",
      "So we reject when X > oO-!(1—a)/ Jn. Equivalently, we reject\n",
      "\n",
      "when —\n",
      "Vu gg\n",
      "o\n",
      "Finding most powerful tests is hard and, in many cases, most\n",
      "powerful tests don’t even exist. Instead of searching for most\n",
      "powerful tests, we'll just consider three widely used tests: the\n",
      "Wald test, the y? test and the permutation test. A fourth test,\n",
      "the likelihood ratio test, is dicussed in the appendix.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-183.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "11.1 The Wald Test 183\n",
      "\n",
      ".1 The Wald Test\n",
      "\n",
      "Let @ be a scalar parameter, let @ be an estimate of @ and let\n",
      "be the estimated standard error of @.\n",
      "\n",
      " \n",
      "\n",
      "Definition 11.3 The Wald Test\n",
      "Consider testing\n",
      "\n",
      "Hy:9=0 versus H,: 046.\n",
      "Assume that 0 is asymptotically Normal:\n",
      "\n",
      "0-4\n",
      "vind — 60) ~ N(0,1).\n",
      "sé\n",
      "The size a Wald test is: reject Hy when |W| > Za/2\n",
      "where\n",
      "—%\n",
      "\n",
      "se\n",
      "\n",
      "W=\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 11.4 Asymptotically, the Wald test has size a, that is,\n",
      "\n",
      "Po, (|Z| > 2a/2) 3 @\n",
      "\n",
      "as n —+ &.\n",
      "\n",
      "PrRooF. Under 6 = 60, (0- 0)/sé ~» N(0,1). Hence, the\n",
      "\n",
      "probability of rejecting when the null 6 = 6p is true is\n",
      "\n",
      "0-6\n",
      "Py, (W|> zap) = Po ( > =)\n",
      "\n",
      "+ P(|N(O,1)| > 2/2)\n",
      "= a. —\n",
      "\n",
      "Remark 11.5 Most texts define the Wald test slightly differently.\n",
      "They use the standard error computed at @ = 09 rather then at\n",
      "\n",
      "the estimated value 0. Both versions are valid.\n",
      "\n",
      "The test is named\n",
      "after Abraham Wald\n",
      "(1902-1950), who\n",
      "was a very influen-\n",
      "tial — mathematical\n",
      "statistician.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-184.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "184 11. Hypothesis Testing and p-values\n",
      "\n",
      "Let us consider the power of the Wald test when the null\n",
      "hypothesis is false.\n",
      "\n",
      "Theorem 11.6 Suppose the true value of 0 is 0, #09. The power\n",
      "B(0,) - the probability of correctly rejecting the null hypothesis\n",
      "— is given (approximately) by\n",
      "\n",
      "9% — 8. % —8\n",
      "of 7 * 42a) +0( = tan). (11.4)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "se\n",
      "\n",
      "Recall that sé tends to 0 as the sample size increases. Inspect-\n",
      "ing (11.4) closely we note that: (i) the power is large if 0, is far\n",
      "from 99 and (ii) the power is large if the sample size is large.\n",
      "\n",
      "Example 11.7 (Comparing Two Prediction Algorithms) We test a\n",
      "prediction algorithm on a test set of sizem and we test a second\n",
      "prediction algorithm on a second test set of size n. Let X be\n",
      "the number of incorrect predictions for algorithm 1 and let Y\n",
      "be the number of incorrect predictions for algorithm 2. Then\n",
      "X ~ Binomial(m,p,) and Y ~ Binomial(n, p2). To test the null\n",
      "hypothesis that p; = po write\n",
      "\n",
      "Ho:5=0 versus H,;:6#40\n",
      "\n",
      "where 6 = py — py. The MLB is f= Pi — py with estimated\n",
      "standard error\n",
      "\n",
      " \n",
      "\n",
      "5-0 Di — De\n",
      "se AG-A) | é\n",
      "Vee +\n",
      "\n",
      "The power of this test will be largest when p, is far from py and\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "when the sample sizes are large.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-185.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "11.1 The Wald Test 185\n",
      "\n",
      "What if we used the same test set to test both algorithms?\n",
      "The two samples are no longer independent. Instead we use the\n",
      "following strategy. Let X; = 1 if algorithm 1 is correct on test\n",
      "case i and X; = 0 otherwise. Let Y; =1 if algorithm 2 is correct\n",
      "on test case i Y; = 0 otherwise. A typical data set will look\n",
      "something like this:\n",
      "\n",
      "Test Case] X; Y; Dj; =X;-Y;\n",
      "\n",
      " \n",
      "\n",
      "i TO 7\n",
      "2 1 1 0\n",
      "3 1 1 0\n",
      "4 0 1 -1\n",
      "5 0 0 0\n",
      "n 0 1 Fr\n",
      "\n",
      "Let\n",
      "\n",
      "5=E(D) =E(X) —E(¥) =\n",
      "Then 6 =D =n“! 1 Dj and sé (6) = S/Yn where S? =\n",
      "ne (Di —D)?. To test Hy :6 =0 versus H,:6 #0 we use\n",
      "W =0/sé and reject Ho if |W| > Zqj2- This is called a paired\n",
      "comparison.\n",
      "\n",
      "Example 11.8 (Comparing Two Means.) Let X\\,...,X  andYj,...\n",
      "\n",
      "be two independent samples from populations with means 1, and\n",
      "Ha, respectively. Let’s test the null hypothesis that pf, = [lo.\n",
      "Write this as Hy : 6 = 0 versus H, : 6 #0 where 6 = py — po.\n",
      "Recall that the nonparametric plug-in estimate of 6 is 6=X-Y\n",
      "\n",
      " \n",
      "\n",
      "with estimated standard error\n",
      "\n",
      " \n",
      "\n",
      "Vn\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-186.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "186 11. Hypothesis Testing and p-values\n",
      "\n",
      "where s{ and s3 are the sample variances. The size a Wald test\n",
      "rejects Hy when |W| > Zaj2 where\n",
      "-_ 5-0\n",
      "W= =\n",
      "\n",
      "sé\n",
      "\n",
      "    \n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 11.9 (Comparing Two Medians.) Consider the previous\n",
      "example again but let us test whether the medians of the two\n",
      "distributions are the same. Thus, Ho : 6 = 0 versus H, : 6 #\n",
      "0 where 6 = v4, — 2 where 4 and 12 are the medians. The\n",
      "nonparametric plug-in estimate of 6 is g= DV, —D where V, and\n",
      "Dy are the sample medians. The estimated standard error sé of\n",
      "5 can be obtained from the bootstrap. The Wald test statistic is\n",
      "W=6/s.\n",
      "\n",
      "There is a relationship between the Wald test and the 1 — a\n",
      "asymptotic confidence interval 6 + sé Za/2-\n",
      "\n",
      "Theorem 11.10 The size a Wald test rejects Hy : 0 = 09 versus\n",
      "H,:0 4 if and only if 0 € C where\n",
      "\n",
      "C= (0-8 Zea /2s 048 aj)\n",
      "\n",
      "Thus, testing the hypothesis is equivalent to checking whether\n",
      "the null value is in the confidence interval.\n",
      "\n",
      "11.2 p-values\n",
      "\n",
      "Reporting “reject Ho” or “retain Ho” is not very informative.\n",
      "Instead, we could ask, for every a, whether the test rejects at\n",
      "that level. Generally, if the test rejects at level a it will also\n",
      "reject at level a’ > a. Hence, there is a smallest a at which the\n",
      "test rejects and we call this number the p-value.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-187.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "11.2 p-values 187\n",
      "\n",
      " \n",
      "\n",
      "Definition 11.11 Suppose that for every a € (0,1) we have\n",
      "a size a test with rejection region Ra. Then,\n",
      "\n",
      "p-value = inf{a : T(X\")eE Ral.\n",
      "\n",
      "That is, the p-value is the smallest level at which we can\n",
      "reject Ho.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Informally, the p-value is a measure of the evidence against\n",
      "Hg: the smaller the p-value, the stronger the evidence against\n",
      "A. Typically, researchers use the following evidence scale:\n",
      "\n",
      "p-value evidence\n",
      "\n",
      "< 01 very strong evidence againt Ho\n",
      "-01- .05 strong evidence againt Ho\n",
      "-05- .10 weak evidence againt Ho\n",
      "\n",
      ">i little or no evidence againt Ho\n",
      "\n",
      " \n",
      "\n",
      "Warning! A large p-value is not strong evidence in favor of\n",
      "Ho. A large p-value can occur for two reasons: (i) Ho is true or\n",
      "(ii) Ho is false but the test has low power.\n",
      "\n",
      "But do not confuse the p-value with P(Ho|Data). The p- We discuss quanti-\n",
      "\n",
      "value is not the probability that the null hypothesis is ties like P(Ho|Data)\n",
      "true. in the chapter on\n",
      "\n",
      "The following result explains how to compute the p-value. Bayesian inference.\n",
      "\n",
      "Theorem 11.12 Suppose that the size a test is of the form\n",
      "reject Ho if and only if T(X\") > cq.\n",
      "\n",
      "Then,\n",
      "\n",
      "p-value = sup Py(T(X\") > T(2\")).\n",
      "0€00\n",
      "\n",
      "In words, the p-value is the probability (under Hg) of observing\n",
      "a value of the test statistic as or more extreme than what was\n",
      "actually observed.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-188.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188 11. Hypothesis Testing and p-values\n",
      "\n",
      "For a Wald test, W has an aproximate N(0,1) distribution\n",
      "under Ho. Hence, the p-value is\n",
      "\n",
      "pevalue ~ P(|Z| > \\~) = 2P(Z < —|wl) = 20(Z < —|wl)\n",
      "(11.5)\n",
      "where Z ~ N(0,1) and w = (— 0)/& is the observed value of\n",
      "the test statistic.\n",
      "Here is an important property of p-values.\n",
      "\n",
      "Theorem 11.13 If the test statistic has a continuous distribu-\n",
      "tion, then under Hy : 0 = 9, the p-value has a Uniform (0,1)\n",
      "distribution.\n",
      "\n",
      "If the p-value is less than .05 then people often report that\n",
      "“the result is statistically significant at the 5 per cent level.”\n",
      "This just means that the null would be rejected if we used a size\n",
      "a = 0.05 test. In general, the size a test rejects if and only if\n",
      "p-value < a.\n",
      "\n",
      "Example 11.14 Recall the cholesterol data from Example 8.11.\n",
      "To test of the means ard different we compute\n",
      "\n",
      "6-0 X-Y _ 2162-1953\n",
      "\n",
      "Pape 38\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "min\n",
      "\n",
      "To compute the p-value, let Z ~ N(0,1) denote a standard Nor-\n",
      "mal random variable. Then,\n",
      "\n",
      "p-value = P(|Z| > 3.78) = 2P(Z < —3.78) = .0002\n",
      "\n",
      "which is very strong evidence against the null hypothesis. To\n",
      "test if the medians are different, let 7, and? denote the sample\n",
      "medians. Then,\n",
      "\n",
      "D-H 225-194 |\n",
      "\n",
      "Woe ap\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-189.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80EAB88>\n",
      "11.3 The x? distribution 189\n",
      "\n",
      "where the standard error 7.7 was found using the boostrap. The\n",
      "p-value is\n",
      "\n",
      "p-value = P(|Z| > 2.4) = 2P(Z < —2.4) = .02\n",
      "which is strong evidence against the null hypothesis.\n",
      "\n",
      "Warning! A result might be statistically significant and yet\n",
      "the size of the effect might be small. In such a case we have a\n",
      "result that is statistically significant but not practically signifi-\n",
      "cant. It is wise to report a confidence interval as well.\n",
      "\n",
      "11.3. The x? distribution\n",
      "\n",
      "Let Z,...,Z, be independent, standard normals. Let V =\n",
      "ae Z?. Then we say that V has a x? distribution with k de-\n",
      "grees of freedom, written V ~ xZ. The probability density of V\n",
      "\n",
      "1s .\n",
      "pk/2)-1e-v/2\n",
      "\n",
      "f= Saar\n",
      "for v > 0. It can be shown that E(V) =k and V(k) = 2k. We\n",
      "define the upper a quantile ew = F-'(1— a) where F is the\n",
      "CDF . That is, P(xj > Xf.) =\n",
      "\n",
      "11.4 Pearson’s x? Test For Multinomial Data\n",
      "\n",
      "Pearson’s x” test is used for multinomial data. Recall that\n",
      "X = (X,...,X;) has a multinomial distribution if\n",
      "\n",
      " \n",
      "\n",
      "n\n",
      "\n",
      ": = 1... th\n",
      "S(t1,---,K5 pP) = (7, )ei Di\n",
      "\n",
      "(..\"..)\"am\n",
      "\n",
      "The MLE of p is P= (i,.--, Px) = (Xi/\n",
      "\n",
      "where\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-190.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "190 11. Hypothesis Testing and p-values\n",
      "\n",
      "Let (poi,---,Pox) be some fixed set of probabilities and sup-\n",
      "pose we want to test\n",
      "\n",
      "Ho: (P1,---5 Pe) = (Por,---,Pox) versus Hy : (p1,---, pe) # (Dor,---sPox)-\n",
      "\n",
      "Pearson’s x? statistic is\n",
      "\n",
      "T x Xj = npo;) Pay (= 5\n",
      "jal\n",
      "\n",
      "ny\n",
      "Po; j=l J\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "where O; = X; is the observed data and E; = E(Xj) = npo; is\n",
      "the expected value of Xj under Ho.\n",
      "\n",
      "Theorem 11.15 Under Ho, T ~+ x%_,. Hence the test: reject Ho\n",
      "if T > Xi-1q has asymptotic level a. The p-value is P(x% > t)\n",
      "where t is the observed value of the test statistic.\n",
      "\n",
      "Example 11.16 (Mendel’s peas.) Mendel bred peas with round yel-\n",
      "low seeds and wrinkled green seeds. There are four types of progeny:\n",
      "round yellow, wrinkled yellow, round green and (4) wrinkled\n",
      "green. The number of each type is multinomial with probability\n",
      "(Pi, P2,P3, Ps). His theory of inheritance predicts that\n",
      "\n",
      "(2% 3 3 1\\_\n",
      "?~\\ ie 16’ 16’ 16) ~?\"\n",
      "\n",
      "In n = 556 trials he observed X = (315,101, 108,32). Since,\n",
      "npio = 312.75, npo = np3o = 104.25 and npyo = 34.75, the test\n",
      "statistic is\n",
      "» _ (315 — 312.75)? (101 — 104.25)? (108 — 104.25)? (32 — 34.75)?\n",
      "~ 312.75 Tolas. SCOOT\n",
      "The a = .05 value for a x3 is 7.815. Since 0.47 is not larger\n",
      "than 7.815 we do not reject the null. The p-value is\n",
      "\n",
      "= 0.47.\n",
      "\n",
      " \n",
      "\n",
      "p-value = P(x} > .47) =.07\n",
      "\n",
      "which is only moderate evidence againt Hy. Hence, the data do\n",
      "not contradict Mendel’s theory. Interestingly, there is some con-\n",
      "troversey about whether Mendel’s results are “too good.”\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-191.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "11.5 The Permutation Test 191\n",
      "11.5 The Permutation Test\n",
      "\n",
      "The permutation test is a nonparametric method for test-\n",
      "ing whether two distribution are the same. This test is “exact”\n",
      "meaning that it is not based on large sample theory approxima-\n",
      "tions. Suppose that X1,...,Xm~ Fy and Yi,...,Y¥; ~ Fy are\n",
      "two independent samples and Hp is the hypothesis that the two\n",
      "samples are identically distributed. This is the type of hypothe-\n",
      "sis we would consider when testing whether a treatment differs\n",
      "from a placebo. More precisely we are testing\n",
      "\n",
      "Ay: Fy =Fy versus H,: Fy #4 Fy.\n",
      "Let T(21,---,2m;Yi,---;Yn) be some test statistic, for example,\n",
      "T (Xi, Xm Yis--a¥a) = [Kn Pale\n",
      "\n",
      "Let N = m+n and consider forming all N! permutations of\n",
      "the data X1,..., Xm, Yi,---, Yn. For each permutation, compute\n",
      "the test statistic T. Denote these values by T\\,..., Ty. Under\n",
      "the null hypothesis, each of these values is equally likely. The\n",
      "distribution Pj that puts mass 1/N! on each T; is called the\n",
      "permutation distribution of T. Let to}, be the observed value\n",
      "of the test statistic. Assuming we reject when T is large, the p-\n",
      "value is\n",
      "\n",
      "N!\n",
      "\n",
      "1\n",
      "p-value = Po(T' > toys) = MI ice Stops)\n",
      "\n",
      "ja\n",
      "\n",
      "Example 11.17 Here is a toy example to make the idea clear.\n",
      "Suppose the data are: (X,, X2, Yi) = (1,9, 3). Let T(X, X2,Y) =\n",
      "[X-Y\n",
      "\n",
      " \n",
      "\n",
      "= 2. The permutations are:\n",
      "\n",
      "Under the null hy-\n",
      "pothesis, given the\n",
      "ordered data values,\n",
      "\n",
      "Xi, ++) Xm: Vis.\n",
      "\n",
      "Vn\n",
      "\n",
      "is uniformly — dis-\n",
      "\n",
      "tributed\n",
      "\n",
      "over\n",
      "\n",
      "the\n",
      "\n",
      "N! permutations of\n",
      "\n",
      "the data.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-192.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "192 11. Hypothesis Testing and p-values\n",
      "\n",
      "permutation value of T probability\n",
      "\n",
      " \n",
      "\n",
      "(2,9,3) 2 1/6\n",
      "(9,1,3) 2 1/6\n",
      "(1,3,9) 7 1/6\n",
      "(3,1,9) 7 1/6\n",
      "(3,9,1) 5 1/6\n",
      "(9,3,1) 5 1/6\n",
      "\n",
      "The p-value is P(T > 2) = 4/6.\n",
      "\n",
      "Usually, it is not practical to evaluate all N! permutations.\n",
      "We can approximate the p-value by sampling randomly from\n",
      "the set of permutations. The fraction of times Tj > t,,. among\n",
      "these samples approximates the p-value.\n",
      "\n",
      " \n",
      "\n",
      "Algorithm for Permutation Test\n",
      "\n",
      "1. Compute the observed value of the test statistic to, =\n",
      "P(X 26, Xue Yigees Ya)\n",
      "\n",
      "2. Randomly permute the data. Compute the statistic again\n",
      "using the permuted data.\n",
      "\n",
      "3. Repeat the previous step B times and let T,,...,T de-\n",
      "note the resulting values.\n",
      "\n",
      "4. The approximate p-value is\n",
      "\n",
      "B\n",
      "BoM (Tj > tobs)-\n",
      "\n",
      "_ol=\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 11.18 DNA microarrays allow researchers to measure\n",
      "the expression levels of thousands of genes. The data are the lev-\n",
      "els of messenger RNA (mRNA) of each gene, which is thought\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-193.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "11.6 Multiple Testing 193\n",
      "\n",
      "to provide a measure how much protein that gene produces.\n",
      "Roughly, the larger the number, the more active the gene. The\n",
      "table below, reproduced from Efron, et. al. (JASA, 2001, p. 1160)\n",
      "shows the expression levels for genes from two types of liver can-\n",
      "cer cells. There are 2,638 genes in this experiment but here we\n",
      "show just the first two. The data are log-ratios of the intensity\n",
      "levels of two different color dyes used on the arrays.\n",
      "\n",
      " \n",
      "\n",
      "Type I Type II\n",
      "Patient 7 2 3 4 5| 6 7 8 9 10\n",
      "Gene 1 230.0 -1,350 -1,580.0 -400 -760|970 110 -50 -190 -200\n",
      "\n",
      "Gene 2 470.0 -850 -.8 -280 120) 390 -1730 -1360\n",
      "\n",
      "Let’s test whether the median level of gene 1 is different be-\n",
      "tween the two groups. Let 1, denote the median level of gene 1\n",
      "of Type I and let v, denote the median level of gene 1 of Type II.\n",
      "The absolute difference of sample medians is T = | —V.| = 710.\n",
      "Now we estimate the permutation distribution by simulation and\n",
      "we find that the estimated p-value is .045. Thus, if we use a\n",
      "a = .05 level of significance, we would say that there is evidence\n",
      "to reject the null hypothesis of no difference. Mi\n",
      "\n",
      "In large samples, the permutation test usually gives similar\n",
      "results to a test that is based on large sample theory. The per-\n",
      "mutation test is thus most useful for small samples.\n",
      "\n",
      "11.6 Multiple Testing\n",
      "\n",
      "In some situations we may conduct many hypothesis tests. In\n",
      "example 11.18, there were actually 2,638 genes. If we tested for a\n",
      "difference for each gene, we would be conducting 2,638 separate\n",
      "hypothesis tests. Suppose each test is conducted at level a. For\n",
      "any one test, the chance of a false rejection of the null is a.\n",
      "But the chance of at least one false rejection is much higher.\n",
      "\n",
      "-.8 -330\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-194.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194 11. Hypothesis Testing and p-values\n",
      "\n",
      "This is the multiple testing problem. The problem comes up\n",
      "\n",
      "in many data mining situations where one may end up testing\n",
      "\n",
      "thousands or even millions of hypotheses. There are many ways\n",
      "\n",
      "to deal with this problem. Here we discuss two methods.\n",
      "Consider m hypothesis tests:\n",
      "\n",
      "Ao; versus Hy;, i =1,...,m\n",
      "\n",
      "and let P,,...,P,, denote m p-values for these tests.\n",
      "\n",
      " \n",
      "\n",
      "The Bonferroni Method\n",
      "\n",
      " \n",
      "\n",
      "Given p-values P;,...,Pmm, reject null hypothesis Ho; if P; < a/m.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 11.19 Using the Bonferroni method, the probability of\n",
      "falsely rejecting any null hypotheses is less than or equal to a.\n",
      "\n",
      "ProoF. Let R be the event that at least one null hypotheses\n",
      "is falsely rejected. Let R; be the event that the 7\" null hypoth-\n",
      "esis is falsely rejected. Recall that if A,,...,A, are events then\n",
      "P( Ba Aj) < whi P(Aj). Hence,\n",
      "\n",
      "m\n",
      "\n",
      "P(R) =P (Ux) < SPUR) =f =a\n",
      "\n",
      "i=l\n",
      "\n",
      "from Theorem 11.13.\n",
      "\n",
      "Example 11.20 In the gene example, using a = .05, we have\n",
      "that .05/2638 = .00001895375. Hence, for any gene with p-value\n",
      "less than .00001895375, we declare that there is a significant\n",
      "difference.\n",
      "\n",
      "The Bonferroni method is very conservative because it is try-\n",
      "ing to make it unlikely that you would make even one false\n",
      "rejection. Sometimes, a more reasonable idea is to control the\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-195.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "11.6 Multiple Testing 195\n",
      "\n",
      "false discovery rate (FDR) which is defined as the mean of the\n",
      "number of false rejections divided by the number of rejections.\n",
      "\n",
      "Suppose we reject all null hypotheses whose p-values fall be-\n",
      "low some threshold. Let mo be the number of null hypotheses\n",
      "are true and m; = m— mo null hypotheses are false. The tests\n",
      "can be categorize in a 2 x 2 as in the following table.\n",
      "\n",
      "Hy Not Rejected Hp Rejected Total\n",
      "\n",
      "Hy True U Vv mo\n",
      "Ho False T S my\n",
      "Total m—R R m\n",
      "\n",
      "Define the False Discovery Proportion (FDP)\n",
      "\n",
      "V/R ifR>0\n",
      "ERP = { 0 ifR=0.\n",
      "The FDP is the proportion of rejections that are incorrect. Next\n",
      "define FDR = E(FDP).\n",
      "\n",
      " \n",
      "\n",
      "The Benjamini-Hochberg (BH) Method\n",
      "\n",
      "1. Let Pay < +++ < Pom) denote the ordered p-values.\n",
      "\n",
      "2. Define\n",
      "\n",
      "t “and R= maxi 7 Pe = ei} (11.6)\n",
      "\n",
      " \n",
      "\n",
      "Cmm\n",
      "\n",
      "where C,,, is defined to be 1 if the p-values are independent\n",
      "and Om = 07, (1/i) otherwise.\n",
      "\n",
      "3. Let ¢ = Pim; we call t the BH rejection threshold.\n",
      "\n",
      "4. Rejects all null hypotheses Ho; for which P; < t.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-196.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "196 11. Hypothesis Testing and p-values\n",
      "\n",
      "Theorem 11.21 (Benjamini and Hochberg) If the procedure\n",
      "above is applied, then regardless of how many nulls are true\n",
      "and regardless of the distribution of the p-values when the null\n",
      "hypthesis is false,\n",
      "\n",
      "FDR =E(FDP) < wa <a.\n",
      "\n",
      "Example 11.22 Figure 11.1 shows 7 ordered p-values plotted as\n",
      "vertical lines. If we tested at level a without doing any corec-\n",
      "tion for mutiple testing, we would reject all hypotheses whose\n",
      "p-values are less than a. In this case, the 5 hypotheses corre-\n",
      "sponding to the 5 smallest p-values are rejected. The Bonferroni\n",
      "method rejects all hypotheses whose p-values are less than a/m.\n",
      "In this example, this leads to no rejections. The BH threshold\n",
      "corresponds to the last p-value that falls under the line with slope\n",
      "a. This leads to three hypotheses being rejected in this case. Wi\n",
      "\n",
      "SumMARY. The Bonferonni method controls the probability\n",
      "of a single false rejection. This is very strict and leads to low\n",
      "power when there are many tests. The FDR method controls the\n",
      "fraction of false discoveries which is a more reasonable criterion\n",
      "when there are many tests.\n",
      "\n",
      "11.7 Technical Appendix\n",
      "\n",
      "11.7.1 The Neyman-Pearson Lemma\n",
      "\n",
      "In the special case of a simple null Ho : 8 = 4 and a simple\n",
      "alternative H; : 9 = 6, we can say precisely what the most\n",
      "powerful test is.\n",
      "\n",
      "Theorem 11.23 (Neyman-Pearson.) Suppose we test Hy : 0 =\n",
      "0) versus H,: 0 =0,. Let\n",
      "L£(91) haf (aes 1)\n",
      "\n",
      "—_ _ st.\n",
      "T= £60) ~ [tas Flas Bo)\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-197.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "11.7 Technical Appendix 197\n",
      "\n",
      "p-values\n",
      "\n",
      "a/m\n",
      "\n",
      " \n",
      "\n",
      "0 reject don’t reject 1\n",
      "\n",
      "threshold\n",
      "\n",
      "FIGURE 11.1. Schematic illustration of Benjamini-Hochberg pro-\n",
      "cedure. All hypotheses corresponding to the last undercrossing are\n",
      "rejected.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-198.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "198 11. Hypothesis Testing and p-values\n",
      "\n",
      "Suppose we reject Hy when T > k. If we choose k so that\n",
      "Py,(I > k) = a then this test is the most powerful, size a\n",
      "test. That is among all tests with size a, this test maximizes the\n",
      "power 8(6).\n",
      "\n",
      "11.7.2 Power of the Wald Test\n",
      "PROOF OF THEOREM 11.6.\n",
      "Let Z ~ N(0,1). Then,\n",
      "\n",
      "Power = {(0,)\n",
      "\n",
      "Py, (Reject Ho)\n",
      "\n",
      "= Py(\\W|> 2a)\n",
      "\n",
      "O— 6\n",
      "= Po (“ | > sn]\n",
      "se\n",
      "0-8, 0-8,\n",
      "= Py ( = > an) + Po, ( — <-~san)\n",
      "se se\n",
      "\n",
      "= Po. ( > +S un) 4% (0 < % — & 20/2)\n",
      "\n",
      "6-98, > ft — 6. ef! — 86.\n",
      "= n(S * oe at un) + Po, G5 = <3 = - sa)\n",
      "\n",
      "p(z> Be boa) +P ze! * can)\n",
      "\n",
      "0\n",
      "1-0(45 + son) +0 (95 - 0) rT]\n",
      "\n",
      "11.7.3 The t-test\n",
      "\n",
      "To test Ho : 1 = {to where ji is the mean, we can use the Wald\n",
      "test. When the data are assumed to be Normal and the sample\n",
      "size is small, it is common instead to use the t-test. A random\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "z\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "variable T has a t-distribution with k degrees of freedom if it has\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-199.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "11.7 Technical Appendix 199\n",
      "\n",
      "density\n",
      "r()\n",
      "vie (9 (1+ ey\n",
      "\n",
      "When the degrees of freedom k — oo, this tends to a Normal\n",
      "distribution. When k = 1 it reduces to a Cauchy.\n",
      "\n",
      "Let X1,..., Xn ~ N(,07) where 6 = (1,07) are both un-\n",
      "known. Suppose we want to test js = jo versus ps A fio. Let\n",
      "\n",
      "f=\n",
      "\n",
      "Jri(Xn = Ho)\n",
      "f= ee\n",
      "By\n",
      "\n",
      "where $? is the sample variance. For large samples T = N(0, 1)\n",
      "under Ho. The exact distribution of T under Ho is t,_;. Hence\n",
      "if we reject when |T| > ty 1.0/2 then we get a sizea test.\n",
      "\n",
      "11.7.4 The Likelihood Ratio Test\n",
      "\n",
      "Let 0 = (01,.--,99,9q41,---,9,) and suppose that Qo consists\n",
      "of all parameter values @ such that (0,41, .--,4+) = (Oo,¢41)---+%,r)-\n",
      "\n",
      " \n",
      "\n",
      "Definition 11.24 Define the likelihood ratio statistic\n",
      "\n",
      "by\n",
      "21o SUD 9co L(8) —2lo L(8)\n",
      "A= 2le (= am) = 208 (=)\n",
      "\n",
      "where @ is the MLE and % is the MLE when 0 is restricted\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "to lie in Oo. The likelihood ratio test is: reject Ho\n",
      "when (2\") > X} ge\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "For example, if @ = (0;, 0, 03,04) and we want to test the null\n",
      "hypothesis that 0; = 6, = 0 then the limiting distribution has\n",
      "4 —2=2 degrees of freedom.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-200.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "200\n",
      "\n",
      "11. Hypothesis Testing and p-values\n",
      "\n",
      "Theorem 11.25 Under Ho,\n",
      "\n",
      "E ny 4.2\n",
      "2log A(x\") > x7_,.\n",
      "\n",
      "Hence, asymptotically, the LR test is level a.\n",
      "\n",
      "11.8 Bibliographic Remarks\n",
      "\n",
      "The most complete book on testing is (1986). See also Casella\n",
      "and Berger (1990, Chapter 8). The FDR method is due to Ben-\n",
      "jamini and Hochberg (1995).\n",
      "\n",
      "11.9 Exercises\n",
      "\n",
      "1. Prove Theorem 11.13.\n",
      "\n",
      "2. Prove Theorem 11.10.\n",
      "\n",
      "3. Let Xy,..., Xn ~ Uniform(0, 0) and let Y = max{Xj,..., X,}-\n",
      "\n",
      "We want to test:\n",
      "Ho: 9 =1/2 versus H, : 0 > 1/2.\n",
      "\n",
      "The Wald test is not appropriate since Y does not converge\n",
      "to a Normal. Suppose we decide to test this hypothesis by\n",
      "rejecting Hy when Y > c.\n",
      "\n",
      "(a) Find the power function.\n",
      "(b) What choice of ¢ will make the size of the test .05?\n",
      "\n",
      "(c) In a sample of size n = 20 with Y=0.48 what is the\n",
      "p-value? What conclusion about Ho would you make?\n",
      "\n",
      "(d) In a sample of size n = 20 with Y=0.52 what is the\n",
      "p-value? What conclusion about Ho would you make?\n",
      "\n",
      ". There is a theory that people can postpone their death\n",
      "\n",
      "until after an important event. To test the theory, Phillips\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-201.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "6.\n",
      "\n",
      "11.9 Exercises 201\n",
      "\n",
      "and King (1988) collected data on deaths around the Jew-\n",
      "ish holiday Passover. Of 1919 deaths, 922 died the week\n",
      "before the holiday and 997 died the week after. Think\n",
      "of this as a binomial and test the null hypothesis that\n",
      "6 = 1/2. Report and interpret the p-value. Also construct\n",
      "a confidence interval for 0.\n",
      "\n",
      "Reference:\n",
      "Phillips, D.P. and King, E.W. (1988).\n",
      "\n",
      "Death takes a holiday: Mortality surrounding major social\n",
      "occasions.\n",
      "\n",
      "The Lancet, 2, 728-732.\n",
      "\n",
      ". In 1861, 10 essays appeared\n",
      "\n",
      "Crescent. They were signed “\n",
      "and some people suspected t\n",
      "Mark Twain. To investigate t\n",
      "\n",
      "in the New Orleans Daily\n",
      "Quintus Curtuis Snodgrass”\n",
      "hey were actually written by\n",
      "is, we will consider the pro-\n",
      "\n",
      "portion of three letter words found in an author’s work.\n",
      "\n",
      "From eight Twain essays we\n",
      "\n",
      "-225 .262 .217 .240 .230 .229 .235 .217\n",
      "\n",
      " \n",
      "\n",
      "ave:\n",
      "\n",
      "From 10 Snodgrass essays we have:\n",
      "-209 .205 .196 .210 .202 .207 .224 .223 .220 .201\n",
      "(source: Rice xxxx)\n",
      "\n",
      "(a) Perform a Wald test for equality of the means. Use\n",
      "the nonparametric plug-in estimator. Report the p-value\n",
      "and a 95 per cent confidence interval for the difference of\n",
      "means. What do you conclude?\n",
      "\n",
      "(b) Now use a permutation test to avoid the use of large\n",
      "\n",
      "sample methods. What is your conclusion?\n",
      "Let X1,...,Xn~ N(0,1). Consider testing\n",
      "\n",
      "Ho : 0 =0 versus 6 = 1.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-202.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202 11. Hypothesis Testing and p-values\n",
      "Let the rejection region be R = {a\" : T(a\") > c} where\n",
      "Te) =a yy Xe\n",
      "(a) Find c so that the test has size a.\n",
      "(b) Find the power under Hy, i.e. find 8(1).\n",
      "(c) Show that 8(1) 4 1 as n > ov.\n",
      "\n",
      "7. Let be the MLE ofa parameter 0 and let & = {nI(9)}~'/?\n",
      "where J(0) is the Fisher information. Consider testing\n",
      "\n",
      "Hy : 0 = 6 versus 0 F 6.\n",
      "Consider the Wald test with rejection region R = {x” :\n",
      "|Z| > 2aj2} where Z = (6 — 0%)/Se. Let 0, > % be some\n",
      "alternative. Show that 3(0:) > 1.\n",
      "8. Here are the number of elderly Jewish and Chinese women\n",
      "\n",
      "who died just before and after the Chinese Harvest Moon\n",
      "Festival.\n",
      "\n",
      "Week | Chinese Jewish\n",
      "\n",
      " \n",
      "\n",
      "-2 55 141\n",
      "-1 33 145\n",
      "dL. 70 139\n",
      "2 49 161\n",
      "\n",
      "Compare the two mortality patterns.\n",
      "\n",
      "9. A randomized, double-blind experiment was conducted to\n",
      "assess the effectiveness of several drugs for reducing post-\n",
      "operative nausea. The data are as follows.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Number of Patients | Incidence of Nausea.\n",
      "Placebo 80 45\n",
      "Chlorpromazine 75 26\n",
      "Dimenhydrinate 85 52\n",
      "Pentobarbital (100 mg) 67 35\n",
      "Pentobarbital (150 mg) 85 37\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-203.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "10.\n",
      "\n",
      "11.9 Exercises 203\n",
      "\n",
      "(source: )\n",
      "\n",
      "(a) Test each drug versus the placebo at the 5 per cent\n",
      "level. Also, report the estimated odds-ratios. Summarize\n",
      "your findings.\n",
      "\n",
      "(b) Use the Bonferoni and the FDR method to adjust for\n",
      "multiple testing.\n",
      "\n",
      "Let Xj, ...,Xn ~ Poisson().\n",
      "(a) Let Ag > 0. Find the size a Wald test for\n",
      "\n",
      "Ay:X =X versus Hy: AF Ao.\n",
      "\n",
      "(b) (Computer Experiment.) Let Ay = 1, n = 20 anda =\n",
      ".05. Simulate X,,...,X, ~ Poisson(A9) and perform the\n",
      "Wald test. Repeat many times and count how often you\n",
      "reject the null. How close is the type I error rate to .05?\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-204.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "204 11. Hypothesis Testing and p-values\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-205.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "ib\n",
      "\n",
      "Bayesian Inference\n",
      "\n",
      "12.1 The Bay esian Philosophy\n",
      "\n",
      "The statistical theory and methods that we have discussed\n",
      "so far are known as frequentist (or classical) inference. The\n",
      "frequentist point of view is based on the following postulates:\n",
      "\n",
      "(F1) Probability refers to limiting relative frequencies. Proba-\n",
      "bilities are objective properties of the real world.\n",
      "\n",
      "(F2) P arametersare fixed, (usually unknown) constants. Be-\n",
      "cause they are not fluctuating, no probability statements can\n",
      "be made about parameters.\n",
      "\n",
      "(F3) Statistical procedures should be designed to have well de-\n",
      "fined long run frequency properties. F or example, a 95 per cei\n",
      "confidence in terwal should trap the true value of the parameter\n",
      "with limiting frequency at least 95per cent.\n",
      "\n",
      "There is another approach to inference called Bay esian in-\n",
      "ference. The Bay esianapproach is based on the following pos-\n",
      "tulates:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-206.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "206 12. Bayesian Inference\n",
      "\n",
      "(B1) Probability describes degree of belief, not limiting fre-\n",
      "quency. As such, we can make probability statements about lots\n",
      "of things, not just data which are subject to random variation.\n",
      "For example, I might say that ‘the probability that Albert Ein-\n",
      "stein drank a cup of tea on August 1 1948” is .35. This does not\n",
      "refer to any limiting frequency. It reflects my strength of belief\n",
      "that the proposition is true.\n",
      "\n",
      "(B2) We can make probability statements about parameters,\n",
      "even though they are fixed constants.\n",
      "\n",
      "(B3) We make inferences about a parameter 0, by producing\n",
      "a probability distribution for 6. Inferences, such as point esti-\n",
      "mates and interval estimates may then be extracted from this\n",
      "distribution.\n",
      "\n",
      "Bayesian inference is a controversial approach because it in-\n",
      "herently embraces a subjective notion of probability. In general,\n",
      "Bayesian methods provide no guarantees on long run perfor-\n",
      "mance. The field of Statistics puts more emphasis on frequentist\n",
      "methods although Bayesian methods certainly have a presence.\n",
      "Certain data mining and machine learning communuties seem to\n",
      "embrace Bayesian methods very strongly. Let’s put aside philo-\n",
      "sophical arguments for now and see how Bayesian inference is\n",
      "done. We'll conclude this chapter with some discussion on the\n",
      "strengths and weaknesses of each approach.\n",
      "\n",
      "12.2 The Bayesian Method\n",
      "\n",
      "Bayesian inference is usually carried out in the following way.\n",
      "1. We choose a probability density f(#) - called the prior\n",
      "distribution — that expresses our degrees of beliefs about\n",
      "\n",
      " \n",
      "\n",
      "a parameter @ before we see any data.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-207.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "12.2 The Bayesian Method 207\n",
      "\n",
      "2. We choose a statistical model f(2|@) that reflects our be-\n",
      "liefs about x given 6. Notice that we now write this as\n",
      "f (|) instead of f (230).\n",
      "\n",
      "3. After observing data X,,...,X,, we update our beliefs\n",
      "and form the posterior distribution f(0|X1,...,Xn)-\n",
      "\n",
      "To see how the third step is carried out, first, suppose that 6 is\n",
      "discrete and that there is a single, discrete observation XY. We\n",
      "should use a capital letter now to denote the parameter since\n",
      "\n",
      "we are treating it like a random variable so let O denote the\n",
      "parameter. Now, in this discrete setting,\n",
      "\n",
      "P(X =2,0 =0) P(X =2|0 =0)P(O0 =80)\n",
      "\n",
      "20 08s\") = Fire a) ~ 35, P(X =2]0 =0)P(O=9)\n",
      "\n",
      " \n",
      "\n",
      "which you may recognize from earlier in the course as Bayes’\n",
      "\n",
      " \n",
      "\n",
      "theorem. The version for continuous variables is obtained by\n",
      "using density functions:\n",
      "\n",
      "f(2l0) £0)\n",
      "F(0|2) = LO) _ (121)\n",
      "= Te) Fa\n",
      "If we have n IID observations X),...,X,, we replace f(2|@)\n",
      "with f(r,-..,%,|0) = TT}, f(#i|@). Let us write X” to mean\n",
      "(Xq,...,X,) and 2” to mean (21,...,2,). Then\n",
      "f(2\" |) F(8) L£n(8)£(9)\n",
      "\n",
      "Me) = Tremp gaae ~ Tele) F(aaa * oO\n",
      "(12.2)\n",
      "\n",
      "In the right hand side of the last equation, we threw away the\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "denominator [ £,(0)f(@)d0 which is a constant that does not\n",
      "depend on 0; we call this quantity the normalizing constant.\n",
      "We can summarize all this by writing:\n",
      "\n",
      "“posterior is proportional to likelihood times prior.” (12.3)\n",
      "\n",
      "You might wonder, doesn’t it cause a problem to throw away\n",
      "the constant f £,,(0)f(0)d0? The answer is that we can always\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-208.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "208 12. Bayesian Inference\n",
      "\n",
      "recover the constant is since we know that f f(0|x\")d0 = 1.\n",
      "Hence, we often omit the constant until we really need it.\n",
      "What do we do with the posterior? First, we can get a point\n",
      "estimate by summarizing the center of the posterior. Typically,\n",
      "we use the mean or mode of the posterior. The posterior mean\n",
      "\n",
      "7 wy £ PLn(8)F (8)\n",
      "I, = [ ortoje \\d8 = a Ohas\n",
      "\n",
      "We can also obtain a Bayesian interval estimate. Define a and b\n",
      "by [°F (O|a\")do = f° f(0|2\")dd = a/2. Let C = (a,b). Then\n",
      "\n",
      "(12.4)\n",
      "\n",
      "PO €C|z”) = [soa =l-a\n",
      "\n",
      "so C is a 1 —a posterior interval.\n",
      "\n",
      "Example 12.1 Let X,,...,X, ~ Bernoulli(p). Suppose we take\n",
      "the uniform distribution f (p) = 1 as a prior. By Bayes’ theorem\n",
      "the posterior has the form\n",
      "\n",
      "F(p|2\")  f(e)La(p) = p'(L =p)\" = ph = pyr\n",
      "\n",
      "where 8 = )),2; is the number of heads. Recall that random\n",
      "variable has a Beta distribution with parameters a and 6 if its\n",
      "density is\n",
      "\n",
      "T+) 44\n",
      "\n",
      "Toray a2\n",
      "\n",
      "f(p; a, 8) =\n",
      "\n",
      "We see that the posterior for p is a Beta distribution with pa-\n",
      "rameters s +1 andn—s +1. That is,\n",
      "\n",
      "n) _ T(n +2) Ghhid: Vine fij2\n",
      "fle\") = Foy prmss aye fe Ce) nae\n",
      "\n",
      "We write this as\n",
      "\n",
      "pia” ~ Beta(s +1,n—s +1).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-209.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "12.2 The Bayesian Method 209\n",
      "\n",
      "Notice that we have figured out the normalizing constant without\n",
      "actually doing the integral f Ln(p)f(p)dp. The mean of a Beta\n",
      "(a, 8) is a/(a + B) so the Bayes estimator is\n",
      "\n",
      "— sti\n",
      "Pena?\n",
      "It is instructive to rewrite the estimator as\n",
      "\n",
      "B=Anpt (1 —An)p\n",
      "\n",
      "where p = s/n is the mle, p =1/2 is the prior mean and \\,, =\n",
      "n/(n +2) 1. A 95 per cent posterior interval can be obtained\n",
      "by numerically finding a and b such that SL? Flp|z”) dp = 95.\n",
      "\n",
      "Suppose that instead of a uniform prior, we use the prior p ~\n",
      "Beta(a, 3). If you repeat the calculations above, you will see that\n",
      "pia” ~ Beta(a +s,8+n—s). The flat prior is just the special\n",
      "case with a = 8 =1. The posterior mean is\n",
      "\n",
      "a+s n a4 a+,\n",
      "at+B+n \\atB+n)”' \\atp+n)™\n",
      "\n",
      "where po = a/(a + 8) is the prior mean.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "In the previous example, the prior was a Beta distribution\n",
      "and the posterior was a Beta distribution. When the prior and\n",
      "the posterior are in the same family, we say that the prior is\n",
      "conjugate.\n",
      "\n",
      "Example 12.2 Let Xi,...,Xn ~ N(0,0°). For simplicity, let us\n",
      "assume that o is known. Suppose we take as a prior 0 ~ N(a, b?).\n",
      "In problem 1 in the homework, it is shown that the posterior for\n",
      "0 is\n",
      "\n",
      "O|X\" ~ N(a,b’) (12.5)\n",
      "\n",
      "where\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-210.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 12. Bayesian Inference\n",
      "\n",
      " \n",
      "\n",
      "where A\n",
      "oy i eel\n",
      "w=>z* 7 and G=atG\n",
      "wtRP T se 6\n",
      "\n",
      "and se = o/,/n is the standard error of the mle X. This is\n",
      "another example of a conjugate prior. Note that w — 1 and\n",
      "t/se +1 asn — oo. So, for large n, the posterior is approx-\n",
      "imately NO, se”). The same is true if n is fixed but b + x,\n",
      "which corresponds to letting the prior become very flat.\n",
      "\n",
      "Continuing with this example, let is find C = (c,d) such that\n",
      "Pr(9 € C|X\") = .95. We can do this by choosing c such that\n",
      "Pr(@<¢|X\") = .025 and Pr(@ > d|X\") = 025. So, we want to\n",
      "find c such that\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "0-0 c-#O\n",
      "PW) <clX\") = P( «é |»)\n",
      "T T\n",
      "\n",
      "=P (z < —) = 025.\n",
      "T\n",
      "\n",
      "Now, we know that P(Z < —1.96) = .025. So\n",
      "\n",
      "c-@\n",
      "= —1.96\n",
      "\n",
      " \n",
      "\n",
      "implying that c = @—1.967. By similar arguments, d = 0 +1.96.\n",
      "So a 95 per cent Bayesian interval is 0 +1967. Since 0 = a\n",
      "and tT & se, the 95 per cent Bayesian interval is approximated\n",
      "by 941.96 se which is the frequentist confidence interval. Mi\n",
      "\n",
      " \n",
      "\n",
      "12.3 Functions of Parameters\n",
      "\n",
      "How do we make inferences about a function T = (0)? Re\n",
      "member in Chapter 3 we solved the following problem: given\n",
      "the density fx for X, find the density for Y = g(X). We now\n",
      "simply apply the same reasoning. The posterior CDF for T is\n",
      "\n",
      "H(rla\") = P(al®) <7) = ff s(ele\")a0\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-211.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "12.4 Simulation 211\n",
      "\n",
      "where A = {@: g(@) <7}. The posterior density is h(r|2\") =\n",
      "H'(r|x\").\n",
      "\n",
      "Example 12.3 Let Xy,...,X, ~ Bernoulli(p) and f(p) = 1 s\n",
      "that p|X\" ~ Beta(s +1,n—s8 +1) with s = SO, 2;. Let y =\n",
      "log(p/(1— p)). Then\n",
      "\n",
      ")\n",
      "\n",
      "3\n",
      "\n",
      " \n",
      "\n",
      "P(W < pa\") =P (108 (+) <p\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "ev\n",
      "=P (? < )\n",
      "l1+e?\n",
      "e¥ /(L+e)\n",
      "= [sole\n",
      "0\n",
      "\n",
      "P(n +2) et Ate)\n",
      "_ fy ay\n",
      "rates | Pi — py\" dp\n",
      "\n",
      "and\n",
      "A(d|a\") = H'(W|2\")\n",
      "= acca (os) (rin) (OO\n",
      "\n",
      "I(s+1T(n—s+1) \\l+er¥ 1+e¥\n",
      "\n",
      "- raroreon (ree) (as) (Ge)\n",
      "\n",
      "i\n",
      "_ T(n +2) aye 1 Vee\n",
      "~ T(s+)Dr(n—s4+l) \\i+ee 1+e\"\n",
      "\n",
      "forvEeRE\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "12.4 Simulation\n",
      "\n",
      "The posterior can often be approximated by simulation. Sup-\n",
      "pose we draw 0;,...,93 ~ p(6|2\"). Then a histogram of 01, ...,45\n",
      "approximates the posterior density p(@|2”). An approximation\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-212.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "212 12. Bayesian Inference\n",
      "\n",
      "to the posterior mean 0, = E(6|x”) is B-! Ya 0;. The poste-\n",
      "rior 1 — a interval can be approximated by (0/2, 41—-a/2) where\n",
      "9,/2 is the a/2 sample quantile of (,...,9p-\n",
      "\n",
      "Once we have a sample 6;,...,9, from f(0|2\"), let 7; = 9(0;).\n",
      "Then 7,...,7, is a sample from f(r|x\"). This avoids the need\n",
      "to do any analytical calculations. Simulation is discussed in more\n",
      "detail later in the book.\n",
      "\n",
      "Example 12.4 Consider again Example 12.8. We can approxi-\n",
      "mate the posterior for y without doing any calculus. Here are\n",
      "the steps:\n",
      "\n",
      "1. Draw P,,...,Pg ~ Beta(s+1,n—s +1).\n",
      "2. Let pj =log(P;)/(1— P,)) fori =1,...,B.\n",
      "\n",
      "Now ,---,Up are ID draws from h(w|2\"). A histogram of\n",
      "these values provides an estimate of h(w|2\"). Ml\n",
      "\n",
      "   \n",
      "\n",
      "12.5 Large Sample Properties of Bayes’\n",
      "Procedures.\n",
      "\n",
      "In the Bernoulli and Normal examples we saw that the pos-\n",
      "terior mean was close to the MLE . This is true in greater gen-\n",
      "erality.\n",
      "\n",
      "Theorem 12.5 Under appropriate regularity conditions, we have\n",
      "that the posterior is approximately NO, &”) where 6, is the\n",
      "MLE and & =1/\\/nI(0,). Hence, 0, © 0,. Also, if C = (On —\n",
      "2a [288 , On +208) is the asymptotic frequentist 1—a confidence\n",
      "interval, then C,, is also an approtimate 1—a Bayesian posterior\n",
      "interval:\n",
      "\n",
      "P(9EC|X\") > 1-a.\n",
      "There is also a Bayesian delta method. Let r = g(@). Then\n",
      "\n",
      "T|X\" = N(F, 5€”)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-213.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "12.6 Flat Priors, Improper Priors and “Noninformative” Priors. 213\n",
      "\n",
      "where 7 = g(@) and se = se |9'(8)|.\n",
      "\n",
      "12.6 Flat Priors, Improper Priors and\n",
      "“Noninformative” Priors.\n",
      "\n",
      "A big question in Bayesian inference is: where do you get\n",
      "the prior f(@)? One school of thought, called “subjectivism”\n",
      "says that the prior should reflect our subjective opinion about\n",
      "@ before the data are collected. This may be possible in some\n",
      "cases but seems impractical in complicated problems especially\n",
      "if there are many parameters. An alternative is to try to define\n",
      "some sort of “noninformative prior.” An obvious candidate for\n",
      "a noninformative prior is to use a “flat” prior f(@) oc constant.\n",
      "\n",
      "In the Bernoulli example, taking f(p) = 1 leads to p|X” ~\n",
      "Beta(s +1,n—s +1) as we saw earlier which seemed very rea-\n",
      "sonable. But unfettered use of flat priors raises some questions.\n",
      "\n",
      "IMPROPER PRIORS. Consider the N(0,1) example. Suppose\n",
      "we adopt a flat prior f() «x c¢ where c > 0 is a constant. Note\n",
      "that f f(0)d0 = oo so this is not a real probability density\n",
      "in the usual sense. We call such a prior an improper prior.\n",
      "Nonetheless, we can still carry out Bayes’ theorem and compute\n",
      "the posterior density f(@) « L,(0)f (0) « L£,(0). In the nor-\n",
      "mal example, this gives 0|X\" ~ N(X,o?/n) and the resulting\n",
      "point and interval estimators agree exactly with their frequen-\n",
      "tist counterparts. In general, improper priors are not a problem\n",
      "as long as the resulting posterior is a well defined probability\n",
      "distribution.\n",
      "\n",
      "FLAT PRIORS ARE Not INVARIANT. Go back to the Bernoulli\n",
      "example and consider using the flat prior f(p) = 1. Recall that\n",
      "a flat prior presumably represents our lack of information about\n",
      "p before the experiment. Now let = log(p/(1 — p)). This is a\n",
      "transformation and we can compute the resulting distribution\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-214.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "214 12. Bayesian Inference\n",
      "\n",
      "for w. It turns out that:\n",
      "\n",
      "ev\n",
      "¢%) = ——:.\n",
      "fol) = oa\n",
      "But one could argue that if we are ignorant about p then we are\n",
      "also ignorant about ¢ so shouldn’t we use a flat prior for 7)? This\n",
      "contradicts the prior fy(w) for that is implied by using a flat\n",
      "prior for p. In short, the notion ofa flat prior is not well-defined\n",
      "because a flat prior on a parameter does not imply a flat prior\n",
      "on a transformed version of the parameter. Flat priors are not\n",
      "transformation invariant.\n",
      "\n",
      "JEFFREYS’ PRIOR. Jeffreys came up with a “rule” for cre-\n",
      "ating priors. The rule is: take f (0) oc I(@)'/? where I(0) is the\n",
      "Fisher information function. This rule turns out to be transfor-\n",
      "mation invariant. There are various reasons for thinking that\n",
      "this prior might be a useful prior but we will not go into details\n",
      "here.\n",
      "\n",
      "Example 12.6 Consider the Bernoulli (p). Recall that\n",
      "\n",
      "at\n",
      "p—p)\n",
      "Jeffrey’s rule says to use the prior\n",
      "\n",
      "f(p) « VT(p) = p7'? (1 =p).\n",
      "\n",
      "This is a Beta (1/2,1/2) density. This is very close to a uniform\n",
      "density.\n",
      "\n",
      "I(p) =\n",
      "\n",
      "In a multiparameter problem, the Jeffreys’ prior is defined to\n",
      "be f(0) x \\/detI(@) where det(A) denotes the determinant of\n",
      "a matrix A.\n",
      "\n",
      "12.7 Multiparameter Problems\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-215.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "12.7 Multiparameter Problems 215\n",
      "\n",
      "In principle, multiparameter problems are handled the same\n",
      "way. Suppose that @ = (6;,...,4,). The posterior density is still\n",
      "given by\n",
      "\n",
      "(Olax) x Ln(9)F (4).\n",
      "The question now arises of how to extract inferences about one\n",
      "parameter. The key is find the marginal posterior density for\n",
      "the parameter of interest. Suppose we want to make inferences\n",
      "about 0;. The marginal posterior for 0; is\n",
      "\n",
      "f(6: |x”) = foe fH. eet).\n",
      "\n",
      "In practice, it might not be feasible to do this integral. Simula-\n",
      "tion can help. Draw randomly from the posterior:\n",
      "\n",
      "O',...,0% ~ f(O|x\")\n",
      "\n",
      "where the superscripts index the different draws. Each @” is a\n",
      "vector 67 = (6},...,03). Now collect together the first compo-\n",
      "nent of each draw:\n",
      "\n",
      "0... 08.\n",
      "These are a sample from f(6,|2\") and we have avoided doing\n",
      "any integrals.\n",
      "Example 12.7 (Comparing two binomials.) Suppose we have n, con-\n",
      "trol patients and nz treatment patients and that X, control pa-\n",
      "tients survive while Xj treatment patients survive. We want to\n",
      "estimate T = g(pi,p2) = p2— pi. Then,\n",
      "\n",
      "X,~ Binomial(ni,pi) and X2 ~ Binomial(na, p2).\n",
      "\n",
      "Suppose we take f(pi,p2) =1. The posterior is\n",
      "\n",
      "ny—21 to\n",
      "\n",
      "J (pi, p2|e1, 22) % py\" (1 — pi)\" \"p37 (1 — pe\n",
      "\n",
      "jpaoe\n",
      "\n",
      "Notice that (p,,p2) live on a rectangle (a square, actually) and\n",
      "that\n",
      "\n",
      "J (pi, p2|1, %2) = f (pilei) f (p2|x2)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-216.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 12. Bayesian Inference\n",
      "\n",
      "where\n",
      "\n",
      "f (piles) x py = pi)\" and f (pele) x py? (L = pe)\"\n",
      "which implies that py and pz are independent under the pos-\n",
      "terior. Also, pi|a, ~ Beta(a, + 1,m — 2 +1) and polar ~\n",
      "Beta(a2.+1, n2—224+1). If we simulate P,1,..., Pi,g ~ Beta(xy+\n",
      "1,m—2,4+1) and P2,,..., Po,3 ~ Beta(22+1,n2—22 41) then\n",
      "7) = Poy — Pip, b=1,...,B, is a sample from f(r\\r1, 22).\n",
      "\n",
      " \n",
      "\n",
      "12.8 Strengths and Weaknesses of Bayesian\n",
      "Inference\n",
      "\n",
      "Bayesian inference is appealing when prior information is avail-\n",
      "able since Bayes’ theorem is a natural way to combine prior in-\n",
      "formation with data. Some people find Bayesian inference psy-\n",
      "chologically appealing because it allows us to make probability\n",
      "statements about parameters. In contrast, frequentist inference\n",
      "provides confidence sets C, which trap the parameter 95 per\n",
      "cent of the time, but we cannot say that P(@ € C,,|X\") is .95.\n",
      "In the frequentist approach we can make probability statements\n",
      "\n",
      " \n",
      "\n",
      "about C,, not 6. However, psychological appeal is not really an\n",
      "argument for using one type of inference over another.\n",
      "\n",
      "In parametric models, with large samples, Bayesian and fre-\n",
      "quentist methods give approximately the same inferences. In\n",
      "general, they need not agree. Consider the following example.\n",
      "\n",
      "Example 12.8 Let X ~ N(6,1) and suppose we use the prior\n",
      "6 ~ N(0,7?). From (12.5), the posterior is\n",
      "£ 1\n",
      "Ola ~N (— a) = N (ez, c)\n",
      "\n",
      "where c = 17/(7? +1). A 1— a per cent posterior interval is\n",
      "C = (a,b) where\n",
      "\n",
      "a=cxr— VeZa/2 and b=cxr+ Vezap2-\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-217.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "12.9 Appendix 217\n",
      "\n",
      "Thus, P(@ € C|X) =1—a. We can now ask, from a frequntist\n",
      "perspective, what is the coverage of C, that is, how often will\n",
      "this interval contain the true value? The answer is\n",
      "\n",
      " \n",
      "\n",
      "Py(a<@0<b) = Po(cX — ZajaVe <0 <cX + Za/2V0)\n",
      "_ py (ev cg <a)\n",
      "fa é\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "- % (“a - Be Fapve og 2@l4 to)\n",
      "_ 9 (“ =o) 4 Zane) 6 (“ =9 = suv\")\n",
      "\n",
      "where Z ~ N(0,1). Figure 12.1 shows the coverage as a function\n",
      "of 0 fort =1 anda = 05. Unless the true value of 6 is close to\n",
      "0, the coverage can be very small. Thus, upon repeated use, the\n",
      "Bayesian 95 per cent interval might contain the true value with\n",
      "frequency near 0! In contrast, a confidence interval has coverage\n",
      "95 per cent coverage no matter what the true value of 6 is.\n",
      "\n",
      "What should we conclude from all this? The important thing\n",
      "is to understand that frequentist and Bayesian methods are an-\n",
      "swering different questions. To combine prior beliefs with data\n",
      "in a principled way, use bayesian inference. To construct proce-\n",
      "dures with guaranteed long run performance, such as confidence\n",
      "intervals, use frequentist methods. It is worth remarking that it\n",
      "is possible to develop nonparametric Bayesian methods similar\n",
      "to plug-in estimation and the bootstrap. be forewarned, how-\n",
      "ever, that the frequency properties of nonparametric Bayesian\n",
      "methods can sometimes be quite poor.\n",
      "\n",
      "12.9 Appendix\n",
      "\n",
      "Proof of Theorem 12.5.\n",
      "It can be shown that the effect of the prior diminishes as\n",
      "n increases so that f(0|X\") « Lnr(O)f(@) = Ln(0). Hence,\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-218.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80EDF48>\n",
      "218 12. Bayesian Inference\n",
      "\n",
      " \n",
      "\n",
      "08\n",
      "\n",
      "coverage\n",
      "\n",
      "04\n",
      "\n",
      "02\n",
      "L\n",
      "\n",
      " \n",
      "\n",
      "0.0\n",
      "L\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "FIGURE 12.1. Frequentist coverage of 95 per cent Bayesian posterior\n",
      "interval as a function of the true value 6. The dotted line marks the\n",
      "95 per cent level.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-219.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "12.10 Bibliographic Remarks 219\n",
      "\n",
      "log f (|X) = £(8). Now, (8) & £(6) + (8 = 8)e(8) + [(0 —\n",
      "0)? /2\\0\"(8) = €(0) + [(0 — 0)?/2)¢\"(0) since ¢'(0) = 0. Exponen-\n",
      "tiating, we get approximately that\n",
      "\n",
      "(0X) oc exp {4 =|\n",
      "\n",
      "on\n",
      "\n",
      " \n",
      "\n",
      "where 02 = — 1/0\", ,)- So the posterion of @ is approximately\n",
      "Normal with mean @ and variance o?. Let ¢; = log f (X;|0), then\n",
      "\n",
      "oy? exe 6G) = Y- G00)\n",
      "\n",
      "Bs WE 0\", n) Ey | — €1(6,)]\n",
      "@,)\n",
      "\n",
      "and hence 0, © sc(0). i\n",
      "\n",
      "12.10 Bibliographic Remarks\n",
      "\n",
      "Some references on Bayesian inference include Carlin and\n",
      "Louis (1996), Gelman, Carlin, Stern and Rubin (1995), Lee\n",
      "(1997), Robert (1994) and Schervish (1995). See Cox (1997), Di-\n",
      "aconis and Freedman (1996), Freedman (2001), Barron, Schervish\n",
      "and Wasserman (1999), Ghosal, Ghosh and van der Vaart (2001),\n",
      "Shen and Wasserman (2001) and Zhao (2001) for discussions of\n",
      "some of the technicalities of nonparametric Bayesian inference.\n",
      "\n",
      "12.11 Exercises\n",
      "1. Verify (12.5).\n",
      "\n",
      "2. Let X,,...,X, Normal(j,1). (a) Simulate a data set (using\n",
      "j= 5) consisting of n=100 observations.\n",
      "\n",
      "(b) Take f(s.) = 1 and find the posterior density. Plot the\n",
      "density.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-220.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "220\n",
      "\n",
      "12. Bayesian Inference\n",
      "\n",
      "(c) Simulate 1000 draws from the posterior. Plot a his-\n",
      "togram of the simulated values and compare the histogram\n",
      "to the answer in (b).\n",
      "\n",
      "(d) Let @ = e“. Find the posterior density for @ analytically\n",
      "and by simulation.\n",
      "\n",
      "(e) Find a 95 per cent posterior interval for 0.\n",
      "\n",
      "(f) Find a 95 per cent confidence interval for 0.\n",
      "\n",
      ". Let X1,...,X, Uniform(0,9). Let f(@) « 1/0. Find the\n",
      "\n",
      "posterior density.\n",
      "\n",
      ". Suppose that 50 people are given a placebo and 50 are\n",
      "\n",
      "given a new treatment. 30 placebo patients show improve-\n",
      "ment while 40 treated patients show improvement. Let\n",
      "T =p2—p where py is the probability of improving under\n",
      "treatment and p, is the probability of improving under\n",
      "placebo.\n",
      "\n",
      "(a) Find the mle of 7. Find the standard error and 90 per\n",
      "cent confidence interval using the delta method.\n",
      "\n",
      " \n",
      "\n",
      "(b) Find the standard error and 90 per cent confidence\n",
      "interval using the parametric bootstrap.\n",
      "\n",
      "(c) Use the prior f(p1,p2) = 1. Use simulation to find the\n",
      "posterior mean and posterior 90 per cent interval for 7.\n",
      "\n",
      "(d) Let\n",
      "Y = log (4) ~ (=)\n",
      "\n",
      "be the log-odds ratio. Note that q = 0 if p, = po. Find\n",
      "the MLE of w. Use the delta method to find a 90 per cent:\n",
      "confidence interval for 4).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "(e) Use simulation to find the posterior mean and posterior\n",
      "90 per cent interval for y.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-221.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "12.11 Exercises 221\n",
      "\n",
      "5. Consider the Bernoulli(p) observations\n",
      "0101000000\n",
      "Plot the posterior for p using these priors: Beta(1/2,1/2),\n",
      "Beta(1,1), Beta(10,10), Beta(100,100).\n",
      "\n",
      "6. Let X),...,X, ~ Poisson(,).\n",
      "\n",
      "(a) Let A ~ Gamma(a, 8) be the prior. Show that the\n",
      "posterior is also a Gamma. Find the posterior mean.\n",
      "\n",
      "(b) Find the Jeffreys’ prior. Find the posterior.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-222.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-223.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "13\n",
      "\n",
      "Statistical Decision Theory\n",
      "\n",
      "13.1 Preliminaries\n",
      "\n",
      "We have considered several point estimators such as the max-\n",
      "imum likelihood estimator, the method of moments estimator\n",
      "and the posterior mean. In fact, there are many other ways to\n",
      "generate estimators. How do we choose among them? The an-\n",
      "swer is found in decision theory which is a formal theory for\n",
      "comparing statistical procedures.\n",
      "\n",
      "Consider a parameter 6 which lives in a parameter space O.\n",
      "Let 0 be an estimator of 0. In the language of decision theory, a\n",
      "estimator is sometimes called a decision rule and the possible\n",
      "values of the decision rule are called actions.\n",
      "\n",
      "We shall measure the discrepancy between 0 and @ using a\n",
      "loss function L(6,8). Formally, L maps © x © into R. Here\n",
      "\n",
      "This is page 227\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-224.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "228 13. Statistical Decision Theory\n",
      "\n",
      "are some examples of loss functions:\n",
      "\n",
      "L(6,0) = (9 — 0)? squared error loss,\n",
      "L(0,0) = |@— | absolute error loss,\n",
      "= |0—9/P L, loss,\n",
      "\n",
      "=0if 0=@and1if 0 # 9  zero-one loss,\n",
      "\n",
      "= Slog (422) f(a; 9)dx  Kullback-Leibler loss.\n",
      "\n",
      "w\n",
      "aes\n",
      "B) DS) BS) BS) B)\n",
      "\n",
      "Bear in mind in what follows that an estimator @ is a function\n",
      "of the data. To emphasize this point, sometimes we will write 6\n",
      "\n",
      "as 0(X). To assess an estimator, we evaluate the average loss or\n",
      "tisk.\n",
      "\n",
      " \n",
      "\n",
      "Definition 13.1 The risk of an estimator 0 is\n",
      "\n",
      "R(0,0) = Ey (z(0,8)) = [ 400.60) 104;0)ae.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "When the loss function is squared error, the risk is just the\n",
      "MSE (mean squared error):\n",
      "R(0,0) = Ey(0— 6)? = sz = Vp(6) + bias3().\n",
      "In the rest of the chapter, if we do not state what loss func-\n",
      "\n",
      "tion we are using, assume the loss function is squared\n",
      "error.\n",
      "\n",
      "13.2 Comparing Risk Functions\n",
      "\n",
      "To compare two estimators we can compare their risk func-\n",
      "tions. However, this does not provide a clear answer as to which\n",
      "estimator is better. Consider the following examples.\n",
      "\n",
      "Example 13.2 Let X ~ N(0,1) and assume we are using squared\n",
      "error loss. Consider two estimators: 0; = X and 6, = 3. The\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-225.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.2 Comparing Risk Functions. 229\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "0 1 2 3 4 5\n",
      "FIGURE 13.1. Comparing two risk functions. Neither dominates the\n",
      "other at all values of 6.\n",
      "\n",
      "risk functions are R(0,0,) = Ey(X — 0)? = 1 and R(0,0,) =\n",
      "E,(3— 6)? = (3 — 6)?. Notice that, if 2 <6 <4 then R(0, 02) <\n",
      "R(0,0;) otherwise R(0,0,) < R(0,0,). Neither estimator uni-\n",
      "formly dominates the other; see Figure 13.1. Obviously 0 isa\n",
      "ridiculous estimator but it serves to illustrate the point that it\n",
      "is not obvious how to compare two risk functions.\n",
      "\n",
      "Example 13.3 Let X,...,Xn ~ Bernoulli(p). Consider squared\n",
      "error loss and let p; = X. Since this has 0 bias, we have that\n",
      "\n",
      "- pa-p\n",
      "Rep,fi) = VX) = PAP).\n",
      "Another estimator is\n",
      "wx Yo\n",
      "ee ot Bon\n",
      "where Y = Dn X; anda and £ are positive constants. This is\n",
      "the posterior mean using a Beta (a, 8) prior. Now,\n",
      "\n",
      "R(p,P) = Vp(B2) + (bias p(P2))” ; ;\n",
      "v. (Seas) * ( Geren) ?)\n",
      "np(1 — p) +( np +a -»).\n",
      "\n",
      "f@+Bent \\atBan\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-226.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "230 13. Statistical Decision Theory\n",
      "\n",
      " \n",
      "\n",
      "RO\n",
      "\n",
      "Risk\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "FIGURE 13.2. Risk functions for p, and 2 in Example 13.3.\n",
      "\n",
      "Now leta = 8= y/n/4. (In Example 13.13 we will explain this\n",
      "choice.) The resulting estimator is\n",
      "\n",
      "n+/J/n\n",
      "and risk function is\n",
      "A n\n",
      "R(p, Po) = Ta+ Vane\n",
      "\n",
      "The risk functions are plotted in figure 13.2. As we can see,\n",
      "neither estimator uniformly dominates the other.\n",
      "\n",
      "These examples highlight the need to be able to compare risk\n",
      "functions. To do so, we need a one-number summary of the risk\n",
      "\n",
      "function. Two such summaries are the maximum risk and the\n",
      "Bayes risk.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-227.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "13.2 Comparing Risk Functions. 231\n",
      "\n",
      " \n",
      "\n",
      "Definition 13.4 The maximum risk is\n",
      "\n",
      "R(9) = sup R(8, 0) (13.1)\n",
      "0\n",
      "and the Bayes risk is\n",
      "r(n,8) = / R(6, dyn (0)d8 (13.2)\n",
      "\n",
      "where (0) is a prior for 0.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 13.5 Consider again the two estimators in Example 13.3.\n",
      "We have\n",
      "\n",
      "a pi-p)_ 1\n",
      "RU) = BK = an\n",
      "and\n",
      "Rf) = max ——__ = —_\" __\n",
      "\n",
      "p A(n+ Jn)? 4(n+ Vn)?”\n",
      "Based on maximum risk, Py is a better estimator since R(p) <\n",
      "R(fi). However, when n is large, R(P,) has smaller risk except\n",
      "for a small region in the parameter space near p = 1/2. Thus,\n",
      "many people prefer p, to p2. This illustrates that one-number\n",
      "summaries like maximum risk are imperfect. Now consider the\n",
      "Bayes risk. For illustration, let us take 7(p) = 1. Then\n",
      "\n",
      "(fi) = [ ®@.5)0= [2 o- i.\n",
      "\n",
      "and\n",
      "n\n",
      "\n",
      "Po) = | R(p,p2)dp = ——— ,.\n",
      "\n",
      "v(m, D2) | (p, p2)dp Tn+ vnp\n",
      "For n > 20, r(a,p2) > (7, pi) which suggests that p, is a better\n",
      "estimator. This might seem intuitively reasonable but this an-\n",
      "swer depends on the choice of prior. The advtantage of using\n",
      "mazimum risk, despite its problems, is that it does not require\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-228.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "232 13. Statistical Decision Theory\n",
      "\n",
      "one to choose a prior. In high-dimensional, complex problems,\n",
      "choosing a defensible prior can be extremely difficult. Ml\n",
      "\n",
      "These two summaries of the risk function suggest two dif-\n",
      "ferent methods for devising estimators: choosing @ to minimize\n",
      "the maximum risk leads to minimax estimators; choosing 0 to\n",
      "minimize the Bayes risk leads to Bayes estimators.\n",
      "\n",
      " \n",
      "\n",
      "Definition 13.6 A decision rule that minimizes the Bayes\n",
      "risk is called a Bayes rule. Formally, 0 isa Byes rule\n",
      "for prior w if\n",
      "R(6,0) = inf r(x, 6) (13.3)\n",
      "0\n",
      "\n",
      "where the infimum is over all estimators 8.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Definition 13.7 An estimator that minimizes the mazi-\n",
      "mum risk is called a minimax rule. Formally, 0 is min-\n",
      "imaz if\n",
      "\n",
      "R(0, 0) = inf sup R(6, @) (13.4)\n",
      "a 0\n",
      "\n",
      "where the infimum is over all estimators 8.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "13.3 Bayes Estimators\n",
      "\n",
      "Let 7 be a prior. From Bayes’ theorem, the posterior density\n",
      "is\n",
      "L(al)(0) _ __ F(xl@)r(@)\n",
      "m(x) J Fal) (@)a0\n",
      "\n",
      "where m(x) = f f(z,0)d0 = f f(x\\@)r(0)d0 is the marginal\n",
      "distribution of X. Define the posterior risk of an estimator\n",
      "\n",
      " \n",
      "\n",
      "f(|x) = (13.5)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-229.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "13.3 Bayes Estimators 233\n",
      "O(x) by\n",
      "r@le) = f 1(6,8(e)) p(0.2)20. (13.6)\n",
      "Theorem 13.8 The Bayes risk r(x, 0) satisfies\n",
      "\n",
      "r(a,0) = J r@eym(c) ac.\n",
      "\n",
      "Let 6(x) be the value of @ that minimizes r(6\\x). Then @ is the\n",
      "Bayes estimator.\n",
      "\n",
      "Proor. We can rewrite the Bayes risk as follows:\n",
      "\n",
      "r(m, 8) [Re 0,0)n(0)d0 = [UJ setenven)-0s\n",
      "{fusion (2,0) jazao = ff 24 (0,0(2)) f(0|x)m\n",
      "\n",
      "If we choose 6(«) to be the value of @ that minimizes r(6|x) then\n",
      "we will minimize the integrand at every x and thus minimize the\n",
      "integral [r(0|x)m(x)dx.\n",
      "\n",
      "Now we can find an explicit formula for the Bayes estimator\n",
      "for some specific loss functions.\n",
      "\n",
      "Theorem 13.9 If L(0,8) = (0- 6) then the Bayes estimator is\n",
      "(x) = [esa =E(6|X = 2). (13.7)\n",
      "\n",
      "If L(0, 8) = |0- 4) then the Bayes estimator is the median of\n",
      "the posterior f(@|x). If L(0,0) is zero-one loss, then the Bayes\n",
      "estimator is the mode of the posterior f(0|x).\n",
      "\n",
      "PROOF. We will prove the theorem for squared error loss. The\n",
      "Bayes rule 0(x) minimizes r(6|x) = [(9—0(a))? f(@|x)d0. Taking\n",
      "\n",
      "(x)dad0\n",
      "\n",
      "[U 1(0,8(2)f(0\\2)a® n(x) ae = [oom eae\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-230.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "234 13. Statistical Decision Theory\n",
      "\n",
      "the derivative of r(x) with respect to 6() and setting it equal\n",
      "to 0 yields the equation 2 [ (9 — 0(x)) f(@|x)d# = 0. Solving for\n",
      "\n",
      "O(x) we get 13.7.\n",
      "\n",
      "Example 13.10 Let X1,...,Xn ~ N(u,07) where o? is known.\n",
      "Suppose we use a N(a,b’) prior for . The Bayes estimator with\n",
      "respect to squared error loss is the posterior mean, which is\n",
      "\n",
      "a Pe 2\n",
      "Bion lll) — ee Tig\n",
      "\n",
      "7\n",
      "\n",
      " \n",
      "\n",
      "13.4 Minimax Rules\n",
      "\n",
      "The problem of finding minimax rules is complicated and we\n",
      "cannot attempt a complete coverage of that theory here but\n",
      "we will mention a few key results. The main message to take\n",
      "away from this section is: Bayes estimators with a constant risk\n",
      "function are minimax.\n",
      "\n",
      "Theorem 13.11 Let 6* be the Bayes rule for some prior 7:\n",
      "\n",
      "r(q, 6\") = inf r(z, 0). (13.8)\n",
      "D\n",
      "\n",
      "Suppose that\n",
      "R(0,0\") <r(a,6\") for all 6. (13.9)\n",
      "Then 6 is minimax and 7 is called a least favorable prior.\n",
      "\n",
      "PROOF. Suppose that 6\" is not minimax. Then there is an-\n",
      "other rule such that sup, R(0, 0) < sup, R(0, 6\"). Since the\n",
      "average of a function is always less than or equal to its maxi-\n",
      "mum, we have that r(z, 00) < sup, R(O, 00). Hence,\n",
      "\n",
      "r(,8)) < sup R(6, 6) < sup RO, 6*) < r(n, 6\")\n",
      "0 0\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-231.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "13.4 Minimax Rules 235\n",
      "\n",
      "which contradicts (13.8). i\n",
      "\n",
      " \n",
      "\n",
      "Theorem 13.12 Suppose that @ is the Bayes rule with re-\n",
      "spect to some prior n. Suppose further that 0 has constant\n",
      "risk: R(0,0) = c for some c. Then 0 is minimac.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Proor. The Bayes risk is r(7,4) = f R(0,0)n(0)d0 = c and\n",
      "\n",
      "hence R(0,@) < r(z, 9) for all 6. Now apply the previous theo-\n",
      "rem.\n",
      "\n",
      "Example 13.13 Consider the Bernoulli model with squared error\n",
      "loss. In example 13.3 we showed that the estimator\n",
      "\n",
      "p(X\") = Die Xi + vn/A\n",
      "n+ /J/n\n",
      "\n",
      "has a constant risk function. This estimator is the posterior\n",
      "mean, and hence the Bayes rule, for the prior Beta(a, 8) with\n",
      "a= B= /n/4. Hence, by the previous theorem, this estimator\n",
      "is minimas.\n",
      "\n",
      "Example 13.14 Consider again the Bernoulli but with loss func-\n",
      "tion\n",
      "\n",
      "~ _ (p-d)\n",
      "\n",
      "Bae pp)\n",
      "Let y\n",
      "AX\") = P= —\n",
      "\n",
      "The risk is\n",
      "\n",
      "R(p,p) = E (o*) = a (4) ~ :\n",
      "\n",
      "which, as a function of p, is constant. It can be shown that, for\n",
      "this loss function, p(X\") is the Bayes estimator under the prior\n",
      "w(p) = 1. Hence, p is minimac.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-232.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "“Well-behaved”\n",
      "means that the level\n",
      "sets must be convex\n",
      "and symmetric\n",
      "about the origin.\n",
      "The result holds up\n",
      "to sets of measure\n",
      "0.\n",
      "\n",
      "Typically, the\n",
      "squared bias is order\n",
      "O(n) while the\n",
      "variance is of order\n",
      "\n",
      "O(n\").\n",
      "\n",
      "236 13. Statistical Decision Theory\n",
      "\n",
      "A natural question to ask is: what is the minimax estimator\n",
      "for a Normal model?\n",
      "\n",
      "Theorem 13.15 Let X1,...,X, ~ N(@,1) and let @=X. Then\n",
      "0 is minimax with respect to any well-behaved loss function. It\n",
      "is the only estimator with this property.\n",
      "\n",
      "If the parameter space is restricted, the theorem above does\n",
      "not apply as the next example shows.\n",
      "\n",
      "Example 13.16 Suppose that X ~ N(0,1) and that 0 is known\n",
      "to lie in the interval [-m,m] where 0 < m < 1. The unique,\n",
      "minimas estimator under squared error loss is\n",
      "\n",
      "6(X) = mtanh(mX)\n",
      "\n",
      "where tanh(z) = (e* —e~*)/(e* + e7*). It can be shown that this\n",
      "is the Bayes rule with respect to the prior that puts mass 1/2 at\n",
      "m and mass 1/2 at —m. Moreover, it can be shown that the risk\n",
      "\n",
      "is not constant but it does satisfy R(@, 0) <r(n, 6) for all 0; see\n",
      "\n",
      "Figure 13.3. Hence, Theorem 13.11 implies that 0 is minima.\n",
      "|\n",
      "\n",
      "13.5 Maximum Likelihood, Minimax and Bayes\n",
      "\n",
      "For parametric models that satisfy weak regularity conditions,\n",
      "the maximum likelihood estimator is approximately minimax.\n",
      "Consider squared error loss which is squared bias plus variance.\n",
      "In parametric models with large samples, it can be shown that\n",
      "the variance term dominates the bias so the risk of the MLE 0\n",
      "roughly equals the variance:\n",
      "\n",
      "R(8,0) = Vo(@) + bias? = Vo(8).\n",
      "\n",
      "As we saw in the Chapter on parametric models, the variance\n",
      "of the MLE is approximately\n",
      "\n",
      " \n",
      "\n",
      "2)\n",
      "\n",
      "vo\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-233.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.5 Maximum Likelihood, Minimax and Bayes 237\n",
      "\n",
      "@\n",
      "FIGURE 13.3. Risk functions for constrained Normal with m=.5.\n",
      "The two short lines show the least favorable prior which puts its\n",
      "mass at two points.\n",
      "\n",
      " \n",
      "\n",
      "where J(@) is the Fisher information. Hence,\n",
      "\n",
      "a a\n",
      "\n",
      "nR(0,8) = > Oh\n",
      "\n",
      "v\n",
      "\n",
      "(13.10)\n",
      "\n",
      "For any other estimator 6’, it can be shown that for large n,\n",
      "R(0,6') > R(@,9). More precisely,\n",
      "1\n",
      "\n",
      "limlimsup sup nR(0',0) > —. 13.11\n",
      "200 meee aoatee (2 1(0) ( }\n",
      "\n",
      "This says that, in a local, large sample sense, the MLE is min-\n",
      "imax. It can also be shown that the MLE is approximately the\n",
      "Bayes rule.\n",
      "\n",
      "In summary, in parametric models with large samples, the\n",
      "MLE is approximately minimax and Bayes. There is a caveat:\n",
      "these results break down when the number of parameters is\n",
      "large as the next example shows.\n",
      "\n",
      "Example 13.17 (Many Normal means) Let Y; ~ N(0;,07/n), i\n",
      "1,...,n. Let Y = (¥%,...,¥,) denote the data and let 0\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-234.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "The many Normal\n",
      "means problem is\n",
      "more general than\n",
      "it looks. Many\n",
      "nonparametric es-\n",
      "timation problems\n",
      "are mathematically\n",
      "equivalent to this\n",
      "model.\n",
      "\n",
      "238 13. Statistical Decision Theory\n",
      "\n",
      "(01,---,9n) denote the unknown parameters. Assume that\n",
      "\n",
      "0€O,= { (+0045) ase}\n",
      "1\n",
      "\n",
      "for some c > 0. In this model, there are as many parameters\n",
      "as observations. The MLE is 0 = Y = (Y%1,---, Yn). Under the\n",
      "loss function L(0,0) = re G — 0;)?, the risk of the MLE is\n",
      "RO, 0) =o. It can be shown that the minimas risk is approsi-\n",
      "mately o?/(0?+¢) and one can find an estimator 6 that achieves\n",
      "this risk. Since o?/(o?+@) < 02, we see that 6 has smaller risk\n",
      "than the MLE . In practice, the difference between the risks can\n",
      "be substantial. This shows that maximum likelihood is not an\n",
      "optimal estimator in high dimensional problems.\n",
      "\n",
      "13.6 Admissibility\n",
      "Minimax estimator and Bayes estimator are “good estimator”\n",
      "\n",
      "in the sense that they have small risk. Sometimes it is also useful\n",
      "to characterize bad estimator.\n",
      "\n",
      " \n",
      "\n",
      "Definition 13.18 An estimator @ is inadmissible if there\n",
      "exists another rule 6’ such that\n",
      "\n",
      "R(6,0’) < R(0,0) forall @ and\n",
      "R(0,0\") < R(0,0) for at least one 0.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 13.19 Let X ~ N(0,1) and consider estimating 0 with\n",
      "squared error loss. Let 0(X) = 3. We will show that 0 is ad-\n",
      "missible. Suppose not. Then there exists a different rule 0 with\n",
      "smaller risk. In particular, R(3, 0) < R(3, 0) = 0. Hence, 0 =\n",
      "R(3, 6) S@o) — 3)? f(x; 3)dz. Thus, 6'(x) = 3. So there is\n",
      "no rule that beats 6. Even though @ is admissible it is clearly a\n",
      "bad decision rule.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-235.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "13.6 Admissibility 239\n",
      "\n",
      "A prior “ow has full support if for every 6 and every\n",
      "€>0, fy) 1(0)d0 > 0.\n",
      "\n",
      "Theorem 13.20 (Bayes’ rules are admissible.) Suppose that\n",
      "9 Cc R and that R(6, 0) is a continuous function of 0 for every\n",
      "@. Let x be a prior density with full support and let 8 be the\n",
      "Bayes’ rule. If the Bayes risk is finite then 6\" is admissible.\n",
      "\n",
      "PROOF. Suppose 6\" is inadmissible. Then there exists a better\n",
      "rule @ such that R(,0) < R(0,47) for all 6 and R(6,0) <\n",
      "R(6, 6\") for some Oy. Let v = R(6,6*) — R(6,0) > 0. Since R\n",
      "is continuous, there is an > 0 such that R(0,6*)—R(8, 0) > v/2\n",
      "for all 0 € (0 —€, 0 +). Now,\n",
      "\n",
      "r(, 0\") — r(r, 8) / R(0,0\")n(0)d0 — / R(0, 0) (0)d0\n",
      "/ [2,8*) — R(0,8)] x(0)a0\n",
      "\n",
      "IV\n",
      "\n",
      "Bo+e\n",
      "| [Re, #) — RO, 8) 1 (0)d0\n",
      "6\n",
      "\n",
      "lo-€\n",
      "\n",
      "v ote\n",
      "of n(0)d8\n",
      "\n",
      "o-€\n",
      "\n",
      "> 0.\n",
      "\n",
      "Hence, r (7, 6\") >r(r, 6). This implies that 4\" does not minimize\n",
      "r(m,0) which contradicts the fact that 6* is the Bayes rule. ll\n",
      "\n",
      "Theorem 13.21 Let X1,...,Xn ~ N(u, 07). Under squared er-\n",
      "ror loss, X is admissible.\n",
      "\n",
      "The proof of the last theorem is quite technical and is omitted\n",
      "but the idea is as follows. The posterior mean is admissible for\n",
      "any strictly positive prior. Take the prior to be N(a, 0”). When\n",
      "b is very large, the posterior mean is approximately equal to\n",
      "x.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-236.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "240 13. Statistical Decision Theory\n",
      "\n",
      "How are minimaxity and admissibility linked? In general, a\n",
      "rule may be one, both or neither. But here are some facts linking\n",
      "admissibility and minimaxity.\n",
      "\n",
      "Theorem 13.22 Suppose that @ has constant risk and is admis-\n",
      "sible. Then it is minimas.\n",
      "\n",
      "PROOF. The risk is R(@,0) = c for some c. If @ were not\n",
      "minimax then there exists a rule 6’ such that\n",
      "\n",
      "R(6, 6) < sup R(0, 6\") < sup R@,6) = c.\n",
      "0 0\n",
      "\n",
      "This would imply that 6 is inadmissible. ll\n",
      "Now we can prove a restricted version of Theorem 13.15 for\n",
      "squared error loss.\n",
      "\n",
      "Theorem 13.23 Let Xi,...,Xn ~ N(0,1). Then, under squared\n",
      "error loss, 0 =X is minimas.\n",
      "\n",
      "Proor. According to Theorem 13.21, @ is admissible. The\n",
      "tisk of @ is 1/n which is constant. The result follows from The-\n",
      "orem 13.22.\n",
      "\n",
      "Although minimax rules are not guaranteed to be admissi-\n",
      "ble they are “close to admissible.” Say that Gis strongly in-\n",
      "admissible if there exists a rule 6’ and an « > 0 such that\n",
      "R(0, 6\") < R(6,) — e for all 6.\n",
      "\n",
      "Theorem 13.24 Ife is minimaz then it is not strongly inadmis-\n",
      "sible.\n",
      "\n",
      "13.7 Stein’s Paradox\n",
      "\n",
      "Suppose that X ~ N(6,1) and consider estimating @ with\n",
      "squared error loss. From the previous section we know that\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-237.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "13.8 Bibliographic Remarks 241\n",
      "\n",
      "6(X) = X is admissible. Now consider estimating two, unre-\n",
      "lated quantities 6 = (01, 62) and suppose that X; ~ N (6,1) and\n",
      "X2 ~ N(O2, 1) independently, with loss L(0,0) = 7}_,(0;-;).\n",
      "Not surprisingly, 6(X) = X is again admissible where X =\n",
      "(Xi, X2). Now consider the generalization to k normal means.\n",
      "Let 0 = (01,...,x), X =(Xi,...,X) with X; ~ N(G;,1) (in-\n",
      "dependent) and loss L(0,0) = ~* (0; — 0,)?. Stein astounded\n",
      "\n",
      "j=l\n",
      "\n",
      "everyone when he proved that, if k > 3, then O(X) = X is inad-\n",
      "missible. It can be shown that the following estimator, known\n",
      "\n",
      "as the James-Stein estimator, has smaller risk:\n",
      "\n",
      "5(X) = (1 Ane *y (13.12)\n",
      "_ ixXi) 7\" :\n",
      "where (z)t = max{z,0}. This estimator shrinks the X;’s to-\n",
      "\n",
      "wards 0. The message is that, when estimating many param-\n",
      "eters, there is great value in “shrinking” the estimates. This\n",
      "observation plays an important role in modern nonparametric\n",
      "function estimation.\n",
      "\n",
      "13.8 Bibliographic Remarks\n",
      "\n",
      "It is difficult to find books that cover modern decision the-\n",
      "ory in great detail. Aspects of decision theory can be found in\n",
      "Casella and Berger (2002), Berger (1985), Ferguson (1967) and\n",
      "Lehmann and Casella (1998).\n",
      "\n",
      "13.9 Exercises\n",
      "\n",
      "1. In each of the following models, find (i) the Bayes risk and\n",
      "the Bayes estimator, using squared error loss.\n",
      "\n",
      "(a) X ~ Binomial(n, p), p ~ Beta(a, 8).\n",
      "(b) X ~ Poisson(\\), \\ ~ Gamma(a, 8).\n",
      "\n",
      "(c) X ~ N(0,07) where o? is known and 6 ~ N(a, 7).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-238.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "242\n",
      "\n",
      "13. Statistical Decision Theory\n",
      "\n",
      "Let Xi,...,Xn ~ N(0,07) and suppose we estimate @\n",
      "with loss function L(0,@) = (@ — 0)?/o?. Show that X is\n",
      "admissible and minimax.\n",
      "\n",
      "Let © = {6;,...,0,} be a finite parameter space. Prove\n",
      "that the posterior mode is the Bayes estimator under zero-\n",
      "one loss.\n",
      "\n",
      "(Casella and Berger.) Let X1,...,Xn be a sample from a\n",
      "distribution with variance o?. Consider estimators of the\n",
      "form }S? where S? is the sample variance. Let the loss\n",
      "function for estimating a? be\n",
      "ped oi\n",
      "a a Gi\n",
      "L(o?, 6\") = aT 1— log (=) :\n",
      "Find the optimal value of b that minimizes the risk for all\n",
      "\n",
      "et\n",
      "\n",
      "(Berliner, 1983). Let X ~ Binomial(n, p) and suppose the\n",
      "\n",
      "loss function is\n",
      "p 2\n",
      "un.) = (1-4)\n",
      "P\n",
      "\n",
      "where 0 < p < 1. Consider the estimator A(X) = 0. This\n",
      "estimator falls outside the parameter space (0,1) but we\n",
      "will allow this. Show that p(X) = 0 is the unique, minimax\n",
      "tule.\n",
      "\n",
      "(Computer Experiment.) Compare the risk of the mle and\n",
      "the James-Stein estimator (13.12) by simulation. Try var-\n",
      "ious values of n and various vectors 6. Summarize your\n",
      "results.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-239.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part III\n",
      "\n",
      "Statistical Models and\n",
      "Methods\n",
      "\n",
      "243\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-240.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "This is page 244\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-241.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "14\n",
      "\n",
      "Linear Regression\n",
      "\n",
      "Regression is a method for studying the relationship be-\n",
      "tween a response variable Y and a covariates X. The co-\n",
      "variate is also called a predictor variable or a feature. Later,\n",
      "we will generalize and allow for more than one covariate. The\n",
      "data are of the form\n",
      "\n",
      "(%4,.%1),---5 (Yas Xn):\n",
      "\n",
      "One way to summarize the relationship between X and Y is\n",
      "through the regression function\n",
      "\n",
      "re) = BOX =2) = fusule)dy. (04a)\n",
      "\n",
      "Most of this chapter is concerned with estimating the regression\n",
      "function.\n",
      "14.1 Simple Linear Regression\n",
      "\n",
      "The simplest version of regression is when X; is simple (a\n",
      "scalar not a vector) and r(x) is assumed to be linear:\n",
      "\n",
      "r(x) = Bot fiz.\n",
      "\n",
      "This is page 245\n",
      "Printer: Opaque this\n",
      "\n",
      "The term “regres-\n",
      "sion” is due to\n",
      "Sir Francis Galton\n",
      "(1822-1911) who\n",
      "noticed that tall\n",
      "and short men tend\n",
      "to have sons with\n",
      "heights closer to the\n",
      "mean. He called this\n",
      "“regression towards\n",
      "the mean.”\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-242.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "246 14. Linear Regression\n",
      "\n",
      "This model is called the the simple linear regression model.\n",
      "\n",
      "Let €; = Y; — (89 + 8:X;). Then,\n",
      "\n",
      "E(e|Xi) = E(¥;— (0 + &Xi)|Xi) =E(%\n",
      "= (Xj) — (Bo + AX)\n",
      "\n",
      "(60 + P:Xj) — (Bo + A Xi)\n",
      "\n",
      "= 0.\n",
      "\n",
      " \n",
      "\n",
      "Xi) — (80 + 1 Xi)\n",
      "\n",
      "Let o?(a) = V(e,|X = x). We will make the further simplifying\n",
      "assumption that o?(7) = 0? does not depend on x. We can thus\n",
      "write the linear regression model as follows.\n",
      "\n",
      " \n",
      "\n",
      "Definition 14.1 The Linear Regresion Model\n",
      "\n",
      "Yi = Bot AX +; (14.2)\n",
      "where E(e;|X;) = 0 and V(e,|X;) = 07.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 14.2 Figure 14.1 shows a plot of Log surface tempera-\n",
      "ture (Y) versus Log light intensity (X) for some nearby stars.\n",
      "Also on the plot is an estimated linear regression line which will\n",
      "be explained shortly.\n",
      "\n",
      "The unknown parameters in the model are the intercept [Jo\n",
      "and the slope }; and the variance o?. Let 89 and '; denote\n",
      "estimates of 39 and 3. The fitted line is defined to be\n",
      "\n",
      "F(a) = Bo + Bye. (14.3)\n",
      "\n",
      "The predicted values or fitted values are ¥, = 7(X;) and the\n",
      "residuals are defined to be\n",
      "\n",
      "a-Y¥,-f=-y- (5 + AX). (14.4)\n",
      "\n",
      "The residual sums of squares or RSS is defined by\n",
      "\n",
      "78\n",
      "Rss =).\n",
      "i=1\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-243.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "Log surface temperature (Y)\n",
      "\n",
      " \n",
      "\n",
      "4.1\n",
      "\n",
      "4.0\n",
      "\n",
      "14.1 Simple Linear Regression 247\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "T T T T\n",
      "4.0 45 5.0 5.5\n",
      "\n",
      "Log light intensity (X)\n",
      "\n",
      "FIGURE 14.1. Data on stars in nearby stars.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-244.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "248 14. Linear Regression\n",
      "\n",
      "The quantity RSS measures how well the fitted line fits the data.\n",
      "\n",
      " \n",
      "\n",
      "Definition 14.3. The least squares estimates are the val-\n",
      "\n",
      "ues By and 3, that minimize RSS = v\"_,@\n",
      "\n",
      "i=l i\"\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 14.4 The least squares estimates are given by\n",
      "\n",
      "3, = Sei FN-7.)\n",
      "-kP\n",
      "\n",
      "2\n",
      "\n",
      "Bo = Yn—BiXn- (14.6)\n",
      "\n",
      "(14.5)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "An unbiased estimate of o? is\n",
      "\n",
      "Be (5) ve (14.7)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 14.5 Consider the star data from Example 14.2. The\n",
      "least squares estimates are 39 = 3.58 and 3; = 0.166. The fitted\n",
      "line F(x) = 3.58 + 0.1662 is shown in Figure 14.1. Wl\n",
      "\n",
      "Example 14.6 (The 2001 Presidential Election.) Figure 14.2 shows\n",
      "the plot of votes for Buchanan (Y) versus votes for Bush (X)\n",
      "in Florida. The least squares estimates (omitting Palm Beach\n",
      "County) and the standard errors are\n",
      "\n",
      "By = 66.0991 €((y) = 17.2926\n",
      "B, = 00.0035 €(,) = 0.0002.\n",
      "\n",
      "The fitted line is\n",
      "Buchanon = 66.0991 + .0035 Bush.\n",
      "\n",
      "(We will see later how the standard errors were computed.) Fig-\n",
      "ure 14.2 also shows the residuals. The inferences from linear\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-245.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "14.2 Least Squares and Maximum Likelihood 249\n",
      "\n",
      "regression are most accurate when the residuals behave like ran-\n",
      "dom normal numbers. Based on the residual plot, this is not the\n",
      "case in this example. If we repeat the analysis replacing votes\n",
      "with log(votes) we get\n",
      "\n",
      "By = —2.3298\n",
      "B, = 0.730300\n",
      "\n",
      "& (Bo) = -3529\n",
      "ea\n",
      "\n",
      "& (31) = 0.0358.\n",
      "This gives the fit\n",
      "log(Buchanon) = —2.3298 + .7303 log(Bush).\n",
      "\n",
      "The residuals look much healthier. Later, we shall address two\n",
      "interesting questions: (1) how do we see if Palm Beach County\n",
      "has a statistically plausible outcome? (2) how do we do this prob-\n",
      "lem nonparametrically ?\n",
      "\n",
      "14.2 Least Squares and Maximum Likelihood\n",
      "\n",
      "Suppose we add the assumption that €;|X; ~ N(0,07), that\n",
      "\n",
      "is,\n",
      "\n",
      "  \n",
      "\n",
      "{|X ~ N (ui, 07)\n",
      "where fj = 39 + 3,X;. The likelihood function is\n",
      "\n",
      "n\n",
      "\n",
      "Tv“. = Tt fvix WiLX)\n",
      "\n",
      "i=l\n",
      "Ti x I Frix(¥iLX)\n",
      "sa] ek\n",
      "\n",
      "Li xX Lo\n",
      "\n",
      "where £; = JJ}, fx(Xj) and\n",
      "\n",
      "Lo= Il fyix(VilX). (14.8)\n",
      "\n",
      "i=1\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-246.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "250\n",
      "\n",
      "Buchanon\n",
      "\n",
      "log Buchanon\n",
      "\n",
      "3000\n",
      "\n",
      "1000\n",
      "\n",
      "ie)\n",
      "\n",
      "2345 6/7 8\n",
      "\n",
      "14. Linear Regression\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "0 50000 150000 250000\n",
      "\n",
      "Bush\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "log Bush\n",
      "\n",
      "FIGURE 14.2. Voting Data for Election 2000.\n",
      "\n",
      "Residuals\n",
      "\n",
      "Residuals\n",
      "\n",
      "Oo 200\n",
      "\n",
      "-400\n",
      "\n",
      "45\n",
      "\n",
      "0.0\n",
      "\n",
      "-1.0\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "0\n",
      "\n",
      "50000\n",
      "\n",
      "150000 250000\n",
      "\n",
      "Bush\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "log Bush\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-247.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "14.3 Properties of the Least Squares Estimators 251\n",
      "\n",
      "The term £; does not involve the parameters (39 and (3,. We shall\n",
      "focus on the second term £2 which is called the conditional\n",
      "likelihood, given by\n",
      "\n",
      "“ a na 1 .\n",
      "L = £(3o, 10) = TT] fix Wil X) 0 0\" exp {st eH - wr}\n",
      "\n",
      "i=l\n",
      "\n",
      "The conditional log-likelihood is\n",
      "; Loy, xa\\\"\n",
      "(80, 81,0) = —nlogo — at — (Bo + 6.X,)) - (14.9)\n",
      "i=\n",
      "\n",
      "To find the MLE of (8, 5) we maximize ¢(/J, 31,0). From (14.9)\n",
      "we see that maximizing the likelihood is the same as minimizing\n",
      "2\n",
      "the RSS SO\", (i am (Go+4.%;)) . Therefore, we have shown the\n",
      "\n",
      "following.\n",
      "\n",
      "Theorem 14.7 Under the assumption of Normality, the least squares\n",
      "estimator is also the maximum likelihood estimator.\n",
      "\n",
      "We can also maximize ¢((%, 3;,0) over o yielding the MLE\n",
      "Es 1\n",
      "P= = 2 (14.10)\n",
      "\n",
      "This estimator is similar to, but not identical to, the unbiased\n",
      "estimator. Common practice is to use the unbiased estimator\n",
      "(14.7).\n",
      "\n",
      "14.3 Properties of the Least Squares Estimators\n",
      "\n",
      "We now record the standard errors and limiting distribution\n",
      "of the least squares estimator. In regression problems, we usu-\n",
      "ally focus on the properties of the estimators conditional on\n",
      "X”\" =(X,...,X,,). Thus, we state the means and variances as\n",
      "\n",
      " \n",
      "\n",
      "conditional means and variances.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-248.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "252 14. Linear Regression\n",
      "\n",
      " \n",
      "\n",
      "Theorem 14.8 Let 7 = (Bo, BJT denote the least squares\n",
      "estimators. Then,\n",
      "\n",
      "E(3|X\") =\n",
      "\n",
      "V(B|X\") =\n",
      "\n",
      " \n",
      "\n",
      "2 psn (yy 2\n",
      "where sy =n fet (Kp oMg)*\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The estimated standard errors of {39 and 3, are obtained by\n",
      "\n",
      " \n",
      "\n",
      "taking the square roots of the corresponding diagonal terms of\n",
      "V(8|X\") and inserting the estimate & for o. Thus,\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "& (Bo) = — (14.12)\n",
      "Bh) = @ (14.13)\n",
      "\n",
      " \n",
      "\n",
      "sx Jn\n",
      "\n",
      "We should really write these as sé (Bo|X\") and sé (B:|X\") but\n",
      "we will use the shorter notation sé (f) and sé (3;).\n",
      "\n",
      "Theorem 14.9 Under appropriate conditions we have:\n",
      "1. (Consistency): Bo—> Bo and i> fr.\n",
      "\n",
      "2. (Asymptotic Normality):\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Bo =o N(0,1) and Br SH sg N(0,1).\n",
      "$e (0) se (81)\n",
      "\n",
      "8. Approximate 1 —a confidence intervals for 89 and 8, are\n",
      "Bo + 2a /2 B (30) and Br + 2a 9 & (i). (14.14)\n",
      "\n",
      "The Wald test statistic for testing Ho: 8; =0 versus H, :\n",
      "Bi £0 is: reject Hy if W > 2a where W = §1/sé (G1).\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-249.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "14.4 Prediction 253\n",
      "\n",
      "Example 14.10 For the election data, on the log scale, a 95 per\n",
      "cent confidence interval is .7303 + 2(.0358) = (.66,.80). The\n",
      "fact that the interval excludes 0 The Wald statistics for testing\n",
      "Ho : 6, = 0 versus H, : 8; £0 is W = |.7303—0|/.0358 = 20.40\n",
      "with a p-value of P(|Z| > 20.40) © 0. This is strong evidence\n",
      "that that the true slope is not 0.\n",
      "\n",
      " \n",
      "\n",
      "14.4 Prediction\n",
      "\n",
      "Suppose we have estimated a regression model 7(x) = Bo + Bi\n",
      "from data (X,,¥i),---,(Xn, Yn). We observe the value X = x,\n",
      "of the covariate for a new subject and we want to predict their\n",
      "outcome Y,. An estimate of Y, is\n",
      "\n",
      "¥, = Bo +B X.. (14.15)\n",
      "\n",
      "Using the formula for the variance of the sum of two random\n",
      "variables,\n",
      "\n",
      "V(¥,) = V(B + Bia.) = V(Bo) + 22V(B,) + 22,Cov(fo, 31).\n",
      "\n",
      "Theorem 14.8 gives the formulas for all the terms in this equa-\n",
      "tion. The estimated standard error sé (Y,) is the square root of\n",
      "this variance, with G in place of 0”. However, the confidence\n",
      "\n",
      "interval for Y, is not of the usual form y. +2a/2- The appendix\n",
      "\n",
      " \n",
      "\n",
      "explains why. The correct form of the confidence interval is given\n",
      "in the following Theorem. We call the interval a prediction in-\n",
      "terval.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-250.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "254 14. Linear Regression\n",
      "\n",
      " \n",
      "\n",
      "Theorem 14.11 (Prediction Interval) Let\n",
      "\n",
      "2 = a(F)+e\n",
      "= toa qa (AG — X\n",
      "ny(Xi—X)?\n",
      "\n",
      "An approximate 1 —a predicition interval for Y, is\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "¥, + zajatn- (14.17)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 14.12 (Election Data Revisited.) On the log-scale, our lin-\n",
      "ear regression gives the following prediction equation: log( Buchanon) =\n",
      "—2.3298 + .7303log(Bush). In Palm Beach, Bush had 152954\n",
      "votes and Buchanan had 3467 votes. On the log scale this is\n",
      "11.93789 and 8.151045. How likely is this outcome, assuming\n",
      "our regression model is appropriate? Our prediction for log Buchanan\n",
      "votes -2.3298 + .7803 (11.93789)=6. 388441. Now 8.151045 is\n",
      "bigger than 6.388441 but is is “significantly” bigger? Let us com-\n",
      "pute a confidence interval. We find that &, = .093775 and the ap-\n",
      "protimate 95 per cent confidence interval is (6.200, 6.578) which\n",
      "clearly excludes 8.151. Indeed, 8.151 is nearly 20 standard er-\n",
      "rors from ¥,. Going back to the vote scale by exponentiating, the\n",
      "confidence interval is (493,717) compared to the actual number\n",
      "\n",
      "of votes which is 3467.\n",
      "\n",
      "14.5 Multiple Regression\n",
      "\n",
      "Now suppose we have k covariates X;,...,X,. The data are\n",
      "of the form\n",
      "\n",
      "(¥1, 1), -+- (Vis Xa), +s (Ys Xn)\n",
      "\n",
      "where\n",
      "Ky = [Xilinx og Nye)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-251.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "14.5 Multiple Regression 255\n",
      "\n",
      "Here, X; is the vector of k covariate values for the i*® observa-\n",
      "tion. The linear regression model is\n",
      "\n",
      "k\n",
      "Vi= Do Bi Xy + €i (14.18)\n",
      "\n",
      "j=l\n",
      "\n",
      "for i=1,...,n, where E(e;|X1,,...,X4;) = 0. Usually we want\n",
      "to include an intercept in the model which we can do by setting\n",
      "Xj, =1 fori =1,...,n. At this point it will be more convenient\n",
      "to express the model in matrix notation. The outcomes will be\n",
      "denoted by\n",
      "Y\n",
      "Yo\n",
      "Yo .\n",
      "Yn\n",
      "\n",
      "and the covariates will be denoted by\n",
      "\n",
      "Xi Xi ... Xp\n",
      "ye Xn Xn oe Xu\n",
      "Xn Xn2 «+. Xnk\n",
      "\n",
      "Each row is one observation; the columns correspond to the k\n",
      "covariates. Thus, X is a (n x k) matrix. Let\n",
      "\n",
      "fern 1\n",
      "B= : and €= s\n",
      "Br En\n",
      "Then we can write (14.18) as\n",
      "Y=NXB+e (14.19)\n",
      "\n",
      "Theorem 14.13 Assuming that the (k x k) matrix X7X is in-\n",
      "vertible, the least squares estimate is\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-252.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "256 14. Linear Regression\n",
      "\n",
      " \n",
      "\n",
      "B=(XTX)-UXTY. (14.20)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The estimated regression function is\n",
      "is ~\n",
      "F(x) = So Bjx;. (14.21)\n",
      "j=l\n",
      "\n",
      "The variance-covariance matrix of Bis\n",
      "V(3|X\") = 0? (XTX).\n",
      "\n",
      "Under appropriate conditions,\n",
      "\n",
      " \n",
      "\n",
      "where € = XB —Y is the vector of residuals. An approximate\n",
      "1 —a confidence interval for 8; is\n",
      "\n",
      "Bj + 228 (3;) (14.22)\n",
      "\n",
      "where sé 2(B;) is the j diagonal element of the matrix G?(X™X)-!.\n",
      "\n",
      "Example 14.14 Crime data on 47 states in 1960 can be obtained\n",
      "at http://lib. stat.cmu. edu/DASL/Stories/USCrime.html. If we\n",
      "fit a linear regression of crime rate on 10 variables we get the\n",
      "following:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-253.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "14.6 Model Selection 257\n",
      "\n",
      "ak\n",
      "*\n",
      "\n",
      "HK\n",
      "\n",
      "Covariate Least Estimated t value p-value\n",
      "\n",
      "Squares — Standard\n",
      "\n",
      "Estimate Error\n",
      "(Intercept) -589.39 167.59  -3.51 0.001\n",
      "Age 1.04 0.45 2.33 0.025\n",
      "Southern State 11.29 13.24 0.85 0.399\n",
      "Education 1.18 0.68 1.7 0.093\n",
      "Expenditures 0.96 0.25 3.86 0.000\n",
      "Labor 0.11 0.15 0.69 0.493\n",
      "Number of Males 0.30 0.22 1.36 0.181\n",
      "Population 0.09 O14 0.65 0.518\n",
      "Unemployment (14-24) -0.68 0.48 -1.4 0.165\n",
      "Unemployment (25-39) 2.15 0.95 2.26 0.030\n",
      "Wealth -0.08 0.09 -0.91 0.367\n",
      "\n",
      "This table is typical of the output of a multiple regression pro-\n",
      "gram. The “t-value” is the Wald test statistic for testing Ho :\n",
      "8; = 0 versus H, : 6; # 0. The asterisks denote “degree of\n",
      "significance” with more asterisks being significant at a smaller\n",
      "level. The example raises several important questions. In partic-\n",
      "ular: (1) should we eliminate some variables from this model?\n",
      "(2) should we interpret this relationships as causal? For exam-\n",
      "ple, should we conclude that low crime prevention expenditures\n",
      "cause high crime rates? We will address question (1) in the next\n",
      "section. We will not address question (2) until a later Chapter.\n",
      "\n",
      "14.6 Model Selection\n",
      "\n",
      "Example 14.14 illustrates a problem that often arises in mul-\n",
      "tiple regression. We may have data on many covariates but we\n",
      "may not want to include all of them in the model. A smaller\n",
      "model with fewer covariates has two advantages: it might give\n",
      "better predictions than a big model and it is more parsimonious\n",
      "(simpler). Generally, as you add more variables to a regression,\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-254.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "258 14. Linear Regression\n",
      "\n",
      "the bias of the predictions decreases and the variance increases.\n",
      "Too few covariates yields high bias; too many covariates yields\n",
      "high variance. Good predictions result from achieving a good\n",
      "balance between bias and variance.\n",
      "\n",
      "In model selection there are two problems: (i) assigning a\n",
      "“score” to each model which measures, in some sense, how good\n",
      "the model is and (ii) searching through all the models to find\n",
      "the model with the best score.\n",
      "\n",
      "Let us first discuss the problem of scoring models. Let S C\n",
      "{1,...,k} and let Ys = {X;: j € S} denote a subset of the\n",
      "covariates. Let 6s denote the coefficients of the corresponding\n",
      "set of covariates and let Bs denote the least squares estimate of\n",
      "Bg. Also, let Xs denote the X matrix for this subset of covari-\n",
      "ates and define 7s(x) to be the estimated regression function\n",
      "from (14.21). The predicted valus from model § are denoted by\n",
      "Y,(S) =7s(X,). The prediction risk is defined to be\n",
      "\n",
      "R(S) = STE) - 9) (14.23)\n",
      "\n",
      "where Y,* denotes the value of a future observation of Y; at\n",
      "covariate value X;. Our goal is to choose S to make R(S) small.\n",
      "The training error is defined to be\n",
      "\n",
      "Ru(S) = S(%i(8) —\n",
      "\n",
      "This estimate is very biased and under-estimates R(S).\n",
      "Theorem 14.15 The training error is a downward biased esti-\n",
      "mate of the prediction risk:\n",
      "\n",
      "E(Ru(S)) < R(S).\n",
      "In fact,\n",
      "\n",
      "bias (R..(S)) =E(R.(S)) —R(S) = (¥;,.¥)). (14.24)\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-255.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "14.6 Model Selection 259\n",
      "\n",
      "The reason for the bias is that the data are being used twice:\n",
      "to estimate the parameters and to estimate the risk. When\n",
      "we fit a complex model with many parameters, the covariance\n",
      "Cov(¥;, ¥;) will be large and the bias of the training error gets\n",
      "worse. In summary, the training error is a poor estimate of risk.\n",
      "Here are some better estimates.\n",
      "\n",
      "Mallow’s C;, statistic is defined by\n",
      "\n",
      "R(S) = R.(S) +2\n",
      "\n",
      " \n",
      "\n",
      "S| (14.25)\n",
      "\n",
      "where |S] denotes the number of terms in S and G? is the es\n",
      "timate of o? obtained from the full model (with all covariates\n",
      "in the model). This is simply the training error plus a bias cor\n",
      "rection. This estimate is named in honor of Colin Mallows who\n",
      "invented it. The first term in (14.25) measures the fit of the\n",
      "model while the second measure the complexity of the model.\n",
      "Think of the C,, statistic as:\n",
      "\n",
      "lack of fit + complexity penalty.\n",
      "\n",
      "Thus, finding a good model involves trading off fit and\n",
      "complexity.\n",
      "\n",
      "A related method for estimating risk is AIC (Akaike Infor-\n",
      "mation Criterion). The idea is to choose S to maximize\n",
      "\n",
      "es —|3| (14.26)\n",
      "\n",
      "where ¢s is the log-likelihood of the model evaluated at the\n",
      "MLE . This can be thought of “goodness of fit” minus “com-\n",
      "plexity.” In linear regression with Normal errors, maximizing\n",
      "AIC is equivalent to minimizing Mallow’s C,; see exercise 8.\n",
      "Yet another method for estimating risk is leave-one-out\n",
      "cross-validation. In this case, the risk estimator is\n",
      "\n",
      "n\n",
      "\n",
      "Rev(S) = 3% - Yo)?\n",
      "\n",
      "i=l\n",
      "\n",
      "(14.27)\n",
      "\n",
      "Some texts use a\n",
      "\n",
      "slightly different\n",
      "definition of AIC\n",
      "which involves\n",
      "multiplying the\n",
      "definition here by\n",
      "\n",
      "2 or -2. This has\n",
      "no effect on which\n",
      "model is selected.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-256.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "260 14. Linear Regression\n",
      "\n",
      "where Yu) is the prediction for Y; obtained by fitting the model\n",
      "with Y; omitted. It can be shown that\n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      "_ nly _=\n",
      "Rev(3) => (8 (14.28)\n",
      "i=l\n",
      "where Uj;(S) is the i\" diagonal element of the matrix\n",
      "U(S) =Xs(XPXs)1 XE. (14.29)\n",
      "\n",
      "Thus, one need not actually drop each observation and re-fit\n",
      "the model. A generalization is k-fold cross-validation. Here\n",
      "we divide the data into k groups; often people take k = 10. We\n",
      "omit one group of data and fit the models to the remaining data.\n",
      "We use the fitted model to predict the data in the group that\n",
      "was omitted. We then estimate the risk by )>,(¥j — Y,)? where\n",
      "the sum is over the the data points in the omitted group. This\n",
      "process is repeated for each of the k groups and the resulting\n",
      "risk estimates are averaged.\n",
      "\n",
      "For linear regression, Mallows C,, and cross-validation often\n",
      "yield essentially the same results so one might as well use Mal-\n",
      "\n",
      " \n",
      "\n",
      "lows’ method. In some of the more complex problems we will\n",
      "discuss later, cross-validation will be more useful.\n",
      "\n",
      "Another scoring method is BIC (Bayesian information crite-\n",
      "rion). Here we choose a model to maximize\n",
      "\n",
      "BIC(S) = rss (S) + 2|5|6?. (14.30)\n",
      "\n",
      "The BIC score has a Bayesian interpretation. Let S = {S,,..., Sin}\n",
      "denote a set of models. Suppose we assign the prior P(S;) = 1/m\n",
      "over the models. Also, assume we put a smooth prior on the pa-\n",
      "rameters within each model. It can be shown that the posterior\n",
      "probability for a model is approximately,\n",
      "\n",
      "eBIC(S))\n",
      "\n",
      "P(S;|data) = > ares\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-257.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.6 Model Selection 261\n",
      "\n",
      "Hence, choosing the model with highest BIC is like choosing the\n",
      "model with highest posterior probability. The BIC score also has\n",
      "an information-theoretic interpretation in terms of something\n",
      "called minimum description length. The BIC score is identical\n",
      "to Mallows C, except that it puts a more severe penalty for\n",
      "complexity. It thus leads one to choose a smaller model than\n",
      "the other methods.\n",
      "\n",
      "Now let us turn to the problem of model search. If there are k\n",
      "covariates then there are 2\" possible models. We need to search\n",
      "rough all these models, assign a score to each one, and choose\n",
      "he model with the best score. If k is not too large we can do\n",
      "a complete search over all the models. When k is large, this is\n",
      "infeasible. In that case we need to search over a subset of all\n",
      "\n",
      "a a0\n",
      "\n",
      " \n",
      "\n",
      "the models. Two common methods are forward and back-\n",
      "ward stepwise regression. In forward stepwise regression, we\n",
      "start with no covariates in the model. We then add the one vari-\n",
      "able that leads to the best score. We continue adding variables\n",
      "one at a time until the score does not improve. Backwards step-\n",
      "\n",
      " \n",
      "\n",
      "wise regression is the same except that we start with the biggest\n",
      "model and drop one variable at a time. Both are greedy searches;\n",
      "nether is guaranteed to find the model with the best score. An-\n",
      "other popular method is to do random searching through the\n",
      "set of all models. However, there is no reason to expect this to\n",
      "\n",
      " \n",
      "\n",
      "be superior to a deterministic search.\n",
      "\n",
      "Example 14.16 We apply backwards stepwise regression to the\n",
      "crime data using AIC. The following was obtained from the pro-\n",
      "gram R. This program uses minus our version of AIC. Hence,\n",
      "we are seeking the smallest possible AIC. This is the same is\n",
      "minimizing Mallows Cy.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-258.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "262 14. Linear Regression\n",
      "\n",
      "The full model (which includes all covariates) has AIC= 310.37.\n",
      "The AIC scores, in ascending order, for deleting one variable are\n",
      "as follows:\n",
      "\n",
      "variable || Pop Labor South Wealth Males U1 Educ. U2 Age _Bxpend\n",
      "\n",
      " \n",
      "\n",
      "AIC l 308 = 309 309 309 310 310 312 814 315 324\n",
      "\n",
      "For example, if we dropped Pop from the model and kept the\n",
      "other terms, then the AIC score would be 308. Based on this in-\n",
      "formation we drop “population” from the model and the current\n",
      "AIC score is 308. Now we consider dropping a variable from the\n",
      "current model. The AIC scores are:\n",
      "\n",
      "variable || South Labor Wealth Males U1 Education U2 Age LExpend\n",
      "AIC l 308 308 308 309 = 309 310 3138 3138 329\n",
      "\n",
      "We then drop “Southern” from the model. This process is con-\n",
      "\n",
      "tinued until there is no gain in AIC by dropping any variables.\n",
      "\n",
      "In the end, we are left with the following model:\n",
      "\n",
      " \n",
      "\n",
      "Crime = 1.2 Age + .75 Education + .87 Expenditure + .34 Males — .86 U1 + 2.31 U2.\n",
      "\n",
      "Warning! This does not yet address the question of which vari-\n",
      "ables are causes of crime.\n",
      "\n",
      "14.7. The Lasso\n",
      "\n",
      "There is an easier model search method although it addresses\n",
      "a slightly different question. The method, due to Tibshirani, is\n",
      "called the Lasso. In this section we assume that the covari-\n",
      "ates have all been rescaled to have the same variance. This\n",
      "puts each covariate on the same scale. Consider estimating 6 =\n",
      "(61,---,8,) by minimizing the loss function\n",
      "n\n",
      "\n",
      "k\n",
      "SH =H? +a SP 13) (14.31)\n",
      "\n",
      "i=l\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-259.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "14.8 Technical Appendix 263\n",
      "\n",
      "where \\ > 0. The idea is to minimize the sums of squares but\n",
      "we include a penalty that gets large if any of the Bis are large.\n",
      "The solution B = (Bi, --+5Bx) can be found numerically and\n",
      "will depend on the choice of A. It can be shown that some of\n",
      "the Bis will be 0. We interpret this is meaning that the j is\n",
      "omitted from the model. Hence, we are doing estimation and\n",
      "model selection simultaneously. We need to choose a value of\n",
      "A. We can do this by estimating the prediction risk R(A) as\n",
      "a function of \\ and choosing \\ to minimze the estimated risk.\n",
      "For example, we can estimate the risk using leave-one-out cross-\n",
      "validation.\n",
      "\n",
      "Example 14.17 Returning to the crime data, Figure 14.3 shows\n",
      "the results of the lasso. The first plot shows the leave-one-out\n",
      "cross-validation score as a function of X. The minimum. occurs\n",
      "at \\ = .7. The second plot shows the estimated coefficients as a\n",
      "function of X. You can see how some estimated parameters are\n",
      "zero until A gets larger. At \\ = .7, all the B; are non-zero so the\n",
      "Lasso chooses the full model.\n",
      "\n",
      "14.8 Technical Appendix\n",
      "\n",
      "Why is the predicition interval of a different form than the\n",
      "other confidence intervals we have seen? The reason is that the\n",
      "quantity we want to estimate, Y,, is not a fixed parameter, it\n",
      "is a random variable. To understand this point better, let 6 =\n",
      "Bo t+ A,X, and let 0 = 8) + B.X,. Thus, Y, = @ while Y, =0 +e.\n",
      "Now, Ox) (0,se”) where\n",
      "\n",
      "se? = V(8) = V(4y + B24).\n",
      "\n",
      " \n",
      "\n",
      "Note that V(@) is the same as V(Y,). Now, 04 2\\/Var(0) is a\n",
      "95 per cent confidence interval for 9 = 89+ 612, using the usual\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-260.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "264 14. Linear Regression\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "    \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "2\n",
      "g 4\n",
      "EA 4\n",
      "3° 4\n",
      "284\n",
      "“a o\n",
      "a T T T T\n",
      "é\n",
      "5 0.2 04 0.6 0.8\n",
      "d\n",
      "Oo\n",
      "be 9-0-0-5-8-8-6-0-9-0-0-0-0-6\n",
      ". 4+ yee\n",
      "BO) 5 |e” e-\n",
      "| 9-0-0262828202058263\n",
      "2 4\n",
      "1\n",
      "\n",
      " \n",
      "\n",
      "0.2 0.4 0.6\n",
      "\n",
      "FIGURE 14.3. The Lasso applied to the crime data.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-261.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "14.8 Technical Appendix 265\n",
      "\n",
      "argument for a confidence interval. It is not a valid confidence\n",
      "interval for Y,. To see why, let’s compute the probability that\n",
      "\n",
      "y. + 2/V(¥.) contains Y,.\n",
      "\n",
      "P(Y, —28 < Y, < ¥, +2s)\n",
      "\n",
      " \n",
      "\n",
      "2\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "-2< N(0,1)-N (0 =) < 2)\n",
      "\n",
      "2\n",
      "-2 <n (01+) <2)\n",
      "Ss\n",
      "# 95.\n",
      "\n",
      "The problem is that the quantity of interest Y, is equal\n",
      "\n",
      "to a parameter @ plus a random variable. We can fix this\n",
      "by defining\n",
      "\n",
      "=VN,)+07= as —- + ie\n",
      "\n",
      "In practice, we substitute G for o and we denote the resulting\n",
      "quantity by é.. Now consider the interval ¥, +26, Then,\n",
      "\n",
      "P(Y, — 26, <¥% < ¥, 426) =\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-262.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "266\n",
      "\n",
      "14. Linear Regression\n",
      "\n",
      "= P(-2< N(0,1) < 2) =.95.\n",
      "\n",
      " \n",
      "\n",
      "Of course, a 1 — a interval is given by ¥, + Zajren-\n",
      "14.9 Exercises\n",
      "\n",
      "sles\n",
      "\n",
      "2.\n",
      "\n",
      "Prove Theorem 14.4.\n",
      "\n",
      "Prove the formulas for the standard errors in Theorem\n",
      "14.8. You should regard the X;’s as fixed constants.\n",
      "\n",
      ". Consider the regression through the origin model:\n",
      "\n",
      "Y, = BX; +e.\n",
      "\n",
      "Find the least squares estimate for 6. Find the standard\n",
      "error of the estimate. Find conditions that guarantee that\n",
      "the estimate is consistent.\n",
      "\n",
      ". Prove equation (14.24).\n",
      "\n",
      ". In the simple linear regression model, construct a Wald\n",
      "\n",
      "test for Ho : 6; = 179 versus H, : 6; #17).\n",
      "\n",
      ". Get the passenger car mileage data from\n",
      "\n",
      "http://lib.stat.cmu.edu/DASL/Datafiles/carmpgdat.html\n",
      "\n",
      "(a) Fit a simple linear regression model to predict MPG\n",
      "(miles per gallon) from HP (horsepower). Summarize your\n",
      "analysis including a plot of the data with the fitted line.\n",
      "(b) Repeat the analysis but use log(MPG) as the response.\n",
      "Compare the analyses.\n",
      "\n",
      ". Get the passenger car mileage data from\n",
      "\n",
      "http://lib.stat.cmu.edu/DASL/Datafiles/carmpgdat.html\n",
      "\n",
      "(a) Fit a multiple linear regression model to predict MPG\n",
      "(miles per gallon) from HP (horsepower). Summarize your\n",
      "analysis.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-263.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A81061C8>\n",
      "14.9 Exercises 267\n",
      "\n",
      "(b) Use Mallow C, to select a best sub-model. To search\n",
      "trhough the models try (i) all possible models, (ii) for-\n",
      "ward stepwise, (ili) backward stepwise. Summarize your\n",
      "findings.\n",
      "\n",
      "(c) Repeat (b) but use BIC. Compare the results.\n",
      "\n",
      "(d) Now use the Lasso and compare the results.\n",
      "\n",
      ". Assume that the errors are Normal. Show that the model\n",
      "with highest AIC (equation (14.26)) is the model with the\n",
      "lowest Mallows C, statistic.\n",
      "\n",
      ". In this question we will take a closer look at the AIC\n",
      "method. Let X1,...,X,, be iid observations. Consider two\n",
      "models Mp and M,. Under Mo the data are assumed to\n",
      "be N(0,1) while under M, the data are assumed to be\n",
      "N(6,1) for some unknown @ € R:\n",
      "\n",
      "Mo: Xi,---;Xn ~ N(0,1)\n",
      "Mi: Xi--,Xn ~ N(O,1), OER\n",
      "\n",
      "This is just another way to view the hypothesis testing\n",
      "problem: Ho : 9 = 0 versus H; : 0 4 0. Let 2,(0) be\n",
      "the log-likelihood function. The AIC score for a model is\n",
      "the log-likelihood at the mle minus the number of param-\n",
      "eters. (Some people multiply this score by 2 but that is\n",
      "irrelevant.) Thus, the AIC score for Mo is AICo = ¢,(0)\n",
      "and the AIC score for M, is AIC; = £,(0) — 1. Suppose\n",
      "we choose the model with the highest AIC score. Let J,\n",
      "denote the selected model:\n",
      "j= { 0 if AIC) > AIC;\n",
      "mn (1 if AIC, > AIC).\n",
      "\n",
      "(a) Suppose that Mo is the true model, i.e. 9 = 0. Find\n",
      "\n",
      "lim P (Jn = 0).\n",
      "\n",
      "n—00\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-264.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268\n",
      "\n",
      "14. Linear Regression\n",
      "\n",
      "Now compute lim,_,.. P(J, =0) when 0 4 0.\n",
      "\n",
      "(b) The fact that lim;,.P (J, =0) 4 1 when 0 = 0 is\n",
      "why some people say that AIC “overfits.” But this is not\n",
      "quite true as we shall now see. Let @g(2) denote a Normal\n",
      "density function with mean @ and variance 1. Define\n",
      "\n",
      "=. f do(x) if Jn =0\n",
      "hey ={ ola) if Jn =1.\n",
      "\n",
      "If 0 =0, show that D(do, f,) 20 as n + 00 where\n",
      "\n",
      "D(f.9)= fH) 108 (42) dx\n",
      "\n",
      "is the Kullback-Leibler distance. Show also that D($9, fn) 2\n",
      "0if@ #0. Hence, AIC consistently estimates the true den-\n",
      "sity even if it “overshoots” the correct model.\n",
      "\n",
      "REMARK: If you are feeling ambitious, repeat this anal-\n",
      "ysis for BIC which is the log-likelihood minus (p/2) logn.\n",
      "where p is the number of parameters and n is sample size.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-265.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "5\n",
      "Multivariate Models\n",
      "\n",
      "In this chapter we revisit the Multinomial model and the mul-\n",
      "tivariate Normal, as we will need them in future chapters.\n",
      "\n",
      "Let us first review some notation from linear algebra. Re-\n",
      "call that if « and y are vectors then 2’ y = Ms xy; If Aisa\n",
      "matrix then det(A) denotes the determinant of A, A? denotes\n",
      "the transpose of A and A~ denotes the the inverse of A (if the\n",
      "inverse exists). The trace of a square matrix A — denoted by\n",
      "tr(A) — is the sum of its diagonal elements. The trace satis-\n",
      "fies tr(AB) = tr(BA) and tr(A) + tr(B). Also, tr(a) = a if a\n",
      "is a scalar (i.e. a real number). A matrix is positive definite if\n",
      "x? Sx > 0 for all non-zero vectors x. If a matrix © is symmet-\n",
      "ric and positive definite, there exists a matrix y!/2 — called the\n",
      "square root of © — with the following properties:\n",
      "\n",
      "1. ©? is symmetric;\n",
      "2. D=DIeyV?,\n",
      "SMe a el wheres EI = (eh,\n",
      "\n",
      "This is page 269\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-266.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "270 15. Multivariate Models\n",
      "15.1 Random Vectors\n",
      "\n",
      "Multivariate models involve a random vector X of the form\n",
      "\n",
      "XY\n",
      "X= :\n",
      "Xx\n",
      "The mean of a random vector X is defined by\n",
      "ma E(X1)\n",
      "w=|{: f= ‘ . (15.1)\n",
      "Me E(Xx)\n",
      "The covariance matrix © is defined to be\n",
      "V(X1) Cov(X1,X2) +++ Cov(X1, Xx)\n",
      "3=V(X) = Cov(X2, X1) V(%2) i Cov(Xa, Xe)\n",
      "Cov(X,,X1) Cov(X,,X2) +++ V(Xx)\n",
      "(15.2)\n",
      "\n",
      "This is also called the variance matrix or the variance-covariance\n",
      "matrix.\n",
      "\n",
      "Theorem 15.1 Leta be a vector of length k and let X be a ran-\n",
      "dom vector of the same length with mean yc and variance XS.\n",
      "Then E(a?X) = ap and V(aT X) = aTSa. If A is a matrix\n",
      "with k columns then E(AX) = Ay and V(AX) = ASAT.\n",
      "\n",
      "Now suppose we have a random sample of n vectors:\n",
      "\n",
      "Xu Xi Xin\n",
      "Xn Xn oo. Xm ; (15.3)\n",
      "Xri Xx Xin\n",
      "\n",
      "The sample mean X is a vector defined by\n",
      "\n",
      "x\n",
      "\n",
      "x :\n",
      "Xp\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-267.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A8106288>\n",
      "15.2 Estimating the Correlation 271\n",
      "\n",
      "ra jot Xig- The sample variance matrix, also\n",
      "\n",
      "called the covariance matrix or the variance-covariance matrix,\n",
      "is\n",
      "\n",
      "where X; = n\n",
      "\n",
      "Sii S12 *** Sik\n",
      "$12 822 *** San\n",
      "\n",
      "1 anne (15.4)\n",
      "Sik 82k *** Skk\n",
      "\n",
      "where\n",
      "\n",
      "1 “ a - >\n",
      "ja du — Xa)(Xvj — Xs).\n",
      "=\n",
      "It follows that E(X) = pt. and E(S) = ¥.\n",
      "\n",
      "15.2 Estimating the Correlation\n",
      "\n",
      "Consider n data points from a bivariate distribution:\n",
      "\n",
      "Xu Xi2 ee Xin\n",
      "Xo J 7 \\ X22 J? \\ Xan J”\n",
      "\n",
      "Recall that the correlation between X, and X is\n",
      "\n",
      "p= E(X ae (15.5)\n",
      "\n",
      "The sample correlation (the plug-in estimator) is\n",
      "\n",
      "i oh (a = 1) (Xai =X) (15.6)\n",
      "\n",
      "S189\n",
      "\n",
      " \n",
      "\n",
      "We can construct a confidence interval for p by applying the\n",
      "delta method as usual. However, it turns out that we get a more\n",
      "accurate confidence interval by first constructing a confidence\n",
      "interval for a function 9 = f(p) and then applying the inverse\n",
      "function p The method, due to Fisher, is as follows. Define\n",
      "\n",
      "f(r) = $(lestt +1) —log(1— r))\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-268.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "272\n",
      "\n",
      "1\n",
      "\n",
      "5. Multivariate Models\n",
      "\n",
      "akd let 9 = f(p). The inverse of r is\n",
      "\n",
      "Now do the following steps:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Approximate Confidence Interval for The Correlation\n",
      "\n",
      "1. Compute\n",
      "\n",
      "A= (8) = 5(log(1 +A) —log(t —7)).\n",
      "\n",
      ". Compute the approximate standard error of @ which can\n",
      "\n",
      "be shown to be\n",
      "\n",
      " \n",
      "\n",
      ". An approximate 1 — a confidence interval for 0 = f(p) is\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "(0) = (0- Ly ee ).\n",
      "yn—-3 yn —3\n",
      "\n",
      ". Apply the inverse transformation f~'(z) to get a confi-\n",
      "\n",
      "dence interval for p:\n",
      "\n",
      "C4 _1 eb _y\n",
      "e244 17 ce +1) °\n",
      "\n",
      " \n",
      "\n",
      "15.3 Multinomial\n",
      "\n",
      "Let us now review the Multinomial distribution. Consider\n",
      "drawing a ball from an urn which has balls with k different\n",
      "\n",
      "colors labeled color 1, color 2, ... , color k. Let p = (p1,.--, Pe)\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-269.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "15.3 Multinomial 273\n",
      "\n",
      "where p; > 0 and Sy pj =1 and suppose that p; is the prob-\n",
      "ability of drawing a ball of color 7. Draw n times (independent\n",
      "draws with replacement) and let X = (X,,...,X,) where X; is\n",
      "the number of times that color j appeared. Hence, n = Mis Xj.\n",
      "We say that X has a Multinomial (n,p) distribution. The prob-\n",
      "ability function is\n",
      "\n",
      "= = o Leen 7 Bid\n",
      "f(x; p) (\"Je Py\n",
      "\n",
      "n nt\n",
      ",...Lp ales)\n",
      "\n",
      "Theorem 15.2 Let X ~ Multinomial(n,p). Then the marginal\n",
      "\n",
      "where\n",
      "\n",
      "distribution of X; is X; ~ Binomial(n,p;). The mean and vari-\n",
      "ance of X are\n",
      "\n",
      "mp1\n",
      "EX)=|] :\n",
      "Pk\n",
      "and\n",
      "np, (1 — pi) —NPi pa aa —NP1PK\n",
      "Y= | “we ee) mee\n",
      "—\"PiPk —Np2Pr +++ mpe(1 — De)\n",
      "\n",
      "Proor. That X; ~ Binomial(n,p;) follows easily. Hence,\n",
      "E(.X;) = np; and V(Xj) = np;(1—p;). To compute Cov(.X;, X;)\n",
      "we proceed as follows. Notice that X;+X; ~ Binomial(n, p;+p,)\n",
      "and so V(X; + Xj) = n(p;+p;)(1 — p; — p;)- On the other hand,\n",
      "\n",
      "V(Xi+ Xj) = V(X) + V(Xj) + 2Cow( Xi, X;)\n",
      "npi(1 — pi) + np; (1 — pj) + 2Cov( Xj, X})-\n",
      "\n",
      "Equating this last expression with n(p;+p;)(1 —p; —p;) implies\n",
      "that Cov(X;,X;) = —npip;.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-270.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "274 15. Multivariate Models\n",
      "\n",
      "Theorem 15.3 The maximum likelihood estimator of p is\n",
      "\n",
      "g\\ (x\n",
      "~(\"\\) (*)\\ ix\n",
      "p= al @ l=%.\n",
      "\n",
      "Be Xe\n",
      "\n",
      "Proof. The log-likelihood (ignoring the constant) is\n",
      "\n",
      "k\n",
      "\n",
      "lp) = > Xjlogpy.-\n",
      "\n",
      "jal\n",
      "\n",
      "When we maximize ¢ we have to be careful since we must enforce\n",
      "the constraint that yj pj = 1. We use the method of Lagrange\n",
      "multipliers and instead maximize\n",
      "\n",
      " \n",
      "\n",
      "k\n",
      "A(p) = SX logp; + (Sop; - 1).\n",
      "j=l j\n",
      "Now .\n",
      "240) _ Xi,\n",
      "Op; Dy\n",
      "\n",
      "Setting “ = 0 yields p; = —X;/A. Since dj p; = 1 we see\n",
      "that \\= <n and hence p; = X;/n as claimed. 7\n",
      "\n",
      "Next we would like to know the variability of the MLE . We\n",
      "can either compute the variance matrix of p directly or we can\n",
      "approximate the variability of the mle by computing the Fisher\n",
      "information matrix. These two approaches give the same answer\n",
      "in this case. The direct approach is easy: V(p) = V(X/n) =\n",
      "n~?V(X) and so\n",
      "\n",
      "pa(l — pi) —PiP2 ae —P1PK\n",
      "—Pip2 — pa(L—pe) ++ —P2Pr\n",
      "\n",
      "vip) =>\n",
      "\n",
      "—PiPr pape ++ pe(L — px)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-271.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "15.4 Multivariate Normal 275\n",
      "\n",
      "15.4 Multivariate Normal\n",
      "\n",
      "Let us recall how the multivariate Normal distribution is de-\n",
      "fined. To begin, let\n",
      "XZ\n",
      "Z= |:\n",
      "Zi\n",
      "where Z,,...,Z, ~ N(0,1) are independent. The density of Z\n",
      "is\n",
      "\n",
      "1 ae 1 1\n",
      "fe) = Gane {3} =a {-5:\"=} ;\n",
      "\n",
      "(15.7)\n",
      "The variance matrix of Z is the identity matrix J. We write\n",
      "Z ~ N(0,1) where it is understood that 0 denotes a vector of\n",
      "\n",
      " \n",
      "\n",
      "k zeroes. We say that Z has a standard multivariate Normal\n",
      "distribution.\n",
      "\n",
      "More generally, a vector X has a multivariate Normal distri-\n",
      "bution, denoted by ¥ ~ N(,¥), if its density is\n",
      "\n",
      "fei 0.2) = pam { ge WTEe—1)}\n",
      "cae (27)*/2det(S)!/2 2\n",
      "(15.8)\n",
      "\n",
      "where det(-) denotes the determinant of a matrix, j1 is a vector\n",
      "of length k and ¥ isa k x k symmetric, positive definite matrix.\n",
      "Then E(X) = and V(X) =®. Setting p= 0 and © =I gives\n",
      "back the standard Normal.\n",
      "\n",
      "Theorem 15.4 The following properties hold:\n",
      "1. If Z~ N(0,1) and X=p+D'?Z then X ~ N(p, >).\n",
      "2. If X ~ N(u,=), then D-/?(X — p) ~ N(0, 1).\n",
      "\n",
      "3. If X ~ N(u,%) a is a vector of the same length as X,\n",
      "then a? X ~ N(a™ p,a7Sa).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-272.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276 15. Multivariate Models\n",
      "\n",
      "4. Let\n",
      "Va(X—p)DO'M(X — yp).\n",
      "\n",
      "Then V ~ xz.\n",
      "\n",
      "Suppose we partition a random Normal vector X into two\n",
      "parts X = (X,,X,) We can similarly partition the the mean\n",
      "t= (Ha; fp) and the variance\n",
      "\n",
      "Zaa Lab\n",
      "L= :\n",
      "(se Los )\n",
      "Theorem 15.5 Let X ~ N(u,¥). Then:\n",
      "(1) The marginal distribition of X, is Xq ~ N (pa; Naa):\n",
      "(2) The conditional distribition of X, given Xq = %q is\n",
      "\n",
      "Xp|Xa = Fa ~ N(Ui(a), Eta)\n",
      "\n",
      "where\n",
      "\n",
      "Hy + Eee (Ta — Ha) (15.9)\n",
      "Dap — Deeg Das- (15.10)\n",
      "\n",
      "Theorem 15.6 Given a random sample of size n from a N(p,™)\n",
      "\n",
      "the log-likelihood is (up to a constant not depending on ys or ©)\n",
      "is given by\n",
      "\n",
      "é(p, E) = -5(X- py So'(X—p)- str(S\"\"S) = ; log det(S).\n",
      "\n",
      "The MLE is\n",
      "\n",
      " \n",
      "\n",
      "fi=X and S= (—) 8. (15.11)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-273.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "15.5 Appendix 277\n",
      "15.5 Appendix\n",
      "\n",
      "Proof of Theorem 15.6. Denote the i‘ random vector by X'.\n",
      "The log-likelihood is\n",
      "\n",
      "as kn . n 1 ri\n",
      "Lu, D) = Dad (X*; pd) = ~ sy loa(27)—F log det (=) —5 SOx —}\n",
      "\n",
      "i\n",
      "\n",
      "Now,\n",
      "\n",
      "Sx —plToxXt-p) = SI (x! —X) + (X = p)PD1 (xt =\n",
      "\n",
      "i i\n",
      "\n",
      "iy Xt\n",
      "\n",
      "X)+(X\n",
      "\n",
      "r — p)]\n",
      "\n",
      " \n",
      "\n",
      "i\n",
      "\n",
      "since S7,(X' — X)\"-1(X — y) = 0. Also, notice that (X' —\n",
      "py)? S-1!(X' — 2) is a scalar, so\n",
      "\n",
      "Sox py ee -~ jp) = Le [(X? = p)7E1 (X= p)]\n",
      "- Dee “TX! = p(X\" = py)\"\n",
      "\n",
      "— dr [ xi =p) (Xe? = pe\n",
      "= ntr[=-'S|\n",
      "\n",
      "and the conclusion follows.\n",
      "15.6 Exercises\n",
      "\n",
      "1. Prove Theorem 15.1.\n",
      "\n",
      "2. Find the Fisher information matrix for the MLE of a Multi-\n",
      "nomial.\n",
      "\n",
      "3. Prove Theorem 15.5.\n",
      "\n",
      "4. (Computer Experiment. Write a function to generate nsim\n",
      "observations from a Multinomial(n, p) distribution.\n",
      "\n",
      "SO [Xt = XEN! = X)] + n(X-;\n",
      "\n",
      "Toa\n",
      "\n",
      "(X\n",
      "\n",
      "Lb).\n",
      "\n",
      "—h)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-274.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "278\n",
      "\n",
      "15. Multivariate Models\n",
      "\n",
      "5. (Computer Experiment. Write a function to generate nsim\n",
      "\n",
      "observations from a Multivariate normal with given mean\n",
      "} and covariance matrix D.\n",
      "\n",
      ". (Computer Experiment. Generate 1000 random vectors from\n",
      "\n",
      "a N(y,%) distribution where\n",
      "\n",
      "(2). 2(29)\n",
      "\n",
      "Plot the simulation as a scatterplot. Find the distribution\n",
      "of X2|X, = 2, using theorem 15.5. In particular, what is\n",
      "the formula for E(X2|X, = 2)? Plot E(X2|X, = 21) on\n",
      "your scatterplot. Find the correlation p between X; and\n",
      "X5. Compare this with the sample correlations from your\n",
      "simulation. Find a 95 per cent confidence interval for p.\n",
      "Estimate the covariance matrix DU.\n",
      "\n",
      "- Generate 100 random vectors from a multivariate Normal\n",
      "\n",
      "with mean (0, 2)\" and variance\n",
      "\n",
      "(31),\n",
      "\n",
      "Find a 95 per cent confidence interval for the correlation\n",
      "p. What is the true value of p?\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-275.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "16\n",
      "\n",
      "Inference about Independence\n",
      "\n",
      "In this chapter we address the following questions:\n",
      "\n",
      "(1) How do we test if two random variables are independent?\n",
      "(2) How do we estimate the strength of dependence between two\n",
      "random variables?\n",
      "\n",
      "When Y and Z are not independent, we say that they are\n",
      "dependent or associated or related. If Y and Z are associ-\n",
      "ated, it does not imply that Y causes Z or that Z causes Y. If\n",
      "Y does cause Z then changing Y will change the distribution of\n",
      "Z, otherwise it will not change the distribution of Z.\n",
      "\n",
      "For example, quitting smoking Y will reduce your probabil-\n",
      "ity of heart disease Z. In this case Y does cause Z. As another\n",
      "example, owning a TV Y is associated with having a lower in-\n",
      "cidence of starvation Z. This is because if you own a TV you\n",
      "are less likely to live in an impoverished nation. But giving a\n",
      "starving person will not cause them to stop being hungry. In\n",
      "this case, Y and Z are associated but the relationship is not\n",
      "causal.\n",
      "\n",
      "This is page 279\n",
      "Printer: Opaque this\n",
      "\n",
      "Recall that we write\n",
      "Y IZ to mean that\n",
      "Y and Z are inde-\n",
      "pendent.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-276.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "280 16. Inference about Independence\n",
      "\n",
      "We defer detailed discussion of the important question of cau-\n",
      "sation until a later chapter.\n",
      "\n",
      "16.1 Two Binary Variables\n",
      "\n",
      "Suppose that Y and Z are both binary. Consider a data set\n",
      "(%,Z,),---, (Yn, Zn). Represent the data as a two-by-two table:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "where the Xj; represent counts:\n",
      "\n",
      "Xj; = number of observations for which Y = 7 and Z = j.\n",
      "\n",
      "The dotted subscripts denote sums. For example, Xj. = My Xe\n",
      "This is a convention we use throughout the remainder of the\n",
      "book. Denote the corresponding probabilities by:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "where pj; P(Z = i,Y = j). Let X (Xoo, Xo1, X10, Xu1)\n",
      "denote the vector of counts. Then X ~ Multinomial(n, p) where\n",
      "P = (Poo, Po1, P10, Pi1)- It is now convenient to introduce two new\n",
      "parameters.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-277.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "16.1 Two Binary Variables 281\n",
      "\n",
      " \n",
      "\n",
      "Definition 16.1 The odds ratio is defined to be\n",
      "\n",
      "— PooPir\n",
      "\n",
      "w 8 16.1\n",
      "PoiPi0 ( )\n",
      "\n",
      "The log odds ratio is defined to be\n",
      "7 = log(w). (16.2)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 16.2 The following statements are equivalent:\n",
      "\n",
      "  \n",
      "\n",
      "& y=.\n",
      "4. For i,j € {0,1},\n",
      "\n",
      "Dij = Die -5- (16.3)\n",
      "Now consider testing\n",
      "\n",
      "Hy: YUZ versus H,: Y WZ.\n",
      "\n",
      "First we consider the likelihood ratio test. Under H,, X ~\n",
      "Multinomial(n, p) and the MLE is p= X/n. Under Ho, we again\n",
      "have that X ~ Multinomial(n,p) but p is subject to the con-\n",
      "straint\n",
      "\n",
      "Diy =PiP-j, J = 0,1.\n",
      "This leads to the following test.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 16.3 (Likelihood Ratio Test for Independence in a 2-by-2 table)\n",
      "Let\n",
      "dt XX\n",
      "_ - agX--\n",
      "r=2 » » Xj; log (==) : (16.4)\n",
      "\n",
      "Under Ho, T ~ x2. Thus, an approximate level a test is\n",
      "obtained by rejecting Ho then T > Xe\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-278.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "282 16. Inference about Independence\n",
      "\n",
      "Another popular test for independence is Pearson’s x? test.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 16.4 (Pearson’s y” test for Independence in a 2-by-2 table)\n",
      "\n",
      "Let i 4\n",
      "Xj; — Bij)?\n",
      "v=y ry Me Fl (16.5)\n",
      "i=0 j=0 ag\n",
      "where Bx\n",
      "By =\n",
      "n\n",
      "\n",
      "Under Ho, U ~ x7. Thus, an approximate level a test is\n",
      "obtained by rejecting Hy then U > a\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Here is the intuition for the Pearson test. Under Ho, pij =\n",
      "\n",
      "Pip.j, 80 the maximum likelihood estimator of p,; under Ho is\n",
      "a we, . MEM\n",
      "Pig = PicP Gj = “He\n",
      "\n",
      "Thus, the expected number of observations in the (i,j) cell is\n",
      "\n",
      "  \n",
      "\n",
      "Ey = np =\n",
      "\n",
      "n\n",
      "\n",
      "The statistic U compares the observed and expected counts.\n",
      "\n",
      "Example 16.5 The following data from Johnson and Johnson\n",
      "(1972) relate tonsillectomy and Hodgkins disease. (The data are\n",
      "actually from a case-control study; we postpone discussion of\n",
      "this point until the next section.)\n",
      "\n",
      "| Hodgkins Disease No Disease |\n",
      "\n",
      "Tonsillectomy 90 165 255\n",
      "No Tonsillectomy | 84 307 391\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Total 14 472 646\n",
      "\n",
      "We would like to know if tonsillectomy is related to Hodgkins\n",
      "disease. The likelihood ratio statistic is T = 14.75 and the p-\n",
      "value is P(x? > 14.75) = .0001. The x? statistic is U = 14.96\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-279.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "16.1 Two Binary Variables 283\n",
      "\n",
      "and the p-value is P(x{ > 14.96) = 0001. We reject the null\n",
      "hypothesis of independence and conclude that tonsillectomy is\n",
      "associated with Hodgkins disease. This does not mean that ton-\n",
      "sillectomies cause Hodgkins disease. Suppose, for example, that\n",
      "doctors gave tonsillectomies to the most seriosuly ill patients.\n",
      "Then the assocation between tonsillectomies and Hodgkins dis-\n",
      "ease may be due to the fact that those with tonsillectomies were\n",
      "the most ill patients and hence more likely to have a serious\n",
      "disease.\n",
      "\n",
      "We can also estimate the strength of dependence by estimat-\n",
      "ing the odds ratio 7 and the log-odds ratio y.\n",
      "Theorem 16.6 The MLE ’s of ) and y are\n",
      "XoXi ~ rT\n",
      "= ——, J=logy. 16.6\n",
      "XoXo a (166)\n",
      "\n",
      "The asymptotic stanrad errors (computed from the delta method)\n",
      "are\n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      "(16.7)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "(16.8\n",
      "\n",
      "Remark 16.7 For small sample sizes, o and ¥ can have a very\n",
      "large variance. In this case, we often use the modified estimator\n",
      "ga (Xoo 5) Sut 5)\n",
      "v=o (16.9\n",
      "\n",
      "(Xo +3) (Mio + 3)\n",
      "\n",
      "Yet another test for indepdnence is the Wald test for y = 0\n",
      "given by W = (7—0)/sé (7). A 1—a confidence interval for 7 is\n",
      "7+ 2a 286 (7). A 1—a confidence interval for y can be obtained\n",
      "in two ways. First, we could use ~ + 2/286 (w). Second, since\n",
      "w =e\" we could use\n",
      "\n",
      " \n",
      "\n",
      "exp {7 + zaj2se (7) }. (16.10\n",
      "\n",
      "This second method is usually more accurate.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-280.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284 16. Inference about Independence\n",
      "\n",
      "Example 16.8 In the previous example,\n",
      "=~ 90 x 307\n",
      "jj ee a\n",
      "165 x 84 99\n",
      "and\n",
      "F =log(1.99) = .69.\n",
      "\n",
      "So tonsillectomy patients were twice as likely to have Hodgkins\n",
      "disease. The standard error of 7 is\n",
      "\n",
      " \n",
      "\n",
      "The Wald statistic is W = .69/.18 = 3.84 whose p-value is\n",
      "P(|Z| > 3.84) = .0001, the same as the other tests. A 95 per\n",
      "cent confidence interval for y is 7 + 2(.18) = (.33, 1.05). A 95\n",
      "per cent confidence interval for w is (e8, e!) = (1.39, 2.86).\n",
      "\n",
      "16.2 Interpreting The Odds Ratios\n",
      "\n",
      "Suppose event A as probability P(A). The odds of A are defined\n",
      "as\n",
      "\n",
      "odds(A) = TET\n",
      "\n",
      "It follows that ;\n",
      "P(A) = ' slag) ;\n",
      "+ odds(A)\n",
      "Let E be the event that someone is exposed to something\n",
      "(smoking, radiation, etc) and let D be the event that they get\n",
      "a disease. The odds of getting the disease given that you are\n",
      "\n",
      "exposed are\n",
      "PIE)\n",
      "1—-P(D|E)\n",
      "\n",
      "and the odds of getting the disease given that you are not ex-\n",
      "\n",
      "odds(D|E) =\n",
      "\n",
      "posed are\n",
      "PDE)\n",
      "\n",
      "odds(D|E*) = TPE)\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-281.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "16.2 Interpreting The Odds Ratios 285\n",
      "\n",
      "The odds ratio is defined to be\n",
      "\n",
      "_ odds(D|E)\n",
      "oe odds(D|E*)”\n",
      "\n",
      "If = 1 then disease probability is the same for exposed and un-\n",
      "exposed. This implies that these events are independent. Recall\n",
      "that the log-odds ratio is defined as y = log(z). Independence\n",
      "corresponds to y = 0.\n",
      "\n",
      "Consider this table of probabilities:\n",
      "\n",
      "Denote the data by\n",
      "\n",
      " \n",
      "\n",
      "Ee\n",
      "B\n",
      "Now\n",
      "P(D|E)= Pu and P(D|E*) = Po.\n",
      "Po+Pu Poo + Por\n",
      "and so\n",
      "odds(D|E) = Pu and odds(D|E*) = Bow\n",
      "Pio Poo\n",
      "and therefore,\n",
      "Pi P11Po00\n",
      "PoiPio™\n",
      "\n",
      "To estimate the parameters, we have to first consider how the\n",
      "data were collected. There are three methods.\n",
      "\n",
      "MULTINOMIAL SAMPLING. We draw a sample from the pop-\n",
      "ulation and, for each person, record their exposure and disease\n",
      "status. In this case, ¥ = (Xoo, Xo1, X10, X11) ~ Multinomial(n, p).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-282.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "286 16. Inference about Independence\n",
      "\n",
      "We then estimate the probabilities in the table by pj; = Xj;/n\n",
      "and\n",
      "i PirPoo as XuXoo\n",
      "Po1Pio Xoi X10\n",
      "PROSPECTIVE SAMPLING. (COHORT SAMPLING). We get\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "some exposed and unexposed people and count the number with\n",
      "disease in each group. Thus,\n",
      "\n",
      "Xo1 ~ Binomial(Xo., P(D|E*))\n",
      "Xi, ~ Binomial(X,., P(D|£)).\n",
      "\n",
      "We should really write xp. and x. instead of Xo. and Xj. since\n",
      "in this case, these are fixed not random but for notational sim-\n",
      "plicity I'll keep using capital letters. We can estimate P(D|E)\n",
      "and P(D|E°) but we cannot estimate all the probabilities in the\n",
      "table. Still, we can estimate ¢% since 7 is a function of P(D|E)\n",
      "and P(D|E*). Now\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Fs} Xu\n",
      "P(D|E) = X,.\n",
      "and x\n",
      "= cy _ Aor\n",
      "P(D|E) = Xe\n",
      "Thus,\n",
      "§= Xi Xoo\n",
      "Xo X10\n",
      "\n",
      "just as before.\n",
      "\n",
      "CASE-CONTROL (RETROSPECTIVE) SAMPLING. Here we get\n",
      "some diseased and non-diseased people and we observe how\n",
      "many are exposed. This is much more efficient if the disease\n",
      "is rare. Hence,\n",
      "\n",
      "Xio ~ Binomial(X., P(E|D*))\n",
      "Xi ~ Binomial(X.;, P(E|D)).\n",
      "\n",
      "From these data we can estimate P(E|D) and P(E|D*). Sur-\n",
      "prisingly, we can also still estimate 7. To understand why, note\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-283.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "16.2 Interpreting The Odds Ratios 287\n",
      "\n",
      "that\n",
      "\n",
      "Po odds(B|D) = 2.\n",
      "\n",
      "P(E|D) = —?4 = _,\n",
      "Poi + pit Poi\n",
      "\n",
      "~ por + pi’\n",
      "\n",
      "By a similar argument,\n",
      "\n",
      "1-P(E|D)\n",
      "\n",
      "odds(B|D*) = ay\n",
      "Poo\n",
      "\n",
      "Hence,\n",
      "odds(E|D) — pupoo _\n",
      "\n",
      "odds(E|D*) ~~ puPio\n",
      "From the data, we form the following estimates:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "a Xu i Xo => Xy — Xg\n",
      "P(B|D) = —, 1—P(E|D) = —, odds(E|D) = —, odds(E|D*°) = —.\n",
      "(ED) =, 1-Plz|D) = 5%, oddb(B|D) = F4, odas(e|D9) =\n",
      "Therefore,\n",
      "— XoXn\n",
      "XoXo\n",
      "\n",
      "So in all three data collection methods, the estimate of w turns\n",
      "out to be the same.\n",
      "\n",
      "It is tempting to try to estimate P(D|E) — P(D|E*). Ina\n",
      "case-control design, this quantity is not estimable. To see this,\n",
      "we apply Bayes’ theorem to get\n",
      "P(E|D)P(D) _ P(E*|D)P(D)\n",
      "\n",
      "P(DIE) - P(DIE) = FES\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Because of the way we obtained the data, P(D) is not estimable\n",
      "from the data. However, we can estimate € = P(D|E£)/P(D|E°),\n",
      "which is called the relative risk, under the rare disease as-\n",
      "sumption.\n",
      "Theorem 16.9 Let € = P(D|E)/P(D|E*). Then\n",
      "\n",
      "wv\n",
      "\n",
      "oe\n",
      "\n",
      "&\n",
      "\n",
      "as P(D) > 0.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-284.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "288 16. Inference about Independence\n",
      "\n",
      "Thus, under the rare disease assumption, the relative risk is\n",
      "approximately the same as the odds ratio and, as we have seen,\n",
      "we can estimate the ods ratio.\n",
      "\n",
      "In a randomized experiment, we can interpret a strong associ-\n",
      "ation, that is 7 # 1, as a causal relationship. In an observational\n",
      "(non-randomized) study, the association can be due to other\n",
      "unobserved confounding variables. We'll discuss causation in\n",
      "more detail later.\n",
      "\n",
      "16.3 Two Discrete Variables\n",
      "\n",
      "Now suppose that Y € {1,...,/} and Z € {1,..., J} are two\n",
      "discrete variables. The data can be represented as an I — by — J\n",
      "table of counts:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Z=i| Xa Xg ot My ct Xi\n",
      "Z=I| Xn Xp Xy eX :\n",
      "x <<: = X = Hy | 2\n",
      "\n",
      "Consider testing Hyp: Y IZ versus H,: Y WZ.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-285.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "16.3 Two Discrete Variables 289\n",
      "\n",
      " \n",
      "\n",
      "of i\n",
      "¥\n",
      "\n",
      " \n",
      "\n",
      "Theorem 16.10 Let\n",
      "\n",
      "The limiting distribution of T under the null hypothesis\n",
      "\n",
      "Asymptotically, under Ho, U has a x2, distribution where\n",
      "y=(I-1)(J—-1).\n",
      "\n",
      " \n",
      "\n",
      "rT J\n",
      "Xi X.,\n",
      "T=2 3 Xyloe (AE). 6.11\n",
      "\n",
      "i=1 j=l 4\n",
      "\n",
      "independence is x? where v = (I—1)(J—1). Pearson’s\n",
      "test statistic is\n",
      "\n",
      "U= yr ae a. (16.12)\n",
      "\n",
      "#=1 j=l\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 16.11 These data are from a study by Hancock et al\n",
      "(1979). Patients with Hodkins disease are classified by their re-\n",
      "sponse to treatment and by histological type.\n",
      "Type | Positive Response Partial Response No Response\n",
      "LP |% 18 12 104\n",
      "NS | 68 16 12 96\n",
      "MC | 154 54 58 266\n",
      "LD | 18 10 44 72\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The x? test statistic is 75.89 with 2x 3=6 degrees of freedom.\n",
      "The p-value is P(x3 > 75.89) % 0. The likelihood ratio test\n",
      "statistic is 68.30 with 2x 3=6 degrees of freedom. The p-value\n",
      "is P(x > 68.30) © 0. Thus there is strong evidence that reponse\n",
      "\n",
      "to trea\n",
      "\n",
      "tment and histological type are associated.\n",
      "\n",
      "There are a variety of ways to quantify the strength of depen-\n",
      "\n",
      "dence\n",
      "\n",
      "between two discrete variables Y and Z. Most of them\n",
      "\n",
      "are not very intuitive. The one we shall use is not standard but\n",
      "\n",
      "is more interpretable.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-286.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "290 16. Inference about Independence\n",
      "We define\n",
      "\n",
      "6(¥,Z) = max [Pyz(¥ € A,Z € B)—Py(¥ € A)~PA(Z € B)|\n",
      "\n",
      "(16.13)\n",
      "where the maximum is over all pairs of events A and B.\n",
      "\n",
      "Theorem 16.12 Properties of 6:\n",
      "L0<6(¥,2Z) <1.\n",
      "2. O(Y,Z) =0 if and only if Y UZ.\n",
      "\n",
      "3. The following identity holds:\n",
      "\n",
      "3(X,Y\n",
      "\n",
      " \n",
      "\n",
      "<P - Pip: (16.14)\n",
      "\n",
      "i Me\n",
      "\n",
      "4. The MLE of 6 is\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "tod\n",
      "Y=5 >) [a- 7.2, (16.15)\n",
      "i=l j=l\n",
      "where\n",
      "a My 2 Xp og My\n",
      "Di » Di. » By z\n",
      "n n n\n",
      "\n",
      "The interpretation of 6 is this: if one person makes probability\n",
      "statements assuming independence and another person makes\n",
      "probability statements without assuming independence, their\n",
      "probability statements may differ by as much as 0. Here is a\n",
      "suggested scale for interpreting 6:\n",
      "\n",
      "0<d<.01 negligible association\n",
      "\n",
      "01 <6 <.05 non-negligible association\n",
      "05 <6<.10 substantial association\n",
      "\n",
      "6 > .10 very strong association\n",
      "\n",
      "A confidence interval for 6 can be obtained by bootstrapping.\n",
      "The steps for bootstrapping are:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-287.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "16.4 Two Continuous Variables 291\n",
      "1. Draw X* ~ Multinomial(n, p);\n",
      "\n",
      ". Compute ij, Dj. Py.\n",
      "\n",
      "nN\n",
      "\n",
      "3. Compute 6*.\n",
      "\n",
      "4. Repeat.\n",
      "\n",
      "Now we use any of the methods we learned earlier for construct-\n",
      "ing bootstrap confidence intervals. However, we should not use\n",
      "a Wald interval in this case. The reason is that if Y Il Z then\n",
      "6 = 0 and we are on the boundary of the parameter space. In\n",
      "this case, the Wald method is not valid.\n",
      "\n",
      "Example 16.13 Returning to Example 16.11 we find that 6= 11.\n",
      "Using a pivotal bootstrap with 10,000 bootstrap samples, a 95\n",
      "per cent confidence interval for 6 is (.09,.22). Our conclusion\n",
      "is that the association between histological type and response is\n",
      "substantial.\n",
      "\n",
      "16.4 Two Continuous Variables\n",
      "\n",
      "Now suppose that Y and Z are both continuous. If we assume\n",
      "that the joint distribution of Y and Z is bivariate Normal, then\n",
      "we mesure the dependence between Y and Z by means of the\n",
      "correaltion coefficient p. Tests, estimates and confidence inter-\n",
      "vals for p in the Normal case are given in the previous chapter.\n",
      "If we do not assume Normality then we need a nonparametric\n",
      "method for assessing dependence.\n",
      "Recall that the correaltion is\n",
      "\n",
      "_ E((Xi — #4) (X2 — bi)\n",
      "\n",
      "102\n",
      "A nonparametric estimator of p is the plug-in estimator which\n",
      "is\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "pe Ea Xu FH) as - Ke)\n",
      "VDm Ou — HPD he - )?\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-288.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "292 16. Inference about Independence\n",
      "\n",
      "which is just the sample correlation. A confidence interval can be\n",
      "constructed using the bootstrap. A test for p = 0 can be based\n",
      "on the Wald test using the bootstrap to estimate the standard\n",
      "error.\n",
      "\n",
      "The plug-in approach is useful for large samples. For small\n",
      "samples, we measure the correlation using the Spearman rank\n",
      "correaltion coefficient f;. We simply replace the data by their\n",
      "ranks — ranking each variable separately — then we compute the\n",
      "correaltion coefficient of the ranks. To test the null hypothesis\n",
      "that p\n",
      "pothesis. This can be obtained easily by simulation. We fix the\n",
      "ranks of the first variable as 1,2,...,n. The ranks of the second\n",
      "\n",
      "s = 0 we need the distribution of fg under the null hy-\n",
      "\n",
      "variable are chosen at random from the set of n! possible order-\n",
      "ings. Then we compute the correlation. This is repeated many\n",
      "times and the resulting distribution Po is the null distribution of\n",
      "fs. The p-value for the test is Po(|R| > |fs|) where R is drawn\n",
      "from the null distribution Po.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 16.14 The following data (Snedecor and Cochran, 1980,\n",
      "p. 191) are systolic blood pressure X, and are diastolic blood\n",
      "pressure Xo in millimeters:\n",
      "\n",
      "X,| 100 105 110 110 120 120 125 130 130 150 170 195\n",
      "\n",
      "X2.| 65 65 7 TW 7 80 1% 82 80 90\n",
      "\n",
      "The sample correlation is p = .88. The bootstrap standard\n",
      "error is .046 and the Wald test statistic is .88/.046 = 19.23.\n",
      "The p-value is near 0 giving strong evidence that the correlation\n",
      "is not 0. The 95 per cent pivotal bootstrap confidence interval is\n",
      "(.78,.94). Because the sample size is small, consider Spearman’s\n",
      "rank correlation. In ranking the data, we will use average ranks\n",
      "if there are ties. So if the third and fourth lowest numbers are\n",
      "the same, they each get rank 3.5. The ranks of the data are:\n",
      "\n",
      "95\n",
      "\n",
      "90\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-289.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "16.5 One Continuous Variable and One Discrete 293\n",
      "\n",
      "xX, it 23.5 8.5 5.5 5.5 7 85 85 10 I\n",
      "Xo] 25 15 4.5 8 6 15 4.5 9 7.5 10.5 12\n",
      "\n",
      "The rank correlation is pg = .94 and the p-value for testing the\n",
      "null hypothesis that there is no correaltion is Po(|R| > .94) +0\n",
      "which is was obtained by simulation.\n",
      "\n",
      "16.5 One Continuous Variable and One Discrete\n",
      "\n",
      "Suppose that Y € {1,...,I} is discrete and Z is continuous.\n",
      "Let F,(z) = P(Z < 2|Y = i) denote the CDF of Z conditional\n",
      "on Y =i.\n",
      "\n",
      "Theorem 16.15 When Y € {1,...,I} is discrete and Z is con-\n",
      "tinuous, then Y I Z if and only if Fy =-++-= Fy.\n",
      "\n",
      "It follows from the previous theorem that to test for indepen-\n",
      "dence, we need to test\n",
      "\n",
      "Ay: Fi =-:-=F;, versus H,: not Hp.\n",
      "\n",
      "For simplicity, we consider the case where J = 2. To test the\n",
      "null hypothesis that F; = F, we will use the two sample\n",
      "Kolmogorov-Smirnov test. Let n, denote the nimber of ob-\n",
      "servations for which Y; = 1 and let nz denote the nimber of\n",
      "observations for which Y; = 2. Let\n",
      "\n",
      "and\n",
      "\n",
      " \n",
      "\n",
      "denote the empirical distribution function of Z given Y = 1 and\n",
      "Y = 2 respectively. Define the test statistic\n",
      "\n",
      "D=sup|Fi(2) — F(x).\n",
      "\n",
      "12\n",
      "10.5\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-290.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "The data here are\n",
      "an approximate re-\n",
      "creation using the\n",
      "information in the\n",
      "article.\n",
      "\n",
      "294 16. Inference about Independence\n",
      "\n",
      "Theorem 16.16 Let\n",
      "\n",
      "Under the null hypthesis that Fy = F»,\n",
      "\n",
      "lim P ( /S™ p< ‘) = H(t).\n",
      "n—00 ny + ng\n",
      "\n",
      "It follows from the theorem that an approximate level a test\n",
      "is obtained by rejecting Hp when\n",
      "\n",
      ",/—_p > #\"(1-a).\n",
      "ny + Ng\n",
      "\n",
      "16.6 Bibliographic Remarks\n",
      "\n",
      "Johnson, $.K. and Johnson, R.E. (1972). New England Journal\n",
      "of Medicine. 287. 1122-1125.\n",
      "Hancock, B.W. (1979). Clinical Oncology, 5, 283-297.\n",
      "\n",
      "16.7 Exercises\n",
      "\n",
      "1. Prove Theorem 16.2.\n",
      "2. Prove Theorem 16.3.\n",
      "3. Prove Theorem 16.9.\n",
      "4. Prove equation (16.14).\n",
      "\n",
      "5. The New York Times (January 8, 2003, page A12) re-\n",
      "ported the following data on death sentencing and race,\n",
      "from a study in Maryland:\n",
      "\n",
      "Death Sentence No Death Sentence\n",
      "Black Victim 14 641\n",
      "White Victim 62 594\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-291.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243B75A2388>\n",
      "16.7 Exercises 295\n",
      "\n",
      "Analyze the data using the tools from this Chapter. In-\n",
      "terpret the results. Explain why, based only on this infor-\n",
      "mation, you can’t make causal conclusions. (The authors\n",
      "of the study did use much more information in their full\n",
      "report.\n",
      "\n",
      ". Analyze the data on the variables Age and Financial Sta-\n",
      "tus from:\n",
      "http://lib.stat.cmu.edu/DASL/Datafiles/montanadat.html\n",
      "\n",
      ". Estimate the correlation between temperature and lati-\n",
      "tude using the data from\n",
      "http://lib.stat.cmu.edu/DASL/Datafiles/USTemperatures.html\n",
      "Use the correlation coefficient and the Spearman rank cor-\n",
      "realtion. Provide estimates, tests and confidence intervals.\n",
      "\n",
      ". Test whether calcium intake and drop in blood pressure\n",
      "are associated. Use the data in\n",
      "\n",
      "http://lib.stat.cmu.edu/DASL/Datafiles/Calcium.html\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-292.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-293.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80EAB88>\n",
      "17\n",
      "\n",
      "Undirected Graphs and Conditional\n",
      "Independence\n",
      "\n",
      "Graphical models are a class of multivariate statistical models\n",
      "that are useful for representing independence relations. They are\n",
      "also useful develop parsimonious models for multivariate data.\n",
      "\n",
      "To see why parsimonious models are useful in the multivariate\n",
      "setting, consider the problem of estimating the joint distribu-\n",
      "tion of several discrete random variables. Two binary variables\n",
      "Yj, and Y) can be represented as a two-by-two table which corre-\n",
      "sponds to a multinomial with four categories. Similarly, k binary\n",
      "variables Y,,..., Y, correspond to a multinomial with N = 2*\n",
      "categories. When k is even moderately large, N = 2\" will be\n",
      "huge. It can be shown in that case that the MLE is a poor es-\n",
      "timator. The reason is that the data are sparse: there are not\n",
      "enough data to estimate so many parameters. Graphical mod-\n",
      "els often require fewer parameters and may lead to estimators\n",
      "with smaller risk. There are two main types of graphical models:\n",
      "undirected and directed. Here, we introduce undirected graphs.\n",
      "We'll discuss directed graphs later.\n",
      "\n",
      "This is page 297\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-294.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "298 17. Undirected Graphs and Conditional Independence\n",
      "17.1 Conditional Independence\n",
      "\n",
      "Underlying graphical models is the concept of conditional inde-\n",
      "pendence.\n",
      "\n",
      " \n",
      "\n",
      "Definition 17.1 Let X, Y and Z be discrete random vari-\n",
      "ables. We say that X and Y are conditionally inde-\n",
      "pendent given Z, written X IY | Z, if\n",
      "\n",
      "P(X =2,Y =y|Z =z) = P(X =2|Z =2z)P(Y =y|Z = 2)\n",
      "(17.1)\n",
      "for alla,y,z.If X,Y and Z are continuous random vari-\n",
      "ables, we say that X and ¥Y are conditionally independent\n",
      "given Z if\n",
      "\n",
      "fxyviz(a, yl2) = fxiz(al2) fyiz(yl2)-\n",
      "\n",
      "for dll x, y andz.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Intuitively, this means that, once you know Z, Y provides no\n",
      "extra information about X.\n",
      "\n",
      "The conditional independence relation satisfies some basic\n",
      "properties.\n",
      "\n",
      "Theorem 17.2 The following implications hold:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "XILY|Z = YUX|Z\n",
      "\n",
      "XUY|Z and U=h(X) = UMY|Z\n",
      "XIY|Z and U=h(X) = XUY|(Z,U)\n",
      "NXUY |Z and XUW|(Y,Z) = XU(WY)|Z\n",
      "\n",
      "XUY|Z and XUZ|Y => XU(,2).\n",
      "\n",
      "The last property requires the assumption that all events have\n",
      "positive probability; the first four do not.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-295.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "17.2 Undirected Graphs 299\n",
      "\n",
      "xX Z\n",
      "\n",
      "FIGURE 17.1. A graph with vertices V = {X,Y, Z}. The edge set is\n",
      "B={(X,Y),(¥,Z)}.\n",
      "\n",
      "17.2 Undirected Graphs\n",
      "\n",
      "An undirected graph G = (V,£) has a finite set V of ver-\n",
      "tices (or nodes) and a set E of edges (or arcs) consisting of\n",
      "pairs of vertices. The vertices correspond to random variables\n",
      "X,Y,Z,... and edges are written as unordered pairs. For exam-\n",
      "ple, (X,Y) € E means that X and Y are joined by an edge.\n",
      "\n",
      "An example of a graph is in Figure 17.1.\n",
      "\n",
      "Two vertices are adjacent, written X ~ Y, if there is an edge\n",
      "between them. In Figure 17.1, X and Y are adjacent but X and\n",
      "Z are not adjacent. A sequence Xo,...,Xn is called a path if\n",
      "Xj-1 ~ X; for each 7. In Figure 17.1, X,Y, Z is a path. A graph\n",
      "is complete if there is an edge between every pair of vertices.\n",
      "A subset U C V of vertices together with their edges is called a\n",
      "subgraph.\n",
      "\n",
      "If A,B and C are three distinct subsets if V, we say that C\n",
      "separates A and B if every path from a variable in A toa\n",
      "variable in B intersects a variable in C. In Figure 17.2 {Y,W}\n",
      "and {Z} are separated by {X}. Also, W and Z are separated\n",
      "by {X,Y}.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-296.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 17. Undirected Graphs and Conditional Independence\n",
      "Ww x\n",
      "\n",
      "FIGURE 17.2. {Y,W} and {Z} are separated by {X}. Also, W and\n",
      "Z are separated by {X,Y}.\n",
      "\n",
      "y\n",
      "\n",
      "x Z\n",
      "\n",
      "FIGURE 17.3. X II Z|Y.\n",
      "\n",
      "17.3 Probability and Graphs\n",
      "\n",
      "Let V be a set of random variables with distribution P. Con-\n",
      "struct a graph with one vertex for each random variable in V.\n",
      "Suppose we omit the edge between a pair of variables if they are\n",
      "independent given the rest of the variables:\n",
      "\n",
      "no edge between X andY <=> X IY|rest\n",
      "\n",
      "where “rest” refers to all the other variables besides X and Y.\n",
      "This type of graph is called a pairwise Markov graph. Some\n",
      "examples are shown in Figures 17.3, 17.4 17.6 and 17.5.\n",
      "\n",
      "The graph encodes a set of pairwise conditional independence\n",
      "relations. These relations imply other conditional independence\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-297.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "17.3 Probability and Graphs 301\n",
      "\n",
      "xX Z\n",
      "\n",
      "FIGURE 17.4. No implied independence relations.\n",
      "\n",
      "FIGURE 17.5. X 1Z|{Y,W} and ¥ I W|{X, Z}.\n",
      "\n",
      "x ¥ Z Ww\n",
      "\n",
      "FIGURE 17.6. Pairwise independence implies that X II Z|{Y,W}.\n",
      "But is X I. Z|Y?\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-298.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "302 17. Undirected Graphs and Conditional Independence\n",
      "\n",
      "relations. How can we figure out what they are? Fortunately, we\n",
      "can read these other conditional independence relations directly\n",
      "from the graph as well, as is explained in the next theorem.\n",
      "\n",
      "Theorem 17.3 Let G = (V,£) be a pairwise Markov graph for\n",
      "a distribution P. Let A,B and C be distinct subsets of V such\n",
      "that C separates A and B. Then AIL B\\C.\n",
      "\n",
      "Remark 17.4 If A and B are not connected (i.e. there is no path\n",
      "from A to B) then we may regard A and B as being separated\n",
      "by the empty set. Then Theorem 17.3 implies that AIL B.\n",
      "\n",
      "The independence condition in Theorem 17.3 is called the\n",
      "global Markov property. We thus see that the pairwise and\n",
      "global Markov properties are equivalent. Let us state this more\n",
      "precisely. Given a graph G, let Myair(G) be the set of distri-\n",
      "butions which satisfy the pairwise Markov property: thus P €\n",
      "Mpair(G) if, under P, X IL Y|rest if and only if there is no edge\n",
      "between X and Y. Let Mgiovai(G) be the set of distributions\n",
      "which satisfy the global Markov property: thus P € Mpair(G) if,\n",
      "under P, ALI BIC if and only if C separates A and B.\n",
      "\n",
      "Theorem 17.5 Let G be a graph. Then, Mpair(G) = Mgiovai(G).\n",
      "\n",
      "This theorem allows us to construct graphs using the simpler\n",
      "pairwise property and then we can deduce other independence\n",
      "relations using the global Markov property. Think how hard this\n",
      "would be to do algebraically. Returning to 17.6, we now see that\n",
      "XUZ|Y and YUW|Z.\n",
      "\n",
      " \n",
      "\n",
      "Example 17.6 Figure 17.7 implies that X UY, X IZ and X I\n",
      "(Y, Z).\n",
      "\n",
      "Example 17.7 Figure 17.8 implies that X ILW|(Y,Z) and X II\n",
      "ZY.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-299.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "17.3 Probability and Graphs\n",
      "\n",
      "Xe Z\n",
      "\n",
      "FIGURE 17.7. XY, XZ and XI(Y, Z).\n",
      "\n",
      "x Y\n",
      "\n",
      "Ww\n",
      "\n",
      "Z\n",
      "\n",
      "FIGURE 17.8. X IW|(Y,Z) and X 1Z|Y.\n",
      "\n",
      "303\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-300.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "304 17. Undirected Graphs and Conditional Independence\n",
      "17.4 Fitting Graphs to Data\n",
      "\n",
      "Given a data set, how do we find a graphical model that fits\n",
      "the data. Some authors have devoted whole books to this sub-\n",
      "ject. We will only treat the discrete case and we will consider a\n",
      "method based on log-linear models which are the subject of\n",
      "the next chapter.\n",
      "\n",
      "17.5 Bibliographic Remarks\n",
      "\n",
      "Thorough treatments of undirected graphs can be found in Whit-\n",
      "taker (1990) and Lauritzen (1996). Some of the exercises below\n",
      "are adapted from Whittaker (1990).\n",
      "\n",
      "17.6 Exercises\n",
      "\n",
      "1. Consider random variables (X,,X2,X3). In each of the\n",
      "following cases, draw a graph that has the given indepen-\n",
      "dence relations.\n",
      "\n",
      "(a) X; X53 | Xo.\n",
      "(b) Xy HE Xp | Xg and Xy IX | Xo.\n",
      "\n",
      "(c) X, IX, | X3 and X, UX; | Xz and X, UX; |X).\n",
      "\n",
      "2. Consider random variables (X,, X), X3, X,). In each of the\n",
      "following cases, draw a graph that has the given indepen-\n",
      "dence relations.\n",
      "\n",
      " \n",
      "\n",
      "(a) X; UX | Xo, X, and X, WX, | Xo, X3 and Xy IX, |\n",
      "X1,X3.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-301.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "(b) Xi 1 Xs\n",
      "i Ki\n",
      "\n",
      "(c) X, IX\n",
      "\n",
      ". A conditiona\n",
      "\n",
      "17.6 Exercises 305\n",
      "\n",
      "xX, Xe xX\n",
      "X3\n",
      "FIGURE 17.9.\n",
      "XG X, X3 X4\n",
      "\n",
      "FIGURE 17.10.\n",
      "\n",
      "X3,.X4 and X, I X3 | Xo, X4 and Xj ILX; |\n",
      "\n",
      "Xy,X, and Xy WX, | X1,X3.\n",
      "\n",
      "independence between a pair of variables is\n",
      "\n",
      "minimal if it is not possible to use the Separation The-\n",
      "\n",
      "orem to elimi\n",
      "\n",
      "nate any variable from the conditioning set,\n",
      "\n",
      "i.e. from the right hand side of the bar (Whittaker 1990).\n",
      "\n",
      " \n",
      "\n",
      "Write down t\n",
      "\n",
      "he minimal conditional independencies from:\n",
      "\n",
      "(a) Figure 17.9; (b) Figure 17.10; (c) Figure 17.11; (d)\n",
      "\n",
      "Figure 17.12.\n",
      "\n",
      ". Here are breast cancer data on diagnostic center (X;),\n",
      "nuclear grade (X2), and survival (X53):\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-302.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "306 17. Undirected Graphs and Conditional Independence\n",
      "\n",
      "X Xs\n",
      "\n",
      "X, XxX\n",
      "\n",
      "FIGURE 17.11.\n",
      "\n",
      " \n",
      "\n",
      "x Xs Xs\n",
      "\n",
      "FIGURE 17.12.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-303.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "17.6 Exercises 307\n",
      "\n",
      " \n",
      "\n",
      "Xj malignant malignant benign benign\n",
      "\n",
      " \n",
      "\n",
      "X3 died survived died survived\n",
      "xX, Boston 35 59 AT 112\n",
      "Glamorgan 42 77 26 76\n",
      "\n",
      " \n",
      "\n",
      "(a) Treat this as a multinomial and find the maximum\n",
      "likelihood estimator.\n",
      "\n",
      "(b) If someone has a tumour classified as benign at the\n",
      "Glamorgan clinic, what is the estimated probability that\n",
      "they will die? Find the standard error for this estimate.\n",
      "\n",
      "(c) Test the following hypotheses:\n",
      "\n",
      "X, UW X|X3 versus X, WXy|X3\n",
      "X, 0 X3|X2 versus X, WX3|X2\n",
      "X_ I X3|X, versus =X» UX3|X,\n",
      "\n",
      "Based on the results of your tests, draw and interpret the\n",
      "resulting graph.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-304.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-305.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "18\n",
      "Loglinear Models\n",
      "\n",
      "In this chapter we study loglinear models which are useful for\n",
      "modelling multivariate discrete data. There is a strong connec-\n",
      "tion between loglinear models and undirected graphs. Parts of\n",
      "this Chapter draw on the material in Whittaker (1990).\n",
      "\n",
      "18.1 The Loglinear Model\n",
      "\n",
      "Let X = (Xj,...,Xm) be a random vector with probability\n",
      "function\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "f(x) = P(X =2) = P(X, = 2,...,Xm = Im)\n",
      "\n",
      "where t = (21,...,2m). Let r; be the number of values that\n",
      "X; takes. Without loss of generality, we can assume that X; €\n",
      "{Og iesenss f= 1}. Suppose now that we have n such random vec-\n",
      "tors. We can think of the data as a sample from a Multinomial\n",
      "with N=r, Xrg X-+++ X 1, categories. The data can be repre-\n",
      "sented as counts inary X72 X+++XTm table. Let p = (pi,---, Py)\n",
      "denote the multinomial parameter.\n",
      "\n",
      "This is page 309\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-306.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "310 18. Loglinear Models\n",
      "\n",
      "Let S = {1,...,m}. Given a vector # = (21,...,2m) anda\n",
      "subset A CS, let #4 = (a;: j € A). For example, if A = {1,3}\n",
      "then x, = (21,23).\n",
      "\n",
      " \n",
      "\n",
      "Theorem 18.1 The joint probability function f(x) of a single\n",
      "\n",
      "random vector X = (X,,...,Xm) can be written as\n",
      "log f(2) = So vale) (18.1)\n",
      "ACS\n",
      "where the sum is over all subsets A of S = {1,...,m} and the\n",
      "\n",
      "w’s satisfy the following conditions:\n",
      "1. up(x) is a constant;\n",
      "2. For every A CS, w(x) is only a function of x4 and not\n",
      "the rest of the ris.\n",
      "3. Ift€ A and x; =0, then W4(x) =0.\n",
      "\n",
      "The formula in equation (18.1) is called the log-linear ex-\n",
      "pansion of f. Note that this is the probability function for a\n",
      "single draw. Each q4() will depend on some unknown parame-\n",
      "ters 84. Let 8 = (84: A CS) be the set of all these parameters.\n",
      "We will write f(x) = f(x; 8) when we want to estimate the de-\n",
      "\n",
      " \n",
      "\n",
      "pendence on the unknown parameters {.\n",
      "In terms of the multinomial, the parameter space is\n",
      "\n",
      "N\n",
      "P= {p= (p1,---pw) pj 0, Sop) = 1}.\n",
      "j=l\n",
      "\n",
      "This is an N — 1 dimensional space. In the log-linear represen-\n",
      "ation, the parameter space is\n",
      "\n",
      "O= {8 = (f:,.--Bn): 6 =lp),pe P}\n",
      "\n",
      "where {(p) is the set of 9 values associated with p. The set © is\n",
      "a N —1 dimensinal surface in RY. We can always go back and\n",
      "forth betwee the two parameterizations we can write 3 = 5(p)\n",
      "\n",
      "and p= p({).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-307.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.1 The Loglinear Model 311\n",
      "\n",
      "Example 18.2 Let X ~ Bernoulli(p) where 0 < p <1. We can\n",
      "write the probability mass function for X as\n",
      "\n",
      "f(a) =p*(L—p)'* =p ps\n",
      "for x=0,1, where p) =p and pp =1—p. Hence,\n",
      "\n",
      "log f (x) = up(x) + vi (x)\n",
      "where\n",
      "\n",
      "vol) = log(ps)\n",
      "wi(z) = «log (4).\n",
      "\n",
      "Notice that yg(a) is a constant (as a function of x) and y1(2)\n",
      "0 when « = 0. Thus the three conditions of the Theorem hold.\n",
      "\n",
      "  \n",
      "\n",
      "The loglinear parameters are\n",
      "\n",
      "f 6 Pp\n",
      "Bo =log(p2), 31 = log (2) .\n",
      "P2\n",
      "The original, multinomial parameter space is P = {(p,,p2) :\n",
      "pj > 0,pi+p2=1}. The log-linear parameter space is\n",
      "\n",
      "© = {(o, fi) ER’: et 4 6% = 1}\n",
      "\n",
      "Given (p,,p2) we can solve for (8, 5). Conversely, given (Bo, 31)\n",
      "we can solve for (p,,p2).\n",
      "\n",
      "Example 18.3 Let X = (X,,X2) where X, € {0,1} and Xz €\n",
      "{0,1,2}. The joint distribution of n such random vectors is a\n",
      "multinomial with 6 categories. The multinomial parameters can\n",
      "be written as a 2-by-3 table as follows:\n",
      "\n",
      " \n",
      "\n",
      "multinomial wy 0 1 2\n",
      "r, 0 Poo Por Po2\n",
      "\n",
      "1 Pio Pu Pi2\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-308.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "312 18. Loglinear Models\n",
      "\n",
      "The n data vectors can be summarized as counts:\n",
      "data © 0 1 2:\n",
      "xr, 0 Coo Cor Coz\n",
      "1 Cio Cu Cr\n",
      "\n",
      "For x = (x1, %), the log-linear expansion takes the form\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "log f (x) = Wo (a) + vi (2) + ga(x) + Via(2)\n",
      "where\n",
      "\n",
      "u(x) = log poo\n",
      "\n",
      "tilt) = x log (22)\n",
      "ola) = I(x, =1) log (=) + I (29 = 2) log ()\n",
      "\n",
      "Poo Poo\n",
      "\n",
      "= I =1, 22 =1)log (a) + (2 = 1,2 = 2) log (a) ;\n",
      "PoiPi0 Po2Pi0\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Convince yourself that the three conditions on the w)’s of the\n",
      "theorem are satisfied. The six parameters of this model are:\n",
      "\n",
      " \n",
      "\n",
      "Bi = log poo Ba = log (=) Bs = log (2)\n",
      "\n",
      "6 =log (#2) 5 =log (Butte) Fg = log (BEE)\n",
      "\n",
      "The next Theorem gives an easy way to check for conditional\n",
      "independence in a loglinear model.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 18.4 Let (X,,X;,X.) be a partition of a vectors (X,...,Xm)-\n",
      "Then X,UX,|Xq if and only if all the y-terms in the log-linear\n",
      "expansion that have at least one coordinate in b and one coordi-\n",
      "\n",
      "nate inc are 0.\n",
      "\n",
      "To prove this Theorem, we will use the following Lemma\n",
      "whose proof follows easily from the definition of conditional in-\n",
      "dependence.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-309.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "18.2 Graphical Log-Linear Models 313\n",
      "\n",
      "Lemma 18.5 A partition (X., Xp, X_,) satisfies X,UX,|Xq if and\n",
      "only if f (a, Xp, Te) = g(a, Zp)h(La, Ze) for some functions g and\n",
      "h\n",
      "\n",
      "PRoorF. (Theorem 18.4.) Suppose that , is 0 whenever ¢ has\n",
      "\n",
      "coordinates in b and c. Hence, yy; is 0 ift GaUdb or t ¢ aUe.\n",
      "Therefore\n",
      "\n",
      "log f(x) = S uy (a) + S Uy (a) — S u(x).\n",
      "\n",
      "tcaUb tcaUe tCa\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Exponentiating, we see that the joint density is of the form\n",
      "G(%a, Tp)h(2q,%-). By Lemma 18.5, X, I X,|X,. The converse\n",
      "follows by reversing the argument. ll\n",
      "\n",
      "18.2 Graphical Log-Linear Models\n",
      "\n",
      "A log-linear model is graphical if missing terms correspond\n",
      "only to conditional independence constraints.\n",
      "\n",
      " \n",
      "\n",
      "Definition 18.6 Let log f(x) = 30 ycg Wa(@) be a log-linear\n",
      "model. Then f is graphical if all y-terms are non-zero\n",
      "except for any pair of coordinates not in the edge set for\n",
      "some graph G. In other words, ~4(x) = 0 if and only if\n",
      "{i,j} C A and (i, j) is not an edge.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Here is way to think about the definition above:\n",
      "\n",
      "If you can add a term to the model and the graph does\n",
      "not change, then the model is not graphical.\n",
      "\n",
      "Example 18.7 Consider the graph in Figure 18.1.\n",
      "The graphical log-linear model that corresponds to this graph\n",
      "is\n",
      "log f(x) = Up + vi (a) + Yo(x) + Ys (x) + vax) + Us (2)\n",
      "+ Wy2(x) + Wo3(x) + bo5(x) + W34(x) + 35 (2) + Was (x) + Ya35 (x) + W345 (x).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-310.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "314 18. Loglinear Models\n",
      "\n",
      "Xs Xs\n",
      "\n",
      "xX, Xy X3\n",
      "FIGURE 18.1. Graph for Example 18.7.\n",
      "\n",
      "Let’s see why this model is graphical. The edge (1,5) is missing\n",
      "in the graph. Hence any term containing that pair of indices is\n",
      "omitted from the model. For example,\n",
      "\n",
      "Vis, Vie, Visas, Viss, Vi235, Vieds, Visas, V12345\n",
      "are all omitted. Similarly, the edge (2,4) is missing and hence\n",
      "Wea, Vira, Wo3a, Was, Vi2aa, Wied, 2345, W12345\n",
      "\n",
      "are all omitted. There are other missing edges as well. You can\n",
      "check that the model omits all the corresponding » terms. Now\n",
      "consider the model\n",
      "\n",
      "log f(x) = d(x) + di (x) + Y2(x) + u(x) + d(x) + s(x)\n",
      "+ Wrox) + Gos (x) + dos (a) + Vaa(2) + 35 (2) + Vas (\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "This is the same model except that the three way interactions\n",
      "were removed. If we draw a graph for this model, we will get the\n",
      "same graph. For example, no w terms contain (1,5) so we omit\n",
      "the edge between X, and X;. But this is not graphical since it has\n",
      "extra terms omitted. The independencies and graphs for the two\n",
      "models are the same but the latter model has other constraints\n",
      "besides conditional independence constraints. This is not a bad\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-311.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "18.3 Hierarchical Log-Linear Models 315\n",
      "\n",
      "thing. It just means that if we are only concerned about presence\n",
      "or absence of conditional independences, then we need not con-\n",
      "sider such a model. The presence of the three-way interaction\n",
      "235 means that the strength of association between Xz and X3\n",
      "varies as a function of X5. Its absence indicates that this is not\n",
      "so.\n",
      "\n",
      "18.3 Hierarchical Log-Linear Models\n",
      "\n",
      "There is a set of log-linear models that is larger than the set of\n",
      "graphical models and that are used quite a bit. These are the\n",
      "hierarchical log-linear models.\n",
      "\n",
      " \n",
      "\n",
      "Definition 18.8 A log-linear model is hierarchical if j=\n",
      "0 and a Ct implies that W,=0.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Lemma 18.9 A graphical model is hierarchical but the reverse\n",
      "need not be true.\n",
      "\n",
      "Example 18.10 Let\n",
      "log f (a) = vo (@) + vn (2) + bo() + Wa(x) + Vie(2) + Yra(2).\n",
      "\n",
      "The model is hierarchical; its graph is given in Figure 18.2. The\n",
      "model is graphical because all terms involving (1,3) are omitted.\n",
      "It is also hierarchical. @\n",
      "\n",
      "Example 18.11 Let\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "log f(x) = Yo(a)+1 (2) +2 (2)-+3 (2) +012 (a) +413 (2) +23 (2).\n",
      "\n",
      "The model is hierarchical. It is not graphical. The graph corre-\n",
      "sponding to this model is complete; see Figure 18.8. It is not\n",
      "graphical because W123(2) =0 which does not correspond to any\n",
      "pairwise conditional independence. Mi\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-312.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "316 18. Loglinear Models\n",
      "\n",
      "Xi Xs Xs\n",
      "\n",
      "FIGURE 18.2. Graph for Example 18.10.\n",
      "\n",
      "xX XS\n",
      "\n",
      "X3\n",
      "\n",
      "FIGURE 18.3. The graph is complete. The model is hierarchical but\n",
      "not graphical.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-313.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A8FA3908>\n",
      "18.4 Model Generators 317\n",
      "\n",
      "—__—_+ °\n",
      "\n",
      "xX, Xe X3\n",
      "FIGURE 18.4. The model for this graph is not hierarchical.\n",
      "Example 18.12 Let\n",
      "log f (7) = Wg (2) + ¥3(a) + d12(2).\n",
      "\n",
      "The graph corresponding is in Figure 18.4. This model is not\n",
      "hierarchical since ty = 0 but YW is not. Since it is not hierar-\n",
      "chical, it is not graphical either. Ml\n",
      "\n",
      "18.4 Model Generators\n",
      "\n",
      "Hierarchical models can be written succinctly using genera-\n",
      "tors. This is most easily explained by example. Suppose that\n",
      "X = (X, Xo, X3). Then, M = 1.2 + 1.3 stands for\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "log f= vg tdi + v2+ 03 + Vie + V3.\n",
      "\n",
      "The formula M = 1.2+1.3 says: “include 2 and q13.” We have\n",
      "to also include the lower order terms or it won’t be hierarchical.\n",
      "\n",
      " \n",
      "\n",
      "The generator M = 1.2.3 is the saturated model\n",
      "log f= Ug +1 t+ Wo +03 4+ Vio + Vis + W238 + Yi23-\n",
      "\n",
      "The saturated models corresponds to fitting an unconstrained\n",
      "multinomial. Consider M = 1+ 2+ 3 which means\n",
      "\n",
      "log f = vp t+ v1 + 24+ U3.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-314.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "318 18. Loglinear Models\n",
      "|\n",
      "7 \\\n",
      "\\ /\n",
      "i)\n",
      "FIGURE 18.5. The lattice with two variables.\n",
      "\n",
      "This is the mutual independence model. Finally, consider M =\n",
      "1.2 which has log-linear expansion\n",
      "\n",
      "log f = vo + yi +y2+ Yr.\n",
      "\n",
      "This model makes X3|X2 = 72, X; = 7, a uniform distribution.\n",
      "\n",
      "8.5 Lattices\n",
      "\n",
      "Hierarchical models can be organized into something called a\n",
      "lattice. This is the set of all hierarchical models partially or-\n",
      "ered by inclusion. The set of all hierarchical models for two\n",
      "variables can be illustrated as in Figure 18.5.\n",
      "\n",
      "M = 1.2 is the saturated model, M = 1 + 2 is the indepen-\n",
      "ence model, M = 1 is independence plus X|X, is uniform,\n",
      "M = 2 is independence plus X,|X2 is uniform, M = 0 is the\n",
      "\n",
      " \n",
      "\n",
      "uniform distribution.\n",
      "The lattice of trivariate models is shown in figure 18.6.\n",
      "\n",
      "18.6 Fitting Log-Linear Models to Data\n",
      "\n",
      "Let 6 denote all the parameters in a log-linear model M. The\n",
      "loglikelihood for {3 is\n",
      "\n",
      "(8) = S32; 108 p;(8)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-315.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.6 Fitting Log-Linear Models to Data 319\n",
      "\n",
      "saturated 1.2.3\n",
      "(graphical)\n",
      "two-way 1.2 +13 + 2:3\n",
      "graphical 1.24 1.3 1.2 + 2.3 1.3 + 2.3\n",
      "graphical 12+3 1.3 +2 2.341\n",
      "mutual independence 14+2+3\n",
      "(graphical)\n",
      "conditional uniform 122 1.3 2.3\n",
      "independence 142 14+3 2+3\n",
      "ul 2 3\n",
      "uniform 0\n",
      "\n",
      "FIGURE 18.6. The lattice of models for three variables.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-316.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "320 18. Loglinear Models\n",
      "\n",
      "where the sum is over the cells and p(/3) denotes the cell proba-\n",
      "bilities corresponding to 6. The MLE B generally has to be found\n",
      "numerically. The model with all possible 7-terms is called the\n",
      "saturated model. We can also fit any sub-model which cor-\n",
      "\n",
      "responds to setting some subset of 7) terms to 0.\n",
      "\n",
      " \n",
      "\n",
      "Definition 18.13 For any submodel M, define the deviance\n",
      "dev(M) by\n",
      "\n",
      "dev(M) = 2(€sat — £11)\n",
      "where Boat is the log-likelihood of the saturated model eval-\n",
      "\n",
      "uated at the MLE andey 1s the log-likelihood of the model\n",
      "M evaluated at its MLE .\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 18.14 The deviance is the likelihood ratio test statistic\n",
      "\n",
      "for\n",
      "Ho:the model is M_ versus A : the model is not M.\n",
      "\n",
      "Under Hg, dev(M) & x2 with v degrees of freedom equal to the\n",
      "difference in the number of parameters between the saturated\n",
      "model and M.\n",
      "\n",
      "One way to find a good model is to use the deviance to test\n",
      "every sub-model. Every model that is not rejected by this test is\n",
      "then considered a plausible model. However, this is not a good\n",
      "strategy for two reasons. First, we will end up doing many tests\n",
      "which means that there is ample opportunity for making Type I\n",
      "and Type II errors. Second, we will end up using models where\n",
      "we failed to reject Hp. But we might fail to reject Ho due to low\n",
      "power. The result is that we end up with a bad model just due\n",
      "to low power.\n",
      "\n",
      "There are many model searching strategie:\n",
      "proach is to use some form of penalized likelihood. One version\n",
      "\n",
      "A common ap-\n",
      "\n",
      " \n",
      "\n",
      "of penalized is the AIC that we used in regression. For any model\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-317.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "18.6 Fitting Log-Linear Models to Data 321\n",
      "\n",
      "M define\n",
      "AIC(M) = —2 (a = ia) (18.2)\n",
      "where || is the number of parameters. Here is the explanation\n",
      "\n",
      " \n",
      "\n",
      "of where AIC comes from.\n",
      "\n",
      "Consider a set of models {M,, Mo, ...}. Let fj(2) denote the\n",
      "estimated probability function obtained by using the maximum\n",
      "likelihood estimator ofr mode M;. Thus, fj (2) =f a: B;) where\n",
      "B; is the MLE of the set of parameters §; for model Mj. We will\n",
      "\n",
      "use the loss function D(f, f) where\n",
      "\n",
      "D(f.9) = Le) log (43)\n",
      "\n",
      "is the Kullback-Leibler distance between two probability func-\n",
      "\n",
      "tions. The corresponding risk function is Rf, f) = E(D(f, f)-\n",
      "\n",
      "Notice that D(f, f) =e A(f,f) where c = >>, f (2) log f(a)\n",
      "\n",
      "deos not depend on f and\n",
      "AULA) = 37 F(2) log F).\n",
      "\n",
      "Thus, minimizing the risk is equivalent to maximizing a(f, f) =\n",
      "E(A(f, f)). . ae\n",
      "\n",
      "It is tempting to estimate a(f,f) by >, f(a) log f(x) but,\n",
      "just as the training error in regression is a highly biased esti-\n",
      "\n",
      "mate of prediction risk, it is also the case that }>,, f (2) log f(x)\n",
      "\n",
      "isa highly biased estimate of a(f, f). In fact, the bias is approx-\n",
      "imately equal to |M;|. Thus:\n",
      "\n",
      "Theorem 18.15 AIC(Mj) is an approximately unbiased estimate\n",
      "of af, f)-\n",
      "\n",
      "After finding a “best model” this way we can draw the corre-\n",
      "sponding graph. We can also check the overall fit of the selected\n",
      "model using the deviance as described above.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-318.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "322 18. Loglinear Models\n",
      "\n",
      "Example 18.16 Data on breast cancer from Morrison et al (1973)\n",
      "were presented in homework question 4 of Chapter 17. The data\n",
      "are on diagnostic center (X,), nuclear grade (X»), and survival\n",
      "\n",
      "(X3): (Morrison et al 1973):\n",
      "\n",
      " \n",
      "\n",
      "X2 malignant malignant\n",
      "\n",
      "X3 died survived\n",
      "\n",
      "Xi Boston 35 59\n",
      "Glamorgan 42 717\n",
      "\n",
      "benign benign\n",
      "died survived\n",
      "77 112\n",
      "\n",
      "26 16\n",
      "\n",
      " \n",
      "\n",
      "The saturated log-linear model is:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Coefficient Standard Error Wald Statistic p-value\n",
      "(Intercept) 8.56 0.17 21.03 0.00 ***\n",
      "center 0.18 0.22 0.79 0.42\n",
      "grade 0.29 0.22 1.82 0.18\n",
      "survival 0.52 0.21 244 0.01 *\n",
      "centerx grade -0.77 0.33 -2.31 0.02 *\n",
      "centerx survival 0.08 0.28 0.29 0.76\n",
      "gradex survival 0.84 0.27 1.25 0.20\n",
      "centerx gradex survival 0.12 0.40 0.29 0.76\n",
      "The best sub-model, selected using AIC and backward searching\n",
      "is\n",
      "Coefficient Standard Error Wald Statistic p-value\n",
      "(Intercept) 8.52 0.13 25.62 7 0.00 ***\n",
      "center 0.23 0.13 1.70 0.08\n",
      "grade 0.26 0.18 143° O15\n",
      "survival 0.56 0.14 3.98 6.65e-05 ***\n",
      "centerx grade -0.67 0.18 -3.62 0.00 ***\n",
      "gradex survival 0.87 0.19 1.90 0.05\n",
      "\n",
      "The graph for this model M is shown in Figure 18.7. To test\n",
      "the fit of this model, we compute the deviance of M which is\n",
      "0.6. The appropriate x? has 8 — 6 = 2 degrees of freedom. The\n",
      "p-value is P(x3 > .6) = .74. So we have no evidence to suggest\n",
      "\n",
      "that the model is a poor fit. 1\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-319.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "18.6 Fitting Log-Linear Models to Data 323\n",
      "\n",
      "Center ————._ Grade ————— Survival\n",
      "\n",
      "FIGURE 18.7. The graph for Example 18.16.\n",
      "\n",
      "The example above is a toy example. With so few variables,\n",
      "there is little to be gained by searching for a best model. The\n",
      "overall multinomial MLE would be sufficient. More importantly,\n",
      "we are probably interested in predicting survivial from grade and\n",
      "center in which case this should really be treated as a regresion\n",
      "problem with outcome Y = survival and covariates center and\n",
      "grade.\n",
      "\n",
      "Example 18.17 Here is a synthetic example. We generate n =\n",
      "100 random vectors X = (X1,...,X5) of length 5. We generated\n",
      "the data as follows:\n",
      "\n",
      "X, ~ Bernoulli(1/2)\n",
      "and\n",
      "\n",
      "1/4 if Xj1=0\n",
      "\n",
      "FG [RB nnn { 3/4 if Xj1=1.\n",
      "\n",
      "It follows that\n",
      "\n",
      "flat,» 28)\n",
      "\n",
      "L) /3\\tite2te8ted 7.3) d-el-a2—ad—24\n",
      "a)\\4 4 .\n",
      "\n",
      "We estimated f using three methods: (i) maximum likelihood\n",
      "treating this as a multinomial with 32 categories, (ii) maximum\n",
      "likelihood from the best loglinear model using AIC and forward\n",
      "selection and (iti) maximum likelihood from the best loglinear\n",
      "\n",
      "(0) Q-@@ “G(\n",
      "\n",
      "2\n",
      "\n",
      "4\n",
      "\n",
      "ye\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "y(\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      ")~\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-320.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "324 18. Loglinear Models\n",
      "\n",
      "model using BIC and forward selection. We estimated the risk\n",
      "by simulating the example 100 times. The average risks were:\n",
      "\n",
      "Method Risk\n",
      "\n",
      "MLE 0.63\n",
      "AIC 0.54\n",
      "BIC 0.53\n",
      "\n",
      "In this ecample, there is little difference between AIC and BIC.\n",
      "Both are better than maximum likelihood. @\n",
      "\n",
      "18.7 Bibliographic Remarks\n",
      "\n",
      "A classic references on loglinear models is Bishop, Fienberg and\n",
      "Holland (1975). See also Whittaker (1990) from which some of\n",
      "the exercises are borrowed.\n",
      "\n",
      "18.8 Exercises\n",
      "1. Solve for the pj;s in terms of the 6’s in Example 18.3.\n",
      "\n",
      "2. Repeat example 18.17 using 7 covariates and n = 1000.\n",
      "To avoid numerical problems, replace any zero count with\n",
      "a one.\n",
      "\n",
      "3. Prove Lemma 18.5.\n",
      "4. Prove Lemma 18.9.\n",
      "\n",
      "5. Consider random variables (X), X,-X3,.X,). Suppose the\n",
      "log-density is\n",
      "\n",
      "log f (x) = dio(a) + di2(x) + di3(x) + bau(a) + Y3a(2).\n",
      "\n",
      "(a) Draw the graph G for these variables.\n",
      "\n",
      "(b) Write down all independence and conditional indepen-\n",
      "dence relations implied by the graph.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-321.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "18.8 Exercises 325\n",
      "(c) Is this model graphical? Is it hierarchical?\n",
      "\n",
      ". Suppose that parameters p(2), 22,73) are proportional to\n",
      "the following values:\n",
      "\n",
      " \n",
      "\n",
      "™ 0 0 1 -\n",
      "tm 0 1 0 1\n",
      "™ 0 2 8 4 16\n",
      "1 16 128 32 256\n",
      "\n",
      " \n",
      "\n",
      "Find the w-terms for the log-linear expansion. Comment\n",
      "on the model.\n",
      "\n",
      ". Let X4,...\n",
      "corresponding to the following log-linear models. Also,\n",
      "\n",
      "X, be binary. Draw the independence graphs\n",
      "\n",
      " \n",
      "\n",
      "identify whether each is graphical and/or hierarchical (or\n",
      "neither).\n",
      "\n",
      "(a) log f = 7+ lla, + 2x9 + 1.523 + 17x,\n",
      "\n",
      "(b) log f = 7+ 11a, +22%94-1.5234+17244+122903+780924+\n",
      "323X4 + 32227324\n",
      "\n",
      " \n",
      "\n",
      "(c) log f = 7+112;+2a94 1.523 +1724 +122203+32304+\n",
      "©4124 +2222\n",
      "\n",
      "(d) log f = 7+ 50552, 2973.24\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-322.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-323.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "19\n",
      "\n",
      "Causal Inference\n",
      "\n",
      "In this Chapter we discuss causation. Roughly speaking, the\n",
      "statement “X causes Y” means that changing the value of XY\n",
      "will change the distribution of Y. When X causes Y, X and Y\n",
      "will be associated but the reverse is not, in general, true. Associ-\n",
      "ation does not necessarily imply causation. We will consider two\n",
      "frameworks for discussing causation. The first uses the notation\n",
      "of counterfactual random variables. The second, presented in\n",
      "the next Chapter, uses directed acyclic graphs.\n",
      "\n",
      "19.1 The Counterfactual Model\n",
      "\n",
      "Suppose that X is a binary treatment variable where X = 1\n",
      "means “treated” and where X = 0 means “not treated.” We are\n",
      "using the word “treatment” in a very broad sense: treatment\n",
      "might refer to a medication or something like smoking. An al-\n",
      "ternative to “treated/not treated” is “exposed/not exposed” but\n",
      "we shall use the former.\n",
      "\n",
      "This is page 327\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-324.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "328 19. Causal Inference\n",
      "\n",
      "Let Y be some outcome variable such as presence or absence of\n",
      "disease. To distinguish the statement “X is associated Y” from\n",
      "the statement “X causes Y” we need to enrich our probabilistic\n",
      "vocabulary. We will decompose the response Y into a more fine-\n",
      "grained object.\n",
      "\n",
      "We introduce two new random variables (Cp, C;), called po-\n",
      "tential outcomes with the following interpretation: Co is the\n",
      "outcome if the subject is not treated (XY = 0) and C; is the\n",
      "outcome if the subject is treated (X = 1). Hence,\n",
      "\n",
      "yas Go #X=0\n",
      "—ViGy ie\n",
      "\n",
      "We can express the relationship between Y and (Co, C,) more\n",
      "succintly by\n",
      "\n",
      "Y =Cx. (19.1)\n",
      "\n",
      "This equation is called the consistency relationship.\n",
      "Here is a toy data set to make the idea clear:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "XY GG\n",
      "0 4 os\n",
      "0 7 7 *\n",
      "0 2 2 *\n",
      "0 8 8 *\n",
      "1 3° * 3\n",
      "L & * 6\n",
      "1 8 * 8\n",
      "19 * 9\n",
      "\n",
      "The asterisks denote unobserved values. When X = 0 we don’t\n",
      "\n",
      "observe C; in which case we say that C; is a counterfactual\n",
      "\n",
      "since it is the outcome you would have had if, counter to the\n",
      "\n",
      "fact, you had been treated (X = 1). Similarly, when X = 1 we\n",
      "\n",
      "don’t observe Co and we say that Cp is counterfactual.\n",
      "Notice that there are four types of subjects:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-325.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "19.1 The Counterfactual Model 329\n",
      "\n",
      "Type Co C1\n",
      "Survivors I a\n",
      "Responders o ol\n",
      "Anti-repsonders 1 0\n",
      "Doomed 0 0\n",
      "\n",
      "Think of the potential outcomes (Co,C) as hidden variables\n",
      "that contain all the relavent information about the subject.\n",
      "Define the average causal effect or average treatment\n",
      "effect to be\n",
      "6 =E(Ci) -E(). (19.2)\n",
      "\n",
      "The parameter @ has the following interpretation: 0 is the mean\n",
      "if everyone were treated (X = 1) minus the mean if everyone\n",
      "were not treated (X = 0). There are other ways of measuring\n",
      "the causal effect. For example, if Co and C; are binary, we define\n",
      "the causal odds ratio\n",
      "\n",
      " \n",
      "\n",
      "The main ideas will be the same whatever causal effect we use.\n",
      "For simplicity, we shall work with the average causal effect 0.\n",
      "Define the association to be\n",
      "\n",
      "a=K(Y|X =1)-E(Y|X =0). (19.3)\n",
      "Again, we could use odds ratios or other summaries if we wish.\n",
      "\n",
      "Theorem 19.1 (Association is not equal to Causation) Jn\n",
      "general, 0#a.\n",
      "\n",
      "Example 19.2 Suppose the whole population is as follows:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-326.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "330 19. Causal Inference\n",
      "\n",
      "Xx YQ OG\n",
      "\n",
      "a\n",
      "a\n",
      "i)\n",
      "a\n",
      "\n",
      "1 1\n",
      "Again, the asterisks denote unobserved values. Notice that Co =\n",
      "C, for every subject, thus, this treatment has no effect. Indeed,\n",
      "\n",
      "8 8\n",
      "1 1\n",
      "9 = E(C,)-E(Q)= 3 er -_ zo Cu\n",
      "i=l aeck\n",
      "\n",
      "14+ $h4+0+0+040 lett +14+0+070+90\n",
      "8 8\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "= 0.\n",
      "\n",
      "Thus the average causal effect is 0. The observed data are X's\n",
      "and Y’s only from which we can estimate the association:\n",
      "\n",
      "a = K(Y|X=1)-E(Y|X =0)\n",
      "1+1+1+1 0+0+0+0\n",
      "4 4\n",
      "\n",
      " \n",
      "\n",
      "I.\n",
      "\n",
      " \n",
      "\n",
      "Hence, 0 #a.\n",
      "\n",
      "To add some intutition to this example, imagine that the out-\n",
      "come variable is 1 if “healthy” and 0 if “sick”. Suppose that\n",
      "X =0 means that the subject does not take vitamin C and that\n",
      "X = 1 means that the subject does take vitamin C. Vitamin\n",
      "C has no causal effect since Co = C\\. In this example there\n",
      "are two types of people: healthy people (Co,C,) = (1,1) and\n",
      "unhealthy people (Cy,C;) = (0,0). Healthy people tend to take\n",
      "vitamin C while unhealty people don’t. It is this association be-\n",
      "tween (Co, C1) and X that creates an association between X and\n",
      "Y. If we only had data on X and Y we would conclude that X\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-327.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "19.1 The Counterfactual Model 331\n",
      "\n",
      "and Y are associated. Suppose we wrongly interpret this causally\n",
      "and conclude that vitamin C prevents illness. Next we might en-\n",
      "courage everyone to take vitamin C. If most people comply the\n",
      "population will look something like this:\n",
      "\n",
      "XY OM GQ\n",
      "0 0 0 OF\n",
      "1 0 0 OF\n",
      "1 0 0 OF\n",
      "1 0 0 OF\n",
      "1 2  &\n",
      "LoL Yt\n",
      "11 oil\n",
      "11 Moi\n",
      "\n",
      "Now a = (4/7) — (0/1) = 4/7. We see that a went down from 1\n",
      "to 4/7. Of course, the causal effect never changed but the naive\n",
      "observer who does not distinguish association and causation will\n",
      "be confused because his advice seems to have made things worse\n",
      "instead of better. Wi\n",
      "\n",
      "In the last example, 9 = 0 and a = 1. It is not hard to create\n",
      "examples in which a > 0 and yet 6 < 0. The fact that the\n",
      "association and causal effects can have different signs is very\n",
      "confusing to many people including well educated statisticians.\n",
      "\n",
      "The example makes it clear that, in general, we cannot use the\n",
      "association to estimate the causal effect #. The reason that 0 4 a\n",
      "is that (Co,C,) was not independent of X. That is, treatment\n",
      "assigment was not independent of person type.\n",
      "\n",
      "Can we ever estimate the causal effect? The answer is: some-\n",
      "times. In particular, random asigment to treatment makes it\n",
      "possible to estimate 0.\n",
      "\n",
      "Theorem 19.3 Suppose we randomly assign subjects to treat-\n",
      "ment and that P(X =0) >0 and P(X =1) >0. Thena=0.\n",
      "Hence, any consistent estimator of a is a consistent estimator\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-328.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "332 19. Causal Inference\n",
      "\n",
      "of 6. In particular, a consistent estimator is\n",
      "\n",
      "R(y|X =1)-E(v\n",
      "= Yi-Yo\n",
      "\n",
      "g\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      ".X = 0)\n",
      "is a consistent estimator of 0, where\n",
      "\n",
      "M=> EN Y= OK\n",
      "\n",
      "&Xj=1 :X;=0\n",
      "\n",
      " \n",
      "\n",
      "ny = OL, Xi and no = YO, (1 — Xi).\n",
      "\n",
      "PROOF. Since X is randomly assigned, X is independent of\n",
      "(Co, C1). Hence,\n",
      "\n",
      "6 = E(Ci))-E()\n",
      "E(C,|X =1) —E(Co|X =0) — since X IL (Co, C1)\n",
      "= E(Y|X =1)—-E(Y|X =0) since Y = Cx\n",
      "\n",
      "= oO\n",
      "\n",
      " \n",
      "\n",
      "The consistency follows from the law of large numbers. il\n",
      "If Z is a covariate, we define the conditional causal effect\n",
      "by\n",
      "\n",
      "6, =E(C,|Z =z) —E(C,|Z =2).\n",
      "\n",
      "For example, if Z denotes gender with values Z = 0 (women)\n",
      "and Z = 1 (men), then 9p is the causal effect among women and\n",
      "0, is the causal effect among men. In a randomized experiment,\n",
      "6, =E(Y|X = 1,27 = z)—E(Y|X = 0,Z = z) and we can\n",
      "estimate the conditional causal effect using appropriate sample\n",
      "averages.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-329.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "19.2 Beyond Binary Treatments 333\n",
      "\n",
      " \n",
      "\n",
      "Summary\n",
      "\n",
      "Random variables: (Cp, C1, X,Y).\n",
      "\n",
      "Consistency relationship: Y = Cy.\n",
      "\n",
      "Causal Effect: @ = E(C,) — E(Cp).\n",
      "\n",
      "Association: a = E(Y|X =1)—E(Y|X =0).\n",
      "Random assigment => (Co,C:) UX = > 0=a.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "19.2 Beyond Binary Treatments\n",
      "\n",
      "Let us now generalize beyond the binary case. Suppose that\n",
      "X € &. For example, X could be the dose of a drug in which\n",
      "case X € R The counterfactual vector (Cp,C) now becomes\n",
      "the counterfactual process\n",
      "\n",
      "{C(x): rE X} (19.4)\n",
      "\n",
      "where C(x) is the outcome a subject would have if he received\n",
      "dose x. The observed reponse is given by the consistency relation\n",
      "\n",
      "¥ GW). (19.5)\n",
      "\n",
      "(2) = E(C(a)). (19.6)\n",
      "The regression function, which measures association, is r(a) =\n",
      "E(Y|X =z).\n",
      "\n",
      "Theorem 19.4 In general, 0(x) # r(x). However, when X is\n",
      "\n",
      "randomly assigned, 0(2) = r(x).\n",
      "\n",
      "Example 19.5 An example in which @(x) is constant but r(2)\n",
      "is not constant is shown in Figure 19.2. The figure shows the\n",
      "counterfactuals processes for four subjects.\n",
      "\n",
      " \n",
      "\n",
      "The dots represent\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-330.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334 19. Causal Inference\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "°o\n",
      "B\n",
      "No\n",
      "ww\n",
      "fs\n",
      "on\n",
      "Ce\n",
      "at\n",
      "(e)\n",
      "©\n",
      "K\n",
      "°\n",
      "=\n",
      "a\n",
      "\n",
      "FIGURE 19.1. A counterfactual process C(a). The outcome Y is the\n",
      "value of the curve C(x) evaluated at the observed dose X.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-331.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "19.3 Observational Studies and Confounding 335\n",
      "\n",
      "their X values X\\,Xo,X3,X 4. Since C;(x) is constant over x\n",
      "for alli, there is no causal effect and hence\n",
      "\n",
      "Cul) + Cal) + Cal) + Cal)\n",
      "A(x) Z\n",
      "\n",
      "is constant. Changing the dose x will not change anyone’s out-\n",
      "come. The four dots in the lower plot represent the observed data\n",
      "points Y, = Ci(X1),¥2 = C2(X2), V3 = C3(X3), Ya = Cu(Xu).\n",
      "The dotted line represents the regression r(x) = E(Y|X = 2).\n",
      "Although there is no causal effect, there is an association since\n",
      "the regression curve r(x) is not constant. Wl\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "19.3 Observational Studies and Confounding\n",
      "\n",
      "A study in which treatment (or exposure) is not randomly as\n",
      "signed, is called an observational study. In these studies, sub-\n",
      "jects select their own value of the exposure X. Many of the\n",
      "health studies your read about in the newspaper are like this.\n",
      "As we saw, association and causation could in general be quite\n",
      "different. This descrepancy occurs in non-randomized studies\n",
      "because the potential outcome C' is not independent of treat-\n",
      "ment X. However, suppose we could find groupings of subjects\n",
      "such that, within groups, X and {C(a) : x € 4} are indepen-\n",
      "dent. This would happen if the subjects are very similar within\n",
      "groups. For example, suppose we find people who are very sim-\n",
      "ilar in age, gender, educational background and ethnic back-\n",
      "ground. Among these people we might feel it is reasonable to\n",
      "assume that the choice of X is essentially random. These other\n",
      "variables are called confounding variables. If we denote these\n",
      "\n",
      " \n",
      "\n",
      "other variables collectively as Z, then we can express this idea\n",
      "by saying that\n",
      "\n",
      "{C(x): 7 €EX}IX|Z. (19.7)\n",
      "Equation (19.7) means that, within groups of Z, the choice of\n",
      "treatment X does not depend on type, as represented by {C(x) :\n",
      "\n",
      "A more precise def-\n",
      "inition of confound-\n",
      "ing is given in the\n",
      "next Chapter.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-332.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "336 19. Causal Inference\n",
      "\n",
      ", (2)\n",
      "3400 —— (22)\n",
      "270 -———_ 6(z)\n",
      "it + (2)\n",
      "\n",
      " \n",
      "\n",
      "=)\n",
      "\n",
      "8\n",
      "\n",
      " \n",
      "\n",
      "“Y\n",
      "37 eo)\n",
      "A(x)\n",
      "2+ Yen\n",
      "1+ Ye\n",
      "0 t t t t\n",
      "\n",
      " \n",
      "\n",
      "FIGURE 19.2. The top plot shows the counterfactual process C(x)\n",
      "for four subjects. The dots represent their X values. Since Cj(xr) is\n",
      "constant over x for all i, there is no causal effect. Changing the dose\n",
      "will not change anyone’s outcome. The lower plot shows the causal\n",
      "regression function O(a) = (Ci(#) + Co(a) + C3(a) + Ca(x))/4.\n",
      "The four dots represent the observed data points\n",
      "Yi = C1(X1),¥2 = Co(X2),¥3 = C3(X3),¥i = Ca(X.). The\n",
      "dotted line represents the regression r(x) = E(Y|X = x). There is\n",
      "no causal effect since C;(”) is constant for all i. But there is an\n",
      "association since the regression curve r(x) is not constant.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-333.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "19.3 Observational Studies and Confounding 337\n",
      "\n",
      "x € X}. If (19.7) holds and we observe Z then we say that there\n",
      "is no unmeasured confounding.\n",
      "\n",
      "Theorem 19.6 Suppose that (19.7) holds. Then,\n",
      "\n",
      "6(2) = / E(Y\n",
      "\n",
      "If? (x, z) is a consistent estimate of the regression function E(Y|X =\n",
      "x,Z =z), then a consistent estimate of 0(x) is\n",
      "\n",
      " \n",
      "\n",
      "IX = 2, Z = z)dFz(z)dz. (19.8)\n",
      "\n",
      "d(x) =~ Se, 2).\n",
      "\n",
      "In particular, if r(x, 2z) = Bo + 6:2 + Boz is linear, then a con-\n",
      "sistent estimate of (x) is\n",
      "\n",
      " \n",
      "\n",
      "Bo + Bat bZ, (19.9)\n",
      "where (Bo, Bs Bo) are the least squares estimators.\n",
      "\n",
      "Remark 19.7 Jt is useful to compare equation (19.8) to E(Y|X =\n",
      "2) which can be written as E(Y|X = 2) = fE(Y|X =2,Z=\n",
      "2)dF zx (2|2).\n",
      "\n",
      " \n",
      "\n",
      "Epidemiologists call (19.8) the adjusted treatment effect.\n",
      "The process of computing adjusted treatment effects is called\n",
      "adjusting (or controlling) for confounding. The selection\n",
      "of what confounders Z to measure and control for requires sci-\n",
      "entific insight. Even after adjusting for confounders, we cannot\n",
      "be sure that there are not other confounding variables that we\n",
      "missed. This is why observational studies must be treated with\n",
      "healthy skepticism. Results from observational studies start to\n",
      "become believable when: (i) the results are replicated in many\n",
      "studies, (ii) each of the studies controlled for plausible confound-\n",
      "ing variables, (iii) there is a plausible scientific explanation for\n",
      "the existence of a causal relationship.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-334.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "338 19. Causal Inference\n",
      "\n",
      "A good example is smoking and cancer. Numerous studies\n",
      "have shown a relationship between smoking and cancer even af-\n",
      "ter adjusting for many confouding variables. Moreover, in labo-\n",
      "ratory studies, smoking has been shown to damage lung cells. Fi-\n",
      "nally, a causal link between smoking and cancer has been found\n",
      "in randomized animal studies. It is this collection of evidence\n",
      "over many years that makes this a convincing case. One single\n",
      "observational study is not, by itself, strong evidence. Remember\n",
      "that when you read the newspaper.\n",
      "\n",
      "19.4 Simpson’s Paradox\n",
      "\n",
      "Simpson’s paradox is a puzzling phenomenon that is dicussed in\n",
      "most statistics texts. Unfortunately, it is explained incorrectly in\n",
      "most statistics texts. The reason is that it is nearly impossible to\n",
      "explain the paradox without using counterfactuals (or directed\n",
      "acyclic graphs).\n",
      "\n",
      "Let X be a binary treatment variable, Y a binary outcome\n",
      "and Z a third binary variable such as gender. Suppose the joint\n",
      "distribution of X,Y, Z is\n",
      "\n",
      "Y=1 Y=0 Y=1 Y=0\n",
      "X=1 = .1500 — .2250 -1000 -0250\n",
      "X=0  .0375 = .0875 -2625 1125\n",
      "Z =1 (men) Z =0 (women)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The marginal distribution for (X,Y) is\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-335.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "19.4 Simpson’s Paradox 339\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Y=1 Y=0\n",
      "X=1 25 -25 | .50\n",
      "X=0 -30 -20 | .50\n",
      "55 45 1\n",
      "\n",
      "Now,\n",
      "\n",
      " \n",
      "\n",
      "P(Y = 1|X =1)-P(Y = 1X =0) PY = DFO 0)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "We might naively interpret this to mean that the treatment\n",
      "is bad for you since P(Y = 1|X = 1) < P(Y = 1|X = 0).\n",
      "Furthermore, among men,\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "P(¥ =1|X=1,Z=1) — P(Y=1|X =0,Z=1)\n",
      "PY¥=1,X=1,Z=1) P(¥=1,X =0,Z=1)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "PX=1Z=1 £P(X=0,7=1)\n",
      "5 375\n",
      "= 01.\n",
      "\n",
      "Among women,\n",
      "\n",
      "P(Y =1|X =0, Z=0)\n",
      "PY =1,X=1,Z=0) P(¥=1,X =0,Z=0)\n",
      "\n",
      "P(Y =1|X =1,Z=0)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "P(X=1,Z=0)  P(X=0,Z=0)\n",
      "_ 1.2625\n",
      "~ 1250 3750\n",
      "\n",
      "= 0.1.\n",
      "\n",
      "To summarize, we seem to have the following information:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-336.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "340 19. Causal Inference\n",
      "\n",
      "Mathematical Statement English Statement?\n",
      "\n",
      " \n",
      "\n",
      "P(Y =1]X =1) < P(Y =1|X =0) treatment is harmful\n",
      "P(Y =1|X =1,Z=1) > P(Y =1|X =0,Z=1) treatment is beneficial to men\n",
      "P(Y =1|X =1,Z=0) > P(Y =1|X =0,Z=0) treatment is beneficial to women\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Clearly, something is amiss. There can’t be a treatment which\n",
      "is good for men, good for women but bad overall. This is non-\n",
      "sense. The problem is with the set of English statements in the\n",
      "table. Our translation from math into english is specious.\n",
      "\n",
      " \n",
      "\n",
      "The inequality P(Y = 1|X =1) < P(Y =1|X\n",
      "0) does not mean that treatment is harmful.\n",
      "\n",
      " \n",
      "\n",
      "The phrase “treatment is harmful” should be written mathe-\n",
      "matically as P(C; = 1) < P(Co = 1). The phrase “treatment is\n",
      "harmful for men” should be written P(C; = 1|Z = 1) < P(Co =\n",
      "1|Z = 1). The three mathematical statements in the table are\n",
      "\n",
      " \n",
      "\n",
      "not at all contradictory. It is only the translation into English\n",
      "that is wrong.\n",
      "\n",
      "Let us now show that a real Simpson’s paradox cannot hap-\n",
      "pen, that is, there cannot be a treatment that is beneficial for\n",
      "men and women but harmful overall. Suppose that treatment is\n",
      "beneficial for both sexes. Then\n",
      "\n",
      "P(C, = 1|Z = z) > P(Cp = 1|Z = 2)\n",
      "for all z. It then follows that\n",
      "\n",
      "P(C, = 1) SOP(C1 = 1/2 = 2)P(Z =2)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "> SOP(Co= 1/2 =2)P(Z =2)\n",
      "\n",
      "= P(%=1).\n",
      "Hence, P(C; = 1) > P(Co = 1) so treatment is beneficial overall.\n",
      "No paradox.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-337.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.5 Bibliographic Remarks 341\n",
      "\n",
      "9.5 Bibliographic Remarks\n",
      "\n",
      "[he use of potential outcomes to clarify causation is due mainly\n",
      "to Jerzy Neyman and Don Rubin. Later developments are due\n",
      "to Jamie Robins, Paul Rosenbaum and others. A parallel devel-\n",
      "opment took place in econometrics by various people including\n",
      "im Heckman and Charles Manski. Currently, there are no text-\n",
      "book treatments of causal inference from the potential outcomes\n",
      "viewpoint.\n",
      "\n",
      " \n",
      "\n",
      "9.6 Exercises\n",
      "\n",
      "1. Create an example like Example 19.2 in which a > 0 and\n",
      "0<0.\n",
      "\n",
      "2. Prove Theorem 19.4.\n",
      "\n",
      "3. Suppose you are given data (X), Yi),---, (Xn, Yn) from an\n",
      "observational study, where X; € {0,1} and Y; € {0,1}.\n",
      "Although it is not possible to estimate the causal effect 0,\n",
      "it is possible to put bounds on 6. Find upper and lower\n",
      "bounds on @ that can be consistently estimated from the\n",
      "data. Show that the bounds have width 1. Hint: Note that\n",
      "E(C,;) =E(C,|X = I)P(X = 1) + E(C,|X = 0)P(X = 0).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "4. Suppose that X € R and that, for each subject 7, Ci(x) =\n",
      "yx. Each subject has their own slope §,;. Construct a\n",
      "joint distribution on (3;,X) such that P(6; > 0) = 1\n",
      "but E(Y|X = 2) is a decreasing function of x, where Y =\n",
      "C(X). Interpret. Hint: Write f (31,2) = f (91) f (2|1). Choose\n",
      "f (21) so that when (3; is large, 2 is small and when (3;\n",
      "is small x is large.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-338.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-339.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "20\n",
      "Directed Graphs\n",
      "\n",
      "20.1 Introduction\n",
      "\n",
      "Directed graphs are similar to undirected graphs except that\n",
      "there are arrows between vertices instead of edges. Like undi-\n",
      "rected graphs, directed graphs can be used to represent inde-\n",
      "pendence relations. They can also be used as an alternative to\n",
      "counterfactuals to represent causal relationships. Some people\n",
      "use the phrase Bayesian network to refer to a directed graph\n",
      "endowed with a probability distribution. This is a poor choice of\n",
      "terminology. Statistical inference for directed graphs can be per-\n",
      "formed using frequentist or Bayesian methods so it is misleading\n",
      "to call them Bayesian networks.\n",
      "\n",
      "20.2 DAG’s\n",
      "\n",
      "A directed graph G consists of a set of vertices V and an edge\n",
      "set E of ordered pairs of variables. If (X,Y) € E then there is\n",
      "an arrow pointing from X to Y. See Figure 20.1.\n",
      "\n",
      "This is page 343\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-340.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "344 20. Directed Graphs\n",
      "\n",
      "x Z\n",
      "\n",
      "FIGURE 20.1. A directed graph with V = {X,Y,Z} and\n",
      "B= {(¥,X),(¥,Z)}.\n",
      "\n",
      "If an arrow connects two variables X and Y (in either direc-\n",
      "tion) we say that X and Y are adjacent. If there is an arrow\n",
      "from X to Y then X is a parent of Y and Y is a child of\n",
      "X. The set of all parents of X is denoted by wx or m(X). A\n",
      "directed path from X to Y is a set of vertices beginning with\n",
      "X, ending with Y such that each pair is connected by an arrow\n",
      "and all the arrows point in the same direction as follows:\n",
      "\n",
      " \n",
      "\n",
      "we mG Y or X Par ¥Y\n",
      "\n",
      "A sequence of adjacent vertices staring with X and ending with\n",
      "Y but ignoring the direction of the arrows is called an undi-\n",
      "rected path. The sequence {X, Y, Z} in Figure 20.1 is an undi-\n",
      "rected path. X is an ancestor of Y is there is a directed path\n",
      "from X to Y. We also say that Y is a descendant of X.\n",
      "\n",
      "A configuration of the form:\n",
      "\n",
      "XYZ\n",
      "\n",
      "is called a collider. A configuration not of that form is called a\n",
      "non-collider, for example,\n",
      "\n",
      "XYZ\n",
      "\n",
      "or\n",
      "XYZ\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-341.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "20.3 Probability and DAG’s 345\n",
      "\n",
      "A directed path that starts and ends at the same variable is\n",
      "called a cycle. A directed graph is acyclic if it has no cycles. In\n",
      "this case we say that the graph is a directed acyclic graph or\n",
      "DAG. From now on, we only deal with graphs that are DAG’s.\n",
      "\n",
      "20.3 Probability and DAG’s\n",
      "\n",
      "Let G be a DAG with vertices V = (X,..., Xx).\n",
      "\n",
      " \n",
      "\n",
      "Definition 20.1 IfP is a distribution for V with probability\n",
      "function p, we say that If P is Markov to G or that G\n",
      "represents P if\n",
      "\n",
      "k\n",
      "\n",
      "plv) = |] pla | ™) (20.1)\n",
      "\n",
      "i=l\n",
      "\n",
      "where 7; are the parents of X;. The set of distributions\n",
      "represented by G is denoted by M(G).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 20.2 For the DAG in Figure 20.2, P € M(GQ) if and\n",
      "only if its probability function p has the form\n",
      "P(x, y,2,w) = p(2)p(y)p(2 | 2,y)p(w| 2).\n",
      "\n",
      "The following theorem says that P € M(G) if and only if the\n",
      "Markov Condition holds. Roughly speaking, the Markov Con-\n",
      "dition says that every variable W is independent of the “past”\n",
      "given its parents.\n",
      "\n",
      "Theorem 20.3 A distribution P € M(G) if and only if the fol-\n",
      "lowing Markov Condition holds: for every variable W,\n",
      "\n",
      "WILW | my (20.2)\n",
      "\n",
      "where W denotes all the other variables except the parents and\n",
      "descendants of W.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-342.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "346 20. Directed Graphs\n",
      "x\n",
      "Te\n",
      "{_—>W\n",
      "—_—\n",
      "y\n",
      "\n",
      "FIGURE 20.2. Another DAG.\n",
      "\n",
      "FIGURE 20.3. Yet another DAG.\n",
      "Example 20.4 In Figure 20.2, the Markov Condition implies that\n",
      "XIUY and WIU{X,Y}|Z. @\n",
      "Example 20.5 Consider the DAG in Figure 20.3. In this case\n",
      "probability function must factor like\n",
      "p(a,b, e,d,€) = p(a)p(d|a)p(cla)p(alb, ¢)p(e|d)-\n",
      "\n",
      "The Markov Condition implies the following independence rela-\n",
      "tions:\n",
      "\n",
      "DUA|{B,C}, EU{A,B,C}|D and BUC|A @\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-343.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "20.4 More Independence Relations 347\n",
      "\n",
      "FIGURE 20.4. And yet another DAG.\n",
      "20.4 More Independence Relations\n",
      "\n",
      "With undirected graphs, we saw that pairwise independence re-\n",
      "lations implied other independence relations. Luckily, we were\n",
      "able to deduce these other relations from the graph. The situ-\n",
      "ation is similar for DAG’s. The Markov Condition allows us to\n",
      "list some independence relations. These relations may logically\n",
      "imply other other independence relations. Consider the DAG in\n",
      "Figure 20.4. The Markov Condition implies:\n",
      "X, WX2, X_ W{X, Xa}, X3 0X4 | {X1, Xo},\n",
      "X, I {Xo, X3} | Mi, X5 UW {X1, Xo} | (Xs, Xa}\n",
      "\n",
      "It turns out (but it is not obvious) that these conditions imply\n",
      "that\n",
      "\n",
      "{X1, X5} IE Xe | {X1, X3}.\n",
      "\n",
      "How do we find these extra independence relations? The an-\n",
      "swer is “d-separation.” d-separation can be summarized by three\n",
      "rules. Consider the four DAG’s in Figure 20.5 and the DAG in\n",
      "Figure 20.6. The first 3 DAG’s in Figure 20.5 are non-colliders.\n",
      "The DAG in the lower right of Figure 20.5 is a collider. The\n",
      "DAG in Figure 20.6 is a collider with a descendent.\n",
      "\n",
      "In what follows, we\n",
      "implicitly | assume\n",
      "that P is faithful\n",
      "to G which means\n",
      "that P has no\n",
      "extra independence\n",
      "relations other than\n",
      "those logically im-\n",
      "plied by the Markov\n",
      "Condition.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-344.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "348 20. Directed Graphs\n",
      "\n",
      "—\\-—_— 7 i——\\Y<—2%\n",
      "\n",
      "X<x—Y—_zZ X——_>yY<«Z\n",
      "FIGURE 20.5. The first three DAG’s have no colliders. The fourth\n",
      "\n",
      "DAG in the lower right corner has a collider at Y.\n",
      "\n",
      "X——~>y<«—Z\n",
      "\n",
      "|\n",
      "\n",
      "Ww\n",
      "\n",
      "FIGURE 20.6. A collider with a descendent.\n",
      "\n",
      " \n",
      "\n",
      "The rules of d-separation.\n",
      "\n",
      "Consider the DAG’s in Figures 20.5 and 20.6.\n",
      "Rule (1). In a non-collider, X and Z are d-connected,\n",
      "but they are d-separated given Y.\n",
      "Rule (2). If X and Z collide at Y then X and Z are\n",
      "d-separated but they are d-connected given Y.\n",
      "Rule (3). Conditioning on the descendant of a collider\n",
      "has the same effect as conditioning on the collider. Thus\n",
      "in Figure 20.6, X and Z are d-separated but they are\n",
      "d-connected given W.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Here is a more formal definition of d-separation. Let X and Y\n",
      "be distinct vertices and let W bea set of vertices not containing\n",
      "X or Y. Then X and Y are d-separated given W if there\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-345.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "20.4 More Independence Relations 349\n",
      "\n",
      "OU << Y\n",
      "\n",
      ",\n",
      "\n",
      "St Sp\n",
      "FIGURE 20.7. d-separation explained.\n",
      "\n",
      "exists no undirected path U between X and Y such that (i)\n",
      "every collider on U has a descendant in W and (ii) no other\n",
      "vertex on U is in W. If U,V and W are distinct sets of vertices\n",
      "and U and V are not empty, then U and V are d-separated given\n",
      "W if for every X € U and Y € V, X and Y are d-separated\n",
      "given W. Vertices that are not d-separated are said to be d-\n",
      "connected.\n",
      "\n",
      "Example 20.6 Consider the DAG in Figure 20.7. From the d-\n",
      "seperation rules we conclude that:\n",
      "\n",
      "X and Y are d-separated (given the empty set)\n",
      "\n",
      "X and Y are d-connected given {5;, 52}\n",
      "\n",
      "X and Y are d-separated given {5,,S2,V}\n",
      "\n",
      "Theorem 20.7 (Spirtes, Glymour and Scheines) Let A, B\n",
      "and C be disjoint sets of vertices. Then AIL B | C if and only\n",
      "if A and B are d-separated by C.\n",
      "\n",
      "Example 20.8 The fact that conditioning on a collider creates\n",
      "dependence might not seem intuitive. Here is a whimsical ex-\n",
      "ample from Jordan (2003) that makes this idea more palatable.\n",
      "Your friend appears to be late for a meeting with you. There\n",
      "are two explanations: she was abducted by aliens or you forgot\n",
      "to set your watch ahead one hour for daylight savings time. See\n",
      "Figure 20.8 Aliens and Watch are blocked by a collider which\n",
      "implies they are marginally independent. This seems reasonable\n",
      "since, before we know anything about your friend being late, we\n",
      "would expect these variables to be independent. We would also\n",
      "expect that P(Aliens = yes|Late = Yes) > P(Aliens = yes);\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-346.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 20. Directed Graphs\n",
      "\n",
      "aliens watch\n",
      "\n",
      "NZ\n",
      "\n",
      "late\n",
      "\n",
      "FIGURE 20.8. Jordan’s alien example. Was your friend kidnapped\n",
      "by aliens or did you forget to set your watch?\n",
      "\n",
      "learning that your friend is late certainly increases the probabil-\n",
      "ity that she was abducted. But when we learn that you forgot to\n",
      "set your watch properly, we would lower the chance that your\n",
      "friend was abducted. Hence, P(Aliens = yes|Late = Yes) #\n",
      "P(Aliens = yes|Late = Yes,Watch = no). Thus, Aliens and\n",
      "Watch are dependent given Late.\n",
      "\n",
      "Graphs that look different may actually imply the same inde-\n",
      "pendence relations. If G is a DAG, we let Z(G) denote all the in-\n",
      "dependence statements implied by G. Two DAG’s G, and Gz for\n",
      "the same variables V are Markov equivalent if Z(G,) = Z(G2).\n",
      "Given a DAG G, let skeleton(G) denote the undirected graph ob-\n",
      "tained by replacing the arrows with undirected edges.\n",
      "\n",
      "Theorem 20.9 Two DAG’s G, and Gz are Markov equivalent if\n",
      "and only if (i) skeleton(G,) = skeleton(G2) and (ii) G, and G2\n",
      "have the same colliders.\n",
      "\n",
      "Example 20.10 The first three DAG’s in Figure 20.5 are Markov\n",
      "equivalent. The DAG in the lower right of the Figure is not\n",
      "Markov equivalent to the others.\n",
      "\n",
      "20.5 Estimation for DAG’s\n",
      "\n",
      "Let G bea DAG. Assume that all the variables V = (X),.-., Xm)\n",
      "are discrete. The probability function can be written\n",
      "\n",
      "k\n",
      "\n",
      "pv) = [] ra | mi). (20.3)\n",
      "\n",
      "i=l\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-347.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "20.5 Estimation for DAG’s 351\n",
      "\n",
      "To estimate p(v) we need to estimate p(x; | 7;) for each i. Think\n",
      "of the parents of X; as one discrete variable X; with many levels.\n",
      "For example, suppose that X3 has parents X; and X, and that\n",
      "X, € {0,1,2} while X, € {0,1}. We can regard the parents X;\n",
      "and X» asa single variable ce defined by\n",
      "\n",
      "1 if X,=0,X,=0\n",
      "\n",
      " \n",
      "\n",
      "@ IfX, SOpiy= 1\n",
      "za) 3 PM =1%=0\n",
      "3) 4 ifX;=1,X,=1\n",
      "\n",
      " \n",
      "\n",
      "5 if X,=2,X).=0\n",
      "6 if X;=2,X),=1.\n",
      "\n",
      "Hence we can write\n",
      "\n",
      "(20.4)\n",
      "\n",
      " \n",
      "\n",
      "Theorem 20.11 Let Vi,...,V, be 1D random vectors from dis-\n",
      "tribution p given in (20.4). The maximum likelihood estimator\n",
      "of p is\n",
      "\n",
      "plo) = [] ax: | %) (20.5)\n",
      "\n",
      "where\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      ": X= a}\n",
      "\n",
      "Example 20.12 Let V = (X,Y, Z) have the DAG in the top left of\n",
      "Figure 20.5. Given n observations (X,,Y\\,Z1),---;(Xns Yn: Zn),\n",
      "the MLE of p(x, y, 2) = p(x)p(y|x)p(zly) ts\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "BAS #{i: X;= 2}\n",
      "\n",
      "aa) =H Sea)\n",
      "was #H4{i: X;= a2 and Y; = y}\n",
      "Ply|x) His vay} j\n",
      "\n",
      "and\n",
      "_ el Ya pond 2= 2}\n",
      "\n",
      "Plzly) Fle Zaz}\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-348.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "352 20. Directed Graphs\n",
      "\n",
      "It is possible to extend these ideas to continuous random vari-\n",
      "ables as well. For example, we might use some parametric model\n",
      "p(2\\|%.;9,) for each condiitonal density. The likelihood function\n",
      "is then\n",
      "\n",
      " \n",
      "\n",
      "where Xj; is the value of X; for the it data point and 6; are the\n",
      "parameters for the j*\" conditional density. We can then proceed\n",
      "using maximum likelihood.\n",
      "\n",
      "So far, we have assumed that the structure of the DAG is\n",
      "given. One can also try to estimate the structure of the DAG it-\n",
      "self from the data. For example, we could fit every possible DAG\n",
      "using maximum likelihood and use AIC (or some other method)\n",
      "to choose a DAG. However, there are many possible DAG’s so\n",
      "you would need much data for such a method to be reliable.\n",
      "Producing a valid, accurate confidence set for the DAG struc-\n",
      "ture would require astronomical sample sizes. DAG’s are thus\n",
      "most useful for encoding conditional independence information\n",
      "rather than discovering it.\n",
      "\n",
      "20.6 Causation Revisited\n",
      "\n",
      "We discussed causation in Chapter 19 using the idea of coun-\n",
      "terfactual random variables. A different approach to causation\n",
      "uses DAG’s. The two approaches are mathematically equivalent\n",
      "though they appear to be quite different. In Chapter 19, the\n",
      "extra element we added to clarify causation was the idea of a\n",
      "counterfactual. In the DAG approach, the extra element is the\n",
      "idea of intervention. Consider the DAG in Figure 20.9.\n",
      "\n",
      "The probability function for a distribution consistent with\n",
      "this DAG has the form p(x, y, z) = p(x)p(y|x)p(z|2, y). Here is\n",
      "pseudo-code for generating from this distribution:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-349.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "20.6 Causation Revisited 353\n",
      "\n",
      "“oN\n",
      "\n",
      "xX ~>Yozs\n",
      "FIGURE 20.9. Conditioning versus intervening.\n",
      "\n",
      " \n",
      "\n",
      "fore SA, nought\n",
      "ti << px(i)\n",
      "Yi S Pyx(yiles)\n",
      "as pyeyla sy)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Suppose we repeat this code many times yielding data (x), 1, 21),---, (ns Yns2n)-\n",
      "Among all the times that we observe Y = y, how often is Z = z?\n",
      "The answer to this question is given by the conditional distri-\n",
      "bution of Z|Y. Specifically,\n",
      "\n",
      " \n",
      "\n",
      "P(Y =y,Z=2z) _ ply,z)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      "Biemali ) PV=) pw)\n",
      "dX. P(t,y,2) _ Yo, plz) p(ylz) p(zlx,y)\n",
      "ply) ply)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Tele! MO PDS ples)\n",
      "Sn .y) P(aly)-\n",
      "\n",
      "EB\n",
      "\n",
      "    \n",
      "\n",
      "Now suppose we intervene by changing the computer code.\n",
      "Specifically, suppose we fix Y at the value y. The code now\n",
      "looks like this:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-350.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "354 20. Directed Graphs\n",
      "\n",
      " \n",
      "\n",
      "setY = y\n",
      "fori = 5M.\n",
      "i <<\n",
      "Bs y(2iltisy)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Having set Y = y, how often was Z = z? To answer, note\n",
      "that the intervention has changed the joint probability to be\n",
      "\n",
      " \n",
      "\n",
      "p’(x,2) = p(x)p(z|2,y).\n",
      "\n",
      "The answer to our question is given by the marginal distribution\n",
      "\n",
      "(2) = So p\"(#,2) = So pla)plz\n",
      "\n",
      " \n",
      "\n",
      "ij)\n",
      "\n",
      "We shall denote this as P(Z = 2|Y := y) or p(z|Y := y). We call\n",
      "(Z = 2|Y = y) conditioning by observation or passive\n",
      "conditioning. We call P(Z = z|Y := y) conditioning by\n",
      "intervention or active conditioning.\n",
      "\n",
      " \n",
      "\n",
      "Passive conditioning is used to answer a predictive question\n",
      "ike:\n",
      "\n",
      "“Given that Joe smokes, what is the probability he will get lung\n",
      "cancer?”\n",
      "\n",
      " \n",
      "\n",
      "Active conditioning is used to answer a causal question like:\n",
      "\n",
      "“If Joe quits smoking, what is the probability he will get lung\n",
      "cancer?”\n",
      "\n",
      "Consider a pair (G, P) where G is a DAG and P is a distribu-\n",
      "tion for the variables V of the DAG. Let p denote the probability\n",
      "function for P. Consider intervening and fixing a variable X to\n",
      "be equal to x. We represent the intervention by doing two things:\n",
      "\n",
      "(1) Create a new DAG G* by removing all arrows pointing\n",
      "into X;\n",
      "\n",
      "(2) Create a new distribution p*(v) = P(V = v[X := x) by\n",
      "removing the term p(x\n",
      "\n",
      " \n",
      "\n",
      "tx) from p(v).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-351.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "20.6 Causation Revisited 355\n",
      "\n",
      "The new pair (G*, P*) represents the intervention “set X =\n",
      "\n",
      "”\n",
      "\n",
      "=.\n",
      "\n",
      "Example 20.13 You may have noticed a correlation between rain\n",
      "and having a wet lawn, that is, the variable “Rain”\n",
      "dependent of the variable “Wet Lawn” and hence priw(r,w) #\n",
      "Pr(t)pw(w) where R denotes Rain and W denotes Wet Lawn.\n",
      "Consider the following two DAGS:\n",
      "\n",
      "is not in-\n",
      "\n",
      "Rain —> Wet Lawn Rain <— Wet Lawn.\n",
      "\n",
      "The first DAG implies that p(w,r) = p(r)p(w|r) while the sec-\n",
      "ond implies that p(w,r) = p(w)p(r|w) No matter what the joint\n",
      "distribution p(w,r) is, both graphs are correct. Both imply that\n",
      "R and W are not independent. But, intuitively, if we want a\n",
      "graph to indicate causation, the first graph is right and the sec-\n",
      "ond is wrong. Throwing water on your lawn doesn’t cause rain.\n",
      "The reason we feel the first is correct while the second is wrong is\n",
      "because the interventions implied by the first graph are correct.\n",
      "\n",
      "Look at the first graph and form the intervention W =1 where\n",
      "1 denotes “wet lawn.” Following the rules of intervention, we\n",
      "break the arrows into W to get the modified graph:\n",
      "\n",
      " \n",
      "\n",
      "Rain set Wet Lawn =/\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "with distribution p*(r) = p(r). Thus P(R =r | W := w) =\n",
      "P(R=r) tells us that “wet lawn” does not cause rain.\n",
      "\n",
      "Suppose we (wrongly) assume that the second graph is the\n",
      "correct causal graph and form the intervention W = 1 on the\n",
      "second graph. There are no arrows into W that need to be broken\n",
      "so the intervention graph is the same as the original graph. Thus\n",
      "P(r) =p(r|w) which would imply that changing “wet” changes\n",
      "“rain.” Clearly, this is nonsense.\n",
      "\n",
      "Both are correct probability graphs but only the first is correct\n",
      "causally. We know the correct causal graph by using background\n",
      "knowledge.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-352.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "356 20. Directed Graphs\n",
      "\n",
      "Remark 20.14 We could try to learn the correct causal graph\n",
      "from data but this is dangerous. In fact it is impossible with two\n",
      "variables. With more than two variables there are methods that\n",
      "can find the causal graph under certain assumptions but they are\n",
      "large sample methods and, furthermore, there is no way to ever\n",
      "know if the sample size you have is large enough to make the\n",
      "methods reliable.\n",
      "\n",
      "We can use DAG’s to represent confouding variables. If X is\n",
      "a treatment and Y is an outcome, a confounding variable Z is\n",
      "a variable with arrows into both X and Y; see Figure 20.10. It\n",
      "is easy to check, using the formalism of interventions, that the\n",
      "following facts are true.\n",
      "\n",
      "In a randomized study, the arrow betwen Z and X is bro-\n",
      "ken. In this case, even with Z unobserved (represented by en-\n",
      "closing Z in a circle), the causal relationship between X and\n",
      "Y is estimable because it can be shown that E(Y|X := 2) =\n",
      "E(Y|X = x) which does not involve the unobserved Z. In\n",
      "an observational study, with all confounders observed, we get\n",
      "E(Y|X := 2) = fE(Y|X = 2,Z = z)dFy(z) as in formula\n",
      "(19.8). If Z is unobserved then we cannot estimate the causal\n",
      "effect because E(Y|X := 2) = fE(Y|X = 2,Z = z)dFz(z)\n",
      "involves the unobserved Z. We can’t just use X and Y since in\n",
      "this case. P(Y = y|X = 2) #4 P(Y = y|X := 2) which is just\n",
      "another way of saying that causation is not association.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "In fact, we can make a precise connection between DAG’s and\n",
      "counterfactuals as follows. Suppose that X and Y are binary.\n",
      "Define the confounding variable Z by\n",
      "\n",
      "1 if (Co,C1) = (0,0)\n",
      "ga) 2 (GC) =(0.1)\n",
      "3 if (Co, C1) = (1,0)\n",
      "4 if (GC) = (10).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-353.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "20.7 Bibliographic Remarks 357\n",
      "\n",
      "ALN SK\n",
      "\n",
      "x —__> Y \\——>> Y\n",
      "\n",
      "HCE 20.10. Homdonriscd | study; Observational study with mea-\n",
      "sured confounders; Observational study with unmeasured con-\n",
      "founders. The circled variables are unobserved.\n",
      "\n",
      "From this, you can make the correspondence between the DAG\n",
      "approach and the counterfactual approach explicit. I leave this\n",
      "for the interested reader.\n",
      "\n",
      "20.7 Bibliographic Remarks\n",
      "\n",
      "There are a number of texts on DAG’s including Edwards (1996)\n",
      "and Jordan (2003). The first use of DAG’s for representing\n",
      "causal relationships was by Wright (1934). Modern tratments\n",
      "are contained in Spirtes, Glymour, and Scheines (1990) and\n",
      "Pearl (2000). Robins, Scheines, Spirtes and Wasserman (2003)\n",
      "discuss the problems with estimating causal structure from data.\n",
      "\n",
      "20.8 Exercises\n",
      "\n",
      "1. Consider the three DAG’s in Figure 20.5 without a col-\n",
      "lider. Prove that X II Z|Y.\n",
      "\n",
      "2. Consider the DAG in Figure 20.5 with a collider. Prove\n",
      "that X Il Z and that X and Z are dependent given Y.\n",
      "\n",
      "3. Let X € {0,1}, Y € {0,1}, Z € {0,1,2}. Suppose the\n",
      "distribution of (X,Y, Z) is Markov to:\n",
      "\n",
      "XYZ\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-354.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "3\n",
      "\n",
      "5\n",
      "\n",
      "8\n",
      "\n",
      "20. Directed Graphs\n",
      "\n",
      "Create a joint distribution p(x, y, z) that is Markov to this\n",
      "DAG. Generate 1000 random vectors from this distribu-\n",
      "tion. Estimate the distribution from the data using max-\n",
      "imum likelihood. Compare the estimated distribution to\n",
      "the true distribution. Let 6 = (O00, 9001,---, 9112) where\n",
      "6,4 = P(X =1,Y =s5,Z = t). Use the bootstrap to get\n",
      "standard errors and 95 per cent confidence intervals for\n",
      "these 12 parameters.\n",
      "\n",
      "Let V = (X,Y, Z) have the following joint distribution\n",
      ". (1\n",
      "X ~ Bernoulli (5)\n",
      "\n",
      "elt-2\n",
      "Bernoulli {| ——~\n",
      "1+et-?\n",
      "\n",
      ": : ; e2le+y)—2\n",
      "Z\\|X=2,Y =y ~ Bernoulli (a5) x\n",
      "\n",
      "2\n",
      "\n",
      "Y|X=a\n",
      "\n",
      "(a) Find an expression for P(Z = z | Y = y). In particular,\n",
      "find (Z=1|Y=1).\n",
      "\n",
      "(b) Write a program to simulate the model. Conduct a\n",
      "simulation and compute P(Z = 1 | Y = 1) empirically.\n",
      "Plot this as a function of the simulation size N. It should\n",
      "converge to the theoretical value you computed in (a).\n",
      "\n",
      "(c) Write down an expression for P(Z = 1 | Y := y). In\n",
      "particular, find P(Z =1|Y :=1).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-355.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "20.8 Exercises 359\n",
      "\n",
      "(d) Modify your program to simulate the intervention “set\n",
      "Y = 1.” Conduct a simulation and compute P(Z = 1 |\n",
      "Y := 1) empirically. Plot this as a function of the simu-\n",
      "lation size N. It should converge to the theoretical value\n",
      "you computed in (c).\n",
      "\n",
      ". This isa continuous, Gaussian version of the last question.\n",
      "Let V = (X,Y, Z) have the following joint distribution\n",
      "\n",
      "X ~ Normal (0,1)\n",
      "Y|X=2\n",
      "Z |X aay sy\n",
      "\n",
      "é\n",
      "\n",
      "Normal (ax, 1)\n",
      "\n",
      "Normal (8y + y2, 1).\n",
      "\n",
      "é\n",
      "\n",
      "Here, a, 3 and ¥ are fixed parameters. Economists refer to\n",
      "models like this as structural equation models.\n",
      "\n",
      "(a) Find an explicit expression for f(z | y) and E(Z | Y =\n",
      "uy) =f 2flz | y)dz.\n",
      "\n",
      "(b) Find an explicit expression for f(z | Y := y) and then\n",
      "find E(Z | Y :=y) = f 2f(z | Y = y)dy. Compare to (b).\n",
      "\n",
      " \n",
      "\n",
      "(c) Find the joint distribution of (Y,Z). Find the correla-\n",
      "tion p between Y and Z.\n",
      "\n",
      "(d) Suppose that X is not observed and we try to make\n",
      "causal conclusions from the marginal distribution of (Y, Z).\n",
      "(Think of X as unobserved confounding variables.) In par-\n",
      "ticular, suppose we declare that Y causes Z if p # 0 and\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-356.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "360\n",
      "\n",
      "20. Directed Graphs\n",
      "\n",
      "we declare that Y does not cause Z if p = 0. Show that\n",
      "this will lead to erroneous conclusions.\n",
      "\n",
      "(e) Suppose we conduct a randomized experiment in which\n",
      "Y is randomly assigned. To be concrete, suppose that\n",
      "\n",
      "X ~ Normal(0, 1)\n",
      "Y ~ Normal(a, 1)\n",
      "Z|X=2,Y=y ~ Normal(Sy+ 2,1).\n",
      "Show that the method in (d) now yields correct conclu-\n",
      "\n",
      "sions i.e. p = 0 ifand only if f(z | Y := y) does not depend\n",
      "on y.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-357.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "21\n",
      "\n",
      "Nonparametric Curve Estimation\n",
      "\n",
      "In this Chapter we discuss nonparametric estimation of proba-\n",
      "bility density functions and regression functions which we refer\n",
      "to as curve estimation.\n",
      "\n",
      "In Chapter 8 we saw that it is possible to consistently estimate\n",
      "a cumulative distribution function F without making any as-\n",
      "sumptions about F’. If we want to estimate a probability density\n",
      "X =x) the\n",
      "situation is different. We cannot estimate these functions con-\n",
      "\n",
      "function f(x) or a regression function r(x) = E(Y\n",
      "\n",
      " \n",
      "\n",
      "sistently without making some smoothness assumptions. Corre-\n",
      "spondingly, we will perform some sort of smoothing operation\n",
      "on the data.\n",
      "\n",
      "A simple example of a density estimator is a histogram,\n",
      "which we discuss in detail in Section 21.2. To form a histogram\n",
      "estimator of a density f, we divide the real line to disjoint sets\n",
      "called bins. The histogram estimator is piecewise constant func-\n",
      "tion where the height of the function is proportional to number\n",
      "of observations in each bin; see Figure 21.3. The number of bins\n",
      "is an example of a smoothing parameter. If we smooth too\n",
      "\n",
      "This is page 359\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-358.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "360 21. Nonparametric Curve Estimation\n",
      "\n",
      "g(@)\n",
      "\n",
      "This is a function of the data This is the point at which we are\n",
      "evaluating G(-)\n",
      "\n",
      " \n",
      "\n",
      "FIGURE 21.1. A curve estimate g is random because it is a function\n",
      "of the data. The point x at which we evaluate g is not a random\n",
      "variable.\n",
      "\n",
      "much (large bins) we get a highly biased estimator while if we\n",
      "smooth too little (small bins) we get a highly variable estimator.\n",
      "Much of curve estimation is concerned with trying to optimally\n",
      "balance variance and bias.\n",
      "\n",
      "21.1 The Bias-Variance Tradeoff\n",
      "\n",
      "Let g denote an unknown function and let g, denote an es-\n",
      "timator of g. Bear in mind that g(x) is a random function\n",
      "evaluated at a point 2. G, is random because it depends on\n",
      "the data. Indeed, we could be more explicit and write g,(x) =\n",
      "\n",
      "h,(X,-..,X,,) to show that g,(z) is a function of the data\n",
      "X,...,X, and that the function could be different for each x.\n",
      "\n",
      "See Figure 21.1.\n",
      "Asa loss function, we will use the integrated squared error\n",
      "(ISE):\n",
      "\n",
      "Lan) = f (al) ~ Gal)? a (21.1)\n",
      "\n",
      "The risk or mean integrated squared error (MISE) is\n",
      "\n",
      "RES) =E(L(9,9))- (21.2)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-359.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.2 Histograms 361\n",
      "\n",
      "Lemma 21.1 The risk can be written as\n",
      "R(G.Gn) = [re ar+ f w(a)ae (21.3)\n",
      "where\n",
      "b(2) =EGal2)) — 9(x) (21.4)\n",
      "is the bias of G,(z) at a fired x and\n",
      "o() = VGu(0)) = E( Galx) —EGu(2))”)) (21.5)\n",
      "is the variance of G,(x) at a fixed x.\n",
      "\n",
      "In summary,\n",
      "\n",
      " \n",
      "\n",
      "RISK = BIAS? + VARIANCE. (21.6)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "When the data are over-smoothed, the bias term is large and\n",
      "the variance is small. When the data are under-smoothed the op-\n",
      "posite is true; see Figure 21.2. This is called the bias-variance\n",
      "trade-off. Minimizing risk corresponds to balancing bias and\n",
      "variance.\n",
      "\n",
      "21.2 Histograms\n",
      "\n",
      "Let X,...,X, be 11D on [0,1] with density f. The restriction\n",
      "to [0,1] is not crucial; we can always rescale the data to be on\n",
      "this interval. Let m be an integer and define bins\n",
      "\n",
      "Bi [° ~) Ba | =). 2, Bin = awe (21.7)\n",
      "m mm m\n",
      "\n",
      "Define the binwidth h = 1/m, let v; be the number of obser-\n",
      "vations in B;, let pj =v;/n and let p; = f, f(u)du.\n",
      "7\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-360.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "362 21. Nonparametric Curve Estimation\n",
      "\n",
      " \n",
      "\n",
      "0.10\n",
      "L\n",
      "\n",
      "0.08\n",
      "i\n",
      "\n",
      "0.06\n",
      "L\n",
      "\n",
      "0.04\n",
      "L\n",
      "\n",
      " \n",
      "\n",
      "= ~ Variance\n",
      "\n",
      "\\\n",
      "\n",
      "0.02\n",
      "L\n",
      "\n",
      "Bias squared =e\n",
      "\n",
      "Optimal Smoothing\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "T T T T T\n",
      "o.4 0.2 0.3 0.4 os\n",
      "\n",
      " \n",
      "\n",
      "more smoothing —\n",
      "\n",
      "FIGURE 21.2. The Bias- Variance trade-off. The bias increases and\n",
      "the variance decreases with the amount of smoothing. The optimal\n",
      "amount of smoothing, indicated by the vertical line, minimizes the\n",
      "risk = bias” + variance.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-361.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "21.2 Histograms 363\n",
      "\n",
      "The histogram estimator is defined by\n",
      "p/h cEeB\n",
      "\n",
      "Fo) _ pil \" € By\n",
      "\n",
      "a /h # € Bn\n",
      "\n",
      "which we can write more succinctly as\n",
      "\n",
      " \n",
      "\n",
      "Pla) = Be € By). (21.8)\n",
      "a To\n",
      "\n",
      "understand the motivation for this estimator, let pj = des f(u)du\n",
      "5\n",
      "and note that, for « € B; and h small,\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Fig) Bi we Pi Sm FH Playh\n",
      "fale) hh h ~ F(x)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "J (2).\n",
      "\n",
      "Example 21.2 Figure 21.3 shows three different histograms based\n",
      "on n = 1,266 data points from an astronomical sky survey.\n",
      "Each data point represents the distance from us to a galazy.\n",
      "The galaxies lie on a “pencilbeam” pointing directly from the\n",
      "Earth out into space. Because of the finite speed of light, look-\n",
      "ing at galaxies farther and farther away corresponds to looking\n",
      "back in time. Choosing the right number of bins involves finding\n",
      "a good tradeoff between bias and variance. We shall see later\n",
      "that the top left histogram has too few bins resulting in over-\n",
      "smoothing and too much bias. The bottom left histogram has too\n",
      "many bins resulting in undersmoothing and too few bins. The\n",
      "top right histogram is just right. The histogram reveals the pres-\n",
      "ence of clusters of galaxies. Seeing how the size and number of\n",
      "galaxy clusters varies with time, helps cosmologists understand\n",
      "the evolution of the universe.\n",
      "\n",
      "The mean and variance of fax) are given in the following\n",
      "Theorem.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-362.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "60\n",
      "\n",
      "40\n",
      "\n",
      "20\n",
      "\n",
      " \n",
      "\n",
      "364 21. Nonparametric Curve Estimation\n",
      "\n",
      " \n",
      "\n",
      "40\n",
      "\n",
      " \n",
      "\n",
      "30\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "20\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "10\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "0.00\n",
      "\n",
      " \n",
      "\n",
      "T T T 1\n",
      "0.05 0.10 0.15 0.20\n",
      "\n",
      "Oversmoothed\n",
      "\n",
      "cross validation score\n",
      "-10\n",
      "\n",
      "~12\n",
      "\n",
      "14\n",
      "\n",
      " \n",
      "\n",
      "0.00\n",
      "\n",
      "T T T 1\n",
      "0.05 0.10 0.15 0.20\n",
      "\n",
      "Undersmoothed\n",
      "\n",
      "FIGURE 21.3. Three versions of a histogram for the as\n",
      "togram has too few bins. The bottom left his\n",
      "\n",
      "The top left. his\n",
      "\n",
      " \n",
      "\n",
      "has too many bins. The top right histogram is\n",
      "timated risk versus the number of bins.\n",
      "\n",
      " \n",
      "\n",
      "right plot shows the e\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "0.00 0.05 0.10 0.15 0.20\n",
      "\n",
      "Just Right\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "T T T T T T\n",
      "0 200 400 600 800 1000\n",
      "\n",
      "number of bins\n",
      "\n",
      " \n",
      "\n",
      "tronomy data.\n",
      "togram\n",
      "just right. The lower,\n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-363.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "21.2 Histograms 365\n",
      "\n",
      "Theorem 21.3 Consider fired x and fired m, and let B; be the\n",
      "bin containing x. Then,\n",
      "\n",
      "E(fol2)) = 2 and Wiala)) =P) ar)\n",
      "\n",
      "Let’s take a closer look at the bias-variance tradeoff using\n",
      "equation (21.9) . Consider some x € B;. For any other u € B;,\n",
      "\n",
      "f(u) © f(a) + (ua) f\"(2)\n",
      "\n",
      "and so\n",
      "\n",
      "Pj =[ f(ujdu x [ (F(a) +(u = 2)f'(0)) du\n",
      "\n",
      ". Foyh+hf'(a) (« (: - *) ) ,\n",
      "\n",
      "Therefore, the bias b(x) is\n",
      "(x) = Elfa(2)) — fle) = — f(a)\n",
      "\n",
      "f(a)h +hf'(x) (h(j— 3) —2)\n",
      "h\n",
      "\n",
      "- r9(6(i-2)-9)\n",
      "\n",
      "If ; is the center of the bin, then\n",
      "\n",
      "[ Pere x [ver (1-2) -2) wu\n",
      "wr@yr f (n(s-3) - n) ds\n",
      "\n",
      "i\n",
      "\n",
      "Il\n",
      "|\n",
      "R\n",
      "\n",
      " \n",
      "\n",
      "— (2)\n",
      "\n",
      "2\n",
      "\n",
      "2\n",
      "\n",
      "bw gh?\n",
      "= (GPS.\n",
      "Therefore,\n",
      "‘o . 2 SA enya yy\n",
      "[ Poe [ Holdrs DIN\n",
      "ee h\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-364.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "366 21. Nonparametric Curve Estimation\n",
      "\n",
      "Note that this increases as a function of h. Now consider the\n",
      "variance. For h small, 1 — p; % 1, so\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "f(a)h+hf'(x) (h (j — 4) —2)\n",
      "nh?\n",
      "~ fee)\n",
      "nh\n",
      "\n",
      "where we have kept only the dominant term. So,\n",
      "\n",
      "Note that this decreases with h. Putting all this together,\n",
      "\n",
      "4\n",
      "\n",
      "we\n",
      "get:\n",
      "\n",
      " \n",
      "\n",
      "Theorem 21.4 Suppose that {(f'(u))?du < oo. Then\n",
      "\n",
      "h?\n",
      "\n",
      "Rif) & 5 freoyeau + —. (21.10)\n",
      "\n",
      "The value h* that minimizes (21.10) is\n",
      "\n",
      "ok 6 1/3\n",
      "=a (gam) een)\n",
      "\n",
      "With this choice of binwidth,\n",
      "\n",
      "Y\n",
      "\n",
      "és 6\n",
      "R(fns f) ~} pe (21.12)\n",
      "\n",
      "where C = (3/4)?/3 (sU\"(w)?au) .\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 21.4 is quite revealing. We see that with an opti-\n",
      "mally chosen binwidth, the MISE decreases to 0 at rate n7?/9,\n",
      "By comparison, most parametric estimators converge at rate\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-365.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "21.2 Histograms 367\n",
      "\n",
      "n-'. The slower rate of convergence is the price we pay for be-\n",
      "ing nonparametric. The formula for the optimal binwidth h* is\n",
      "of theoretical interest but it is not useful in practice since it\n",
      "depends on the unknown function /f.\n",
      "\n",
      "A practical way to choose the binwidth is to estimate the\n",
      "risk function and minimize over h. Recall that the loss function,\n",
      "which we now write as a function of h, is\n",
      "\n",
      "Ln) =f (fale) = swyPae\n",
      "= [ Rejie—2f Rooars f Peas\n",
      "\n",
      "The last term does not depend on the binwidth h so minimizing\n",
      "the risk is equivalent to minimizing the expected value of\n",
      "\n",
      "10) = [ Playae—2 f Falays ac.\n",
      "\n",
      "We shall refer to E(J(h)) as the risk, although it differs from\n",
      "the true risk by the constant term f f?(x) dx.\n",
      "\n",
      " \n",
      "\n",
      "Definition 21.5 The cross-validation estimator of risk\n",
      "\n",
      "To) = f (fala) de 2 Fol%) (21.18)\n",
      "\n",
      "where fi» is the histogram estimator obtained after re-\n",
      "moving the i*” observation. We refer to J(h) as the cross-\n",
      "validation score or estimated risk.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 21.6 The cross-validation estimator is nearly unbiased:\n",
      "\n",
      "E(J(x)) & E(J(2)).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-366.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "368 21. Nonparametric Curve Estimation\n",
      "\n",
      "In principle, we need to recompute the histogram n times to\n",
      "\n",
      "compute J(h). Moreover, this has to be done for all values of h.\n",
      "Fortunately, there is a shortcut formula.\n",
      "\n",
      "Theorem 21.7 The following identity holds:\n",
      "\n",
      " \n",
      "\n",
      "> 2 n+1 5 ;\n",
      "T(h) = hor @—D » 2 (21.14)\n",
      "Example 21.8 We used cross-validation in the astronomy exam-\n",
      "ple. The cross-validation function is quite flat near its mini-\n",
      "mum. Any m in the range of 73 to 310 is an approximate min-\n",
      "imizer but the resulting histogram does not change much over\n",
      "this range. The histogram in the top right plot in Figure 21.3\n",
      "was constructed using m= 73 bins. The bottom right plot shows\n",
      "the estimated risk, or more precisely, A, plotted versus the num-\n",
      "ber of bins.\n",
      "\n",
      "Next we want some sort of confidence set for f. Suppose i\n",
      "is a histogram with m bins and binwidth h = 1/m. We cannot\n",
      "realistically make confidence statements about the fine details of\n",
      "the true density f. Instead, we shall make confidence statements\n",
      "about f at the resolution of the histogram. To this end, define\n",
      "\n",
      "f(s) = a for « € B; (21.15)\n",
      "\n",
      "where pj = ie f(u)du which is a “histogramized” version of f.\n",
      "5\n",
      "\n",
      "Theorem 21.9 Let m = m(n) be the number of bins in the his-\n",
      "togram f,. Assume that m(n) + co and m(n)logn/n + 0 as\n",
      "n —> oo. Define\n",
      "\n",
      "&\n",
      "&\n",
      "\n",
      "Il\n",
      "——\n",
      "B\n",
      "B\n",
      "Ed\n",
      "a\n",
      "=”\n",
      "a\n",
      "|\n",
      "in\n",
      "=\n",
      "ecw,\n",
      "bet)\n",
      "\n",
      "©\n",
      "\n",
      "u(r) = ( Fata) +e) (21.16)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-367.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.2 Histograms 369\n",
      "\n",
      "= =n [= (21.17)\n",
      "\n",
      "lim P( E(x (x) < f(x) <u(z) for all ) >l-a. (21.18)\n",
      "\n",
      "n—¥00\n",
      "\n",
      "where\n",
      "\n",
      "Then,\n",
      "\n",
      "Proor. Here is an outline of the proof. A rigorous proof\n",
      "requires fairly sophisticated tools. From the central limit the-\n",
      "\n",
      "orem, p; © N(p;,p;(1 — p;)/n). By the delta method, \\/p; ~\n",
      "N( yp; 1/(4n)). Moreover, it can be shown that the \\/p;’s are\n",
      "approximately independent. Therefore,\n",
      "\n",
      "2vn( VB - vB,) © Zi (21.19)\n",
      "\n",
      "where Zj,..., Zm ~ N(0,1). Let A be the event that ¢(x) <\n",
      "f(x) < u(z) for all x. So,\n",
      "\n",
      "A = bx) < f(z) <u(2) forall a}\n",
      "= {May < JF) < Vue for atte}\n",
      "= VR) -c< Vila) < Vile) +e for all a}\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "    \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "    \n",
      "\n",
      " \n",
      "\n",
      "= fala) — V Fla)| <c}.\n",
      "Therefore,\n",
      "sam = Cr i\n",
      "\n",
      " \n",
      "\n",
      "Sala\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "max\n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      "Dj Dj —\n",
      "\n",
      "Se sot) alee\n",
      "\n",
      "(ws\n",
      "rly\n",
      "\n",
      "= mcs vB5| > zaram\n",
      "(»\n",
      "\n",
      "      \n",
      "\n",
      "> 2ev in\n",
      "\n",
      "  \n",
      "\n",
      "m\n",
      "\n",
      "x= P| max|Z,| > nj) < SEP (\\Z)| > zaj2m)\n",
      "\n",
      "ja\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-368.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "370 21. Nonparametric Curve Estimation\n",
      "\n",
      "“a\n",
      "= y —=a. —\n",
      "mr\n",
      "\n",
      "j=l\n",
      "\n",
      "Example 21.10 Figure 21.4 shows a 95 per cent confidence enve-\n",
      "lope for the astronomy data. We see that even with over 1,000\n",
      "data points, there is still substantial uncertainty. Mi\n",
      "\n",
      "21.3 Kernel Density Estimation\n",
      "\n",
      "Histograms are discontinuous. Kernel density estimators are\n",
      "smoother and they converge faster to the true density than his-\n",
      "tograms.\n",
      "\n",
      "Let X,,...,X,, denote the observed data, a sample from /f.\n",
      "In this chapter, a kernel is defined to be any smooth func-\n",
      "tion K such that K(x) > 0, f K(x)dz = 1, fxK(x)dx = 0\n",
      "and oj, = f x? K(x)dz > 0. Two examples of kernels are the\n",
      "Epanechnikov kernel\n",
      "\n",
      "xa={ 41 —2/5)/VB |al < v5 (1.20)\n",
      "\n",
      "°o\n",
      "\n",
      "otherwise\n",
      "\n",
      "and the Gaussian (Normal) kernel K(a) = (27)7!?e-\"'?.\n",
      "\n",
      " \n",
      "\n",
      "Definition 21.11 Given a kernel K and a positive number\n",
      "h, called the bandwidth, the kernel density estima-\n",
      "tor is defined to be\n",
      "\n",
      "fla) = > 7K (- =) . (21.21)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "An example of a kernel density estimator is show in Figure\n",
      "21.5. The kernel estimator effectively puts a smoothed out lump\n",
      "of mass of size 1/n over each data point X;. The bandwidth h\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-369.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "371\n",
      "\n",
      "21.3 Kernel Density Estimation\n",
      "\n",
      " \n",
      "\n",
      "     \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "0.20\n",
      "\n",
      "0.15\n",
      "\n",
      "0.10\n",
      "\n",
      "0.05\n",
      "\n",
      "0.00\n",
      "\n",
      "FIGURE 21.4. 95 per cent confidence envelope for astronomy data\n",
      "\n",
      "= 73 bins.\n",
      "\n",
      "using m.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-370.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "372 21. Nonparametric Curve Estimation\n",
      "\n",
      " \n",
      "\n",
      "0.10\n",
      "\n",
      " \n",
      "\n",
      "0.00\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "10 -5 ie)\n",
      "\n",
      "FIGURE 21.5. A kernel density estimator 2 At each point «, flz)\n",
      "is the average of the kernels centered over the data points X;. The\n",
      "data points are indicated by short vertical bars.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-371.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "21.3 Kernel Density Estimation 373\n",
      "\n",
      "controls the amount of smoothing. When h is close to 0, fi\n",
      "consists of a set of spikes, one at each data point. The height of\n",
      "the spikes tends to infinity as h + 0. When h — ov, fr tends\n",
      "to a uniform density.\n",
      "\n",
      "Example 21.12 Figure 21.6 shows kernel density estimators for\n",
      "the astronomy data using for three different bandwidths. In each\n",
      "case we used a Gaussian kernel. The properly smoothed kernel\n",
      "density estimator in the top right panel shows similar structure\n",
      "as the histogram. However, it is easier to see the clusters in the\n",
      "kernel estimator.\n",
      "\n",
      "To construct a kernel density estimator, we need to choose\n",
      "a kernel AK and a bandwidth h. It can be shown theoretically\n",
      "and empirically that the choice of K is not crucial. However, the\n",
      "choice of bandwidth h is very important. As with the histogram,\n",
      "we can make a theoretical statement about how the risk of the\n",
      "estimator depends on the bandwidth.\n",
      "\n",
      "Theorem 21.13 Under weak assumptions on f and K,\n",
      "\n",
      "RUF fn) & rok fue + Oe (21.22)\n",
      "\n",
      "nh\n",
      "where 0% = [2K (2) dx. The optimal bandwidth is\n",
      "\n",
      "-2/5 1/5 1/5\n",
      "eae\n",
      "hes — (21.23)\n",
      "where cy, = f 2? K(x)dz, c= f K(x)?dx and cs = [(f\"(x))?dz.\n",
      "With this choice of ane\n",
      "\n",
      "Rf fa)\n",
      "\n",
      "C4\n",
      "ni/s\n",
      "\n",
      "-\n",
      "\n",
      "for some constant cy > 0.\n",
      "\n",
      "PRooF. Write Ky(2,X) = h-'K ((a — X)/h) and Jn(x) =\n",
      "nS, Ki (v, X,). Thus, E[f, (2)] = E[K;,(w, X)] and Vif (0)] =\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-372.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "374 21. Nonparametric Curve Estimation\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "ae\n",
      "at\n",
      "a .\n",
      "24 si\n",
      "ou T T T T ° 4G T T T T J\n",
      "2\n",
      "3\n",
      ":\n",
      "2 :\n",
      "BZ 0\n",
      "8 Bs 7 7\n",
      "\n",
      "bandwidth\n",
      "undersmoothed\n",
      "\n",
      "FIGURE 21.6. Kernel density estimators and estimated risk for the\n",
      "astronomy data.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-373.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "21.3 Kernel Density Estimation 375\n",
      "\n",
      "n—'V[K,(x, X)]. Now,\n",
      "\n",
      " \n",
      "\n",
      "El, (2,X)] = eC) H(t)at\n",
      "[ Ketu=t)au\n",
      "[200 [Fe nr + Se 4) de\n",
      "\n",
      "f(wt+ sh p\"(u) [exw du-+:\n",
      "since f K(x) dx =1 and [2 K(x) dz =0. The bias is\n",
      "E[Ky(2, X)] — f(2) = SoRhs (0).\n",
      "\n",
      "By a similar calculation,\n",
      "\n",
      "Vso) = ALEC te,\n",
      "\n",
      "The second result follows from integrating the squared bias plus\n",
      "the variance. Hl\n",
      "\n",
      "We see that kernel estimators converge at rate n~“/> while his-\n",
      "tograms converge at the slower rate n~?/*. It can be shown that,\n",
      "under weak assumptions, there does not exist a nonparametric\n",
      "estimator that converges faster than n~4/>.\n",
      "\n",
      "The expression for h* depends on the unknown density f\n",
      "which makes the result of little practical use. As with the his-\n",
      "tograms, we shall use cross-validation to find a bandwidth. Thus,\n",
      "we estimate the risk (up to a constant) by\n",
      "\n",
      "Fh) = [ Pee: — 20 Ate) (21.24)\n",
      "\n",
      "where fa is the kernel density estimator after omitting the i\"\n",
      "observation.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-374.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "For large data sets,\n",
      "f and = (21.25)\n",
      "can be computed\n",
      "quickly using the\n",
      "fast Fourier trans-\n",
      "form.\n",
      "\n",
      "376 21. Nonparametric Curve Estimation\n",
      "\n",
      "Theorem 21.14 For any h > 0,\n",
      "E [F0H)] =E[J(h)].\n",
      "\n",
      "Also,\n",
      "\n",
      " \n",
      "\n",
      "s 1 wef REIGN DB we\n",
      "i) = EDK ( 7 ) + 2K (21.25)\n",
      "\n",
      "where K*(x) = K@)(x)-2K(x) and K(z) = [ K(z-y)K(y)dy.\n",
      "In particular, if K is a N(0,1) Gaussian kernel then K@(z) is\n",
      "the N(0,2) density.\n",
      "\n",
      "We then choose the bandwidth h, that minimizes J(h). A\n",
      "justification for this method is given by the following remarkable\n",
      "theorem due to Stone.\n",
      "\n",
      "Theorem 21.15 (Stone’s Theorem.) Suppose that f is bounded.\n",
      "Let f;, denote the kernel estimator with bandwidth h and let h,\n",
      "denote the bandwidth chosen by cross-validation. Then,\n",
      "\n",
      "f (F(@) _ faa(2)- dx P\n",
      "ints { (f(2) — fala) dn\n",
      "\n",
      " \n",
      "\n",
      "1. (21.26)\n",
      "\n",
      "Example 21.16 The top right panel of Figure 21.6 is based on\n",
      "cross-validation. These data are rounded which problems for\n",
      "cross-validation. Specifically, it causes the minimizer to be h =\n",
      "0. To overcome this problem, we added a small amount of ran-\n",
      "\n",
      "dom Normal noise to the data. The result is that J(h) is very\n",
      "smooth with a well defined minimum.\n",
      "\n",
      "Remark 21.17 Do not assume that, if the estimator fis wiggly,\n",
      "then cross-validation has let you down. The eye is not a good\n",
      "judge of risk.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-375.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "21.3 Kernel Density Estimation 377\n",
      "\n",
      "To construct confidence bands, we use something similar to\n",
      "histograms although the details are more complicated. The ver-\n",
      "sion described here is from Chaudhuri and Marron (1999).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-376.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "378 21. Nonparametric Curve Estimation\n",
      "Confidence Band for Kernel Density Estimator\n",
      "1. Choose an evenly spaced grid of points V = {v;,...,vy}-\n",
      "For every v € VY, define\n",
      "1 v—X;\n",
      "Y,(v) = —K -). 212\n",
      "i= px (S*) (21.27)\n",
      "Note that f,(v) =Y,(v), the average of the Y;(v)'s.\n",
      "2. Define\n",
      "(21.28)\n",
      "where s?(v) = (n — 1)! 0 (¥i(v) — Yn(v))?.\n",
      "3. Compute the effective sample size\n",
      "ESS' a (*) 21.29\n",
      "=k)\n",
      "Roughly, this is the number of data points being averaged\n",
      "together to form f,,(v).\n",
      "4. Let Vt = {v EV: ESS(v) > 5}. Now define the number\n",
      "of independent blocks m by\n",
      "1 BSS\n",
      "—= (21.30)\n",
      "m n\n",
      "where E'SS is the average of ESS over V*.\n",
      "5. Let Vy)\n",
      "1+(1—a)'/™\n",
      "q=o\" (At) (21.31)\n",
      "and define\n",
      "£(v) = f,(v) —qse(v) and u(v) =f,(v) + se (v)\n",
      "(21.32)\n",
      "where we replace ¢(v) with 0 if it becomes negative.\n",
      "Then.\n",
      "\n",
      "9)\n",
      "\n",
      "P{¢(v) < f(v) < u(v) for all v} wl-a.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-377.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.3 Kernel Density Estimation 379\n",
      "\n",
      "Example 21.18 Figure 21.7 shows approximate 95 per cent con-\n",
      "fidence bands for the astronomy data. Hi\n",
      "\n",
      "Suppose now that the data X; = (Xj,..., Xia) are d-dimensional.\n",
      "The kernel estimator can easily be generalized to d dimensions.\n",
      "Let h = (hy,..., hg) be a vector of bandwidths and define\n",
      "\n",
      "ix .\n",
      "= 5 Kalo — Xj) (21.33)\n",
      "i=l\n",
      "where\n",
      "K,(x — X) = {Ul K(4 ny (21.34)\n",
      "—.\n",
      "where h,,...,hqg are bandwidths. For simplicity, we might take\n",
      "\n",
      "hj = s;h where s, is the standard deviation of the j variable.\n",
      "There is now only a single bandwidth h to choose. Using calcu-\n",
      "lations like those in the one-dimensional case, the risk is given\n",
      "by\n",
      "\n",
      "R(fi fa) & rk bay ic oye St [Stat\n",
      "\n",
      "jfk\n",
      "LR\"\n",
      "nhy-++hg\n",
      "\n",
      "where fj; is the second partial derivative of f. The optimal\n",
      "bandwidth satisfies hy & c¢;n7/\"+®) leading to a risk of order\n",
      "n~/@+4_ Fyom this fact, we see that the risk increases quickly\n",
      "with dimension, a problem usually called the curse of dimen-\n",
      "sionality. To get a sense of how serious this problem is, con-\n",
      "sider the following table from Silverman (1986) which shows the\n",
      "sample size required to ensure a relative mean squared error less\n",
      "than 0.1 at 0 when the density is multivariate normal and the\n",
      "optimal bandwidth is selected.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-378.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "21. Nonparametric Curve Estimation\n",
      "\n",
      "380\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "See\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "ky\n",
      "\n",
      " \n",
      "\n",
      "  \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Or\n",
      "\n",
      "02\n",
      "\n",
      "oO\n",
      "\n",
      "0.20\n",
      "\n",
      "0.15\n",
      "\n",
      "0.10\n",
      "\n",
      "0.05\n",
      "\n",
      "0.00\n",
      "\n",
      "FIGURE 21.7. 95 per cent Confidence bands for kernel density esti-\n",
      "\n",
      "mate for the astronomy data.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-379.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "21.4 Nonparametric Regression 381\n",
      "\n",
      "Dimension Sample Size\n",
      "1 4\n",
      "\n",
      "19\n",
      "\n",
      "67\n",
      "\n",
      "223\n",
      "\n",
      "768\n",
      "2790\n",
      "10,700\n",
      "43,700\n",
      "187,000\n",
      "842,000\n",
      "\n",
      "No\n",
      "\n",
      "Noun\n",
      "\n",
      "ono)\n",
      "\n",
      "This is bad news indeed. It says that having 842,000 observa-\n",
      "tions in a ten dimensional problem is really like having 4 obser-\n",
      "\n",
      "vations.\n",
      "\n",
      "21.4 Nonparametric Regression\n",
      "\n",
      "Consider pairs of points (21, Yi),---,(@n, Yn) related by\n",
      "\n",
      "Y¥,=r(x) +6; (21.35)\n",
      "\n",
      "where E(e;) = 0 and we are treating the 2;’s as fixed. In nonpara-\n",
      "metric regression, we want to estimate the regression function\n",
      "r(z) =E(Y|X = 2).\n",
      "\n",
      "There are many nonparametric regression estimators. Most\n",
      "involve estimating r(x) by taking some sort of weighted average\n",
      "of the Y;’s, giving higher weight to those points near x. In par-\n",
      "ticular, the Nadaraya-Watson kernel estimator is defined\n",
      "by\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-380.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "382 21. Nonparametric Curve Estimation\n",
      "\n",
      " \n",
      "\n",
      "F(x) = So wi()¥i (21.36)\n",
      "i=l\n",
      "where the weights w,(x) are given by\n",
      "\n",
      "w;(x -* GS) _ 21.37\n",
      "=e SD\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The form of this estimator comes from first estimating the\n",
      "\n",
      "joint density f(z,y) using kernel density estimation and then\n",
      "inserting this into:\n",
      "\n",
      "r(z) = AX =e of a y\\n)dy — 1 vfleway\n",
      "re) =B(X =2)= f uf(vle)ay = ae\n",
      "\n",
      "Theorem 21.19 Suppose that V(e;) = 0”. The risk of the Nadaraya-\n",
      "Watson kernel estimator is\n",
      "\n",
      "Rar) ®& “(/ Kade) [ (+22) ao\n",
      "\n",
      "2 f K?(x)d:\n",
      "£ / o? f K?(x)dx -\n",
      "nhf (x)\n",
      "The optimal bandwidth decreases at rate n~'/> and with this\n",
      "choice the risk decreases at rate n~'/?,\n",
      "\n",
      " \n",
      "\n",
      "(21.38)\n",
      "\n",
      "In practice, to choose the bandwidth h we minimize the cross\n",
      "validation score\n",
      "\n",
      "n\n",
      "\n",
      "T(h) = (Vi -—Falai))? (21.39)\n",
      "\n",
      "i=l\n",
      "where 7_; is the estimator we get by omitting the i\" variable.\n",
      "An approximation to J is given by\n",
      "\n",
      "n\n",
      "\n",
      "Th) = 2%) - a)». 21.40)\n",
      "\n",
      "i=l fhe K(0)\n",
      "a KS\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-381.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80ED788>\n",
      "21.4 Nonparametric Regression 383\n",
      "\n",
      "Example 21.20 Figures 21.8 shows cosmic microwave background\n",
      "(CMB) data from BOOMERGNG (Netterfield et al. 2001), Maz-\n",
      "ima (Lee et al. 2001) and DASI (Halverson 2001). The data\n",
      "consist of n pairs (11,Y;),---,(@n,Y¥n) where x; is called the\n",
      "multipole moment and Y; is the estimated power spectrum of\n",
      "the temperature fluctuations. What your seeing are sound waves\n",
      "in the cosmic microwave background radiation which is the heat,\n",
      "left over from the big bang. If r(x) denotes the true power spec-\n",
      "trum then\n",
      "Y= r(ui) +6\n",
      "\n",
      "where €; is a random error with mean 0. The location and size\n",
      "of peaks in r(x) provides valuable clues about the behavior of\n",
      "the early universe. Figure 21.8 shows the fit based on cross-\n",
      "validation as well as an undersmoothed and oversmoothed fit.\n",
      "The cross-validation fit shows the presence of three well defined\n",
      "peaks, as predicted by the physics of the big bang. Ml\n",
      "\n",
      "The procedure for finding confidence bands is similar to that\n",
      "for density estimation. However, we first need to estimate o?.\n",
      "Suppose that the 2;’s are ordered. Assuming r(x) is smooth, we\n",
      "have r(x;41) — r(a;) © 0 and hence\n",
      "\n",
      "Yin — Yi = |r(taga) + esi m [r(e.) + a % €i41 — &\n",
      "and hence\n",
      "V(Yigt — ¥i) © Weis — 6) = V(eiy1) + V(€i) = 20°.\n",
      "\n",
      "We can thus use the average of the n — 1 differences Y;,, — Yj;\n",
      "to estimate o?. Hence, define\n",
      "n=l\n",
      "(Yi -¥)2. (21.41)\n",
      "\n",
      "i=l\n",
      "\n",
      "gait\n",
      "2(n — 1)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-382.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "384 21. Nonparametric Curve Estimation\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "6\n",
      "Ly\n",
      "\n",
      "3000 «4000» 5000\n",
      "L\n",
      "\n",
      "3000 «4000» 5000\n",
      "L\n",
      "\n",
      " \n",
      "\n",
      "1000 = 2000\n",
      "L\n",
      "\n",
      "1000 = 2000\n",
      "L\n",
      "\n",
      "0\n",
      "0\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "T T T T T T T T T T\n",
      "200 400 600 800 1000 200 400 600 800 1000\n",
      "\n",
      "Undersmoothed Oversmocthed\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Bet05\n",
      "L\n",
      "\n",
      "4000 = 5000-6000\n",
      "L L\n",
      "estimated risk\n",
      "Set05 = Get05 Tet 05\n",
      "L L L\n",
      "\n",
      "1000 ©2000» 3000\n",
      "4eH05\n",
      "L\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "3eH05\n",
      "L\n",
      "\n",
      "0\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "T T T T i i T T T i T\n",
      "200 400 600 800 1000 20 40 60 80 100120\n",
      "\n",
      "bandwidth\n",
      "Just Right (Using cross-valdiation)\n",
      "\n",
      "FIGURE 21.8. Regression analysis of the CMB data. The first fit is\n",
      "undersmoothed, the second is oversmoothed and the third is based\n",
      "on cross-validation. The last panel shows the estimated risk versus\n",
      "the bandwidth of the smoother. The data are from BOOMERaNG,\n",
      "Maxima and DASI.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-383.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "21.4 Nonparametric Regression 385\n",
      "\n",
      " \n",
      "\n",
      "Confidence Bands for Kernel Regression\n",
      "\n",
      "Follow the same procedure as for kernel density esti-\n",
      "mators except, change the definition of se (v) to\n",
      "\n",
      "(21.42)\n",
      "\n",
      " \n",
      "\n",
      "where @ is defined in (21.41).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 21.21 Figure 21.9 shows a 95 per cent confidence enve-\n",
      "lope for the CMB data. We see that we are highly confidence of\n",
      "the existence and position of the first peak. We are more uncer-\n",
      "tain about the second and third peak. At the time of this writing,\n",
      "more accurate data are becoming available that apparently pro-\n",
      "vide sharper estimates of the second and third peak.\n",
      "\n",
      "The extension to multiple regressors X = (X1,...,X;,) is\n",
      "straightforward. As with kernel density estimation we just re-\n",
      "place the kernel with a multivariate kernel. However, the same\n",
      "caveats about the curse of dimensionality apply. In some cases,\n",
      "we might consider putting some restrictions on the regression\n",
      "function which will then reduce the curse of dimensionality. For\n",
      "example, additive regression is based on the model\n",
      "\n",
      "Y=So1,(Xj) +6. (21.43)\n",
      "\n",
      "Now we only need to fit p one-dimensional functions. The model\n",
      "can be enriched by adding various interactions, for example,\n",
      "\n",
      "p\n",
      "Y= Sori(Xj) + 2 rj XX) + (21.44)\n",
      "j=l j<k\n",
      "Additive models are usually fit by an algorithm called backfit-\n",
      "ting.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-384.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-385.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "21.5 Appendix: Confidence Sets and Bias 387\n",
      "\n",
      " \n",
      "\n",
      "Backfitting\n",
      "\n",
      "1. Initialize r}(21), ..., rp(2,\n",
      "\n",
      " \n",
      "\n",
      "2. For j=1,...,p:\n",
      "\n",
      "(a) Let ¢; =¥i—Dyyjtolad-\n",
      "\n",
      "the €;’s on the j*\" covariate.\n",
      "\n",
      "3. If converged STOP. Else, go back to step 2.\n",
      "\n",
      " \n",
      "\n",
      "(b) Let r; be the function estimate obtained by regressing\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Additive models have the advantage that they avoid the curse\n",
      "\n",
      " \n",
      "\n",
      "of dimensionality and they can be fit quickly but they have one\n",
      "disadvantage: the model is not fully nonparametric. In other\n",
      "words, the true regression function r(x) may not be of the form\n",
      "(21.43).\n",
      "\n",
      "21.5 Appendix: Confidence Sets and Bias\n",
      "\n",
      "The confidence bands we computed are not for the density func-\n",
      "tion or regression function but rather, for the smoothed function.\n",
      "For example, the confidence band for a kernel density estimate\n",
      "with bandwidth h isa band for the function one gets by smooth-\n",
      "ing the true function with a kernel with the same bandwidth.\n",
      "Getting a confidence set for the true function is complicated for\n",
      "reasons we now explain.\n",
      "\n",
      "First, let’s review the parametric case. When estimating a\n",
      "scalar quantity 6 with an estimator 8, the usual confidence in-\n",
      "\n",
      " \n",
      "\n",
      "‘a/25n Where @ is the maximum likelihood\n",
      "\n",
      " \n",
      "\n",
      "terval is of the form 6:\n",
      "estimator and s, = 1/ V(@) is the estimated standard error of the\n",
      "\n",
      "estimator. Under standard regularity conditions, Ox N(O,8n)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-386.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "388 21. Nonparametric Curve Estimation\n",
      "\n",
      "and\n",
      "lim P (@—25, <0< 8+2sn) =1-a.\n",
      "\n",
      "n—400\n",
      "But let us take a closer look.\n",
      "\n",
      "Let b, = E(0,) — @ We know that 0 ~ N(0,s,) but a more\n",
      "accurate statement is 0 N(@+ bn, 8). We usually ignore the\n",
      "bias term 6, in large sample calculations but let’s keep track of\n",
      "it. The coverage of the usual confidence interval is\n",
      "\n",
      " \n",
      "\n",
      "Sn\n",
      "\n",
      "N (bn, Sn\n",
      "=P (sn as Midst ES «ap)\n",
      "\n",
      "bn\n",
      "=P (-s9n <N (=. :) < cap) .\n",
      "Sn\n",
      "\n",
      "In a well behaved parametric model, s, is of size n~!/? b,, is\n",
      "of size n-!. Hence, b,/s,, — 0 so the last displayed probability\n",
      "statement becomes P(—za/2 < N(0,1) < 2a) = 1 —a. What\n",
      "makes parametric confidence intervals have the right coverage\n",
      "is the fact that b,,/s, — 0.\n",
      "\n",
      "The situation is more complicated for kernel methods. Con-\n",
      "\n",
      "5 ws 0-0\n",
      "P (0 — zaps <0<0+ ps) = P (sr god can)\n",
      "\n",
      "A\n",
      "\n",
      "sider estimating a density f(z) at a single point x with a kernel\n",
      "density estimator. Since f (2) is a sum if iid random variables,\n",
      "the central limit theorem implies that\n",
      "\n",
      " \n",
      "\n",
      "Fla) eN (110 +b, (2), ai) (21.45)\n",
      "\n",
      "where 1\n",
      "b, (2) = gh\" (aes (21.46)\n",
      "\n",
      "is the bias, c, = fa?K(x)dx and cy, = f K?(x)dx. The esti-\n",
      "mated standard error is\n",
      "\n",
      "(21.47)\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-387.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "21.6 Bibliographic Remarks 389\n",
      "\n",
      "Suppose we use the usual interval f(«) + 24/25,. Arguing as\n",
      "above, the coverage is approximately,\n",
      "\n",
      "P (se sN (HB.1) < aap).\n",
      "\n",
      "The optimal bandwidth is of the form h = cn~'/> for some\n",
      "constant c. If we plug h = en—'/> into (21.46) and(21.47) we\n",
      "see that b,(x)/s,(2) does not tend to 0. Thus, the confidence\n",
      "interval will have coverage less than 1 — a.\n",
      "\n",
      "21.6 Bibliographic Remarks\n",
      "\n",
      "Two very good books on curve estimation are Scott (1992) and\n",
      "Silverman (1986). I drew heavily on those books for this Chap-\n",
      "ter.\n",
      "\n",
      "21.7 Exercises\n",
      "\n",
      "1. Let Xy,...,X, ~ f and let fa be the kernel density esti-\n",
      "mator using the boxcar kernel:\n",
      "\n",
      "vy fl -i<a<d\n",
      "K(x) = { 0 otherwise.\n",
      "(a) Show that\n",
      ". 1 pettn/2)\n",
      "BA) =5f selay\n",
      "\n",
      "h —(h/2)\n",
      "\n",
      ". 2+(h/2) 2+(h/2) 3\n",
      "VF) =—a] fo Fora -( / sw)\n",
      "\n",
      "—(h/2) —(h/2)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-388.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "390\n",
      "\n",
      "21. Nonparametric Curve Estimation\n",
      "\n",
      " \n",
      "\n",
      "(b) Show that if h 4 0 and nh - o6 as n - oo then\n",
      "\n",
      "Fr(e)— f(a).\n",
      "\n",
      ". Get the data on fragments of glass collected in forensic\n",
      "\n",
      "work. You need to download the MASS library for R then:\n",
      "\n",
      "library (MASS)\n",
      "data(fgl)\n",
      "x <- fg1[[1]]\n",
      "help(fgl)\n",
      "\n",
      "The data are also on my website. Estimate the density\n",
      "of the first variable (refractive index) using a histogram\n",
      "and use a kernel density estimator. Use cross-validation\n",
      "to choose the amount of smoothing. Experiment with dif-\n",
      "ferent binwidths and bandwidths. Comment on the simi-\n",
      "larities and differences. Construct 95 per cent confidence\n",
      "bands for your estimators.\n",
      "\n",
      ". Consider the data from question 2. Let Y be refractive\n",
      "\n",
      "index and let x be aluminium content (the fourth vari-\n",
      "able). Do a nonparametric regression to fit the model Y =\n",
      "f(a) +e. Use cross-validation to estimate the bandwidth.\n",
      "Construct 95 per cent confidence bands for your estimate.\n",
      "\n",
      ". Prove Lemma 21.1.\n",
      "\n",
      ". Prove Theorem 21.3.\n",
      "\n",
      ". Prove Theorem 21.7.\n",
      "\n",
      ". Prove Theorem 21.14.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-389.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "21.7 Exercises 391\n",
      "\n",
      "8. Consider regression data (x1, Yi), ---, (tn, Yn). Suppose that\n",
      "0 < 2; <1 for all i. Define bins B; as in equation (21.7).\n",
      "For x € B; define\n",
      "\n",
      "P(g =Yy\n",
      "where Y; is the mean of all the Y;’s corresponding to those\n",
      "z;’s in B;. Find the approximate risk of this estimator.\n",
      "From this expression for the risk, find the optimal band-\n",
      "width. At what rate does the risk go to zero?\n",
      "\n",
      "9. Show that with suitable smoothness assumptions on r(x),\n",
      "6? in equation (21.41) is a consistent estimator of 0.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-390.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-391.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "This is page 393\n",
      "Printer: Opaque this\n",
      "\n",
      "22\n",
      "\n",
      "Smoothing Using Orthogonal\n",
      "Functions\n",
      "\n",
      "In this Chapter we study a different approach to nonparametric\n",
      "curve estimation based on orthogonal functions. We begin\n",
      "with a brief introduction to the theory of orthogonal functions.\n",
      "Then we turn to density estimation and regression.\n",
      "\n",
      "22.1 Orthogonal Functions and Ly Spaces\n",
      "\n",
      "Let v = (v,v2,v3) denote a three dimensional vector, that is,\n",
      "a list of three real numbers. Let Y denote the set of all such\n",
      "vectors. If a is a scalar (a number) and v is a vector, we define\n",
      "av = (av;,av2,av3). The sum of vectors v and w is defined by\n",
      "o+w = (v, +1, 2+ We, v3+w3). The inner product between\n",
      "two vectors v and w is defined by (uv, w) = SA 5 v,w;. The norm\n",
      "(or length) of a vector v is defined by\n",
      "\n",
      "llull = Ve, 2) =\n",
      "\n",
      "(22.1)\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-392.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "394 22. Smoothing Using Orthogonal Functions\n",
      "\n",
      "Two vectors are orthogonal (or perpendicular) if (v, w) = 0.\n",
      "A set of vectors are orthogonal if each pair in the set is orthog-\n",
      "onal. A vector is normal if ||v|| =1.\n",
      "\n",
      "Let ¢; = (1,0,0), @2 = (0,1,0), d3 = (0,0,1). These vectors\n",
      "are said to be an orthonormal basis for V since they have the\n",
      "following properties:\n",
      "\n",
      "(i) they are orthogonal;\n",
      "(ii) they are normal;\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "(iii) they form a basis for V which means that if v € V then v\n",
      "can be written as a linear combination of ¢1, ¢2, $3:\n",
      "\n",
      "3\n",
      "v= > Bi where 5; = (¢;,0)- (22.2)\n",
      "jal\n",
      "\n",
      "For example, if v = (12,3, 4) then v = 124, +3¢2+4¢3. There\n",
      "are other orthonormal bases for Y, for example,\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "‘You can check that these three vectors also form an orthonormal\n",
      "basis for Y. Again, if v is any vector then we can write\n",
      "3\n",
      "\n",
      "v= SB where 6; = (vj, v).\n",
      "\n",
      "j=l\n",
      "For example, if v = (12,3, 4) then\n",
      "v = 10.97y) + 6.362 + 2.86y)3.\n",
      "Now we make the leap from vectors to functions. Basically, we\n",
      "just replace vectors with functions and sums with integrals. Let\n",
      "\n",
      "L»(a,b) denote all functions defined on the interval [a,b] such\n",
      "that fo f(a)Pde < 00:\n",
      "\n",
      " \n",
      "\n",
      "Lola, b) = {4 [2,5] >, [sora < oo} _ 223)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-393.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "22.1 Orthogonal Functions and Lz Spaces 395\n",
      "\n",
      "We sometimes write Ly instead of Ly(a, b). The inner product\n",
      "between two functions f,g € Lz is defined by [ f(x)g(x)dx. The\n",
      "\n",
      "norm of f is\n",
      "lsi=\\/f\n",
      "\n",
      "Two functions are orthogonal if [ f(x)g(x)dx = 0. A function\n",
      "is normal if || f|| =1.\n",
      "\n",
      " \n",
      "\n",
      "2d. (22.4)\n",
      "\n",
      "A sequence of functions @1, $2, 3, ...isorthonormal if f o;(x\n",
      "\n",
      "1 for each j and f 4;(1)¢,(x)dx = 0 for i A j. An orthonor-\n",
      "mal sequence is complete if the only function that is orthog-\n",
      "onal to each ¢; is the zero function. In this case, the functions\n",
      "\n",
      " \n",
      "\n",
      "1, ¢2, @3,.-. form in basis, meaning that if f € Ly then f can\n",
      "be written as\n",
      "\n",
      " \n",
      "\n",
      "eo\n",
      "\n",
      "f(z) = > 6j9;(2), where a= f(x), (a) de.\n",
      "\n",
      "j=l\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "(22.5)\n",
      "\n",
      " \n",
      "\n",
      "Parseval’s relation says that\n",
      "lsiP= f Pede= aes lil? 206)\n",
      "j=l\n",
      "\n",
      "where 6 = (1, /o,---)-\n",
      "\n",
      "Example 22.1 An example of an orthonormal basis for L»{0, 1]\n",
      "is the cosine basis defined as follows. Let (x) = 1 and for\n",
      "j = 2 define\n",
      "\n",
      ";(«) = V2cos((j — 1)r2). (22.7)\n",
      "\n",
      "The first siz functions are plotted in Figure 22.1.\n",
      "\n",
      "Example 22.2 Let\n",
      "\n",
      "f(z) = A= sin (=)\n",
      "\n",
      "dx =\n",
      "\n",
      "The = equality\n",
      "\n",
      "in\n",
      "\n",
      "the displayed\n",
      "equation means\n",
      "\n",
      "that f( f(x)\n",
      "fa(a))’dx\n",
      "where f,(x)\n",
      "\n",
      "fa 85;(a).\n",
      "\n",
      "lol\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-394.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "396 22. Smoothing Using Orthogonal Functions\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "o 34\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "   \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "FIGURE 22.1. The first six functions in the cosine basis.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-395.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "22.1 Orthogonal Functions and Lz Spaces 397\n",
      "\n",
      "which is called the “doppler function.” Figure 22.2 shows f (top\n",
      "left) and its approximation\n",
      "\n",
      "= > Bj; (x)\n",
      "\n",
      "with J equal to 5 (top right), 20 (bottom left) and 200 (bottom\n",
      "right). As J aaa we see that f;(2) gets closer to f(x). The\n",
      "coefficients 8; = ft (x)$;(x)dxz were computed numerically.\n",
      "\n",
      "Example 22.3 The Legendre polynomials on [—1,1] are de-\n",
      "fine by\n",
      "\n",
      "1 @\n",
      "\n",
      "a ge? — _ E ¢\n",
      "Shas ® af, 7 =0,1,2,... (22.8)\n",
      "\n",
      "P(x) =\n",
      "It can be shown that these functions are complete and orthogonal\n",
      "and that\n",
      "1 2\n",
      ": = ——.. 22:\n",
      "[ rieoae Bri (22.9)\n",
      "Tt follows that the functions ¢j(x) = y/(2j + 1)/2P;(x),\n",
      "0,1,... form an orthonormal basis for Ly(—1,1). The first fe\n",
      "Legendre polynomials are\n",
      "\n",
      " \n",
      "\n",
      "Pz) =\n",
      "P(x) =\n",
      "\n",
      "Pir) = 5 (3° -1)\n",
      "P(r) = 5 (5° -3z).\n",
      "\n",
      "NIE RB\n",
      "\n",
      "These polynomials may be constructed explicitly using the fol-\n",
      "lowing recursive relation:\n",
      "\n",
      "(2) + 1)2P; (x) — jPj-1(2)\n",
      "j+1 °\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Praa(2) mB (22.10)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-396.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "398\n",
      "\n",
      "02 00 02 04\n",
      "\n",
      "~04\n",
      "\n",
      "~02 00 02 04\n",
      "\n",
      "~04\n",
      "\n",
      "22. Smoothing Using Orthogonal Functions\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "02\n",
      "L\n",
      "\n",
      "04\n",
      "\n",
      "0.1\n",
      "L\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "04\n",
      "\n",
      "02\n",
      "\n",
      "00\n",
      "L\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "00 8 §©602 04 06 08 1.0 00 8©602\n",
      "\n",
      "FIGURE 22.2. Approximating the doppler function\n",
      "f(x) = /z0—a) sin ( 2\n",
      "the cosine basis. The function f (top left) and its ap-\n",
      "proximation fj(%) = DLs Bjb;(2) with J equal to 5\n",
      "(top right), 20 (bottom left) and 200 (bottom right). The\n",
      "coefficients 6; = Se f(2)@;(«)da were computed numer-\n",
      "ically.\n",
      "\n",
      "no\n",
      "\n",
      "with its expansion in\n",
      "\n",
      " \n",
      "\n",
      "oe\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-397.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "22.2 Density Estimation 399\n",
      "\n",
      "The coefficients (6), 32,... are related to the smoothness of\n",
      "the function f. To see why, note that if f is smooth, then\n",
      "its derivatives will be finite. Thus we expect that, for some k,\n",
      "Io (F™ (x))?dx < co where f™ is ther k*\" derivative of f. Now\n",
      "consider the cosine basis (22.7) and let f(x) = 375°, 3;0;(x)-\n",
      "Then,\n",
      "\n",
      "[re Harm 2 50 ile (j -— 1))°*.\n",
      "0\n",
      "\n",
      "The only way that }0°, 57(m(j — 1))°* can be finite is if the\n",
      "6;’s get small when j gets large. To summarize:\n",
      "\n",
      "If the function f is smooth then the coeffcients 3; will\n",
      "be small when 7 is large.\n",
      "\n",
      "For the rest of this chapter, assume we are using the cosine\n",
      "basis unless otherwise specified.\n",
      "\n",
      "22.2 Density Estimation\n",
      "\n",
      "Let X1,...,X, be UD observations from a distribution on [0, 1]\n",
      "with density f. Assuming f € L2 we can write\n",
      "\n",
      "~\n",
      "\n",
      "F(2) = 7 5/6;(2)\n",
      "j=l\n",
      "where @, @2,... is an orthonormal basis. Define\n",
      "\n",
      " \n",
      "\n",
      "1¥ 4X). (2.11)\n",
      "\n",
      "Theorem 22.4 The mean and variance of B; are\n",
      "\n",
      "E (4) = 8 (22.12)\n",
      "v (4) = a (22.13)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-398.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "400 22. Smoothing Using Orthogonal Functions\n",
      "where\n",
      "af = V(oi(X)) = f (6j(«) — Ay)? Fle)ar. (22.18)\n",
      "\n",
      "ProoF. We have\n",
      "\n",
      "z (i) = eden\n",
      "\n",
      "= _\n",
      "\n",
      "Patan x)dx = B;.\n",
      "\n",
      "The calculation for the variance is similar. ll\n",
      "\n",
      "Hence, 8; is an unbiased estimate of 4;. It is tempting to\n",
      "estimate f by })5-; 3;9,(x) but this turns out to have a very\n",
      "high variance. Instead, consider the estimator\n",
      "\n",
      "of\n",
      "\n",
      "F(x) = 32 354;(2). (22.15)\n",
      "j=l\n",
      "The number of terms J is a smoothing parameter. Increasing J\n",
      "will decrease bias while increasing variance. For technical rea-\n",
      "sons, we restrict J to lie in the range\n",
      "\n",
      "1<J<p\n",
      "where p = p(n) = \\/n. To emphasize the dependence of the risk\n",
      "function on - we write the risk function as R(J).\n",
      "Theorem 22.5 The risk of f is given by\n",
      "J ge oo\n",
      "= j g2 93°\n",
      "= ses + SO 6. (22.16)\n",
      "got j=J+1\n",
      "In kernel estimation, we used cross-validation to estimate the\n",
      "\n",
      "risk. In the orthogonal function approach, we instead use the\n",
      "risk estimator\n",
      "\n",
      "BS (p_@\n",
      "d= at > (3 - “) (22.17)\n",
      "j=J+1 +\n",
      "\n",
      "j=l\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-399.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "22.2 Density Estimation 401\n",
      "\n",
      "where a, = max{a,0} and\n",
      "\n",
      "al\n",
      "\n",
      "= (40) - b\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "(22.18)\n",
      "\n",
      "To motivate this estimator, note that a; is an unbiased estmate\n",
      "of oF and B = a; is an unbiased estimator of 63. We take the\n",
      "positive part of the latter term since we know that 53 cannot be\n",
      "negative. We now choose 1 < J < p to minimize R(f, f). Here\n",
      "is a summary.\n",
      "\n",
      " \n",
      "\n",
      "Summary of Orthogonal Function Density Estimation\n",
      "\n",
      "1. Let\n",
      "= Lats | .\n",
      "B= nb).\n",
      "\n",
      "2. Choose J to minimize R(J) over 1 < J < p= yn where\n",
      "\n",
      " \n",
      "\n",
      "j=l j\n",
      "and\n",
      "a Loi . >\n",
      "f= FTL (Hl - 5)\n",
      "3. Let .\n",
      "a xu ~\n",
      "(x) =) 6j9;(2)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The estimator fa can be negative. If we are interested in\n",
      "exploring the shape of f, this is not a problem. However, if\n",
      "we need our estimate to be a probability density function, we\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-400.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "402 22. Smoothing Using Orthogonal Functions\n",
      "\n",
      "can truncate the estimate and then normalize it. That, we take\n",
      "ft = max{f,(x), O}/ fo max{ f,(u), 0}du.\n",
      "\n",
      "Now let us construct a confidence band for f. Suppose we\n",
      "estimate f using J orthogonal functions. We are essentially\n",
      "estimating f(x) = aa 8;0;(%) not the true density f(r) =\n",
      "\n",
      "jz1 6;4;(x). Thus, the confidence band should be regarded as\n",
      "\n",
      "a band for f(z).\n",
      "\n",
      "Theorem 22.6 An approximate 1 — a confidence band for f is\n",
      "(C(x), u(x)) where\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "U(x) = fr(t)-e, u (22.19)\n",
      "where\n",
      "JK?\n",
      "c= 1+ (22.20)\n",
      "and\n",
      "\n",
      "ACS Thx Tne |0;(x)|.\n",
      "For the cosine basis, K = V2.\n",
      "\n",
      "Example 22.7 Let\n",
      "\n",
      "5\n",
      "\n",
      "A(x) = 2605 0,1) + ZY le Hj.)\n",
      "\n",
      "j=l\n",
      "\n",
      "where $(x; 1,0) denotes a Normal density with mean ys and\n",
      "standard deviation o, and (ju,---, $5) = (—1,—-1/2,0,1/2,1).\n",
      "Marron and Wand (1990) call this “the claw” although I prefer\n",
      "the “Bart Simpson.” Figure 22.3 shows the true density as well\n",
      "as the estimated density based on n = 5,000 observations and\n",
      "a 95 per cent confidence band. The density has been rescaled to\n",
      "have most of its mass between 0 and 1 using the transformation\n",
      "y = («+3)/6.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-401.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "403\n",
      "\n",
      "22.2 Density Estimation\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "           \n",
      "\n",
      "-\n",
      "\n",
      "            \n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "      \n",
      "\n",
      "  \n",
      "\n",
      "             \n",
      "\n",
      "  \n",
      "\n",
      "          \n",
      "\n",
      " \n",
      "\n",
      "           \n",
      "\n",
      "                       \n",
      "  \n",
      "\n",
      "\\-\n",
      ".\n",
      "NX 4\n",
      "\\Y\n",
      "it\n",
      "N\n",
      "\\\n",
      "N\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "distribution (rescaled to have most of its mass between 0 and 1). The\n",
      "bottom plot is the orthogonal function density estimate and 95 per\n",
      "\n",
      "FIGURE 22.3. The top plot is the true density for the Bart Simpson\n",
      "cent confidence band.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-402.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "404 22. Smoothing Using Orthogonal Functions\n",
      "22.3 Regression\n",
      "Consider the regression model\n",
      "Y¥,=r(a)+6, i=1,...,n (22.21)\n",
      "\n",
      "where the €; are independent with mean 0 and variance o?. We\n",
      "will initially focus on the special case where x; = i/n. We assume\n",
      "that r € L2[0, 1] and hence we can write\n",
      "\n",
      " \n",
      "\n",
      "r( = >> 6;4;(2) where 5=[ r(a)oj(a)dx (22.22)\n",
      "\n",
      "where ¢1, ¢2,... Where is an orthonormal basis for [0, 1].\n",
      "Define\n",
      "= to, ; . -\n",
      "ar ie $;(2)), f=1,2,-.- (22.23)\n",
      "Since B is an average, the central limit theorem tells us that B;\n",
      "will be approximately normally distributed.\n",
      "Theorem 22.8\n",
      "@\n",
      "By N (0. “) x (22.24)\n",
      "n\n",
      "\n",
      "Proor. The mean of 8; is\n",
      "\n",
      "* ig le\n",
      "E(B) = — STE(¥)Oj(4) = = YI r(m) oj)\n",
      "i=l 1\n",
      "a | reojle)ae= 9,\n",
      "\n",
      "where the approximate nd follows from the definition of a\n",
      "Riemann integral: 53, A,h(«;) > Je h h(a)da where A, = 1/n.\n",
      "The variance is\n",
      "\n",
      "VA) = SLVOVH)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-403.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "22.3 Regression 405\n",
      "2g 2 2, 7\n",
      "_ 6 2 ail 2\n",
      "= 2 » 5 (ai) Th » 93 (xi)\n",
      "o? . o2\n",
      "x —/ ¢ a\n",
      "n [ao a\n",
      "\n",
      "since f ¢5(x)dx = 1.\n",
      "As we did for density estimation, we will estimate r by\n",
      "\n",
      "A= So\n",
      "\n",
      ")\n",
      "\n",
      "D\n",
      "\n",
      "Bj0;(2).\n",
      "\n",
      "Let\n",
      "\n",
      " \n",
      "\n",
      "be the risk of the estimator.\n",
      "\n",
      "Theorem 22.9 The risk R(J) of the estimator?,(«) = pe Bo; (zx)\n",
      "is\n",
      "Jo? 52 99 ¢\n",
      "RU) =—-+ SP G5. (22.25)\n",
      "\n",
      "j=J+1\n",
      "\n",
      "To estimate for 0? = Var(e;) we use\n",
      "\n",
      "eat SB (22.26)\n",
      "isn—k4l\n",
      "where k = n/4. To motivate this estimator, recall that if f is\n",
      "smooth then §; % 0 for large j. So, for j > k, 8; © N(0,0?/n).\n",
      "Thus, 8; © 08;/\\/n for for j > k, where 8; ~ N(0,1). There-\n",
      "fore,\n",
      "\n",
      " \n",
      "\n",
      "n\n",
      "\n",
      "25 -R\n",
      "\n",
      "i=n-k41\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-404.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "406 22. Smoothing Using Orthogonal Functions\n",
      "\n",
      "FF\n",
      "\n",
      "go 2\n",
      "k Xk\n",
      "\n",
      "id\n",
      "3\n",
      "\n",
      " \n",
      "  \n",
      "\n",
      "since a sum of k Normals has a yj distribution. Now E(x7) =\n",
      "and hence E(@?) = 0”. Also, V(xz) = 2k and hence V(6?) =\n",
      "(04 /k?) (2k) = (20'/k) + 0 as n — oo. Thus we expect G? to be\n",
      "a consistent estimator of o?. There is nothing special about the\n",
      "choice k = n/4. Any k that increases with n at an appropriate\n",
      "rate will suffice.\n",
      "\n",
      "We estimate the risk with\n",
      "\n",
      ". ap on a9\n",
      "RiyN=Ie+ (@ “ =) . (22.27)\n",
      "n fe\n",
      "\n",
      "We are now ready to give a complete description of the method,\n",
      "which Beran (2000) calls REACT (Risk Estimation and Adap-\n",
      "tation by Coordinate Transformation).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-405.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "22.3 Regression 407\n",
      "\n",
      " \n",
      "\n",
      "Orthogonal Series Regression Estimator\n",
      "\n",
      " \n",
      "\n",
      "where k & n/4.\n",
      "3. For 1 < J <n, compute the risk estimate\n",
      "\n",
      "PP Nl» @\n",
      "J— Ber\n",
      "“+ > (3 ~)\n",
      "\n",
      "j=d+1 +\n",
      "\n",
      "Rid)\n",
      "\n",
      "4. Choose J € {1,...n} to minimize R(J).\n",
      "\n",
      "5. Let\n",
      "\n",
      "b)\n",
      "\n",
      "w\n",
      "\n",
      "4; (a).-\n",
      "\n",
      "fw) =i\n",
      "\n",
      " \n",
      "\n",
      "(22.28)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Example 22.10 Figure 22.4 shows the doppler function f and\n",
      "n = 2,048 observations generated from the model\n",
      "Yi =r(ai) +6\n",
      "\n",
      "where x; = i/n, €; ~ N(0,(.1)?). Then figure shows the true\n",
      "function, the data, the estimated function and the estimated risk.\n",
      "The estimated figure was based on J = 234 terms.\n",
      "\n",
      "Finally, we turn to confidence bands. As before, these bands\n",
      "are not really for the true function r(x) but rather for the\n",
      "\n",
      "smoothed version of the function 7(z) = Wa Bj; (a).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-406.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408\n",
      "\n",
      "02 00 02 04\n",
      "\n",
      "~04\n",
      "\n",
      "0.2 00 02 04\n",
      "\n",
      "~04\n",
      "\n",
      "22. Smoothing Using Orthogonal Functions\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Estimated Risk\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "05\n",
      "\n",
      "0.0\n",
      "\n",
      "05\n",
      "\n",
      "0.08\n",
      "\n",
      "0.06\n",
      "\n",
      "0.04\n",
      "\n",
      "0.02\n",
      "\n",
      "0.00\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "° 500 1000 1500 2000\n",
      "\n",
      "J\n",
      "\n",
      "FIGURE 22.4. The doppler test function.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-407.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "22.3 Regression 409\n",
      "\n",
      "Theorem 22.11 Suppose the estimate? is based on J terms and\n",
      "G is defined as in equation (22.28). Assume that J<n—k+1.\n",
      "An approximate 1 — a confidence band for F is (£,u) where\n",
      "\n",
      "Zqt Ja?\n",
      "\n",
      "Wx) = F(a) - KVI mt (22.29)\n",
      "o> 2\n",
      "ult) = F(a) +KVI = + 230)\n",
      "where\n",
      "A pa tak 10, (z)|\n",
      "and\n",
      "zg: , 24 J .\n",
      "Pa=_—(1+5 (22.31)\n",
      "\n",
      "and k = n/4 is from equation (22.28). In the cosine basis, we\n",
      "may use K = V2.\n",
      "\n",
      "Example 22.12 Figure 22.5 shows the confidence envelope for the\n",
      "doppler signal. Th first plot is based on J = 234 (the value of\n",
      "J that minimizes the estimated risk). The second is based on\n",
      "J=45 = Jn. Larger J yields a higher resolution estimator\n",
      "at the cost of large confidence bands. Smaller J yields a lower\n",
      "resolution estimator but has tighter confidence bands. 1\n",
      "\n",
      "So far, we have assumed that the 2;;’s are of the form {1/n,2/n,...\n",
      "\n",
      "If the x;’s are on interval [a, }] then we can rescale them so that\n",
      "are in the interval [0,1]. If the 2;’s are not equally spaced the\n",
      "methods we have discussed still apply so long as the 2;’s “fill\n",
      "out” the interval [0,1] in such a way so as to not be too clumped\n",
      "together. If we want to treat the a;’s as random instead of fixed,\n",
      "then the method needs significant modifications which we shall\n",
      "not deal with here.\n",
      "\n",
      "1h.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-408.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "410\n",
      "\n",
      "10\n",
      "\n",
      "05\n",
      "\n",
      "0.0\n",
      "\n",
      "1.0\n",
      "\n",
      "10\n",
      "\n",
      "05\n",
      "\n",
      "0.0\n",
      "\n",
      "1.0\n",
      "\n",
      "22. Smoothing Using Orthogonal Functions\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "FIGURE 22.5. The confidence envelope for the doppler\n",
      "test function using n = 2,048 observations. The top plot\n",
      "shows the estimate and envelope using J = 234 terms.\n",
      "The bottom plot shows the estimate and envelope using\n",
      "J =45 terms.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-409.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "22.4 Wavelets 411\n",
      "\n",
      "22.4 Wavelets\n",
      "\n",
      "Suppose there is a sharp jump in a regression function f at some\n",
      "point « but that f is otherwise very smooth. Such a function\n",
      "f is said to be spatially inhomogeneous. See Figure 22.6 for\n",
      "an example.\n",
      "\n",
      "It is hard to estimate f using the methods we have discussed\n",
      "so far. If we use a cosine basis and only keep low order terms, we\n",
      "will miss the peak; if we allow higher order terms we will find\n",
      "the peak but we will make the rest of the curve very wiggly.\n",
      "Similar comments apply to kernel regression. If we use a large\n",
      "bandwidth, then we will smooth out the peak; if we use a small\n",
      "bandwidth, then we will find the peak but we will make the rest\n",
      "of the curve very wiggly.\n",
      "\n",
      "One way to estimate inhomogeneous functions is to use a more\n",
      "carefully chosen basis that allows us to place a “blip” in some\n",
      "small region without adding wiggles elsewhere. In this section,\n",
      "we describe a special class of bases called wavelets, that are\n",
      "aimed at fixing this problem. Statistical inference suing wavelets\n",
      "is a large and active area. We will just discuss a few of the main\n",
      "ideas to get a flavor of this approach.\n",
      "\n",
      "We start with a particular wavelet called the Haar wavelet.\n",
      "The Haar father wavelet or Haar scaling function is de-\n",
      "\n",
      "fined by\n",
      "1 if0<a<1 5 «\n",
      "9(2) = { 0 otherwise. (22.32)\n",
      "\n",
      " \n",
      "\n",
      "The mother Haar wavelet is defined by\n",
      "\n",
      "vay J ch if0<e<h 59 a\n",
      "(a) = { L ifler<i. (22.33)\n",
      "For any integers j and k define\n",
      "\n",
      "Oja(v) = 29? b(2a—k) and yyy (x) = 24/%y(2a—k). (22.34)\n",
      "\n",
      "The function ~;, has the same shape as y but it has been\n",
      "rescaled by a factor of 24/2 and shifted by a factor of k.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-410.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "412\n",
      "\n",
      "22. Smoothing Using Orthogonal Functions\n",
      "\n",
      " \n",
      "\n",
      "14\n",
      "\n",
      "12\n",
      "\n",
      "1.0\n",
      "\n",
      "08\n",
      "\n",
      "06\n",
      "L\n",
      "\n",
      "04\n",
      "L\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "FIGURE 22.6. An inhomogeneous function. The func-\n",
      "tion is smooth except for a sharp peak in one place.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-411.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "22.4 Wavelets 413\n",
      "\n",
      "See Figure 22.7 for some examples of Haar wavelets. Notice\n",
      "\n",
      " \n",
      "\n",
      "that for large 7,\n",
      "possible to add a blip to a function in one place without adding\n",
      "\n",
      "Wy,z is a very localized function. This makes it\n",
      "\n",
      "wiggles elsewhere. Increasing j is like looking in a microscope at\n",
      "increasing degrees of resolution. In technical terms, we say that\n",
      "wavelets provide a multiresolution analysis of L2(0,1).\n",
      "\n",
      "Let\n",
      "\n",
      "We (jeg, B05 lyeocrng2? 1}\n",
      "\n",
      "be the set of rescaled and shifted mother wavelets at resolution\n",
      "Theorem 22.13 The set of functions\n",
      "{¢, Wo, Wi, Wa,...,}\n",
      "\n",
      "is an orthonormal basis for L2(0,1).\n",
      "\n",
      "It follows from this theorem that we can expand any function\n",
      "f € 12(0,1) in this basis. Because each W; is itself a set of\n",
      "functions, we write the expansion as a double sum:\n",
      "\n",
      " \n",
      "\n",
      "co 2-1\n",
      "\n",
      "F(2) = 0 6(2) + 32 YS Bia (2) (22.35)\n",
      "\n",
      "j=0 k=0\n",
      "\n",
      "1\n",
      "\n",
      "a= f seyota)ar, Bu= f° Hoviyala) ar\n",
      "\n",
      "0\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "We call a the scaling coefficient and the {;,’s are called\n",
      "the detail coefficients. We call the finite sum\n",
      "\n",
      "J-12i-1\n",
      "\n",
      "f(x) 2) + >> So Bavin(a) (22.36)\n",
      "\n",
      "j=0 k=0\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-412.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "414\n",
      "\n",
      "22. Smoothing Using Orthogonal Functions\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "2\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "2\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "00 §©60.2 04 06 08 1.0 00 8602\n",
      "\n",
      "FIGURE 22.7. Some Haar wavelets. Top left: the father\n",
      "wavelet ¢(x); top right: the mother wavelet 7)(); bot-\n",
      "tom left: q2(x); bottom right: 4 ,10(x)-\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-413.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "22.4 Wavelets 415\n",
      "\n",
      "the resolution J approximation to f. The total number of\n",
      "terms in this sum is\n",
      "\n",
      "J-1\n",
      "14+ $02 =142’-1=27.\n",
      "\n",
      "j=0\n",
      "\n",
      "Example 22.14 Figure 22.8 shows the doppler signal, and its res-\n",
      "olution J approximation for J =3,5 and J=8. 8\n",
      "\n",
      "Haar wavelets are localized, meaning that they are zero out-\n",
      "side an interval. But they are not smooth. This raises the ques-\n",
      "tion of whether there exist smooth, localized wavelets that from\n",
      "an orthonormal basis. In 1988, Ingrid Daubechie showed that\n",
      "such wavelets do exist. These smooth wavelets are difficult to\n",
      "describe. They can be constructed numerically but there is no\n",
      "closed form formual for the smoother wavelets. To keep things\n",
      "simple, we will continue to use Haar wavelets. For your interest,\n",
      "figure 22.9 shows an example of a smooth wavelet called the\n",
      "ymmiet 8.”\n",
      "\n",
      "We can now use wavelets to do density estimation and re-\n",
      "gression. We shall only discuss the regression problem Y; =\n",
      "r(a;) + o€; where €; ~ N(0,1) and 2; = i/n. To simplify the\n",
      "\n",
      " \n",
      "\n",
      "discussion we assume that n = 27 for some J.\n",
      "\n",
      "There is one major difference between estimation using wavelets\n",
      "instead of a cosine (or polynomial) basis. With the cosine basis,\n",
      "we used all the terms 1 < j < J for some J. With wavelets,\n",
      "we use a method called thresholding where we keep a term in\n",
      "the function approximation if its coefficient is large, otherwise,\n",
      "we throw we don’t use that term. There are many versions of\n",
      "thresholding. The simplest is called hard, universal threshold-\n",
      "ing.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-414.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "416\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "22. Smoothing Using Orthogonal Functions\n",
      "\n",
      "3 3\n",
      "= ES\n",
      "?\n",
      "3 3\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "FIGURE 22.8. The doppler signal and its reconstruction\n",
      "f(x) = ag(x) + Sa De bjrYjn(2) based on J = 3, J = 5 and\n",
      "J=8.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-415.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "22.4 Wavelets 417\n",
      "\n",
      " \n",
      "\n",
      "05 1.0 1.5\n",
      "\n",
      "0.0\n",
      "\n",
      " \n",
      "\n",
      "B\n",
      "\n",
      " \n",
      "\n",
      "1.0\n",
      "\n",
      "0.6\n",
      "\n",
      "02\n",
      "\n",
      " \n",
      "\n",
      "0.2\n",
      "\n",
      " \n",
      "\n",
      "°\n",
      "Ny\n",
      "EN\n",
      "\n",
      "FIGURE 22.9. The symmlet 8 wavelet. The top plot is the mother\n",
      "and the bottom plot is the father.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-416.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n",
      "\n",
      "22. Smoothing Using Orthogonal Functions\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Haar Wavelet Regression\n",
      "\n",
      ". Let J = log,(n) and define\n",
      "\n",
      "x, b - 1 i > (95\n",
      "oF = Se be (e¥ and Dix = A So bial (22.37)\n",
      "\n",
      "for0<j<J-1.\n",
      "\n",
      ". Estimate o by\n",
      "\n",
      "median (|Ds-.4l 2 BS pe0.g2F* 1)\n",
      "0.6745\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "G= nx\n",
      "\n",
      "(09.38)\n",
      "\n",
      ". Apply universal thresholding:\n",
      "\n",
      "0 otherwise.\n",
      "\n",
      "~ . x [2\n",
      "Bra ~| Dyx if |Djal > Gy/SE* \\ (22.39)\n",
      "\n",
      "Set\n",
      "\n",
      "J-1 2-1\n",
      "\n",
      "Fla) = a6(0) + SD Bavijal2).\n",
      "\n",
      "j=jo k=0\n",
      "\n",
      " \n",
      "\n",
      "In practice, we do not compute 5S; and Dj, using (22.37). In-\n",
      "stead, we use the discrete wavelet transform (DWT) which\n",
      "is very fast. For Haar wavelets, the DWT works as follows.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-417.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "22.4 Wavelets 419\n",
      "\n",
      " \n",
      "\n",
      "DWT for Haar Wavelets\n",
      "\n",
      "Let y be the vector of ¥;’s (length n) and let J =\n",
      "log, (n). Create a list D with elements\n",
      "\n",
      "D{{0l], ---, DllL7— 1].\n",
      "\n",
      "Set:\n",
      "temp < y/Vn.\n",
      "Then do:\n",
      "\n",
      "for(j in ve 1) :0){\n",
      "m<\n",
      "\n",
      "m)\n",
      "\n",
      "temp|2 * I] — temp|(2 * I) — 1] v3\n",
      "\n",
      "a.\n",
      "Dili] <\n",
      "\n",
      "a:\n",
      "(\n",
      "\n",
      "temp < (temp [2 « I] + temp|(2 * I) — 1) /v2\n",
      "}\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Remark 22.15 If you use a programming language that does not\n",
      "allow a 0 index for a list, then index the list as\n",
      "\n",
      "D(t]], ---» Pll]\n",
      "and replace the second last line with\n",
      "Dii+i < = (templ2 * I] — temp[(2 * I) — 1) /v2\n",
      "\n",
      "The estimate for o probably looks strange. It is similar to the\n",
      "estimate we used for the cosine basis but it is designed to be\n",
      "insensitive to sharp peaks in the function.\n",
      "\n",
      "To understand the intuition behind universal thresholding,\n",
      "consider what happens when there is no signal, that is, when\n",
      "5;.~ = 0 for all j and k.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-418.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "420 22. Smoothing Using Orthogonal Functions\n",
      "\n",
      "Theorem 22.16 Suppose that 8; = 0 for all j and k and let\n",
      "Bj be the universal threshold estimator. Then\n",
      "\n",
      "P(8,4 =0 for all j,k) +1\n",
      "as n — Co.\n",
      "\n",
      "Proor. To simplify the proof, assume that o is known. Now\n",
      "Djx © N(0,0?/n). We will need Mill’s inequality: if Z ~\n",
      "N(0,1) then P(|Z| > #) < (c/t)e~*/? where c = \\/2/r is a\n",
      "constant. Thus,\n",
      "\n",
      " \n",
      "\n",
      "P(max|Djxl >) < S>P(\\Dja) > A)\n",
      "jk\n",
      "Z ye Val Dial | VTA\n",
      "~ ik o o\n",
      "co 1nd?\n",
      "a pane a ca\n",
      "< Dean{ 12}\n",
      "= 8 _. 36) ||\n",
      "\n",
      "VJ2logn\n",
      "\n",
      "Example 22.17 Consider Y; = 1r(2;) + 0¢; where f is the doppler\n",
      "signal, o = .1 and n = 2048. Figure 22.10 shows the data and\n",
      "the estimated function using universal thresholding. Of course,\n",
      "the estimate is not smooth since Haar wavelets are not smooth.\n",
      "Nonetheless, the estimate is quite accurate.\n",
      "\n",
      "22.5 Bibliographic Remarks\n",
      "\n",
      "A reference for orthogonal function methods is Efromovich (1999).\n",
      "See also Beran (2000) and Beran and Diimbgen (1998). An in-\n",
      "troduction to wavelets is given in Ogden (1997). A more ad-\n",
      "vanced treatment can be found in Hardle, Kerkyacharian, Pi-\n",
      "card and Tsybakov (1998). The theory of statistical estimation\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-419.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "22.5 Bibliographic Remarks 421\n",
      "\n",
      " \n",
      "\n",
      "0.5\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "00 02 04\n",
      "1\n",
      "\n",
      "0.2\n",
      "|\n",
      "\n",
      "-04\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "FIGURE 22.10. Estimate of the Doppler function using Haar\n",
      "wavelets and universal thresholding.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-420.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "422 22. Smoothing Using Orthogonal Functions\n",
      "\n",
      "using wavelets has been developed by many authors, especially\n",
      "David Donoho and Ian Johnstone. There is a series of papers es-\n",
      "pecially worth reading: Donoho and Johnstone (1994), Donoho\n",
      "and Johnstone (1995a), Donoho and Johnstone (1995b), and\n",
      "Donoho and Johnstone (1998).\n",
      "\n",
      "22.6 Exercises\n",
      "\n",
      "1. Prove Theorem 22.5.\n",
      "2. Prove Theorem 22.9.\n",
      "\n",
      "3. Let\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "( 111 ) ( 1 ol 0) ( 1 1 2 )\n",
      "7 7 + U sa, 0], vs i 7\n",
      "Wa vB vB) Va V8 Ve Ve VE\n",
      "\n",
      "Show that these vectors have norm 1 and are orthogonal.\n",
      "\n",
      "4. Prove Parseval’s relation equation (22.6).\n",
      "\n",
      "5. Plot the first five Legendre polynomials. Very, numerically,\n",
      "that they are orthonormal.\n",
      "\n",
      "6. Expand the following functions in the cosine basis on [0, 1].\n",
      "For (a) and (b), find the coefficients §; analytically. For\n",
      "(c) and (d), find the coefficients 8; numerically, i.e.\n",
      "\n",
      "a= [ room=h>s (Z)o (4)\n",
      "\n",
      "for some large integer N. Then plot the partial sum )77_, 5;;(x)\n",
      "for increasing values of n.\n",
      "\n",
      "(a) f(2) = V2cos(3rz).\n",
      "(b) f(a) = sin(na).\n",
      "(c) f(z) = aan hj K(x—t;) where K(t) = (1+sign(t))/2,\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-421.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "10.\n",
      "\n",
      "22.6 Exercises 423\n",
      "\n",
      "(t)) = (1, 13, .15, 23, .25, 40, .44, .65, .76, 78, .81),\n",
      "(hy) = (4,-5,3, —4, 5, 4.2, 2.1, 4.3, -3.1, 2.1, -4.2).\n",
      "\n",
      "(a) f = Ve 2) sin (2455) -\n",
      "\n",
      ". Consider the glass fragments data. Let Y be refractive in-\n",
      "\n",
      "dex and let X be aluminium content (the fourth variable).\n",
      "\n",
      "(a) Do a nonparametric regression to fit the model Y =\n",
      "f(x)+e using the cosine basis method. The data are not on\n",
      "a regular grid. Ignore this when estimating the function.\n",
      "(But do sort the data first.) Provide a function estimate,\n",
      "an estimate of the risk and a confidence band.\n",
      "\n",
      "(b) Use the wavelet method to estimate f.\n",
      "\n",
      ". Show that the Haar wavelets are orthonormal.\n",
      "\n",
      ". Consider again the doppler signal:\n",
      "\n",
      "2.17\n",
      "Z)= 1 — x) sin | ——— }.\n",
      "f(z) a(1— x) sin (; = =)\n",
      "Let n = 1024 and let (21,..., an) = (1/n,...,1). Generate\n",
      "\n",
      "data\n",
      "Yi = f(x) +06\n",
      "where €; ~ N(0, 1).\n",
      "(a) Fit the curve using the cosine basis method. Plot the\n",
      "\n",
      "function estimate and confidence band for J = 10, 20, ..., 100.\n",
      "\n",
      "(b) Use Haar wavelets to fit the curve.\n",
      "\n",
      "(Haar density Estimation.) Let X,,...,X, ~ f for some\n",
      "density f on [0,1]. Let’s consider constructing a wavelet\n",
      "histogram. Let @ and w be the Haar father and mother\n",
      "wavelet. Write\n",
      "\n",
      "J-1 25-1\n",
      "\n",
      "f(x) © 6(2) + 32 SS Badia (a)\n",
      "\n",
      "j=0 k=0\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-422.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "424\n",
      "\n",
      "11.\n",
      "\n",
      "22. Smoothing Using Orthogonal Functions\n",
      "\n",
      "where J ~ log,(n). Let\n",
      "& Tw\n",
      "Bua = z dersalX).-\n",
      "\n",
      "(a) Show that Bix is an unbiased estimate of (3;,,.\n",
      "\n",
      "(b) Define the Haar histogram\n",
      "\n",
      "iy\n",
      "\n",
      "1\n",
      "\n",
      "Byadya (x)\n",
      "0\n",
      "\n",
      "Fle) = 6) +0\n",
      "\n",
      "k=\n",
      "for0< B< J-1.\n",
      "\n",
      "(c) Find an approximate expression for the MSE as a func-\n",
      "tion of B.\n",
      "\n",
      "(d) Generate n = 1000 observations from a Beta (15,4)\n",
      "density. Estimate the density using the Haar histogram.\n",
      "Use leave-one-out cross vaidation to choose B.\n",
      "\n",
      "In this question, we will explore the motivation for equa-\n",
      "tion (22.38). Let Xi,...,Xn~ N(0,07). Let\n",
      "\n",
      "Kal)\n",
      "\n",
      " \n",
      "\n",
      "median (|Xj|,..-,\n",
      "0.6745\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "@=Vnx\n",
      "\n",
      "(a) Show that E(@) =o.\n",
      "(b) Simulate n = 100 observations from a N(0,1) distribu-\n",
      "\n",
      "tion. Compute G as well as the usual estimate of o. Repeat\n",
      "1000 times and compare the MSE.\n",
      "\n",
      "(c) Repeat (b) but add some outliers to the data. To do\n",
      "this, simulate each observation from a N(0,1) with prob-\n",
      "ability .95 and simulate each observation from a N(0,10)\n",
      "with probability .95.\n",
      "\n",
      "12. Repeat question 6 using the Haar basis.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-423.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "This is page 425\n",
      "Printer: Opaque this\n",
      "\n",
      "23\n",
      "\n",
      "Classification\n",
      "\n",
      "23.1 Introduction\n",
      "\n",
      "The problem of predicting a discrete random variable Y from an-\n",
      "other random variable X is called classification, supervised\n",
      "learning, discrimination or pattern recognition.\n",
      "\n",
      "In more detail, consider 11D data (X,, Yi), -.-, (Xn, Y;) where\n",
      "\n",
      "X; = (Xa,-..,Xia) € ¥ CR\"\n",
      "\n",
      "is a d-dimensional vector and Y; takes values in some finite set\n",
      "y. A classification rule is a function h : X — Y. When we\n",
      "observe a new X, we predict Y to be h(X).\n",
      "\n",
      "Example 23.1 Here is a an example with fake data. Figure 23.1\n",
      "shows 100 data points. The covariate X = (X,, X2) is 2-dimensional\n",
      "and the outcome Y € Y = {0,1}. The Y values are indicated on\n",
      "\n",
      "the plot with the triangles representing Y = 1 and the squares\n",
      "representing Y = 0. Also shown is a linear classification rule\n",
      "represented by the solid line. This is a rule of the form\n",
      "\n",
      "{ 1 if a+d,2 + bor > 0\n",
      "\n",
      "h(z) = 0 otherwise.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-424.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "426 23. Classification\n",
      "\n",
      "Everything above the line is classified as a 0 and everything below\n",
      "the line is classified as a 1. Ml\n",
      "\n",
      "Example 23.2 The Coronary Risk-Factor Study (CORIS) data\n",
      "involve 462 males between the ages of 15 and 64 from three ru-\n",
      "ral areas in South Africa (Rousseauw et al 1983, Hastie and\n",
      "Tibshirani, 1987). The outcome Y is the presence (Y = 1) or\n",
      "absence (Y =0) of coronary heart disease. There are 9 covari-\n",
      "ates: systolic blood pressure, cumulative tobacco (kg), ldl (low\n",
      "density lipoprotein cholesterol), adiposity, famhist (family his-\n",
      "tory of heart disease), typea (type-A behavior), obesity, alco-\n",
      "hol (current alcohol consumption), and age. Figure 23.2 shows\n",
      "the outcomes and two of the covariates, systolic blood pressure\n",
      "and tobaco consumption. People with/without heart disease are\n",
      "coded with triangles and squares. Also shown is a linear deci-\n",
      "sion boundary which we will explain shortly. In this example,\n",
      "the groups are much harder to tell apart. In fact, 141 people are\n",
      "misclassified using this classification rule.\n",
      "\n",
      "At this point, it is worth revisiting the Statistics/Data Mining\n",
      "dictionary:\n",
      "\n",
      "Statistics Computer Science | Meaning\n",
      "\n",
      "classification supervised learning predicting a discrete Y from XY\n",
      "data training sample (Gc¥) ere Si)\n",
      "covariates features the X;’s\n",
      "\n",
      "classifier hypothesis maph:¥ + Y\n",
      "\n",
      "estimation learning finding a good classifier\n",
      "\n",
      "In most cases in this Chapter, we deal with the case Y =\n",
      "\n",
      "{0,1}.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-425.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x2\n",
      "\n",
      " \n",
      "\n",
      "23.1 Introduction 427\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "xl\n",
      "\n",
      "FIGURE 23.1. Two covariates and a linear decision\n",
      "boundary. A means Y = 1.0 means Y = 0. You won't\n",
      "\n",
      "see real data like this.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-426.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "428\n",
      "\n",
      "Tobacco\n",
      "\n",
      "30\n",
      "\n",
      "25\n",
      "\n",
      "20\n",
      "\n",
      "15\n",
      "\n",
      "10\n",
      "\n",
      "23. Classification\n",
      "\n",
      " \n",
      "\n",
      "    \n",
      " \n",
      "\n",
      "cit, rs\n",
      "| * Aaah, bath ;\n",
      "1 ‘p i\n",
      "4 cha tilt HS a\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "T T T T T\n",
      "100 120 140 160 180 200 220\n",
      "\n",
      "Systolic Blood Pressure\n",
      "\n",
      "FIGURE 23.2. Two covariates and and a linear deci-\n",
      "sion boundary with data from the Coronary Risk-Factor\n",
      "Study (CORIS). A means Y = 1.0 means Y = 0. This\n",
      "is real life.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-427.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "23.2 Error Rates and The Bayes Classifier 429\n",
      "\n",
      "23.2 Error Rates and The Bayes Classifier\n",
      "\n",
      "Our goal is to find a classification rule h that makes accurate\n",
      "\n",
      "predictions. We start with the following definitions. One can use other\n",
      "measures of error as\n",
      "well.\n",
      "\n",
      " \n",
      "\n",
      "Definition 23.3 The true error rate of a classifier h is\n",
      "L(h) = P({W(X) # Y}) (28.1)\n",
      "\n",
      "and the empirical error rate or training error rate\n",
      "is\n",
      "\n",
      "Ex(h) =~ So1W(X) AY). (28.2)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "First we consider the special case where Y = {0,1}. Let\n",
      "\n",
      "r(z) =E(Y|X =2) =P(Y =1|X =2)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "denote the regression function. From Bayes’ theorem we have\n",
      "that:\n",
      "\n",
      "mfi(z)\n",
      "\n",
      "\"© = Fw +0 —- he)\n",
      "\n",
      "(23.3)\n",
      "where\n",
      "folz) = f(x\n",
      "filz) = f(x\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "and 7 = P(Y = 1).\n",
      "\n",
      " \n",
      "\n",
      "Definition 23.4 The Bayes classification rule h* is de-\n",
      "fined to be\n",
      "x) _f 1 if r(2) > 4\n",
      "He) ={ 0 otherwise. (23.4)\n",
      "The set Dih) = {x: P(Y =1|X =2) =P(Y =O|X\n",
      "x)} is called the decision boundary.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-428.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "430 23. Classification\n",
      "\n",
      "Warning! The Bayes rule has nothing to do with Bayesian\n",
      "inference. We could estimate the Bayes rule using either fre-\n",
      "quentist or Bayesian methods.\n",
      "\n",
      "The Bayes’ rule may be written in several equivalent forms:\n",
      "\n",
      "1 if P(Y =1]X =2) > P(Y =0|X =2)\n",
      "\n",
      "#4.) — 96\n",
      "\n",
      "HG) ={ 0 otherwise (28.5)\n",
      "\n",
      "~ | 0. otherwise.\n",
      "\n",
      "mo ={ 1 if wfi(x) > (L— 12) fo(z) (23.6)\n",
      "\n",
      "Theorem 23.5 The Bayes rule is optimal, that is, if h is any\n",
      "other classification rule then L(h*) < L(h).\n",
      "\n",
      "The Bayes rule depends on unknown quantities so we need to\n",
      "use the data to find some approximation to the Bayes rule. At\n",
      "the risk of oversimplifying, there are three main approaches:\n",
      "\n",
      "1. Empirical Risk Minimization. Choose a set of classifiers H.\n",
      "and find h € 1 that minimizes some estimate of L(h).\n",
      "\n",
      "2. Regression. Find an estimate 7 of the regression function\n",
      "r and define\n",
      "\n",
      "h(x) = { 1 if F(z) >\n",
      "\n",
      "0 otherwise.\n",
      "\n",
      "3. Density Estimation. Estimate fo from the X;’s for which\n",
      "Y; = 0, estimate f, from the X;’s for which Y; = 1 and let\n",
      "@=n-' 1 Vi. Define\n",
      "\n",
      "#filo)\n",
      "Fle) + (1— A fala)\n",
      "\n",
      "ila) = { 1 ifa)>4\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "F(x) = P(Y =1|X =2)\n",
      "\n",
      "and\n",
      "\n",
      "0 otherwise.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-429.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "23.3 Gaussian and Linear Classifiers 431\n",
      "\n",
      "ow let us generalize to the case where Y takes on more than\n",
      "two values as follows.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 23.6 Suppose that Y € Y = {1,...,K}. The\n",
      "optimal rule is\n",
      "\n",
      "h(x) = argmax,P(Y =k|X = 2) (23.7)\n",
      "es angina refill (238)\n",
      "\n",
      "where\n",
      "RIX = 2) — —fela)me ;\n",
      "P(Y =k|X = 2) = flan Feaym (23.9)\n",
      "\n",
      "Tt, =P(Y =r), f,(%) = f(x|Y =r) and argmax, means\n",
      "“the value of k that maximizes that expression.”\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "23.3 Gaussian and Linear Classifiers\n",
      "\n",
      "Perhaps the simplest approach to classification is to use the den-\n",
      "sity estimation strategy and assume a parametric model for the\n",
      "densities. Suppose that Y = {0,1} and that fo(2) = f(a|Y¥ =0)\n",
      "and f,(x) = f(x|Y = 1) are both multivariate Gaussians:\n",
      "\n",
      " \n",
      "\n",
      "1 1\n",
      "Se (2) = sper {$l = pn) Sp\" (a — m)} »k=0,1.\n",
      "\n",
      "Thus, X|Y =0 ~ N(jip, No) and X|Y =1 ~ N(w, D1).\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-430.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "432 23. Classification\n",
      "\n",
      " \n",
      "\n",
      "Theorem 23.7 If X|¥ =0 ~ N(p9, No) and X|Y =1~\n",
      "N(11, 1), then the Bayes rule is\n",
      "\n",
      "h*(2) = 1 ifr? <rj+2log (2) + log (=)\n",
      "0 otherwise\n",
      "\n",
      " \n",
      "\n",
      "(23.10)\n",
      "where\n",
      "\n",
      "re =(e- pm) Op '(e@-—pm), i=1,2 (23.11)\n",
      "\n",
      "i\n",
      "\n",
      "is the Manalahobis distance. An equivalent way of ex-\n",
      "pressing the Bayes’ rule is\n",
      "\n",
      "h(a) = argmax,0,(2)\n",
      "where\n",
      "\n",
      "1 1\n",
      "O(a) = —9 18 [Ex] — 52 = fg) Ey\" (e — pp) + log ty\n",
      "(23.12)\n",
      "\n",
      "and |A| denotes the determinant of a matrix A.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The decision boundary of the above classifier is quadratic so\n",
      "this procedure is often called quadratic discriminant analy-\n",
      "sis (QDA). In practice, we use sample estimates of 7, j1, fl2, No, Uy\n",
      "in place of the true value, namely:\n",
      "\n",
      "= lag® 1 a 1S,\n",
      "To = pa -¥ i= 5a\n",
      "\n",
      "1 ma. & 1 -\n",
      "Ho = — Xi, h=— » Xj\n",
      "70 yao M1 yma\n",
      "1 1\n",
      "= = YX -w)(X%-7)\", S=— YD (-)(X- HH)\"\n",
      "So Ta , ote! i— fo)(Xi — fio)\", St m | 7i)( ji)\n",
      "\n",
      "where no = 3);(1 — Yj) and m = 30, Yi.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-431.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "23.3 Gaussian and Linear Classifiers 433\n",
      "\n",
      "A simplification occurs if we assume that No = No = DU. In\n",
      "that case, the Bayes rule is\n",
      "\n",
      "h(x) = argmax,0,(2) (23.13)\n",
      "\n",
      "a\n",
      "\n",
      "vhere now\n",
      "1\n",
      "Ce ea ies ste + log m,. (23.14)\n",
      "\n",
      "The parameters are estimated as before except that the MLE of\n",
      "\n",
      "x is\n",
      "noSo +7191\n",
      "\n",
      "No +N,\n",
      "\n",
      "The classification rule is\n",
      "\n",
      "h*(2) = { 1 if 0,(x) > d9(x) (23.15)\n",
      "\n",
      "0 otherwise\n",
      "\n",
      " \n",
      "\n",
      "where\n",
      "\n",
      " \n",
      "\n",
      "a dx = =\n",
      "6;(2) = a7 Sf; — si ST + logit;\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "is called the discriminant function. The decision boundary\n",
      "{x : 69(x) = 6;(a)} is linear so this method is called linear\n",
      "discrimination analysis (LDA).\n",
      "\n",
      "Example 23.8 Let us return to the South African heart disease\n",
      "data, The decision rule in Figure 23.2 was obtained by linear\n",
      "discrimination. The outcome was\n",
      "classified as 0 classified as 1\n",
      "y=0 277 25\n",
      "y=1 116 44\n",
      "The observed misclassification rate is 141/462 = .31. Including\n",
      "all the covariates reduces the error rate to .27. The results from\n",
      "quadratic discrimination are\n",
      "classified as 0 classified as 1\n",
      "y=0 272 30\n",
      "y=1 113 47\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-432.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "The quantity J\n",
      "arises. in physics,\n",
      "where it is called the\n",
      "Rayleigh coefficient.\n",
      "\n",
      "434 23. Classification\n",
      "\n",
      "which has about the same error rate 143/462 = 31. Including\n",
      "all the covariates reduces the error rate to .26. In this example,\n",
      "there is little advantage to QDA over LDA.\n",
      "\n",
      "Now we generalize to the case where Y takes on more than\n",
      "two values.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 23.9 Suppose that Y € {1,...,K}. If fx(z) =\n",
      "f(a|Y =k) is Gaussian, the Bayes rule is\n",
      "\n",
      "h(x) = argmax,,d; (x)\n",
      "where\n",
      "\n",
      "1\n",
      "64(x) = —5 log [Lx| — 5(@ — Me) Ee (a — we) + log mp.\n",
      "(23.16)\n",
      "If the variances of the Gaussians are equal then\n",
      "\n",
      "1\n",
      "O(a) = 2 pe sie + log 7. (23.17)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "We estimate 6,(x) by by inserting estimates of jz, D, and 7.\n",
      "There is another version of linear discriminant analysis due to\n",
      "Fisher. The idea is to first reduce the dimension of covariates to\n",
      "one dimension by projecting the data onto a line. Algebraically,\n",
      "\n",
      "   \n",
      "\n",
      "this meansd replacing the covariate X = (X1,...,Xq) witha\n",
      "linear combination U = w?X = ia ;X;. The goal is to\n",
      "choose the vector w = (w),...,w,) that “best separates the\n",
      "\n",
      "data.” Then we perform classification with the new covariate Z\n",
      "instead of X.\n",
      "\n",
      "We need define what we mean by separation of the groups.\n",
      "We would like the two groups to have means that are far apart\n",
      "relative to their spread. Let j; denote the mean of X for Y;\n",
      "and let © be the variance matrix of X. Then E(U|Y = j) =\n",
      "E(w? X|Y = j) = w\"y; and V(U) = w’ Sw. Define the separa-\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-433.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "23.3 Gaussian and Linear Classifiers 435\n",
      "\n",
      "tion by\n",
      "\n",
      "eu y=1))?\n",
      "\n",
      " \n",
      "\n",
      "YY =0) -E(U\n",
      "wlDw\n",
      "(w? to — wT 1)?\n",
      "ww\n",
      "wT (uo — mi) (40 — pnw\n",
      "wldDw .\n",
      "We estimate J as follows. Let nj }3/_, I(Y; = j) be the number\n",
      "of observations in group J, let X; be the sample mean vector of\n",
      "the X’s for group j, and let S; be the sample covariance matrix\n",
      "in group j. Define\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "> w Spw\n",
      "\n",
      " \n",
      "\n",
      "y) = 23.1\n",
      "Am) wl Sww (23.18)\n",
      "where\n",
      "Sp = (Xo—X1)(Xo- Xi)\n",
      "dy = (no — 1) So + (m1 = 1)S1\n",
      "¥ (no = 1) + (m = 1)\n",
      "Theorem 23.10 The vector\n",
      "w = Sy) (Xo — X1) (23.19)\n",
      "is a minimizer of J(w). We call\n",
      "U=w'X = (Xo — 1)\" Sy X (23.20)\n",
      "\n",
      "the Fisher linear discriminant function. The midpoint m\n",
      "between Xo and X, is\n",
      "\n",
      " \n",
      "\n",
      "1 1\n",
      "m= 5% +X) = 5% —X1)7S5'(Xo+X1) (23.21)\n",
      "Fisher’s classification rule is\n",
      "h(a) = 0 ifw?X >m\n",
      "we) 1 ifw?X <m\n",
      "_ O if X%o—X,)' Soe > m\n",
      "~ Lif (ig —8,) Sy a = me\n",
      "Fisher’s rule is the same as the Bayes linear classifier in equa-\n",
      "tion (23.14) when % = 1/2.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-434.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "436 23. Classification\n",
      "23.4 Linear Regression and Logistic Regression\n",
      "\n",
      "A more direct approach to classification is to estimate the regres-\n",
      "sion function r(«) =E(¥Y|X =) without bothering to estimste\n",
      "the densities f,. For the rest of this section, we will only con-\n",
      "sider the case where Y = {0,1}. Thus, r(x) = P(Y =1|X = 2)\n",
      "and once we have an estimate 7, we will use the classification\n",
      "\n",
      "_ fl iff) >4 >\n",
      "A(x) = { 0 otherwise. (23.22)\n",
      "\n",
      "rule\n",
      "\n",
      "The simplest regression model is the linear regression model\n",
      "\n",
      "d\n",
      "Y=r(z)+e= 60+ > 8X; +e (23.23)\n",
      "\n",
      "j=l\n",
      "\n",
      "where E(e) = 0. This model can’t be correct since it does not\n",
      "force Y = 0 or 1. Nonetheless, it can sometimes lead to a decent\n",
      "classifier.\n",
      "\n",
      "Recall that the least squares estimate of 8 = (4, 41,---, 8a)\"\n",
      "minimizes the residual sums of squares\n",
      "\n",
      "n d\n",
      "\n",
      "RSS(8) = yu By = Yo XB) -\n",
      "\n",
      "i=l j=l\n",
      "\n",
      "Let us briefly review what that estimator is. Let X denote the\n",
      "N x (d+ 1) matrix of the form\n",
      "\n",
      "1 Xy ... Xa\n",
      "xa]! = 3\n",
      "1 Xn... Xna\n",
      "\n",
      "Also let Y = (Yi,-..,Y,)?. Then,\n",
      "\n",
      "RSS(B) = (Y — Xp)\" (¥ — Xf)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-435.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "23.4 Linear Regression and Logistic Regression 437\n",
      "\n",
      "and the model can be written as\n",
      "Y=Xf6+e\n",
      "\n",
      "where € = (€,-..,€,)7. The least squares solution 8 that min-\n",
      "imizes RSS can be found by elementary calculus and is given\n",
      "by\n",
      "\n",
      "B= (KEXP.\n",
      "The predicted values are\n",
      "\n",
      "Y=xp.\n",
      "\n",
      "Now we use (23.22) to classify, where F(2) = By + yj Bix.\n",
      "\n",
      "It seems sensible to use a regression method that takes into\n",
      "account that Y € {0,1}. The most common method for doing\n",
      "so is logistic regression. Recall that the model is\n",
      "\n",
      "@80+d; Bi\n",
      "\n",
      "1 4 eS04d; 5a; \"\n",
      "\n",
      " \n",
      "\n",
      "r(z) = P(Y =1|X =a) = (23.24)\n",
      "\n",
      "We may write this is\n",
      "\n",
      "logit P(Y =1,X = 2) = & + > 8,2;\n",
      "J\n",
      "\n",
      "where logit (a) = log(a/(1—«a)). Under this model, each Y; is a\n",
      "Bernoulli with success probability\n",
      "\n",
      "Bot Ej Bij\n",
      "\n",
      "pi(B) = 1p @b0Fhj BX”\n",
      "\n",
      "The likelihood function for the data set is\n",
      "\n",
      "(3) = pila).\n",
      "\n",
      " \n",
      "\n",
      "We obtain the MLE numerically.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-436.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "438 23. Classification\n",
      "\n",
      "Example 23.11 Let us return to the heart disease data. A logistic\n",
      "regression yields the following estimates and Wald tests for the\n",
      "\n",
      " \n",
      "\n",
      "coefficients:\n",
      "Covariate Estimate Std. Error Wald statistic p-value\n",
      "Intercept -6.145 1.300 -4.788 0.000\n",
      "sbp 0.007 0.006 1.1388 0.255\n",
      "tobacco 0.079 0.027 2.991 0.003\n",
      "Idl 0.174 0.059 2.925 0.003\n",
      "adiposity 0.019 0.029 0.687 0.524\n",
      "famhist 0.925 0.227 4.078 0.000\n",
      "typea 0.040 0.012 3.233 0.001\n",
      "obesity -0.068 0.044 “1.427 0.153\n",
      "alcohol 0.000 0.004 0.027 0.979\n",
      "age 0.045 0.012 3.754 0.000\n",
      "\n",
      "Are surprised by the fact that systolic blood pressure is not\n",
      "significant or by the minus sign for the obesity coefficient? If\n",
      "yes, then you are confusing correlation and causation. There\n",
      "are lots of unobserved confounding variables so the coefficients\n",
      "above cannot be interpreted causally. The fact that blood pres-\n",
      "sure is not significant does not mean that blood pressure is not an\n",
      "important cause of heart disease. It means that it is not an im-\n",
      "portant predictor of heart disease relative to the other variables\n",
      "in the model. The error rate, using this model for classification,\n",
      "is .27. The error rate from a linear regression is .26.\n",
      "\n",
      "We can get a better classifier by fitting a richer model. For\n",
      "example we could fit\n",
      "\n",
      "logit P(Y = 1X = 2) = Bo + 57> By; + S> Bjxxjxy- (23.25)\n",
      "j ik\n",
      "\n",
      "More generally, we could add terms of up to order r for some\n",
      "integer r. Large values of r give a more complicated model which\n",
      "should fit the data better. But there is a bias-variance tradeoff\n",
      "which we'll discuss later.\n",
      "\n",
      "Example 23.12 If we use model (23.25) for the heart disease data\n",
      "with r = 2, the error rate is reduced to .22.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-437.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "23.5 Relationship Between Logistic Regression and LDA 439\n",
      "\n",
      "The logistic regression model can easily be extended to k\n",
      "groups but we shall not give the details here.\n",
      "\n",
      "23.5 Relationship Between Logistic Regression and\n",
      "LDA\n",
      "\n",
      "LDA and logistic regression are almost the same thing. If we\n",
      "assume that each group is Gaussian with the same covariance\n",
      "matrix then we saw that\n",
      "\n",
      "PY =1|X =2 TX 1\n",
      "log (Fr = ate = +) log (2) = (uo + pa) Ei = Ho) + 27S\" (un — Ho)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "= a+ ala.\n",
      "\n",
      "On the other hand, the logistic model is, by assumption,\n",
      "P(Y =1|X =2) ar.\n",
      "\n",
      "log Gor =x +) = fot Bs.\n",
      "These are the same model since they both lead to classification\n",
      "rules that are linear in a. The difference is in how we estimate\n",
      "the parameters.\n",
      "\n",
      "The joint density ofa single observation is f(x,y) = f («]y) f(y) =\n",
      "\n",
      "f (y|x) f (2). In LDA we estimated the whole joint distribution by\n",
      "estimating f(x|y) and f(y); specifically, we estimated f,(x) =\n",
      "f(alY =k), m = fy(1) and 7 = fy(0). We maximized the\n",
      "likelihood\n",
      "\n",
      "[] fi.) =T] feityd TI @0- (23.26)\n",
      "\n",
      "Gaussian Bernoulli\n",
      "\n",
      "In logistic regression we maximized the conditional likelihood\n",
      "IL, f(yilei) but we ignored the second term f (2;):\n",
      "\n",
      "[] f(9) = 11 foiled TT fo - (23.27)\n",
      "\n",
      "logistic ignored\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-438.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "440 23. Classification\n",
      "\n",
      "Since classification only requires knowing f (y|x), we don’t really\n",
      "need to estimate the whole joint distribution. Here is an analogy:\n",
      "we can estimate the mean p of a distribution with the sample\n",
      "mean X without bothering to estimate the whole density func-\n",
      "tion. Logistic regression leaves the marginal distribution f (x)\n",
      "un-specified so it is more nonparametric than LDA.\n",
      "\n",
      "To summarize: LDA and logistic regression both lead to a\n",
      "linear classification rule. In LDA we estimate the entire joint\n",
      "distribution f(x,y) = f(xly) f(y). In logistic regression we only\n",
      "estimate f(y|z) and we don’t bother estimating f(x).\n",
      "\n",
      "23.6 Density Estimation and Naive Bayes\n",
      "\n",
      "Recall that the Bayes rule is h(x) = argmax, 7} f,(2). If we\n",
      "can estimate 7, and f; then we can estimate the Bayes classi-\n",
      "fication rule. Estimating 7; is easy but what about f,? We did\n",
      "this previously by assuming f, was Gaussian. Another strategy\n",
      "is to estimate f, with some nonparametric density estimator\n",
      "fj such as a kernel estimator. But if « = (2,...,2g) is high\n",
      "dimensional, nonparametric density estimation is not very reli-\n",
      "able. This problem is ameliorated if we assume that X,,...,Xa\n",
      "are independent, for then, f.(v1,---,%4) = Th Fuj(aj). This\n",
      "reduces the problem to d one-dimensional density estimation\n",
      "problems, within each of the k groups. The resulting classifier\n",
      "is called the naive Bayes classifier. The assumption that the\n",
      "components of X are independent is usually wrong yet the re-\n",
      "sulting classifier might still be accurate. Here is a summary of\n",
      "the steps in the naive Bayes classifier:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-439.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "23.7 Trees 441\n",
      "\n",
      " \n",
      "\n",
      "The Naive Bayes Classifier\n",
      "\n",
      "1. For each group k, compute an estimate fi of the density\n",
      "fx for Xj, using the data for which Y; = k.\n",
      "\n",
      "d\n",
      "fi(@) = fie(ti,.--,€a) = [] fule-\n",
      "\n",
      "~ 14 ,,-\n",
      "fe= 5 DK)\n",
      "\n",
      "where I(Y¥; =k) =1if Yj; =k and I(Y¥; =k) =0if Yk.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "4. Let\n",
      "h(x) = argmax, 7, f,(2)-\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The naive Bayes classifier is especially popular when z is high\n",
      "dimensional and discrete. In that case, fj,;(;) is especially sim-\n",
      "\n",
      "ple.\n",
      "\n",
      "23.7 Trees\n",
      "\n",
      "Trees are classification methods that partition the covariate\n",
      "space ¥ into disjoint pieces and then classify the observations\n",
      "according to which partition element they fall in. As the name\n",
      "implies, the classifier can be represented as a tree.\n",
      "\n",
      "For illustration, suppose there are two covariates, X; = age\n",
      "and X»2 = blood pressure. Figure 23.3 shows a classification tree\n",
      "using these variables.\n",
      "\n",
      "The tree is used in the following way. If a subject has Age\n",
      "> 50 then we classify him as Y = 1. If a subject has Age < 50\n",
      "then we check his blood pressure. If blood pressure is < 100\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-440.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "442 23. Classification\n",
      "\n",
      "Age\n",
      "\n",
      "Blood Pressure\n",
      "\n",
      "< am K Ne 100\n",
      "\n",
      "0 uh\n",
      "\n",
      "FIGURE 23.3. A simple classification tree.\n",
      "\n",
      "then we classify him as Y = 1, otherwise we classify him as\n",
      "Y = 0. Figure 23.4 shows the same classifier as a partition of\n",
      "the covariate space.\n",
      "\n",
      "Here is how a tree is constructed. For simplicity, we focus on\n",
      "the case in which € Y = {0,1}. First, suppose there is only a\n",
      "single covariate X. We choose a split point ¢ that divides the\n",
      "real line into two sets A; = (—ov, t] and Ay = (t, 00). Let p,(j)\n",
      "be the proportion of observations in A, such that Y; = j:\n",
      "\n",
      " \n",
      "\n",
      "a). 5, Xi € As)\n",
      "Ps(9) = ST eA)\n",
      "\n",
      "it\n",
      "\n",
      " \n",
      "\n",
      "(23.28)\n",
      "\n",
      "for s = 1,2 and j = 0,1. The impurity of the split ¢ is defined\n",
      "to be\n",
      "\n",
      "2\n",
      "I) = 0% (23.29)\n",
      "s=l\n",
      "where\n",
      "\n",
      "ye =1- 7B). (23.30)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-441.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.7 Trees 443\n",
      "\n",
      "110\n",
      "\n",
      "Blood Pressure\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Age\n",
      "\n",
      "FIGURE 23.4. Partition representation of classification tree.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-442.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "444 23. Classification\n",
      "\n",
      "This measure of impurity is known as the Gini index. If a\n",
      "partition element A, contains all 0’s or all 1’s, then y, = 0.\n",
      "Otherwise, 7, > 0. We choose the split point ¢ to minimize the\n",
      "impurity. (Other indices of impurity besides can be used besides\n",
      "the Gini index.)\n",
      "\n",
      "When there are several covariates, we choose whichever co-\n",
      "variate and split that leads to the lowest impurity. This process\n",
      "is continued until some stopping criterion is met. For example,\n",
      "we might stop when every partition element has fewer than no\n",
      "data points, where no is some fixed number. The bottom nodes\n",
      "of the tree are called the leaves. Each leaf is assigned a 0 or 1\n",
      "depending on whether there are more data points with Y = 0\n",
      "or Y =1 in that partition element.\n",
      "\n",
      "This procedure is easily generalized to the case where Y €\n",
      "{1,..., A}. We simply define the impurity by\n",
      "\n",
      "%=1- 0B. (23.31)\n",
      "\n",
      "where p;(j) is the proportion of observations in the partition\n",
      "element for which Y = j.\n",
      "\n",
      "Example 23.13 Figure 23.5 shows a classification tree for the\n",
      "heart disease data. The misclassification rate is .21. Suppose we\n",
      "do the same example but only using two covariates: tobacco and\n",
      "age. The misclassification rate is then .29. The tree is shown in\n",
      "Figure 23.6. Because there are only two covariates, we can also\n",
      "display the tree as a partition of X = R’ as shown in Figure\n",
      "23.7. 0\n",
      "\n",
      "Our description of how to build trees is incomplete. If we keep\n",
      "splitting until there are few cases in each leaf of the tree, we are\n",
      "likely to overfit the data. We should choose the complexity of\n",
      "the tree in such a way that the estimated true error rate is low.\n",
      "In the next section, we discuss estimation of the error rate.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-443.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "23.7 Trees 445\n",
      "\n",
      "age < 31.5\n",
      "\n",
      "    \n",
      "     \n",
      "     \n",
      "      \n",
      "     \n",
      "\n",
      "tobaceq < 0.51\n",
      "\n",
      "alcohol ¢ 11.105 typeak 68.5\n",
      "\n",
      "tobacco|< 7.605\n",
      "\n",
      "  \n",
      "\n",
      "typeak 42.5 adiposity|< 28.955 tobaccq < 4.15\n",
      "\n",
      "adiposity|< 24.435\n",
      "\n",
      "FIGURE 23.5. A classification tree for the heart disease data.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-444.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "446 23. Classification\n",
      "\n",
      " \n",
      "\n",
      "FIGURE 23.6. A classification tree for the heart disease\n",
      "data using two covariates.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-445.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "tobacco\n",
      "\n",
      "30\n",
      "\n",
      "25\n",
      "\n",
      "20\n",
      "\n",
      "15\n",
      "\n",
      "10\n",
      "\n",
      "23.7 Trees\n",
      "\n",
      "447\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "T T T T\n",
      "20 30 40 50\n",
      "\n",
      "age\n",
      "\n",
      "FIGURE 23.7. The tree pictured as a partition of the\n",
      "covariate space.\n",
      "\n",
      "60\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-446.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "448 23. Classification\n",
      "\n",
      "23.8 Assessing Error Rates and Choosing a Good\n",
      "Classifier\n",
      "\n",
      "How do we choose a good classifier? We would like to have a\n",
      "classifier h with a low true error rate L(h). Usually, we can’t use\n",
      "the training error rate In (h) as an estimate of the true error rate\n",
      "because it is biased downward.\n",
      "\n",
      "Example 23.14 Consider the heart disease data again. Suppose\n",
      "we fit a sequence logistic regression models. In the first model we\n",
      "include one covariate. In the second model we include two co-\n",
      "variates and so on. The ninth model includes all the covariates.\n",
      "We can go even further. Let’s also fit a tenth model that in-\n",
      "cludes all nine covariates plus the first covariate squared. Then\n",
      "we fit an eleventh model that includes all nine covariates plus the\n",
      "first covariate squared and the second covariate squared. Con-\n",
      "tinuing this way we will get a sequence of 18 classifiers of in-\n",
      "creasing complexity. The solid line in Figure 23.8 shows the ob-\n",
      "served classification error which steadily decreases as we make\n",
      "the model more complex. If we keep going, we can make a model\n",
      "with zero observed classification error. The dotted line shows the\n",
      "cross-validation estimate of the error rate (to be explained\n",
      "shortly) which is a better estimate of the true error rate than the\n",
      "observed classification error. The estimated error decreases for\n",
      "a while then increases. This is the bias-variance tradeoff phe-\n",
      "nomenon we have seen before. Hi\n",
      "\n",
      "There are many ways to estimate the error rate. We'll consider\n",
      "two: cross-validation and probability inequalities.\n",
      "\n",
      "Cross- VALIDATION. The basic idea of cross-validation, which\n",
      "we have already encountered in curve estmation, is to leave out\n",
      "some of the data when fitting a model. The simplest version of\n",
      "cross-validation involves randomply splitting the data into two\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-447.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "23.8 Assessing Error Rates and Choosing a Good Classifier 449\n",
      "\n",
      " \n",
      "\n",
      "0.32\n",
      "L\n",
      "\n",
      "error rate\n",
      "\n",
      "0.28\n",
      "L\n",
      "\n",
      "0.26\n",
      "L\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "5 10 15\n",
      "Number of terms in the model\n",
      "FIGURE 23.8. Solid line is the observed error rate and\n",
      "\n",
      "dashed line is the cross-validation estimate of true error\n",
      "rate.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-448.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "450 23. Classification\n",
      "\n",
      "Training Data 7 Validation Data YV\n",
      "\n",
      " \n",
      "\n",
      "FIGURE 23.9. Cross-validation.\n",
      "\n",
      "pieces: the training set 7 and the validation set V. Often,\n",
      "about 10 per cent of the data might be set aside as the valida-\n",
      "tion set. The classifier h is constructed from the training set.\n",
      "We then estimate the error by\n",
      "\n",
      "Zh) = = Sy A(X) #¥)- (23.32)\n",
      "\n",
      "where m is the size of the validation set. See Figure 23.9.\n",
      "Another approach to cross-validation is K-fold cross-validation\n",
      "which is obtained from the following algorithm.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-449.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "23.8 Assessing Error Rates and Choosing a Good Classifier 451\n",
      "\n",
      " \n",
      "\n",
      "K-fold cross-validation.\n",
      "\n",
      "1. Randomly divide the data into K chunks of approximately\n",
      "equal size. A common choice is K = 10.\n",
      "\n",
      "2. For k = 1 to K do the following:\n",
      "(a) Delete chunk k from the data.\n",
      "\n",
      "(b) Compute the classifier hay from the rest of the data.\n",
      "\n",
      "(c) Use hay to the predict the data in chunk k. Let La\n",
      "denote the observed error rate.\n",
      "\n",
      "3. Let\n",
      "~ i a en\n",
      "Li=_ > Lw- (23.33)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "This is how dotted line in Figure 23.8 was computed. An al-\n",
      "ternative approach to K-fold cross-validation is to split the data\n",
      "into two pieces called the training set and the test set. All\n",
      "the models are fit on the training set. The models are then used\n",
      "to do predictions on the test set and the errors are estimated\n",
      "from these predictions. When the clasification procedure is time\n",
      "consuming and there are lots of data, this approach is preferred\n",
      "to K-fold cross-validation.\n",
      "\n",
      "Cross-validation can be applied to any classification method.\n",
      "To apply it to trees, one begins by fitting an initial tree. Smaller\n",
      "trees are obtained by pruning tree, that is removing splits from\n",
      "the tree. We can do this for trees of various sizes where size refers\n",
      "to the number of terminal nodes on the tree. Cross-validation is\n",
      "then used to estimate error rate as a function of tree size.\n",
      "\n",
      "Example 23.15 Figure 23.10 shows the cross-validation plot for\n",
      "a classification tree fitted to the heart disease data. The error\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-450.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "452 23. Classification\n",
      "\n",
      "is shown as a function of tree size. Figure 23.11 shows the tree\n",
      "with the size chosen by minimizing the cross-validation error. Mi\n",
      "\n",
      "PROBABILITY INEQUALITIES. Another approach to estimat-\n",
      "ing the error rate is to find a confidence interval for In (h) using\n",
      "probability inequalities. This method is useful in the context of\n",
      "empirical risk minimization.\n",
      "\n",
      "Let H be a set of classifiers, for example, all linear classifiers.\n",
      "Empirical risk minimization means choosing the classifier hex\n",
      "to minimize the training error L,(h), also called the empirical\n",
      "risk. Thus,\n",
      "\n",
      "x > 1\n",
      "h = argminyeyLn(h) = argminy ey (: SAX) FY) }.\n",
      "i\n",
      "\n",
      "(23.34)\n",
      "Typically, Zn (h) underestimates the true error rate L(h) because\n",
      "fh was chosen to make L,.(h) small. Our goal is to assess how\n",
      "much underestimation is taking place. Our main tool for this\n",
      "analysis is Hoeffding’s inequality. Recall that if X,,...,X,~\n",
      "Bernoulli(p), then, for any € > 0,\n",
      "\n",
      "P (p— pl > 6) < 2e°\"\" (23.35)\n",
      "\n",
      "where p=n7-! Sv, Xj.\n",
      "\n",
      "First, suppose that H = {h,,...,h,,} consists of finitely many\n",
      "classifiers. For any fixed h, L,,(h) converges in almost surely\n",
      "to L(h) by the law of large numbers. We will now establish a\n",
      "\n",
      "stronger result.\n",
      "\n",
      "Theorem 23.16 (Uniform Convergence.) Assume H is finite\n",
      "and has m elements. Then,\n",
      "\n",
      "P L,,(h) — Lit <2me?ne.\n",
      "(ns n(h) ml>e) < 2me\n",
      "\n",
      "ProoF. We will use Hoeffding’s inequality and we will also\n",
      "use the fact that if Ai,...,Am isaset of events then P(Uj\", Ai) <\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-451.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n",
      "score\n",
      "\n",
      "160\n",
      "\n",
      "155\n",
      "\n",
      "150\n",
      "\n",
      "145\n",
      "\n",
      "140\n",
      "\n",
      "135\n",
      "\n",
      "130\n",
      "\n",
      "ssing Error Rates and Choosing a Good Classifier\n",
      "\n",
      "4\n",
      "\n",
      "5\n",
      "\n",
      "3\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "size\n",
      "\n",
      "FIGURE 23.10. Cross validation plot\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-452.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "4\n",
      "\n",
      "23. Classification\n",
      "\n",
      "age <$ 31.5\n",
      "\n",
      "   \n",
      "\n",
      "tobaccol< 7.605\n",
      "\n",
      "FIGURE 23.11. Smaller classification tree with size cho-\n",
      "sen by cross-validation.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-453.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "23.8 Assessing Error Rates and Choosing a Good Classifier 455\n",
      "\n",
      "2 P(Ai). Now,\n",
      "\n",
      "P (nu |L,(h) — D(h)| > )\n",
      "\n",
      "P (U |E,(h) — L(h)| > ‘\n",
      "\n",
      "hen\n",
      "\n",
      "< SP (\\n(h) —L(h)| > c)\n",
      "HEH\n",
      "\n",
      "< > 2022 = Ome?\n",
      "HEH\n",
      "\n",
      "Theorem 23.17 Let\n",
      "e= 2 log (\n",
      "n\n",
      "\n",
      " \n",
      "\n",
      "Then L,,(h) £e is a1 — a confidence interval for L(h).\n",
      "\n",
      " \n",
      "\n",
      "PROOF. This follows from the fact that\n",
      "\n",
      "P(lEa(h) D0) > 9. <P (max olf) ~ 10) > €)\n",
      "\n",
      "2me\"\" =a.\n",
      "\n",
      "IA\n",
      "\n",
      "When H is large the confidence interval for L(h) is large.\n",
      "The more functions there are in H the more likely it is we have\n",
      "“overfit” which we compensate for by having a larger confidence\n",
      "interval.\n",
      "\n",
      "In practice we usually use sets H that are infinite, such as the\n",
      "set of linear classifiers. To extend our analysis to these cases we\n",
      "want to be able to say something like\n",
      "\n",
      "P (sp |E,(h) — L(h)| > ‘J < something not too big.\n",
      "heH\n",
      "\n",
      "All the other results followed from this inequality. One way\n",
      "\n",
      "to develop such a generalization is by way of the Vapnik-\n",
      "\n",
      "Chervonenkis or VC dimension. We now consider the main\n",
      "\n",
      "ideas in VC theory.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-454.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "456 23. Classification\n",
      "\n",
      "Let A be a class of sets. Give a finite set F = {2,,...,,} let\n",
      "\n",
      " \n",
      "\n",
      "Na(F) = #{F(\\A Ae A} (23.36)\n",
      "\n",
      "be the number of subsets of F “picked out” by A. Here #(B)\n",
      "denotes the number of elements of a set B. The shatter coef\n",
      "ficient is defined by\n",
      "\n",
      "s(A,n) = max Na(F) (23.37)\n",
      "where F,, consists of all finite sets of size n. Now let X1,...,X, ~\n",
      "\n",
      "P and let ‘\n",
      "P,(A) == LN € A)\n",
      "\n",
      "denote the empirical probability measure. The following remark-\n",
      "able theorem bounds the distance between P and P,,.\n",
      "\n",
      "Theorem 23.18 (Vapnik and Chervonenkis (1971).) For any\n",
      "P, n ande > 0,\n",
      "\n",
      "P{sup IP, (A) — P(A)| > 3 < 88(A,n)emr 2. (23.38)\n",
      "AEA\n",
      "\n",
      "The proof, though very elegant, is long and we omit it. If\n",
      "is a set of classifiers, define A to be the class of sets of the form\n",
      "{x: h(x) =1}. When then define s(H,n) = s(A,n).\n",
      "\n",
      "Theorem 23.19\n",
      "\n",
      "P {sup |L,(h) —L(h)| > 7 <8s8(H, ner,\n",
      "hex\n",
      "\n",
      "A 1 —a confidence interval for L(h) is L,(h) + & where\n",
      "\n",
      "a 4 log (ae =)\n",
      "n\n",
      "\n",
      "n a\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-455.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "23.8 Assessing Error Rates and Choosing a Good Classifier 457\n",
      "\n",
      "These theorems are only useful if the shatter coefficients do\n",
      "not grow too quickly with n. This is where VC dimension enters.\n",
      "\n",
      " \n",
      "\n",
      "Definition 23.20 The VC (Vapnik-Chervonenkis) dimen-\n",
      "sion of a class of sets A is defined as follows. If s(A,n) =\n",
      "2” for all n set VC(A) = 00. Otherwise, define VC (A)\n",
      "to be the largest k for which s(A,n) = 2*.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Thus, the VC-dimension is the size of the largest finite set\n",
      "F than can be shattered by A meaning that A picks out each\n",
      "subset of F. If #1 is aset of classifiers we define VC(H) = VC(A)\n",
      "where A is the class of sets of the form {7 : h(x) = 1} ash\n",
      "varies in H. The following theorem shows that if A has finite\n",
      "\n",
      " \n",
      "\n",
      "VC-dimension then the shatter coefficients grow as a polynomial\n",
      "inn.\n",
      "\n",
      "Theorem 23.21 If A has finite VC-dimension v, then\n",
      "s(A,n) <n’ +1.\n",
      "\n",
      "Example 23.22 Let A = {(—o0,a]; a € R}. The A shatters\n",
      "ever one point set {x} but it shatters no set of the form {x,y}.\n",
      "Therefore, VC(A) =1.\n",
      "\n",
      "Example 23.23 Let A be the set of closed intervals on the real\n",
      "line. Then A shatters S = {x,y} but it cannot shatter sets with\n",
      "8 points. Consider S = {x,y,z} where x < y <z. One cannot\n",
      "find an interval A such that A()S = {x,z}. So, VO(A) =2.\n",
      "\n",
      "Example 23.24 Let A be all linear half-spaces on the plane. Any\n",
      "three point set (not all on a line) can be shattered. No 4 point\n",
      "set can be shattered. Consider, for example, 4 points forming a\n",
      "diamond. Let T be the left and rightmost points. This can’t be\n",
      "picked out. Other configurations can also be seen to be unshat-\n",
      "terable. So VC(A) = 3. In general, halfspaces in R¢ have VC\n",
      "dimension d+ 1.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-456.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "458 23. Classification\n",
      "\n",
      "Example 23.25 Let A be all rectangles on the plane with sides\n",
      "parallel to the ates. Any 4 point set can be shattered. Let S be\n",
      "a 5 point set. There is one point that is not leftmost, rightmost,\n",
      "uppermost, or lowermost. Let T be all points in S except this\n",
      "point. Then T can’t be picked out. So VC(A) =4.\n",
      "\n",
      "Theorem 23.26 Let x have dimension d and let H be th set of\n",
      "linear classifiers. The VC dimension of H. is d+1. Hence, al—a\n",
      "confidence interval for the true error rate is L(h) + € where\n",
      "\n",
      "gy 32 (on + )\n",
      "\n",
      "&, = — log\n",
      "n a\n",
      "\n",
      "23.9 Support Vector Machines\n",
      "\n",
      "In this section we consider a class of linear classifiers called sup-\n",
      "port vector machines. Throughout this section, we assume\n",
      "that Y is binary. It will be convenient to label the outcomes as\n",
      "—1 and +1 instead of 0 and 1. A linear classifier can then be\n",
      "written as\n",
      "\n",
      "h(x) = sign (H@)\n",
      "\n",
      "where x = (%,..-,%a),\n",
      "\n",
      "a\n",
      "A(x) =a) + Sar;\n",
      "i=l\n",
      "\n",
      "and\n",
      "-1 ifz<0\n",
      "sign(z) = 0 ifz=0\n",
      "1 ifz>0.\n",
      "\n",
      "First, suppose that the data are linearly separable, that\n",
      "is, there exists a hyperplane that perfectly separates the two\n",
      "classes.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-457.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "23.9 Support Vector Machines 459\n",
      "\n",
      "Lemma 23.27 The data can be separated by some hyperplane if\n",
      "and only if there exists a hyperplane H(x) = ant yot a,x; such\n",
      "that\n",
      "\n",
      "YjH(a,) >1, i=1,...,n. (23.39)\n",
      "\n",
      "PROOF. Suppose the data can be separated by a hyperplane\n",
      "W(a) = byt, b,x;. It follows that there exists some constant\n",
      "c such that Y; = 1 implies W(X;) > c and Y; = —1 implies\n",
      "W(X) < —c. Therefore, Y¥,;W(X;) > ¢ for all i. Let A(x) =\n",
      "ag +, ax; where a; = b;/c. Then Y;H(X;) > 1 for all i. The\n",
      "reverse direction is straightforward.\n",
      "\n",
      "In the separable case, there will be many separating hyper-\n",
      "planes. How should we choose one? Intuitively, it seems reason-\n",
      "able to choose the hyperplane “furthest” from the data in the\n",
      "sense that it separates the +1’a and -1’s and maximizes the dis-\n",
      "tance to the closest point. This hyperplane is called the maxi-\n",
      "mum margin hyperplane. The margin is the distance to from\n",
      "the hyperplane to the nearest point. Points on the boundary of\n",
      "the margin are called support vectors. See Figure 23.12.\n",
      "\n",
      "Theorem 23.28 The hyperplane H(x) = G+, G2; that sep-\n",
      "arates the data and maximizes the margin is given by minimizing\n",
      "(1/2) 374_, B3 subject to (23.39).\n",
      "\n",
      "jal g\n",
      "\n",
      "It turns out that this problem can be recast as a quadratic\n",
      "programming problem. Recall that (X;,.X,) = X7X;, is the in-\n",
      "ner product of X; and X,.\n",
      "\n",
      "Theorem 23.29 Let H(x) =o + Yh Ga; denote the optimal\n",
      "(largest margin) hyperplane. Then, for j =1,...,d,\n",
      "\n",
      "4; = S_aY;X;(i)\n",
      "i=1\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-458.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80ED608>\n",
      "460 23. Classification\n",
      "\n",
      " \n",
      "\n",
      "H(z) =ay+aTx=0\n",
      "\n",
      " \n",
      "\n",
      "FIGURE 23.12. The hyperplane H(z) has the largest margin of all\n",
      "hyperplanes that separate the two classes.\n",
      "\n",
      "where X;(i) is the value of the covariate X; for the i data\n",
      "\n",
      "point, and @ = (@i,...,Q@,) is the vector that maximizes\n",
      "n pe ae\n",
      "Mai 5 EV ain Vive (Xs, Xe) (23.40)\n",
      "i=l i=l k=l\n",
      "subject to\n",
      "a; >0\n",
      "and\n",
      "\n",
      "0=So ay.\n",
      "F\n",
      "\n",
      "The points X; for which @ F 0 are called support vectors. Go\n",
      "can be found by solving\n",
      "\n",
      "a (Vi(X?a+ fo) =0\n",
      "for any support point X;. ri may be written as\n",
      "\n",
      "H(x) = + D> 4,¥i(x, Xi).\n",
      "i=l\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-459.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "23.10 Kernelization 461\n",
      "\n",
      "There are many software packages that will solve this problem\n",
      "quickly. If there is no perfect linear classifier, then one allows\n",
      "overlap between the groups by replacing the condition (23.39)\n",
      "with\n",
      "\n",
      "Y;H(a;)) >1-&, &>0, i=1,...,n. (23.41)\n",
      "\n",
      "The variables €,...,&, are called slack variables.\n",
      "We now maximize (23.40) subject to\n",
      "\n",
      "0<&<e i=1\n",
      "\n",
      " \n",
      "\n",
      "and hn\n",
      "Sai =0.\n",
      "eek\n",
      "\n",
      "The constant ¢ is a tuning parameter that controls the amount\n",
      "of overlap.\n",
      "\n",
      "23.10 Kernelization\n",
      "\n",
      "There is a trick for improving a computationally simple classifier\n",
      "h called kernelization. The idea is to map the covariate XY —\n",
      "which takes values in 4 — into a higher dimensional space Z and\n",
      "apply the classifier in the bigger space Z. This can yield a more\n",
      "flexible classifier while retaining computationally simplicity.\n",
      "\n",
      "The standard example of this idea is illustrated in Figure\n",
      "23.13. The covariate x = (21,22). The Y;’s can be separated\n",
      "into two groups using an ellipse. Define a mapping @ by\n",
      "\n",
      "2 = (21, 22,23) = O(a) = (a7, V20129, 23).\n",
      "\n",
      "This @ maps Y = R’ into Z = R’. In the higher dimensional\n",
      "space Z, the Y;’s are seperable by a linear decision boundary.\n",
      "In other words, a linear classifier in a higher dimenional space\n",
      "corresponds to a non-linear classifier in the original space.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-460.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "462 23. Classification\n",
      "\n",
      "  \n",
      "\n",
      "FIGURE 23.13. Kernelization. Mapping the covariates into a higher\n",
      "dimensional space can make a complicated decision boundary into a\n",
      "simpler decision boundary.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-461.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9FC8>\n",
      "23.10 Kernelization 463\n",
      "\n",
      "There is a potential drawback. If we signicantly expand the\n",
      "dimension of the problem, we might increase the computational\n",
      "burden. For example, x has dimension d = 256 and we wanted\n",
      "to use all fourth order terms, then z = ¢(x) has dimension\n",
      "183,181,376. What saves us is two observations. First, many\n",
      "classifiers do not require that we know the values of the indi-\n",
      "vidual points but, rather, just the inner product between pairs\n",
      "of points. Second, notice in our example that the inner product\n",
      "in Z can be written\n",
      "\n",
      "(2.2) = (¢(2), 6(@))\n",
      "2pE? + 2x1 F, ToT + 1353\n",
      "((@, ®))?\n",
      "Thus, we can compute (z,Z) without ever computing Z; =\n",
      "o(X).\n",
      "\n",
      "To summarize, kernelization means involves finding a map-\n",
      "ping ¢: X > Z and a classifier such that:\n",
      "\n",
      "1. Z has higher dimension than ¥ and so leads a richer set\n",
      "of classifiers.\n",
      "\n",
      "2. The classifier only requires computing inner products.\n",
      "\n",
      "3. There isa function K, called a kernel, such that (¢(2), 6(Z))\n",
      "\n",
      "4. Everywhere the term (x,Z) appears in the algorithm, re-\n",
      "place it with K(x, 7Z).\n",
      "\n",
      "  \n",
      "\n",
      "  \n",
      "\n",
      "In fact, we never need to construct the mapping ¢ at all.\n",
      "We only need to specify a kernel k(x,Z) that corresponds to\n",
      "(x), 6(%)) for some ¢. This raises an interesting question: given\n",
      "a function of two variables K(2,y), does there exist a function\n",
      "o(x) such that K(x, y) = (6(x),d(y)). The answer is provided\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-462.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "464 23. Classification\n",
      "\n",
      "by Mercer’s theorem which says, roughly, that if K is positive\n",
      "definite — meaning that\n",
      "\n",
      "| [Kemsemrtnaray > 0\n",
      "\n",
      "for square integrable functions f — then such a ¢ exists. Exam-\n",
      "ples of commonly used kernels are:\n",
      "\n",
      "polynomial K(z,7) = ((, Z)+ a)\n",
      "sigmoid K(2,%) = tanh(a(2, 7) +6)\n",
      "Gaussian K(r,%) = exp (-lle —2| /?/(20%))\n",
      "\n",
      "Let us now see how we can use this trick in LDA and in\n",
      "support vector machines.\n",
      "\n",
      "Recall that the Fisher linear discriminant method replaces X\n",
      "with U = w\"X where w is chosen to maximize the Rayleigh\n",
      "\n",
      "coefficient .\n",
      "w' Spw\n",
      "\n",
      "wl Syww'\n",
      "\n",
      "Sp = (Xo —X1)(Xo-™)”\n",
      "\n",
      " \n",
      "\n",
      "J(w) =\n",
      "\n",
      "and\n",
      "\n",
      "sv= (Gare) * mane)\n",
      "\n",
      "In the kernelized version, we replace X; with Z; = ¢(X;) and\n",
      "we find w to maximize\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "(23.42)\n",
      "\n",
      "where\n",
      "\n",
      " \n",
      "\n",
      "and\n",
      "\n",
      ": (no = 1)5o (m =1)51\n",
      "Sw (oes): ( By).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-463.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "23.10 Kernelization 465\n",
      "\n",
      "Here, S; is the sample of covariance of the Z;’s for which Y =\n",
      "j. However, to take advantage of kernelization, we need to re-\n",
      "express this in terms of inner products and then replace the\n",
      "inner products with kernels.\n",
      "\n",
      "It can be shown that the maximizing vector w is a linear\n",
      "combination of the Z;’s. Hence we can write\n",
      "\n",
      "n\n",
      "\n",
      "w= > aj Zj.\n",
      "\n",
      "Also,\n",
      "7-2 y X14 =)\n",
      "Therefore,\n",
      "= a TIALS\n",
      "w'Z, = (Yaz) (| Leai=9)\n",
      "7 EE Lat = 2H)\n",
      "\n",
      "= ty aS = DoOLXITHX,)\n",
      "\n",
      "n\n",
      "Ji=t sal\n",
      "\n",
      "Fee\n",
      "\n",
      "= —Vay Sl = 5s) K(X, X,)\n",
      "rn s=l\n",
      "\n",
      "= a? M;\n",
      "\n",
      "where M; is a vector whose i” component is\n",
      "(i) = — YO K(X, XY = 2).\n",
      "J s=1\n",
      "It follows that\n",
      "w' Saw = al Ma\n",
      "where M = (Mo — M,)(Mo — M,)\". By similar calcuations, we\n",
      "can write\n",
      "w' Sww =alNa\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-464.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "466 23. Classification\n",
      "\n",
      "where\n",
      "1 oT - 1 oT\n",
      "N=Ko(l-—1)ho +h (1-—1) kk,\n",
      "no ny\n",
      "\n",
      "J is the identity matrix, 1 is a matrix of all one’s, and K; is\n",
      "the n x n; matrix with entries (K;),. = K(#,,#,) with x, vary-\n",
      "ing over the observations in group j. Hence, we now find a to\n",
      "maximize r\n",
      "\n",
      "(a) = eee\n",
      "\n",
      "alNa\n",
      "\n",
      "Notice that all the qauntities are expressed in terms of the ker-\n",
      "nel. Formally, the solution is a = N~!(Mgj — M,). However, N\n",
      "might be non-invertible. In this case on replaces N by N + bI\n",
      "form some constant b. Finally, the projection onto the new sub-\n",
      "space can be written as\n",
      "\n",
      " \n",
      "\n",
      "U =u\" (2) = Sak (2, x).\n",
      "i=l\n",
      "\n",
      "The support vector machine can similarly be kernelized. We\n",
      "simply replace (X;,X;) with K(X;,Xj). For example, instead\n",
      "of maximizing (23.40), we now maximize\n",
      "\n",
      "55, - ESS aia ¥¥ek (XX). (23.43)\n",
      "i=1\n",
      "\n",
      "i=l k=1\n",
      "\n",
      "The hyperplane can be written as A(x) = Got) op @iY,K(X, Xj).\n",
      "\n",
      "23.11 Other Classifiers\n",
      "\n",
      "There are many other classifiers and space precludes a full di-\n",
      "cussion of all of them. Let us briefly mention a few.\n",
      "\n",
      "The k-nearest-neighbors classifier is very simple. Given a\n",
      "point x, find the k data points closest to #. Classify x using the\n",
      "majority vote of these k neighbors. Ties can be broke randomly.\n",
      "The parameter k can be chosen by cross-validation.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-465.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "23.11 Other Classifiers 467\n",
      "\n",
      "Bagging is a method for reducing the variability of a clas-\n",
      "sifier. It is most helpful for highly non-linear classifiers such as\n",
      "a tree. We draw B boostrap samples from the data. The b'\n",
      "bootstrap sample yields a classifier h,. The final classifier is\n",
      "\n",
      "> 1 if 402 A(z) >4\n",
      "= B 2ub=1 2\n",
      "H(z) { 0 otherwise.\n",
      "\n",
      "Boosting is a method for starting with a simple classifier\n",
      "and gradually improving it by refitting the data giving higher\n",
      "weight to misclassified samples. Suppose that is a collection of\n",
      "classifiers, for example, tress with onely one split. Assume that\n",
      "Y; € {1,1} and that each h is such that h(x) € {—1,1}. We\n",
      "usually give equal weight to all data points in the methods we\n",
      "have discussed. But one can incorporate unequal weights quite\n",
      "easily in most algorithms. For example, in constructing a tree,\n",
      "we could replace the impurity measure with a weighted impurity\n",
      "measure. The original version of bossting, called AdaBoost, is\n",
      "as follows.\n",
      "\n",
      "1. Set the weights w;=1/n,i=1,...,n.\n",
      "2. For j =1,...,J, do the following steps:\n",
      "\n",
      "(a) Constructing a classifier h; from the data using the\n",
      "weights w1,...,Wn-\n",
      "\n",
      "(b) Compute the weighted error estimate:\n",
      "\n",
      "7 Dw Vi A hl XD)\n",
      "\n",
      "‘ Die Wi :\n",
      "\n",
      "(c) Let aj =log((1 — Z,)/LZ,;).\n",
      "(d) Update the weights:\n",
      "\n",
      "wi wet T(¥i#h; (Xi)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-466.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "468 23. Classification\n",
      "\n",
      "3. The final classifier is\n",
      ". J\n",
      "h(x) = sign(S> ayhj(a)).\n",
      "j=l\n",
      "\n",
      "There is now an enormous literature trying to explain and\n",
      "improve on boosting. Whereas bagging is a variance reduction\n",
      "technique, boosting can be thought of as a bias reduction tech-\n",
      "nique. We starting with a simple — and hence highly biased —\n",
      "classifier, and we gradually reduce the bias. The disadvantage\n",
      "of boosting is that the final classifier is quite complicated.\n",
      "\n",
      "23.12 Bibliographic Remarks\n",
      "\n",
      "An excellent reference on classification is Hastie, Tibshirani and\n",
      "Friedman (2001). For more on the theory, see Devroye, Gyérfi,\n",
      "Lugosi (1996) and Vapnik (2001).\n",
      "23.13 Exercises\n",
      "\n",
      "1. Prove Theorem 23.5.\n",
      "\n",
      "2. Prove Theorem 23.7.\n",
      "\n",
      "3. Download the spam data from:\n",
      "http://www-stat.stanford.edu/~tibs/ElemS tat Learn/index.html\n",
      "\n",
      "The data file can also be found on the course web page.\n",
      "The data contain 57 covariates relating to email messages.\n",
      "Each email message was classified as spam (Y=1) or not\n",
      "spam (Y=0). The outcome Y is the last column in the file.\n",
      "The goal is to predict whether an email is spam or not.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-467.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "23.13 Exercises 469\n",
      "\n",
      "(a) Construct classification rules using (i) LDA, (ii) QDA,\n",
      "(ili) logistic regression and (iv) a classification tree. For\n",
      "each, report the observed misclassification error rate and\n",
      "construct a 2-by-2 table of the form\n",
      "\n",
      " \n",
      "\n",
      "Hint: Here are some things to remember when using R.\n",
      "\n",
      "(i) To fit a logistic regression and get the classified values:\n",
      "\n",
      "d <- data.frame (x,y)\n",
      "\n",
      "n <- nrow(x)\n",
      "\n",
      "out <- glm(y ~ .,family=binomial ,data=d)\n",
      "p <- out$fitted.values\n",
      "\n",
      "pred <- rep(0,n)\n",
      "\n",
      "pred[p > .5] <- 1\n",
      "\n",
      "table(y, pred)\n",
      "\n",
      "(ii) To build a tree you must turn y into a factor and you\n",
      "must do it before you make the data frame:\n",
      "\n",
      "y <- as.factor(y)\n",
      "\n",
      "d <- data.frame (x,y)\n",
      "\n",
      "library (tree)\n",
      "\n",
      "out <- tree(y ~ .,data=d)\n",
      "\n",
      "print (summary (out) )\n",
      "\n",
      "plot (out ,type=\"u\", lwd=3)\n",
      "\n",
      "text (out)\n",
      "\n",
      "pred <- predict(out,d,type=\"class\")\n",
      "table(y, pred)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-468.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470\n",
      "\n",
      "23. Classification\n",
      "\n",
      "(b) Use 5-fold cross-validation to estimate the prediction\n",
      "accuracy of LDA and logistic regression.\n",
      "\n",
      "(c) Sometimes it helps to reduce the number of covariates.\n",
      "One strategy is to compare X; for the spam and email\n",
      "group.. For each of the 57 covariates, test whether the\n",
      "mean of the covriate is the same or different between the\n",
      "two groups. Keep the 10 covariates with the smallest p-\n",
      "values. Try LDA and logstic regression using only these 10\n",
      "variables.\n",
      "\n",
      ". Let A be the set of two-dimensional spheres. That is, A €\n",
      "\n",
      "AifA = {(2,y) + (w—a)?+(y—b) < c?} for some a, b,c.\n",
      "Find the VC dimension of A.\n",
      "\n",
      ". Classify the spam data using support vector machines.\n",
      "\n",
      "Free software for the support vector machine is at\n",
      "http://svmlight.joachims.org/\n",
      "\n",
      "It is very easy to use. After installing, do the following.\n",
      "First, you must recode the Y;’s as +1 and -1. Then type\n",
      "\n",
      "svm_learn data output\n",
      "\n",
      "where “data” is the file with the data. It expects the data\n",
      "in the following form:\n",
      "\n",
      "fi 1:3.4 2:.17 3:129 etc\n",
      "\n",
      "where the first number is Y (+1 or -1) and the rest of the\n",
      "line means: Xj is 3.4, X» is .17, etc.\n",
      "\n",
      "If the file “test” contains test data, then type\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-469.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "10.\n",
      "\n",
      "23.13 Exercises 471\n",
      "\n",
      "svm_classify test output predictions\n",
      "\n",
      "to get the predictions on the test data. There are many\n",
      "options available, as explained on the web site.\n",
      "\n",
      ". Use VC theory to get a confidence interval on the true\n",
      "\n",
      "error rate of the LDA classifier in question 3. Suppose\n",
      "that the covariate X = (X,,...,X 4) has d dimensions.\n",
      "Assume that each variable has a continuous distribution.\n",
      "\n",
      ". Suppose that X; € R and that Y; = 1 whenever |X;| <\n",
      "\n",
      "1 and Y; = 0 whenever |X;| > 1. Show that no linear\n",
      "classifier can perfectly classify these data. Show that the\n",
      "kernelized data Z; = (Xj, X?) can be linearly separated.\n",
      "\n",
      ". Repeat question 5 using the kernel K(x, %) = (1+ 27%)’.\n",
      "\n",
      "Choose p by cross-validation.\n",
      "\n",
      ". Apply the k nearest neighbors classifier to the “iris data.”\n",
      "\n",
      "Get the data in R from the command data(iris). Choose\n",
      "k by cross-validation.\n",
      "\n",
      "(Curse of Dimensionality.) Suppose that X has a uniform\n",
      "distribution on the d-dimensional cube [—1/2, 1/2]7. Let\n",
      "R be the distance from the origin to the closest neighbor.\n",
      "Show that the median of R is\n",
      "\n",
      "(-~@'”)\\\"\"\n",
      "\n",
      "va(l)\n",
      "\n",
      "where\n",
      "\n",
      "a/2\n",
      "va(r) =r\n",
      "\n",
      "T((d/2) + 1)\n",
      "is the volume of a sphere of radius r. When does R exceed\n",
      "the edge of the cube when n = 100, 1000, 10000?\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-470.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "A72\n",
      "\n",
      "11.\n",
      "\n",
      "13.\n",
      "\n",
      "23. Classification\n",
      "\n",
      "Fit a tree to the data in question 3. Now apply bagging\n",
      "and report your results.\n",
      "\n",
      ". Fit a tree that uses only one split on one variable to the\n",
      "\n",
      "data in question 3. Now apply boosting.\n",
      "Let r(x) = P(Y = 1|X = 2) and let 7(x) be an estimate\n",
      "\n",
      "of r(x). Consider the classifier\n",
      "\n",
      "ate) ={ 1 if F(z) > 1/2\n",
      "\n",
      "0 otherwise.\n",
      "\n",
      "Assume that 7(2) & N(F(«),0?(x)) for some functions\n",
      "T(z) and o?(x)). Show that\n",
      "\n",
      "abr is eam sign ( (Fla) — (1/2))(r(2) - (1/2)))\n",
      "\n",
      " \n",
      "\n",
      "a(x)\n",
      "\n",
      "where © is the standard Normal cdf. Regard sign (( (x) —\n",
      "(1/2)) (F(x) — (1/2))) as a type of bias term. Explain the\n",
      "\n",
      "implications for the bias-variance trade-off in classification\n",
      "(Friedman, 1997).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-471.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "24\n",
      "\n",
      "Stochastic Processes\n",
      "\n",
      "24.1 Introduction\n",
      "\n",
      "Most of this book has focused on IID sequences of random vari-\n",
      "ables. Now we consider sequences of dependent random vari-\n",
      "ables. For example, daily temperatures will form a sequence of\n",
      "time-ordered random variables and clearly the temperature on\n",
      "one day is not independent of the temperature on the previous\n",
      "day.\n",
      "\n",
      "A stochastic process {X;: t € T} is a collection of ran-\n",
      "dom variables. We shall sometimes write X(t) instead of X;.\n",
      "The variables X; take values in some set 4 called the state\n",
      "space. The set T is called the index set and for our pur-\n",
      "poses can be thought of as time. The index set can be discrete\n",
      "T = {0,1,2,\n",
      "application.\n",
      "\n",
      "..} or continuous T = [0,0o) depending on the\n",
      "\n",
      " \n",
      "\n",
      "Example 24.1 (11D observations.) A sequence of 11D random vari-\n",
      "ables can be written as {X,: t € T} where T = {1,2,3,..., }.\n",
      "\n",
      "This is page 473\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-472.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "AT4 24. Stochastic Processes\n",
      "\n",
      "Thus, a sequence of tid random variables is an example of a\n",
      "stochastic process.\n",
      "\n",
      "Example 24.2 (The Weather.) Let Y = {sunny, cloudy}. A typi-\n",
      "cal sequence (depending on where you live) might be\n",
      "\n",
      "sunny, sunny, cloudy, sunny, cloudy, cloudy, ---\n",
      "\n",
      "This process has a discrete state space and a discrete index set.\n",
      "a\n",
      "\n",
      "Example 24.3 (Stock Prices.) Figure 24.1 shows the price of a\n",
      "stock over time. The price is monitored continuously so the index\n",
      "set T is not discrete. Price is discrete but, for practical purposes\n",
      "we can imagine treating it as a continuous variable.\n",
      "\n",
      "Example 24.4 (Empirical Distribution Function.) Let X),...,X, ~\n",
      "F where F is some cpr on [0,1]. Let\n",
      "\n",
      "A) =A SK <9)\n",
      "\n",
      "be the empirical CDF . For any fixed value t, F(t) is a random\n",
      "variable. But the whole empirical CDF\n",
      "\n",
      "{Falt): te (o,1)}\n",
      "\n",
      "is a stochastic process with a continuous state space and a con-\n",
      "tinuous index set. Wi\n",
      "\n",
      "We end this section by recalling a basic fact. If X,,...,X,\n",
      "are random variables then we can write the joint density as\n",
      "\n",
      "F(t1,---5%n) = f(ei)f (v2)ai) +++ f(@alei,---, tn)\n",
      "\n",
      "n\n",
      "\n",
      "[[/(eslpast,) (24.1)\n",
      "\n",
      "ist\n",
      "\n",
      "where past; refers to all the variables before X;.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-473.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "oud\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-474.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "A476 24. Stochastic Processes\n",
      "\n",
      "24.2 Markov Chains\n",
      "\n",
      "The simplest stochastic process is a Markov chain in which the\n",
      "distribution of X, depends only on X;,_;. In this section we as-\n",
      "sume that the state space is discrete, either Y = {1,...,N}\n",
      "or X = {1,2,...,} and that the index set is T = {0,1,2,...}.\n",
      "Typically, most authors write X,, instead of X, when discussing\n",
      "Markov chains and I will do so as well.\n",
      "\n",
      " \n",
      "\n",
      "Definition 24.5 The process {X,: n € T} is a Markov\n",
      "chain if\n",
      "\n",
      "P(X, = 2 | Xo,---, Xn) =P(Xp =| Xp) (24.2)\n",
      "\n",
      "for din and for allxe Xx.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "For a Markov chain, equation (24.1) simplifes to\n",
      "\n",
      "F(t1, +++, Un) = f (a1) f (v2|21)f (ws|22) +++ f(@nlen-1)-\n",
      "\n",
      "A Markov chain can be represented by the following DAG:\n",
      "\n",
      "XxX, —— XY) —— Ke so TX, Oe\n",
      "\n",
      "Each variable has a single parent, namely, the previous obser-\n",
      "vation.\n",
      "\n",
      "The theory of Markov chains is a very rich and complex. We\n",
      "have to get through many definitions before we can do anything\n",
      "interesting. Our goal is to answer the following questions:\n",
      "\n",
      "1. When does a Markov chain “settle down” into some sort\n",
      "of equilibrium?\n",
      "\n",
      "2. How do we estimate the parameters of a Markov chain?\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-475.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "24.2 Markov Chains ATT\n",
      "\n",
      "3. How can we construct Markov chains that converge to a\n",
      "given equilibrium distribution and why would we want to\n",
      "do that?\n",
      "\n",
      "We will answer questions 1 and 2 in this chapter. We will\n",
      "answer question 3 in the next chapter. To understand question\n",
      "1, look at the two chains in Figure 24.2. The first chain oscillates\n",
      "all over the place and will continue to do so forever. The second\n",
      "chain eventually settles into an equilibrium. If we constructed a\n",
      "histogram of the first process, it would keep changing as we got\n",
      "more and more observations. But a histogram from the second\n",
      "chain would eventually converge to some fixed distribution.\n",
      "\n",
      "TRANSITION PROBABILITIES. The key quantities of a Markov\n",
      "chain are the probabilities of jumping from one state into an-\n",
      "other state.\n",
      "\n",
      " \n",
      "\n",
      "Definition 24.6 We call\n",
      "P(Xn4 = J|Xn = 4) (24.3)\n",
      "\n",
      "the transition probabilities. If the transition probabil-\n",
      "ities do not change with time, we say the chain is homo-\n",
      "geneous. In this case we define pij = P(Xnyi = j|Xn =\n",
      "i). The matria P whose (i,j) element is pj; is called the\n",
      "transition matrix.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "We will only consider homogeneous chains. Notice that P has\n",
      "two properties: (i) pj; > 0 and (ii) 0, p;; = 1. Each row is\n",
      "\n",
      "a probability mass function. A matrix with these properties is\n",
      "\n",
      "3\n",
      "\n",
      " \n",
      "\n",
      "called a stochastic matrix.\n",
      "\n",
      "Example 24.7 (Random Walk With Absorbing Barriers.) Let ¥ =\n",
      "{1,...,N}. Suppose you are standing at one of these points.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-476.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478\n",
      "\n",
      "°\n",
      "\n",
      "-100 50\n",
      "L\n",
      "\n",
      "150\n",
      "L\n",
      "\n",
      "-2 0 2 4 6 8 10\n",
      "L pop\n",
      "\n",
      "24. Stochastic Processes\n",
      "\n",
      " \n",
      "\n",
      "1 \\ Ny\n",
      "We Wad ) Me\n",
      "\n",
      "Mn Nh\n",
      "Mya ‘nh\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "0 200 400 600 800 1000\n",
      "|\n",
      "4\n",
      "\\\n",
      "srw sald rd AA\n",
      "} 200 400 600 800 1000\n",
      "\n",
      "FIGURE 24.2. The first chain does not settle down into\n",
      "an equilibrium. The second does.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-477.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "24.2 Markov Chains 479\n",
      "\n",
      "Flip a coin with P(Heads) = p and P(Tails) = gq = 1 —p. If\n",
      "it is heads, take one step to the right. If it is tails, take one\n",
      "step to the left. If you hit one of the endpoints, stay there. The\n",
      "transition matrix is\n",
      "\n",
      "1 0 0 0 = 0 0\n",
      "\n",
      "q O p O +: 0 0\n",
      "pa[0 0 0 Pot) |e\n",
      "\n",
      "0 0 0 0 q O p\n",
      "\n",
      "0 0 0 0 0 0 1\n",
      "\n",
      "Example 24.8 Suppose the state space is ¥ = {sunny, cloudy}.\n",
      "Then X,,Xo,... represents the weather for a sequence of days.\n",
      "The weather today clearly depends on yesterday’s weather. It\n",
      "might also depend on the weather two days ago but as a first\n",
      "approximation we might assume that the dependence is only one\n",
      "day back. In that case the weather is a Markov chain and a\n",
      "typical transition matrix might be\n",
      "Sunny Cloudy\n",
      "\n",
      "Sunny 0.4 0.6\n",
      "\n",
      "Cloudy 0.8 0.2\n",
      "For example, if it is sunny today, there is a 60 per cent chance\n",
      "it will be cloudy tommorrow. @\n",
      "\n",
      "Let.\n",
      "pil) = P(Xmin = j|Xm = 1) (24.4)\n",
      "be the probability of of going from state i to state j in n steps.\n",
      "\n",
      "Let P,, be the matrix whose (i,j) element is p;;(n). These are\n",
      "called the n-step transition probabilities.\n",
      "\n",
      "Theorem 24.9 (The Chapman-Kolmogorov equations.) The\n",
      "n-step probabilities satisfy\n",
      "\n",
      "pig(m +n) = Spin (m)peg(n). (24.5)\n",
      "k\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-478.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "480 24. Stochastic Processes\n",
      "\n",
      "ProoF. Recall that, in general,\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "P(X =2,Y =y) =P(X =2)P(Y =y|X =2).\n",
      "This fact is true in the more general form\n",
      "\n",
      "P(X =2,Y =y|Z =z) = P(X =2|Z =z)P(Y =y|[X = 2,2\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Also, recall the law of total probability:\n",
      "\n",
      "P(X =2)=) P(X =2,Y =y).\n",
      "\n",
      "Using these facts and the Markov property we have\n",
      "\n",
      "py(m+n) = P(Xnan = j|Xo = 4)\n",
      "SOP(Xmin = 5, Xm = kl Xo =i)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "k\n",
      "= SOP Xin = j|Xm =k, Xo = t)P(Xm = k|Xo\n",
      "k\n",
      "\n",
      " \n",
      "\n",
      "= SOP Xin = j|Xm = k)P(Xm = k|Xo = 4)\n",
      "\n",
      " \n",
      "\n",
      "k\n",
      "= Vpilm)pg(n).\n",
      "5\n",
      "\n",
      "Look closely at equation (24.5). This is nothing more than the\n",
      "equation for matrix multiplication. Hence we have shown that\n",
      "\n",
      "Prin = PmPn. (24.6)\n",
      "\n",
      "By definition, P| = P. Using the above theorem, Py = P\\,) =\n",
      "P,P, = PP = P”. Continuing this way, we see that\n",
      "\n",
      "P,=P\"= PxPx--xP . (24.7)\n",
      "EEA\n",
      "\n",
      "multiply the matrix n times\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-479.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "24.2 Markov Chains 481\n",
      "\n",
      "Let fin = (Hn(1),---;Hn(N)) be a row vector where\n",
      "\n",
      "Mn(i) = P(Xn = 4) (24.8)\n",
      "\n",
      "is the marginal probability that the chain is in state 7 at time\n",
      "n. In particular, ji is called the initial distribution. To sim-\n",
      "ulate a Markov chain, all you need to know is ji9 and P. The\n",
      "simulation would look like this:\n",
      "\n",
      "Step 1: Draw Xp ~ pip. Thus, P(Xo = i) = puo(i).\n",
      "Step 2: Suppose the outcome of step 1 is 7. Draw X, ~ P. In\n",
      "other words, P(X, = j|Xo = 1) = pi.\n",
      "Step 3: Suppose the outcome of step 2 is 7. Draw X) ~ P. In\n",
      "other words, P(X2 = k|X, = j) = pjp-\n",
      "\n",
      "And so on.\n",
      "\n",
      "It might be difficult to understand the meaning of j1,. Imag-\n",
      "ing simulating the chain many times. Collect all the outcomes at\n",
      "time n from all the chains. This histogram would look approxi-\n",
      "mately like j1,. A consequence of theorem 24.9 is the following.\n",
      "\n",
      "Lemma 24.10 The marginal probabilities are given by\n",
      "Ln = [oP”.\n",
      "PROOF.\n",
      "\n",
      "tn(j) = P(Xn=3)\n",
      "SEP(X, = i|Xo = P(X = 4)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "= SS yo(A)pij(n) = oP\".\n",
      "\n",
      "i\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-480.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "482 24. Stochastic Processes\n",
      "\n",
      " \n",
      "\n",
      "Summary\n",
      "1. Transition matrix: P(i, j) = P(Xn41 = j|Xn =).\n",
      "2. n-step matrix: P,(7, 7) = P(Xnim = j|Xm = 1).\n",
      "3. P, =P\".\n",
      "4. Marginal: y2,,(i) = P(X, = 4).\n",
      "\n",
      "5. [ln = [loP”.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "STATES. The states of a Markov chain can be classified ac-\n",
      "cording to various properties.\n",
      "\n",
      " \n",
      "\n",
      "Definition 24.11 We say that i reaches j (or j is ac-\n",
      "cessible from i) if pi;(n) > 0 for some n, and we write\n",
      "ij. Ifi- j andj >i then we write i + j and we\n",
      "say that i and j communicate.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 24.12 The communication relation satisfies the fol-\n",
      "lowing properties:\n",
      "\n",
      "Ligei.\n",
      "2. Ifises then j oi.\n",
      "8 Iftejandj ok theniok.\n",
      "\n",
      "4. The set of states X can be written as a disjoint union\n",
      "of classes X = XJ 42U-:: where two states i and j\n",
      "communicate with each other if and only if they are in the\n",
      "same class.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-481.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "24.2 Markov Chains 483\n",
      "\n",
      "If all states communicate with each other then the chain is\n",
      "called irreducible. A set of states is closed if, once you enter\n",
      "that set of states you never leave. A closed set consisting of a\n",
      "single state is called an absorbing state.\n",
      "\n",
      "Example 24.13 Let ¥ = {1,2,3,4} and\n",
      "\n",
      "+200\n",
      "2 2\n",
      "2100\n",
      "3 3\n",
      "P=)iiia\n",
      "464 4 4\n",
      "0001\n",
      "\n",
      "The classes are {1,2}, {3} and {4}. State 4 is an absorbing state.\n",
      "a\n",
      "\n",
      "Suppose we start a chain in state 7. Will the chain ever return\n",
      "to state 7? If so, that state is called persistent or recurrent.\n",
      "\n",
      " \n",
      "\n",
      "Definition 24.14 State i is recurrent or persistent if\n",
      "P(X, =i for some n > 1| Xp =i) =1.\n",
      "\n",
      "Otherwise, state i is transient.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Theorem 24.15 A state i is recurrent if and only if\n",
      "\n",
      "So pii(n) = cv. (24.9)\n",
      "\n",
      "n\n",
      "\n",
      "A state i is transient if and only if\n",
      "Spin) < 00. (24.10)\n",
      "n\n",
      "Proor. Define\n",
      "\n",
      "1={ 1 if X,=7\n",
      "\n",
      "0 if X, 4i.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-482.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "484 24. Stochastic Processes\n",
      "\n",
      "The number of times that the chain is in state iis Y = Soni Te.\n",
      "The mean of Y, given that the chain starts in state i, is\n",
      "\n",
      "E(Y|Xo =i) = SOEU|Xo = 4)\n",
      "\n",
      "oo\n",
      "SOP(Xn = 1X0 = 4)\n",
      "n=0\n",
      "\n",
      "Se pis(n).\n",
      "\n",
      "Define a; = P(X,, =i for some n > 1| Xo = i). If i is recurrent,\n",
      "a; = 1. Thus, the chain will eventually return to i. Once it\n",
      "does return to i, we argue again that since a; = 1, the chain will\n",
      "\n",
      "return to state i again. By repeating this argument, we conclude\n",
      "that E(Y|Xo = 7%) = oo. If é is transient, then a; < 1. When the\n",
      "chain is in state 7, there is a probability 1 — a; > 0 that it will\n",
      "never return to state 7. Thus, the probability that the chain is\n",
      "in state 7 exactly n times is a?~'(1— aj). This is a geometric\n",
      "distribution which has finite mean. ll\n",
      "\n",
      "Theorem 24.16 Facts about recurrence.\n",
      "1. If state i is recurrent andi <j then j is recurrent.\n",
      "2. If state i is transient andi <j then j is transient.\n",
      "\n",
      "3. A finite Markov chain must have at least one recurrent\n",
      "state.\n",
      "\n",
      "4. The states of a finite, irreducible Markov chain are all\n",
      "recurrent.\n",
      "\n",
      "Theorem 24.17 (Decomposition Theorem.) The state space\n",
      "&X can be written as the disjoint union\n",
      "\n",
      "X= arJuUa--:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-483.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "24.2 Markov Chains 485\n",
      "\n",
      "where Xr are the transient states and each X; is a closed, irre-\n",
      "ducible set of recurrent states.\n",
      "\n",
      "CONVERGENCE OF MARKOV CHAINS. To discuss the convre-\n",
      "gence of chains, we need a few more definitions. Suppose that\n",
      "Xo =i. Define the recurrence time\n",
      "\n",
      "Tj =min{n >0: X,=j} (24.11)\n",
      "\n",
      "assuming X,, ever returns to state 7, otherwise define Tj; = oo.\n",
      "The mean recurrence time of a recurrent state 7 is\n",
      "\n",
      "m; = E(T,) = > nfii(n) (24.12)\n",
      "\n",
      "n\n",
      "\n",
      "where\n",
      "\n",
      " \n",
      "\n",
      "Fig(n) = P(X. #5, Xo F Gy. 025 Xn-1 FH Xn = 51 X0 = 7).\n",
      "\n",
      " \n",
      "\n",
      "A recurrent state is null if m; = 00 otherwise it is called non-\n",
      "null or positive.\n",
      "\n",
      "Lemma 24.18 If a state is null and recurrent, then p? — 0.\n",
      "\n",
      "Lemma 24.19 In a finite state Markov chain, all recurrent states\n",
      "are positive.\n",
      "\n",
      "Consider a three state chain with transition matrix\n",
      "010\n",
      "001\n",
      "100\n",
      "\n",
      "Suppose we start the chain in state 1. Then we will be in state\n",
      "3 at times 3, 6, 9, .... This is an example of a periodic chain.\n",
      "Formally, the period of state i is d if p;;(n) = 0 whenever n is\n",
      "not divisible by d and d is the largest integer with this property.\n",
      "Thus, d = ged{n : p,i(n) > 0} where gcd means “greater\n",
      "common divisor.” State 7 is periodic if d(i) > 1 and aperiodic\n",
      "if d(i) = 1. A state with period 1 is called aperiodic.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-484.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "486 24. Stochastic Processes\n",
      "\n",
      "Lemma 24.20 If state i has period d andi <+ j then j has period\n",
      "d.\n",
      "\n",
      " \n",
      "\n",
      "Definition 24.21 A state is ergodic if it is recurrent, non-\n",
      "null and aperiodic. A chain is ergodic if all its states are\n",
      "ergodic.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Let 7 = (7; : i € X) be a vector of non-negative numbers\n",
      "that sum to one. Thus 7 can be thought of as a probability mass\n",
      "function.\n",
      "\n",
      " \n",
      "\n",
      "Definition 24.22 We say that x is a stationary (or in-\n",
      "variant) distribution if 7 = 7P.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Here is the intuition. Draw Xp from distribution 7 and sup-\n",
      "pose that 7 is a stationary distribution. Now draw X, accord-\n",
      "ing to the transition probability of the chain. The distribition\n",
      "of X, is then py = foP = 7P = 7. The distribution of X9 is\n",
      "TP? = (nP)P=7 7. Continuing this way, we see that the\n",
      "distribution of X,, is TP” = 7. In other words:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "If at any time the chain has distribution 7 then it will\n",
      "continue to have distribution 7 forever.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-485.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "24.2 Markov Chains\n",
      "\n",
      "487\n",
      "\n",
      " \n",
      "\n",
      "Definition 24.23 We say that a chain has limiting dis-\n",
      "tribution if\n",
      "\n",
      "for some m. In other words, 1 = limp oo Pi exists and\n",
      "is independent of i.\n",
      "\n",
      " \n",
      "\n",
      "Here is the main theorem.\n",
      "\n",
      " \n",
      "\n",
      "Theorem 24.24 An irreducible, ergodic Markov chain has\n",
      "aunique stationary distribution x. The limiting distrubu-\n",
      "tion exists and is equal to 7. If g is any bounded function,\n",
      "then, with probability 1,\n",
      "\n",
      "N\n",
      "lim ~S29(Xn) + Ex(9) = Sooli)my. (24.13)\n",
      "\n",
      "Noo N\n",
      "j\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The last statement of the theorem, is the law of large numb\n",
      "\n",
      "for Markoy chains. It says that sample averages converge to their\n",
      "\n",
      "ers\n",
      "\n",
      " \n",
      "\n",
      "expectations. Finally, there is a special condition which will be\n",
      "useful later. We say that 7 satisfies detailed balance if\n",
      "TiDij = DjiTj- (24.14)\n",
      "\n",
      "Detailed balance guarantees that 7 is a stationary distribution.\n",
      "\n",
      "Theorem 24.25 If m satisfies detailed balance then 7 is a si\n",
      "tionary distribution.\n",
      "\n",
      "Proor. We need to show that tP = 7. The j'® element\n",
      "TP is 00; Mipig = LO, MDH = 7} DG = Tj.\n",
      "\n",
      "ta-\n",
      "\n",
      "of\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-486.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "488 24. Stochastic Processes\n",
      "\n",
      "The importance of detailed balance will become clear when\n",
      "we discuss Markov chain Monte Carlo methods.\n",
      "\n",
      "Warning! Just because a chain has a stationary distribution\n",
      "does not mean it converges.\n",
      "\n",
      "Example 24.26 Let\n",
      "010\n",
      "P=|001\n",
      "100\n",
      "\n",
      "Let « = (1/3,1/3,1/3). Then tP = 7 so m is a stationary\n",
      "distribution. If the chain is started with the distribution 7 it will\n",
      "stay in that distribution. Imagine simulating many chains and\n",
      "checking the marginal distribution at each time n. It will always\n",
      "be the uniform distribution 7. But this chain does not have a\n",
      "limit. It continues to cycle around forever.\n",
      "\n",
      "EXAMPLES OF MARKOV CHAINS\n",
      "\n",
      "Example 24.27 (Random Walk) . Let ¥ = {...,—2,—1,0,1,2,...\n",
      "\n",
      "and suppose that piiz1 =P, Pii-1 =q = 1—p. All states com-\n",
      "municate hence either all the states are recurrent or all are tran-\n",
      "sient. To see which, suppose we start at Xo =0. Note that\n",
      "\n",
      "2n\n",
      "Poo(2n) = ( h par (24.15)\n",
      "since, the only way to get back to 0 is to have n heads (steps\n",
      "\n",
      "to the right) and n tails (steps to the left). We can approximate\n",
      "this expression using Stirling’s formula which says that\n",
      "\n",
      "nl wn J/ne\"V2n.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-487.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "24.2 Markov Chains 489\n",
      "\n",
      "Inserting this approximation into (24.15) shows that\n",
      "\n",
      "(4pq)\"\n",
      "2n)~ :\n",
      "Poo (2n) Vint\n",
      "It is easy to check that >, poo(n) < 00 if and only if >, poo(2n) <\n",
      "oo. Moreover, 53, poo(2n) = co if and only if p= q = 1/2. By\n",
      "Theorem (24.15), the chain is recurrent if p = 1/2 otherwise it\n",
      "is transient. 1\n",
      "\n",
      " \n",
      "\n",
      "Example 24.28 Let ¥ = {1,2,3,4,5,6}. Let\n",
      "\n",
      "1\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "+ 40000\n",
      "+20000\n",
      "eu|rea res\n",
      "“fig iigt\n",
      "1 4 4 4\n",
      "000034\n",
      "loooo048\n",
      "\n",
      "Then C, = {1,2} and Cz = {5,6} are irreducible closed sets.\n",
      "States 8 and 4 are transient because the of the path 3 +4 —>6\n",
      "and once you hit state 6 you cannot return to 3 or 4. Since\n",
      "pii(1) > 0, all the states are aperiodic. In summary, 3 and 4 are\n",
      "transient while 1,2,5 and 6 are ergodic.\n",
      "\n",
      "Example 24.29 (Hardy-Weinberg.) Here is a famous example from\n",
      "genetics. Suppose a gene can be type A or type a. There are three\n",
      "types of people (called genotypes): AA, Aa and aa. Let (p,q,7)\n",
      "denote the fraction of people of each genotype. We assume that\n",
      "everyone contributes one of their two copies of the gene at ran-\n",
      "dom to their children. We also assume that mates are selected\n",
      "at random. The latter is not realistic however, it is often rea-\n",
      "sonable to assume that you do not choose your mate based on\n",
      "whether they are AA, Aa or aa. (This would be false if the gene\n",
      "was for eye color and if people chose mates based on eye color.)\n",
      "Imagine if we pooled eveyone’s genes together. The proportion\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-488.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "490 24. Stochastic Processes\n",
      "\n",
      "of A genes is P = p+ (q/2) and the proportion of a genes is\n",
      "Q=1r+(q/2). A child is AA with probability P?, aA with prob-\n",
      "ability 2PQ and aa with probability Q?. Thus, the fraction of A\n",
      "genes in this generation is\n",
      "2\n",
      "\n",
      "P?+PQ= (p+ 4) + (p+ 4) (r+2).\n",
      "However, r =1—p—q. Substitute this in the above equation\n",
      "and you get P? + PQ = P. A similar calculation shows that\n",
      "fraction of a genes is Q. We have shown that the proportion of\n",
      "type A and type ais P and Q and this remains stable after the\n",
      "first generation. The proportion of people of type AA, Aa, aa is\n",
      "thus (P?,2PQ,Q?) from the second generation and on. This is\n",
      "called the Hardy-Weinberg law.\n",
      "\n",
      "Assume everyone has exactly one child. Now consider a fixed\n",
      "person and let Xp be the genotype of their n'\" descendent. This\n",
      "is a Markov chain with state space X = {AA, Aa,aa}. Some\n",
      "basic calculations will show you that the transition matria is\n",
      "\n",
      "PQ 0\n",
      "Pk PQ Q\n",
      "2 2 2\n",
      "0 P Q\n",
      "\n",
      "The stationary distribution is t = (P?,2PQ, Q?).\n",
      "\n",
      "INFERENCE FOR MARKOV CHAINS. Consider a chain with finite\n",
      "state space V = {1,2,...,.N}. Suppose we observe n observa-\n",
      "tions X,,...,X, from this chain. The unknown parameters of a\n",
      "Markov chain are the initial probabilities fig = (Ji9(1), 0(2),---,)\n",
      "and the elements of the transition matrix P. Each row of P is\n",
      "a multinomial distribution. So we are essentially estimating Nv\n",
      "distributions (plus the initial probabilities). Let n,; be the ob-\n",
      "served number of transitions from state 7 to state 7. The likeli-\n",
      "hood function is\n",
      "NON\n",
      "\n",
      "L(10, P) = Ho(20) [[ px... = Lo(xo) Il TL.\n",
      "ral\n",
      "\n",
      "i=1 j=1\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-489.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "24.3 Poisson Processes 491\n",
      "\n",
      "There is only one observation on [49 so we can’t estimate that.\n",
      "Rather, we focus on estimating P. The MLE is obtained by max-\n",
      "imizing £(j, P) subject to the constraint that the elements are\n",
      "non-negative and the rows sum to 1. The solution is\n",
      "\n",
      "N .\n",
      "where n; = pa nij- Here we are assuming that n; > 0. If note,\n",
      "then we set pj; = 0 by convention.\n",
      "\n",
      "Theorem 24.30 (Consistency and Asymptotic Normality of the MLE .)\n",
      "Assume that the chain is ergodic. Let p,;(n) denote the mle after\n",
      "\n",
      "n observations. Then Bij(n)—> piy- Also,\n",
      "\n",
      "[VN@(By — Ps)] ~ NOE)\n",
      "\n",
      "where the left hand side is a matrix, N;(n) = )>?_, 1(X, = i)\n",
      "\n",
      "rai\n",
      "and\n",
      "Pyll Py) (1) = (9\n",
      "Lij,ne = 1 —PigDie i=k,j Fel\n",
      "0 otherwise.\n",
      "\n",
      "24.3 Poisson Processes\n",
      "\n",
      "One of the most studied and useful stochastic processes is the\n",
      "Poisson process. It arises when we count occurences of events\n",
      "over time, for example, traffic accidents, radioactive decay, ar-\n",
      "rival of email messages etc. As the name suggests, the Poisson\n",
      "process is intimately related to the Poisson distribution. Let’s\n",
      "first review the Poisson distribution.\n",
      "\n",
      "Recall that X has a Poisson distribution with parameter \\ —\n",
      "written X ~ Poisson(A) - if\n",
      "\n",
      "P(X = 2) = p(z;\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-490.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "492 24. Stochastic Processes\n",
      "\n",
      "Also recall that E(X) = \\ and V(X) = A. If X ~ Poisson(A),\n",
      "Y ~ Poisson(v) and X IY, then X +Y ~ Poisson(A + v).\n",
      "Finally, if N ~ Poisson(\\) and Y|N =n ~ Binomial(n, p), then\n",
      "the marginal distribution of Y is Y ~ Poisson(Ap).\n",
      "\n",
      "Now we describe the Poisson process. Imaging you are at your\n",
      "computer. Each time a new email message arrives you record the\n",
      "time. Let X, be the number of messages you have received up\n",
      "to and including time ¢. Then, {X;: t € [0, 00)} is a stochastic\n",
      "process with state space Y = {0,1,2,.\n",
      "\n",
      " \n",
      "\n",
      "}. A process of this form\n",
      "is called a counting process. A Poisson process is a counting\n",
      "process that satisfies certain conditions. In what follows, we will\n",
      "sometimes write X(t) instead of X,. Also, we need the following\n",
      "notation. Write f(h) = o(h) if f(h)/h + 0 as h + 0. This\n",
      "means that f(h) is smaller than h when h is close to 0. For\n",
      "example, h? = o(h).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-491.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.3 Poisson Processes 493\n",
      "\n",
      " \n",
      "\n",
      "Definition 24.31 A Poisson process is a stochastic pro-\n",
      "cess {X,: t € [0,co)} with state space X = {0,1,2,...}\n",
      "such that\n",
      "\n",
      "1. X(0) =0.\n",
      "2. For any 0 =ty < ty < tg <+++< ty, the increments\n",
      "X(t) — X(to), X(tz) —X (ti), -++, X(t) —X baa)\n",
      "are independent.\n",
      "3. There is a function X(t) such that\n",
      "\n",
      "P(X(t+h) —X(t)=1) = A(t)h +o(h) (24.16)\n",
      "P(X(t+h)— X(t) >2) = ofh). (24.17)\n",
      "\n",
      "We call \\(t) the intensity function.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The last condition means that the probability of an event in\n",
      "[t,t + h] is approximately hA(t) while the probability of more\n",
      "than one event is small.\n",
      "\n",
      "Theorem 24.32 If X; is a Poisson process with intensity func-\n",
      "tion A(t), then\n",
      "\n",
      "X(s +t) — X(s) ~ Poisson(m(s + t) — m(s))\n",
      "\n",
      "where 1\n",
      "m(t)= f X(s) ds.\n",
      "0\n",
      "\n",
      "In particular, X(t) ~ Poisson(m(t)). Hence, E(X(t)) = m(t)\n",
      "and V(X (t)) = m(t).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-492.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "494 24. Stochastic Processes\n",
      "\n",
      " \n",
      "\n",
      "Definition 24.33 A Poisson process with intensity func-\n",
      "tion X(t) =X for some > 0 is called a homogeneous\n",
      "Poisson process with rate A. In this case,\n",
      "\n",
      "X(t) ~ Poisson(At).\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Let X(¢) be a homogeneous Poisson process with rate \\. Let\n",
      "W,, be the time at which the n'\" event occurs and set Wo = 0.\n",
      "The random variables Wo, W1,-..., are called waiting times.\n",
      "Let $,, = Wr4i-—Wn- Then So, $),..., are called sojourn times\n",
      "\n",
      " \n",
      "\n",
      "or interarraival times.\n",
      "\n",
      "Theorem 24.34 The sojourn times So,Si,... are UD random\n",
      "variables. Their distribution is exponential with mean 1/2, that\n",
      "is, they have density\n",
      "\n",
      "The waiting time W, ~ Gamma(n, 1/A) i.e. it has density\n",
      "\n",
      "yn—1,—At\n",
      "\n",
      "Flu) = yNwrte\n",
      "Hence, E(W,) = n/X and V(W,) = n/d?.\n",
      "PROOF. First, we have\n",
      "P(S; > t) = P(X(t) =0) =e\n",
      "\n",
      "with shows that the CDF for S; is 1—e~*’. This shows the result\n",
      "for S$). Now,\n",
      "\n",
      "P(S2 > t)S1 =s) P(no events in (s,s + ¢]|Si = s)\n",
      "\n",
      "= P(no events in (s,s+¢]) (increments are independent)\n",
      "\n",
      "= eo,\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-493.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "24.3 Poisson Processes 495\n",
      "\n",
      "Hence, 5S) has an exponential distribution and is independent\n",
      "of S;. The result follows by repeating the argument. The re-\n",
      "sult for W,, follows since a sum of exponentials has a Gamma\n",
      "distribution. I\n",
      "\n",
      "Example 24.35 Figures 24.3 and 24.4 show requests toa WWW\n",
      "server in Calgary.! Assuming that this is a homogeneous Pois-\n",
      "son process, N = X(T) ~ Poisson(AT). The likelihood is\n",
      "\n",
      "L(A) x eX\" (AT)®\n",
      "which is maxmimized at\n",
      "\n",
      "~_ N\n",
      "\n",
      "A= 7 48.0077\n",
      "in units per minute.\n",
      "\n",
      "In the preceding example we might want to check if the as-\n",
      "sumption that the data follow a Poisson process is reasonable.\n",
      "To proceed, let us first note that if a Poisson process is observed\n",
      "on [0,T] then, given N = X(T), the event times W,,...,Wy\n",
      "are a sample from a Uniform(0, T) distribution. To see intutively\n",
      "why this is true, note that the probability of finding an event in\n",
      "a small interval [t,t + h] is proportional to h. But this doesn’t\n",
      "depend on t. Since the probability is constant over t it must be\n",
      "uniform.\n",
      "\n",
      " \n",
      "\n",
      "Figure 24.5 shows a histogram and a some density estimates.\n",
      "The density does not appear to be uniform. To test this formally,\n",
      "let is divide into 4 equal length intervals I), Io, I, I;. Let p; be\n",
      "the probability of a point being in J;. The null hypothesis is that\n",
      "Pi = Po = p3 = py. We can test this hypothesis using either a\n",
      "likelihood ratio test or a x? test. The latter is\n",
      "\n",
      "4\n",
      "\n",
      "O; — E;)?\n",
      "x! )\n",
      "\n",
      "i=l\n",
      "\n",
      " \n",
      "\n",
      "‘See httip://ita.ee.Ibl.gov/html/contrib/Calgary-HTTP.html for more information.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-494.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "496\n",
      "\n",
      "24. Stochastic Processes\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "4 ; : }\n",
      "0 2 4 6\n",
      "time\n",
      "0 200 400 600 800 1000 1200\n",
      "\n",
      "time\n",
      "\n",
      "FIGURE 24.3. Hits on a web server. First plot is first\n",
      "20 events. Second plot is several hours worth of data.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-495.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "x(t)\n",
      "\n",
      "X(t)\n",
      "\n",
      "15 20\n",
      "\n",
      "10\n",
      "\n",
      "600 1000\n",
      "\n",
      "200\n",
      "\n",
      "24.3 Poisson Processes 497\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "24/09 24109 24/09 24/09 24109 24109\n",
      "time (seconds)\n",
      "% T T T T T\n",
      "24/09 24/09 25/09 25109 25/09\n",
      "time (seconds)\n",
      "\n",
      "FIGURE 24.4. X(t) for hits on a web server. First plot\n",
      "is first 20 events. Second plot is several hours worth of\n",
      "data.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-496.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "498 24. Stochastic Processes\n",
      "\n",
      "where O; is the number of observations in J; and E; = n/4 is the\n",
      "expected number under the null. This yields x? = 252 which a\n",
      "p-value of 0. Strong evidence againt the null.\n",
      "\n",
      "24.4 Hidden Markov Models\n",
      "24.5 Bibliographic Remarks\n",
      "\n",
      "This is standard material and there are many good references.\n",
      "\n",
      " \n",
      "\n",
      "In this Chapter, I followed the following references quite closely:\n",
      "Probability and Random Processes by Grimmett and Stirzaker\n",
      "(1982), An Introduction to Stochastic Modeling by Taylor and\n",
      "Karlin (1998), Stochastic Modeling of Scientific Data by Guttorp\n",
      "(1995) and Probability Models for Computer Science by Ross\n",
      "(2002). Some of the exercises are from these texts.\n",
      "\n",
      "24.6 Exercises\n",
      "\n",
      "1. Let Xo, X1,... be a Markov chain with states {0,1,2} and\n",
      "transition matrix\n",
      "\n",
      "0.1 0.2 0.7\n",
      "P=] 0.9 01 0.0\n",
      "0.1 0.8 0.1\n",
      "\n",
      "Assume that jo = (0.3,0.4,0.3). Find P(Xo = 0,X1 =\n",
      "1, X_ = 2) and P(X) = 0, X; =1, X_ = 1).\n",
      "\n",
      "2. Let Y1,¥5,... be a sequence of iid observations such that\n",
      "P(Y = 0) = 0.1, P(Y = 1) = 03, P(Y = 2) = 02,\n",
      "P(Y = 3) =0.4. Let Xp) =0 and let\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "X, = max{Yj,...,¥p}-\n",
      "\n",
      "Show that Xo, X1,... isa Markov chain and find the tran-\n",
      "sition matrix.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-497.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "Frequency\n",
      "\n",
      "Density\n",
      "\n",
      "100 150 200\n",
      "\n",
      "50\n",
      "\n",
      "0.001 0.002 0.003 0.004\n",
      "\n",
      "0.000\n",
      "\n",
      "Histogram of w/60\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "24.6 Exercises\n",
      "\n",
      "Density\n",
      "\n",
      "0.0010 0.0015\n",
      "\n",
      "0.0005\n",
      "\n",
      "499\n",
      "\n",
      "density(x = w/60)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "200 400 600 800 = 10001200\n",
      "wi\n",
      "density(x = w/60, bw = “ucv\")\n",
      "\n",
      "\\\n",
      "|\n",
      "l|\n",
      "\n",
      "|\n",
      "\\\n",
      "\n",
      "yA\n",
      "\n",
      "|\n",
      "\\\n",
      "\\\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "T T\n",
      "0 200 400 600 800 = 1000\n",
      "\n",
      "FIGURE 24.5. Density estimate of event times.\n",
      "\n",
      "| | |\n",
      "fatal\n",
      "\n",
      "T\n",
      "1200\n",
      "\n",
      " \n",
      "\n",
      "0.0000\n",
      "\n",
      "500\n",
      "\n",
      "T\n",
      "1000\n",
      "\n",
      "T\n",
      "1500\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-498.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "500\n",
      "\n",
      "3.\n",
      "\n",
      "24. Stochastic Processes\n",
      "\n",
      "Consider a two state Markov chain with states V = {1,2}\n",
      "and transition matrix\n",
      "\n",
      "l-a a\n",
      "p=| b v's\n",
      "\n",
      "where 0<a<1and0<0<1. Prove that\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "b\n",
      "tim pr= | 3 ts |.\n",
      "\n",
      "mrvea a+b a+b\n",
      "\n",
      ". Consider the chain from question 3 and set a = .1 and\n",
      "\n",
      "6 =.3. Simulate the chain. Let\n",
      "1\n",
      "—) (X;=1\n",
      "e » (Xi =1)\n",
      "\n",
      "x 1< =\n",
      "pat i=2)\n",
      "\n",
      "be the proportion of times the chain is in state 1 and\n",
      "state 2. Plot p,,(1) and p,(2) versus n and verify that they\n",
      "converge to the values predicted from the answer in the\n",
      "previous question.\n",
      "\n",
      "=)\n",
      "S\n",
      "i\n",
      "\n",
      "3\n",
      "\n",
      "pi\n",
      "\n",
      "We\n",
      "Il\n",
      "\n",
      ". An important Markov chain is the branching process\n",
      "\n",
      "which is used in biology, genetics, nuclear physics and\n",
      "many other fields. Suppose that an animal has Y chil-\n",
      "dren. Let p, = P(Y = k). Hence, py > 0 for all & and\n",
      "ro Pk = 1. Assume each animal has the same lifespan\n",
      "and that they produce offspring according to the distri-\n",
      "bution p,. Let X,, be the number of animals in the n‘*\n",
      "generation. Let ¥, 28 Vv be the offspring produced\n",
      "\n",
      "in the n“* generation. Note that\n",
      "\n",
      "Xa = VE) + + VO.\n",
      "\n",
      "Let = E(Y) and o? = V(Y). Assume throughout this\n",
      "question that Xo = 1. Let M(n) = E(X,) and V(n) =\n",
      "V(Xn)-\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-499.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "24.6 Exercises 501\n",
      "\n",
      "(a) Show that M(n+1) = »M(n) and V(n+1) = 0?M(n)+\n",
      "wV (n).\n",
      "\n",
      "(b) Show that M(n) = yw\" and that V(n) = o?v\"-!(1+\n",
      "pee prt).\n",
      "\n",
      "(c) What happens to the variance if . > 1? What happens\n",
      "to the variance if ys = 1? What happens to the variance if\n",
      "ee\n",
      "\n",
      "(d) The population goes extinct if X,, = 0 for some n. Let\n",
      "us thus define the extinction time N by\n",
      "\n",
      "N=min{n: X, =0}.\n",
      "\n",
      "Let F(n) = P(N <n) be the cdf of the random variable\n",
      "N. Show that\n",
      "\n",
      "~\n",
      "\n",
      "F(n) = Sop. (F(n—1))*, n=1,2,...\n",
      "\n",
      "k=0\n",
      "\n",
      "Hint: Note that the event {N <n} is the same as event\n",
      "{X, = 0}. Thus, P({N < n}) = P({X, = 0}). Let k be\n",
      "the number of offspring of the original parent. The pop-\n",
      "ulation becomes extinct at time n if and only if each of\n",
      "the k sub-populations generated from the k offspring goes\n",
      "extinct inn — 1 generations.\n",
      "\n",
      "(e) Suppose that po = 1/4, p; = 1/2, pp = 1/4. Use the\n",
      "formula from (5d) to compute the cdf F(n).\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-500.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502\n",
      "\n",
      "10.\n",
      "\n",
      "11.\n",
      "\n",
      "24. Stochastic Processes\n",
      "\n",
      "Let\n",
      "[ 0.40 0.50 0.10\n",
      "P= jj 0.05 0.70 0.25\n",
      "[ 0.05 0.50 0.45\n",
      "\n",
      "Find the stationary distribution 7.\n",
      "\n",
      ". Show that if 7 is a recurrent state and i ¢+ j, then j isa\n",
      "\n",
      "recurrent state.\n",
      "\n",
      "Let 1 1 1\n",
      "Seren\n",
      "377000\n",
      "000010\n",
      "P= tio.\n",
      "pi p00g\n",
      "001000\n",
      "000001\n",
      "Which states are transient? Which states are recurrent?\n",
      ". Let\n",
      "O01\n",
      "r=[1 9]\n",
      "\n",
      "Show that 7 = (1/2, 1/2) isa stationary distribution. Does\n",
      "this chain converge? Why/why not?\n",
      "\n",
      "Let 0 < p< land q=1-—p. Let\n",
      "\n",
      "qpo000\n",
      "q0p00\n",
      "P=|]¢q00p70\n",
      "q 00 0p\n",
      "10000\n",
      "\n",
      "Find the limiting distribution of the chain.\n",
      "\n",
      "Let X(t) be an inhomogeneous Poisson process with in-\n",
      "tensity function A(t) > 0. Let A(t) = fp Madu. Define\n",
      "Y(s) = X(t) where s = A(t). Show that Y(s) is a homo-\n",
      "geneous Poisson process with intensity \\ = 1.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-501.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "12.\n",
      "\n",
      "13.\n",
      "\n",
      "14.\n",
      "\n",
      "15.\n",
      "\n",
      "16.\n",
      "\n",
      "24.6 Exercises 503\n",
      "\n",
      "Let X(t) be a Poisson process with intensity \\. Find the\n",
      "conditional distribution of X(t) given that X(t+s) =n.\n",
      "\n",
      "Let X(t) be a Poisson process with intensity \\. Find the\n",
      "probability that X(t) is odd, i.e. P(X(t) =1,3,5,...).\n",
      "\n",
      "Suppose that people logging in to the University computer\n",
      "system is described by a Poisson process X(t) with inten-\n",
      "sity A. Assume that a person stays logged in for some\n",
      "random time with cdf G. Assume these times are all inde-\n",
      "pendent. Let Y(t) be the number of people on the system\n",
      "at time t. Find the distribution of Y(t).\n",
      "\n",
      " \n",
      "\n",
      "Let X(t) bea Poisson process with intensity \\. Let W,, Wa,.-.\n",
      "\n",
      "be the waiting times. Let f be an arbitrary function. Show\n",
      "\n",
      "that\n",
      "X(t)\n",
      "\n",
      "E[S*s0m)) =a | \" F(w)dw.\n",
      "\n",
      "A two dimensional Poisson point process is a process of\n",
      "random points on the plane such that (i) for any set A,\n",
      "the number of points falling in A is Poisson with mean\n",
      "Au(A) where (A) is the area of A, (ii) the number of\n",
      "events in nonoverlapping regions is independent. Consider\n",
      "an arbitrary point zo in the plane. Let X denote the dis-\n",
      "tance from x to the nearest random point. Show that:\n",
      "\n",
      "P(X >t) =e\"\n",
      "\n",
      "and\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-502.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-503.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "25\n",
      "Simulation Methods\n",
      "\n",
      "In this we will see that by generating data in a clever way, we\n",
      "can solve a number of problems such as integrating or maxi-\n",
      "mizing a complicated function.! For integration, we will study\n",
      "three methods: (i) basic Monte Carlo integration, (ii) impor-\n",
      "tance sampling and (iii) Markov chain Monte Carlo (MCMC).\n",
      "\n",
      "25.1 Bayesian Inference Revisited\n",
      "\n",
      "Simulation methods are especially useful in Bayesian inference\n",
      "so let us briefly review the main ideas in Bayesian inference.\n",
      "Given a prior f(@) and data X\" = (Xj,...,X,) the posterior\n",
      "density is\n",
      "\n",
      "L£(9)f (8)\n",
      "\n",
      "HON) = Tea Fulda\n",
      "\n",
      "‘My main source for this chapter is Monte Carlo Statistical Methods by C. Robert\n",
      "and G. Casella.\n",
      "\n",
      "This is page 505\n",
      "Printer: Opaque this\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-504.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "506 25. Simulation Methods\n",
      "\n",
      "where £(9) is the likelihood function. The posterior mean is\n",
      "\n",
      "x 0£(0) f (0)a0\n",
      "p= for alxr)ag — LOLA) F(0)a0\n",
      "(1X\") J £(0)f (0)d0\n",
      "If 0 = (0;,...,0,) is multidimensional, then we might be inter-\n",
      "\n",
      "ested in the posterior for one of the components, 6;, say. This\n",
      "marginal posterior density is\n",
      "\n",
      "HO) = fh foo flO 1.--- 8X) ly\n",
      "\n",
      "which involves high dimensional integration.\n",
      "\n",
      "‘You can see that integrals play a big role in Bayesian infer-\n",
      "ence. When @ is high dimensional, it may not be feasible to\n",
      "calculate these integrals analytically. Simulation methods will\n",
      "often be very helpful.\n",
      "\n",
      "25.2 Basic Monte Carlo Integration\n",
      "\n",
      "Suppose you want to evaluate the integral J = fie h(x)dx for\n",
      "some function h. If h is an “easy” function like a polynomial\n",
      "or triginometric function then we can do the integral in closed\n",
      "form. In practice, h can be very complicated and there may be no\n",
      "known closed form expression for J. There are many numerical\n",
      "techniques for evaluating J such as Simpson’s rule, the trape-\n",
      "zoidal rule, Guassian quadrature and so on. In some cases these\n",
      "techniques work very well. But other times they may not work\n",
      "so well. In particular, it is hard to extend them to higher dimen-\n",
      "sions. Monte Carlo integration is another approach to evaluating\n",
      "I which is notable for its simplicity, generality and scalability.\n",
      "Let us begin by writing\n",
      "\n",
      "l= [rear = [orc\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-505.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "25.2 Basic Monte Carlo Integration 507\n",
      "\n",
      "where w(x) = h(x)(b—a) and f(x) = 1/(b —a). Notice that f\n",
      "is the density for a uniform random variable over (a,b). Hence,\n",
      "\n",
      "T= Bj(w(X)\n",
      "\n",
      "where X ~ Unif(a, }).\n",
      "Suppose we generate X,,...,X\\y ~ Unif(a,b) where N is\n",
      "large. By the law of large numbers\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "j= emt % B(w(X)) <1.\n",
      "\n",
      "This is the basic Monte Carlo integration method. We can\n",
      "also compute the standard error of the estimate\n",
      "\n",
      "s\n",
      "\n",
      "VN\n",
      "\n",
      "=\n",
      "\n",
      " \n",
      "\n",
      "where\n",
      "\n",
      "2_ L(4—D?\n",
      "\n",
      "\"N=\n",
      "\n",
      "  \n",
      "\n",
      "where Y; = w(X;). A 1—a confidence interval for I is Ttzaps.\n",
      "We can take N as large as we want and hence make the length\n",
      "of the confidence inteval very small.\n",
      "\n",
      "Example 25.1 Let’s try this on an example where we know the\n",
      "true answer. Let h(x) = 2°. Hence, I = fi wdx = 1/4. Based\n",
      "on N = 10,000 J got I = .2481101 with a standard error of\n",
      "0028. Not bad at all.\n",
      "\n",
      " \n",
      "\n",
      "A simple generalization of the basic method is to consider\n",
      "integrals of the form\n",
      "\n",
      "l= [sorter\n",
      "\n",
      "where f(z) is a probability density function. Taking f to be a\n",
      "Unif (a,b) gives us the special case above. The only difference is\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-506.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "508 25. Simulation Methods\n",
      "\n",
      "that now we draw Xj,...\n",
      "\n",
      " \n",
      "\n",
      "nv ~ f and take\n",
      "\n",
      "as before.\n",
      "\n",
      "Example 25.2 Let\n",
      "1 2\n",
      "_ —2?/2\n",
      "az) = ——=e\n",
      "fo=s\n",
      "be the standard Normal pdf. Suppose we want to compute the cdf\n",
      "at some point x:\n",
      "\n",
      "r= [ soyas=aln.\n",
      "\n",
      "Of course, we could look this up in a table or use the pnorm\n",
      "command in R. But suppose you don’t have access to either.\n",
      "Write\n",
      "\n",
      "te [Hoitioas\n",
      "\n",
      "moy={ 9\n",
      "\n",
      "Now we generate X;,..., Xy ~ N(0,1) and set\n",
      "\n",
      "where\n",
      "\n",
      "a\n",
      "D.\n",
      "\n",
      "IVA\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "> 1 . number of observations <x\n",
      "T= PP = ‘\n",
      "\n",
      "For example, with x = 2, the true answer is ®(2) = .9772 and\n",
      "the Monte Carlo estimate with N = 10,000 yields .9751. Using\n",
      "N= 100,000 J get .9771.\n",
      "\n",
      "Example 25.3 (Two Binomials) Let X ~ Binomial(n,p,) andY ~\n",
      "Binomial(m, pz). We would like to estimate 6 = py—pi. The mle\n",
      "is 0 = Py — Pi = (Y/m) —(X/n). We can get the standard error\n",
      "Se@ using the delta method (remember?) which yields\n",
      "\n",
      "P=), BAB)\n",
      "n m\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-507.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "25.2 Basic Monte Carlo Integration 509\n",
      "\n",
      "and then construct a 95 per cent confidence interval 5428.\n",
      "Now consider a Bayesian analysis. Suppose we use the prior\n",
      "\n",
      "J (Pi, P2) = F(p1) f (Pa) = 1, that is, a flat prior on (py, p2). The\n",
      "\n",
      "posterior is\n",
      "F(pispalX,¥) x pid — pi)\" pp (t= po).\n",
      "\n",
      "The posterior mean of 0 is\n",
      "\n",
      "B= f [o.roro.raxyy= f° [ -posonraX.y).\n",
      "\n",
      "If we want the posterior density of 6 we can first get the posterior\n",
      "cdf\n",
      "\n",
      "F(lX,Y) = P(8 < X,Y) = [ fonnlxy)\n",
      "\n",
      "where A = {(pi,p2): po —pi < c}. The density can then be\n",
      "obtained by differentiating F.\n",
      "\n",
      "To avoid all these integrals, let’s use simulation. Note that\n",
      "f(p1,P2|X,Y) = f(pi|X)f(p2|Y) which implies that p, and po\n",
      "are independent under the posterior distribution. Also, we see\n",
      "that p,|X ~ Beta(X +1,n—X +1) and po|Y ~ Beta(Y +1, m—\n",
      "Y +1). Hence, we can simulate (P\\, PS?),..., (PO, PL”)\n",
      "from the posterior by drawing\n",
      "\n",
      "PO ~ Beta(X +1,n—-X+1)\n",
      "PO ~ Beta(Y+1,m—Y +1)\n",
      "\n",
      "LN. Now let 5 = P® — P. Then,\n",
      "\n",
      "= 1 i\n",
      "de >> 5.\n",
      "\n",
      "i\n",
      "\n",
      "fori=1\n",
      "\n",
      " \n",
      "\n",
      "We can also get a 95 per cent posterior interval for 6 by sorting\n",
      "the simulated values, and finding the .025 and .975 quantile. The\n",
      "posterior density f(5|X,Y) can be obtained by applying density\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-508.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510 25. Simulation Methods\n",
      "\n",
      "estimation techniques to 6,...,6) or, simply by plotting a\n",
      "histogram.\n",
      "\n",
      "For example, suppose thatn = m= 10, X =8 and Y = 6.\n",
      "The R code is:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "X Kp Sp ke |6\n",
      "\n",
      "n <- 10; m <- 10\n",
      "\n",
      "B <- 10000\n",
      "\n",
      "pi <- rbeta(B,x+1,n-x+1)\n",
      "\n",
      "p2 <- rbeta(B,y+i,m-y+1)\n",
      "delta <- p2 - pl\n",
      "\n",
      "print (mean (delta) )\n",
      "\n",
      "left <- quantile(delta, .025)\n",
      "right <- quantile(delta, .975)\n",
      "print (c(left,right) )\n",
      "\n",
      "I found that 3 and a 95 per cent posterior interval of (-0.52,0.20).\n",
      "The posterior density can be estimated from a histogram of the\n",
      "simulated values as shown in Figure 25.1.\n",
      "\n",
      "Example 25.4 (Dose Response) Suppose we conduct an experiment\n",
      "by giving rats one of ten possible doses of a drug, denoted by\n",
      "@ <_<... < Xo. For each dose level x; we use n rats and\n",
      "we observe Yj, the number that survive. Thus we have ten inde-\n",
      "pendent binomials Y; ~ Binomial(n,p;). Suppose we know from\n",
      "biological considerations that higher doses should have higher\n",
      "probability of death. Thus, py < po < +++ < pio. Suppose we\n",
      "want to estimate the dose at which the animals have a 50 per\n",
      "cent chance of dying. This is called the LD50. Formally, 5 = x;\n",
      "where\n",
      "\n",
      "j=min{i: p; > -50}-.\n",
      "Notice that 6 is implicitly a (complicated) function of pi,.--,Pi0\n",
      "\n",
      "so we can write 6 = g(pi,---,Pio) for some g. This just means\n",
      "that if we know (pi,---,pio) then we can find 6. The posterior\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-509.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "f(delta | data)\n",
      "\n",
      " \n",
      "\n",
      "25.2 Basic Monte Carlo Integration 511\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "-08\n",
      "\n",
      "T T T T T T\n",
      "-06 -04 0.2 0.0 02 04\n",
      "\n",
      "delta = p2-p1\n",
      "\n",
      "FIGURE 25.1. Posterior of 5 from simulation.\n",
      "\n",
      "06\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-510.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "512 25. Simulation Methods\n",
      "\n",
      "mean. of 6 is\n",
      "\n",
      "Df f germ Fer. Diol¥is.--s Yolanda. dv.\n",
      "A\n",
      "\n",
      "The integral is over the region\n",
      "\n",
      "A={(pi,---,Pio): pi <+*+ < pro}-\n",
      "\n",
      "Similarly, the posterior cdf of 6 is\n",
      "\n",
      "F(el%,---, Yio) = P(6 <elM,.-., Yo)\n",
      "\n",
      "where\n",
      "\n",
      "B= Af \\{(pi.---. P10) : g(Pi,---, Pio) < ch.\n",
      "\n",
      "We would need to do a 10 dimensional integral over a resticted\n",
      "region A. Instead, let’s use simulation.\n",
      "\n",
      "Let us take a flat prior truncated over A. Except for the trun-\n",
      "cation, each P; has once again a Beta distribution. To draw from\n",
      "the posterior we do the following steps:\n",
      "\n",
      "(1) Draw P, ~ Beta(Y;+1,n — ¥,;+1),i=1,...,10.\n",
      "\n",
      "(2) If Pi) < Py <+++ < Pro keep this draw. Otherwise, throw\n",
      "it away and draw again until you get one you can keep.\n",
      "\n",
      "(3) Let 6 = x; where\n",
      "\n",
      "j=min{i: P; > .50}.\n",
      "\n",
      "We repeat this N times to get 0,...,6) and take\n",
      "\n",
      ". 1 i\n",
      "E(5|%i,..-, Yio) © Fe\"\n",
      "\n",
      "I [- «PFO Plies: Yio)dpidps...dpro\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-511.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9EC8>\n",
      "25.2 Basic Monte Carlo Integration 513\n",
      "\n",
      "6 is a discrete variable. We can estimate its probability mass\n",
      "function by\n",
      "\n",
      "N\n",
      "\n",
      "1\n",
      "= Z PY oe. @ 5\n",
      "P(6=2j|V1,---, Yio) & wot = j).\n",
      "For example, suppose that x = (1,2,...,10). Figure 25.2 shows\n",
      "some data with n = 15 animals at each dose. The R code is a\n",
      "bit tricky.\n",
      "\n",
      "## assume x and y are given\n",
      "n <- rep(15,10)\n",
      "B <- 10000\n",
      "p <- matrix(0,B, 10)\n",
      "check <- rep(0,B)\n",
      "for(i in 1:10){\n",
      "pL,i] <- rbeta(B, ylil+1, nfil-yfil+1)\n",
      "3\n",
      "for(i in 1:B){\n",
      "check[i] <- min(diff(p[i,]))\n",
      "}\n",
      "good <- (1:B) [check >= 0]\n",
      "p <- plgood,]\n",
      "B <- nrow(p)\n",
      "delta <- rep(0,B)\n",
      "for(i in 1:B){\n",
      "temp <- cumsum(p[i,])\n",
      "j <- (1:10) [temp > .5]\n",
      "j <- min(j)\n",
      "delta[i] <- x[j]\n",
      "3\n",
      "print (mean (delta) )\n",
      "left <- quantile(delta, .025)\n",
      "right <- quantile(delta, .975)\n",
      "print(c(left ,right))\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-512.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "proportion who died\n",
      "\n",
      "f(delta | data)\n",
      "\n",
      "02 04 08 o8 °\n",
      "\n",
      "0.0\n",
      "\n",
      "0.2 0.4 os os\n",
      "\n",
      "0.0\n",
      "\n",
      "Simulation Methods\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "tp)\n",
      "\n",
      "0.2 04 06 08 °\n",
      "\n",
      "0.0\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "dose\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "FIGURE 25.2. Dose repoonse data.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-513.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9B08>\n",
      "25.3 Importance Sampling 515\n",
      "\n",
      "The posterior draws for pi,..-,Pio are shown in the second\n",
      "panel in the figure. We find that that 6 = 4.04 with a 95 per\n",
      "cent interval of (3,5). The third panel shows the posterior for 5\n",
      "which is necessarily a discrete distribution.\n",
      "\n",
      "25.3 Importance Sampling\n",
      "\n",
      "Consider the integral J = f h(x) f(2)da where f is a probability\n",
      "density. The basic Monte Carlo method involves sampling from\n",
      "f{. However, there are cases where we may not know how to\n",
      "sample from f. For example, in Bayesian inference, the posterior\n",
      "density density is is obtained by multiplying the likelihood £(@)\n",
      "times the prior f (0). There is no guarantee that f(6|2) will be\n",
      "a known distribution like a Normal or Gamma or whatever.\n",
      "\n",
      "Importance sampling is a generalization of basic Monte Carlo\n",
      "which overcomes this problem. Let g be a probability density\n",
      "that we know how to simulate from. Then\n",
      "\n",
      "l= [restorer = [OR oar = E,(Y)\n",
      "\n",
      "(2)\n",
      "\n",
      "where Y = h(X)f(X)/g(X) and the expectation E,(Y) is with\n",
      "respect to g. We can simulate X),..., Ny ~ g and estimate\n",
      "\n",
      "by\n",
      "= Unie _ LanllXOrORD\n",
      "! D> ¥ rpy aX)\n",
      "\n",
      "This is called importance sampling. By the law of large num-\n",
      "bers, 1 4 I. However, there isa catch. It’s possible that J might\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "have an infinite standard error. To see why, recall that J is the\n",
      "mean of w(x) = h(x) f(z)/g(x). The second moment of this\n",
      "quantity is\n",
      "\n",
      "yu\") = / (Meena nz peer an.\n",
      "\n",
      "(2) gx)\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-514.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "516 25. Simulation Methods\n",
      "\n",
      "If g has thinner tails than f, then this integral might be infinite.\n",
      "To avoid this, a basic rule in importance sampling is to sample\n",
      "from a density g with thicker tails than f. Also, suppose that\n",
      "g(x) is small over some set A where f(z) is large. Again, the\n",
      "ratio of f/g could be large leading to a large variance. This\n",
      "implies that we should choose g to be similar in shape to f. In\n",
      "summary, a good choice for an importance sampling density g\n",
      "should be similar to f but with thicker tails. In fact, we can say\n",
      "what the optimal choice of g is.\n",
      "\n",
      "Theorem 25.5 The choice of g that minimizes the variance of I\n",
      "\n",
      "° sy [bled la)\n",
      "8) = The Oe\n",
      "\n",
      "PROOF. The variance of w = fh/g is\n",
      "\n",
      "Flu?) — (E(w)? =f w2teyateyae— ( f waar)\n",
      "\n",
      "- [BSP rote “ (ners) .\n",
      "\n",
      "The second integral does not depend on g so we only need to\n",
      "minimize the first integral. Now, from Jensen’s inequality we\n",
      "\n",
      "have\n",
      "E,(W*) > (,(W))? = (fim ited)\n",
      "\n",
      "This establishes a lower bound on E,(W?). However, Ey» (W?)\n",
      "equals this lower bound which proves the claim.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "This theorem is interesting but it is only of theoretical inter-\n",
      "est. If we did not know how to sample from f then it is unlikely\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-515.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "25.3 Importance Sampling 517\n",
      "\n",
      "that we could sample from |h(2)|f(x)/ f |h(s)|f(s)ds. In prac-\n",
      "tice, we simply try to find a thick tailed distribution g which is\n",
      "similar to fh].\n",
      "\n",
      "Example 25.6 (Tail Probability) Let’s estimate I = P(Z > 3) =\n",
      "0013 where Z ~~ N(0,1). Write I = f[ h(x) f(x)dx where f(x) is\n",
      "the standard Normal density and h(x) =1 if « > 3 and 0 other-\n",
      "wise. The basic Monte Carlo estimator is T = N-! A(X).\n",
      "Using N = 100 we find (from simulating many times) that\n",
      "E(1) = .0015 and Var(I) = .0039. Notice that most obser-\n",
      "vations are wasted in the sense that most are not near the right\n",
      "tail. We will estimate this with importance sampling taking g\n",
      "to be a Normal(4,1) density. We draw values from g and the\n",
      "estimate is now T= N-! Dd, f(XDA(X)/g(Xj). In this case we\n",
      "find that E(1) = .0011 and Var(I) = .0002. We have reduced\n",
      "the standard deviation by a factor of 20.\n",
      "\n",
      "Example 25.7 (Measurement Model With Outliers) Suppose we have\n",
      "measurements X1,...,Xn of some physical quantity 0. We might\n",
      "model this as\n",
      "\n",
      "X,=0+6.\n",
      "\n",
      "If we assume that ¢; ~ N(0,1) then X; ~ N(0;,1). However,\n",
      "when taking measurements, it is often the case that we get the\n",
      "occasional wild obvervation, or outlier. This suggests that a Nor-\n",
      "mal might be a poor model since Normals have thin tails which\n",
      "implies that extreme observations are rare. One way to improve\n",
      "the model is to use a density for €; with a thicker tail, for ex-\n",
      "ample, a t-distribution with v degrees of freedom which has the\n",
      "form\n",
      "\n",
      " \n",
      "\n",
      "(x)=\n",
      "\n",
      "Vv\n",
      "\n",
      "P (44) (1+ “) “me\n",
      "r (4) vn\n",
      "\n",
      "Smaller values of v correspond to thicker tails. For the sake of\n",
      "illustration we will take v = 3. Suppose we observe n X; = 0+€;,\n",
      "i=1,...,n where €; has at distribution with v = 3. We will\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-516.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "518 25. Simulation Methods\n",
      "\n",
      "take a flat prior on 0. The likelihood is £(0) = T];_, (Xi — 4)\n",
      "and the posterior mean of 0 is\n",
      "\n",
      "‘[9L(0)d0\n",
      "\n",
      "= Tra\n",
      "\n",
      "We can estimate the top and bottom integral using importance\n",
      "sampling. We draw 0,,...,9y ~ g and then\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "il 0; £(0;)\n",
      "Ba 9(95)\n",
      "\n",
      "Se N L0;) ~\n",
      "w Lj=1 9)\n",
      "\n",
      "To illustrate the idea, I drew n = 2 observations. The posterior\n",
      "mean (ccomputed numerically) is -0.54. Using a Normal impor-\n",
      "tance sampler g yields an estimate of -0.74. Using a Cauchy (t-\n",
      "distribution with 1 degree of freedom) importance sampler yields\n",
      "an estimate of -0.53.\n",
      "\n",
      "25.4 MCMC Part I: The Metropolis-Hastings\n",
      "Algorithm\n",
      "\n",
      "Consider again the problem of estimating the integral J = f h(a) f («)dx.\n",
      "In this chapter we introduce Markov chain Monte Carlo (MCMC)\n",
      "methods. The idea is to constuct a Markov chain X), Xo,...,\n",
      "\n",
      "whose stationary distribution is f. Under certain conditions it\n",
      "\n",
      "will then follow that\n",
      "\n",
      "N\n",
      "Fn) 4, Bh(X)) <1.\n",
      "i=\n",
      "This works because there is a law of large numbers for Markov\n",
      "chains called the ergodic theorem.\n",
      "\n",
      "The Metropolis-Hastings algorithm is a specific MCMC\n",
      "method that works as follows. Let ¢(y|x) be an arbitrary, friendly\n",
      "distribution (i.e. we know how to sample from q(y|x)). The con-\n",
      "ditional density q(y|x) is called the proposal distribution.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-517.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "25.4 MCMC Part I: The Metropolis-Hastings Algorithm 519\n",
      "\n",
      "The Metropolis-Hastings algorithm creates a sequence of obser-\n",
      "vations Xo, X1,..., as follows.\n",
      "\n",
      " \n",
      "\n",
      "(1) Generate a proposal or candidate value Y ~ q(y|X;)-\n",
      "(2) Evaluate r = r(X;, Y) where\n",
      "\n",
      "r(z,y) = min fapatiy\n",
      "r(a,y) eovnne i}.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "(3) Set\n",
      "Y= Y with probability r\n",
      "“el ) X, with probability 1 — r.\n",
      "\n",
      " \n",
      "\n",
      "Metropolis-Hastings Algorithm. Choose Xo arbitrarily. Suppose\n",
      "we have generated Xo, X1,..., X;. To generate Xj; do the following:\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "REMARK 1: A simple way to execute step (3) is to generate\n",
      "U ~ (0,1). If U <r set X41 = Y otherwise set X;,1 = Xj.\n",
      "\n",
      "REMARK 2: A common choice for g(y|x) is N(2, 6?) for some\n",
      "6 > 0. This means that the proposal is draw from a Normal,\n",
      "centered at the current value.\n",
      "\n",
      " \n",
      "\n",
      "REMARK 3: If the proposal density g is symmetric, q(y|x) =\n",
      "q(z|y), then r simplifies to\n",
      "\n",
      "r= min { FO a}.\n",
      "\n",
      "The Normal proposal distribution mentioned in Remark 2 is an\n",
      "example of a symmetric proposal density.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-518.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "520 25. Simulation Methods\n",
      "\n",
      "By construction, Xo,Xy,...is a Markov chain. But why does\n",
      "this Markov chain have f as its stationary distribution? Before\n",
      "we explain why, let us first do an example.\n",
      "\n",
      "Example 25.8 The Cauchy distribution has density\n",
      "\n",
      "f(2)=++\n",
      "\n",
      "~aT+ a\n",
      "Our goal is to simulate a Markov chain whose stationary distri-\n",
      "\n",
      "bution is f. As suggested in the remark above, we take q(y|x) to\n",
      "be a N(x,b?). So in this case,\n",
      "\n",
      "rea) =min{ £9, 1} in {2a}\n",
      "\n",
      "So the algorithm is to draw Y ~ N(X;,b*) and set\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "You Y with probability r(X;, Y)\n",
      "“itt =) X; with probability 1—r(X;,Y).\n",
      "\n",
      "The R code is very simple:\n",
      "\n",
      "metrop <- function(N,b){\n",
      "\n",
      "x <- rep(0,N)\n",
      "\n",
      "for(i in 2:N){\n",
      "y <- rmorm(1,x[i-1],b)\n",
      "vr <- (14x72) /(1+y72)\n",
      "u <- runif(1)\n",
      "if(u < r)x[i] <- y else x[i] <- x[i-1]\n",
      ":\n",
      "\n",
      "return (x)\n",
      "\n",
      "ip\n",
      "\n",
      "The simulator requires a choice of b. Figure 25.3 shows three\n",
      "chains of length N = 1000 using b= 1, b=1 and b= 10. The\n",
      "histograms of the simulated values are shown in Figure 25.4. Set-\n",
      "ting b = .1 forces the chain to take small steps. As a result, the\n",
      "chain doesn’t “explore” much of the sample space. The histogram\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-519.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E88>\n",
      "25.4 MCMC Part I: The Metropolis-Hastings Algorithm 521\n",
      "\n",
      "from the sample does not approximate the true density very well.\n",
      "Setting b = 10 causes the proposals to often be far in the tails,\n",
      "making r small and hence we reject the proposal and keep the\n",
      "chain at its current position. The result is that the chain “gets\n",
      "stuck” at the same place quite often. Again, this means that the\n",
      "histogram from the sample does not approximate the true density\n",
      "very well. The middle choice avoids these extremes and results in\n",
      "a Markov chain sample that better represents the density sooner.\n",
      "In summary, there are are tuning parameters and the efficiency\n",
      "of the chain depends on these parameters. We'll discus this in\n",
      "more detail later.\n",
      "\n",
      "If the sample from the Markov chain starts to “look like”\n",
      "the target distribution f quickly, then we say that the chain is\n",
      "“mixing well.” Constructing a chain that mixes well is somewhat\n",
      "of an art.\n",
      "\n",
      "Wry Ir Works. Recall from the previous chapter that a\n",
      "distribution 7 satisfies detailed balance for a Markov chain if\n",
      "\n",
      "Dig Ti = DjiT-\n",
      "\n",
      "We then showed that if 7 satisfies detailed balance, then it is a\n",
      "stationary distribution for the chain.\n",
      "\n",
      "Because we are now dealing with continuous state Markov\n",
      "chains, we will change notation a little and write p(x, y) for the\n",
      "probability of making a transition from x to y. Also, let’s use\n",
      "f(x) instead of 7 for a distribution. In this new notation, f is\n",
      "a stationary distribution if f(x) = f[ f(y)p(y, 7)dy and detailed\n",
      "balance holds for f if\n",
      "\n",
      "F(x)p(x, y) = f(y)ply, 2)-\n",
      "\n",
      "Detailed balance implies that f is a stationary distribution since,\n",
      "if detailed balance holds, then\n",
      "\n",
      "[ tort2y= [ sere. nay= se) f pte, v)av= (0)\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-520.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A8106308>\n",
      "522\n",
      "\n",
      "25. Simulation Methods\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "al ‘gait, we\n",
      "\n",
      "T T T T T\n",
      "m0 400 a0 200 1000\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "‘WA fio Ni 7 ll Vy /l iy hw\n",
      "\n",
      " \n",
      "\n",
      "T T T\n",
      "é i a 0 200 1000\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "- woud\n",
      "\n",
      " \n",
      "\n",
      "hr al HI yu LA I\n",
      "\n",
      " \n",
      "\n",
      "T T\n",
      "0 200 400 60 0 1000\n",
      "\n",
      "FIGURE 25.3. Three Metropolis chains corresponding\n",
      "tob=.1,b=1,b=10.\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-521.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A8106048>\n",
      "0.0 04 02 08 04 0S\n",
      "\n",
      "0.0 04 02 03 04 0S\n",
      "\n",
      "0.0 04 02 03 04 05\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "25.4 MCMC Part I: The Metropolis-Hastings Algorithm 523\n",
      "T T\n",
      "5 a\n",
      "ail PRY\n",
      "r T\n",
      "5 a\n",
      "er\n",
      "T T\n",
      "f a\n",
      "\n",
      "FIGURE 25.4. Histograms from the three chains. The\n",
      "true density is also plotted.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-522.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "524 25. Simulation Methods\n",
      "\n",
      "which shows that f(x) = f f(y)p(y,7)dy as required. Our goal\n",
      "is to show that f satisfies detailed balance which will imply that\n",
      "f is a stationary distribution for the chain.\n",
      "\n",
      "Consider two points x and y. Either\n",
      "\n",
      "F(z)a(ylz) <f(y)a(zly) or f(x)a(ylz) > F(y)a(aly)-\n",
      "\n",
      "We will ignore ties which we can do in the continuous setting.\n",
      "Without loss of generality, assume that f(x)q(y|x) > f(y)q(z|y)-\n",
      "This implies that\n",
      "\n",
      "f(y) alaly\n",
      "F(x) q(yl|x\n",
      "\n",
      "and that r(y,z) = 1. Now p(z,y) is the probability of jumping\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "r(a,y) =\n",
      "\n",
      "from x to y. This requires two things: (i) the proposal distribu-\n",
      "tion must generate y and (ii) you must accept y. Thus,\n",
      "fy) (ely) _ fly)\n",
      "\n",
      "p(x, y) = a(yl2)r (a, y) = ale) Fr ala) Feayll)-\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Therefore,\n",
      "\n",
      "f(2)p(2,y) = F(y)a(ely)- (25.1)\n",
      "On the other hand, p(y,x) is the probability of jumping from\n",
      "y to x. This requires two things: (i) the proposal distribution\n",
      "must generate x and (ii) you must accept x. This occurs with\n",
      "probability p(y, x) = q(xly)r(y, x) = q(xly). Hence,\n",
      "\n",
      "F(y)a(aly)- (25.2)\n",
      "\n",
      "Comparing (25.1) and (25.2), we see that we have shown that\n",
      "detailed balance holds.\n",
      "\n",
      " \n",
      "\n",
      "F(y)ply, 2)\n",
      "\n",
      "25.5 MCMC Part II: Different Flavors\n",
      "\n",
      "There are different types of MCMC algorithm. Here we will\n",
      "consider a few of the most popular versions.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-523.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C08>\n",
      "25.5 MCMC Part II: Different Flavors 525\n",
      "\n",
      "RANDOM- WALK- METROPOLIS- HASTINGS. In the previous sec-\n",
      "tion we considered drawing a proposal Y of the form\n",
      "\n",
      "Y=Xj+6\n",
      "\n",
      "where €; comes from some distribution with density g. In other\n",
      "words, ¢(y|2) = g(y — 2). We saw that in this case,\n",
      "\n",
      "roy\n",
      "\n",
      "This is called a random-walk-Metropolis-Hastings method.\n",
      "The reason for the name is that, if we did not do the accept-\n",
      "reject step, we would be simulating a random walk. The most\n",
      "common choice for g is a N(0,b\"). The hard part is choosng b\n",
      "\n",
      " \n",
      "\n",
      "r(a,y) = min {1\n",
      "\n",
      "so that the chain mixes well. A good rule of thumb is: choose b\n",
      "so that you accept the proposals about 50 per cent of the time.\n",
      "\n",
      "Warning: This method doesn’t make sense unless X takes\n",
      "values on the whole real line. If X is restricted to some interval\n",
      "then it is best to transform X, say, Y = m(X) say, where Y takes\n",
      "values on the whole real line. For example, if X € (0,00) then\n",
      "you might take Y = log X and then simulate the distribution\n",
      "for Y instead of X.\n",
      "\n",
      "INDEPENDENCE-METROPOLIS-HASTINGS. This is like an importance-\n",
      "sampling version of MCMC. In this we draw the proposal from\n",
      "a fixed distribution g. Generally, g is chosen to be an approxi-\n",
      "mation to f. The acceptance probability becomes\n",
      "\n",
      "HOLES\n",
      "\n",
      " \n",
      "\n",
      "r(2,y) = min {1 F(x) gy)\n",
      "\n",
      "GIBBS SAMPLING. The two previous methods can be easily\n",
      "adapted, in principle, to work in higher dimensions. In practice,\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-524.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A81061C8>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "526 25. Simulation Methods\n",
      "\n",
      "tuning the chains to make them mix well is hard. Gibbs sampling\n",
      "is a way to turn a high-dimensional problem into several one\n",
      "dimensional problems.\n",
      "\n",
      "Here’s how it works for a bivariate problem. Suppose that\n",
      "(X,Y) has density fy(z,y). First, suppose that it is possi-\n",
      "ble to simulate from the conditional distributions fxy(2|y) and\n",
      "fyx(ylz). Let (Xo, Yo) be starting values. Asume we have drawn\n",
      "(Xo, Yo);---; (Xn, Yn)- Then the Gibbs sampling algorithm for\n",
      "getting (Xn41, Yn41) is:\n",
      "\n",
      " \n",
      "\n",
      "The n+ 1 step in Gibbs sampling:\n",
      "\n",
      "Xn © fxqy(2l¥n)\n",
      "Yaut ~~ frpx(y|Xnyi)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "This generalizes in the obvious way to higher dimensions.\n",
      "\n",
      "Example 25.9 (Normal Hierarchical Model) Gibbs sampling is very\n",
      "useful for a class of models called hierarchical models. Here\n",
      "is a simple case. Suppose we draw a sample of k cities. From\n",
      "each city we draw n; people and observe how many people Y;\n",
      "have a disease. Thus, Y; ~ Binomial(n;,p;). We are allowing\n",
      "for different disease rates in different cities. We can also think\n",
      "of the pis as random draws from some distribution F. We can\n",
      "write this model in the following way:\n",
      "\n",
      "Pow F\n",
      "YP, =p; ~ Binomial(n,,p;)-\n",
      "\n",
      "We are interested in estimating the ps and the overall disease\n",
      "\n",
      "rate { pdF(p).\n",
      "To proceed, it will simplify matters if we make some transfor-\n",
      "mations that allow us to use some Normal apprormations. Let\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-525.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "25.5 MCMC Part II: Different Flavors 527\n",
      "\n",
      "pi = Y;/nj. Recall that p; » N(p;,8;) where s; = \\/pi(1 — pi)/ni-\n",
      "Let v; = log(p;/(1 — p;)) and define Z; =\n",
      "By the delta method,\n",
      "\n",
      " \n",
      "\n",
      "be & Nis a?)\n",
      "\n",
      "where 0? = 1/(np;(1 — p;)). Experience shows that the Normal\n",
      "approximation for w is more accurate than the Normal approx-\n",
      "imation for p so we shall work with w. We shall treat o; as\n",
      "known. Furthermore, we shall take the distribution of the wis to\n",
      "be Normal. The hierarchical model is now\n",
      "\n",
      "vi ~ N(u,7?)\n",
      "Zilbi ~ N(dy, 07).\n",
      "\n",
      "As yet another simplification we take T = 1. The unknown pa-\n",
      "rameter are 0 = (1, U1,---, Ux). The likelihood function is\n",
      "\n",
      "L(9) «x [1 Ftd [17 4y)\n",
      "« Tle {30:0 fx { spr — wtf\n",
      "\n",
      "If we use the prior f (p) x 1 then the posterior is proportional\n",
      "to the likelihood. To use Gibbs sampling, we need to find the\n",
      "conditional distribution of each parameter conditional on all the\n",
      "others. Let us begin by finding f(jlrest) where “rest” refers to\n",
      "all the other variables. We can throw away any terms that don’t\n",
      "involve pt. Thus,\n",
      "\n",
      "Flulrest) oc Too {3-9}\n",
      "x exp {Sua}\n",
      "\n",
      "1\n",
      "b= Fh\n",
      "\n",
      "where\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-526.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F99C8>\n",
      "528 25. Simulation Methods\n",
      "\n",
      "Hence we see that p\\rest ~ N(b,1/k). Next we will find f (v|rest).\n",
      "Again, we can throw away any terms not involving w; leaving us\n",
      "with\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "where\n",
      "4 + pe\n",
      "e; —- and d 7\n",
      "145 1+3\n",
      "\n",
      "and so wy|rest ~ N(e;,d?). The Gibbs sampling algorithm then\n",
      "\n",
      "in d;\n",
      "involves iterating the following steps N times:\n",
      "\n",
      "draw pp ~ N(b,v?\n",
      "\n",
      "draw vy ~ N(ey,\n",
      "\n",
      "Bes\n",
      "\n",
      ")\n",
      "\n",
      "draw Wy, ~ N(ex, di)-\n",
      "\n",
      "It is understood that at each step, the most recently drawn ver-\n",
      "sion of each variable is used.\n",
      "\n",
      "Let’s now consider a numerical example. Suppose that there\n",
      "are k = 20 cities and we sample n = 20 people from each city.\n",
      "The R function for this problem is:\n",
      "\n",
      "gibbs.fun <- function(y,n,N){\n",
      "k <- length(y)\n",
      "p-hat <- y/n\n",
      "Z <- log(p.hat/(1-p.hat))\n",
      "sigma <- sqrt(1/(n*p.hat*(1-p.hat)))\n",
      "v <- sqrt(1/sum(1/sigma*2) )\n",
      "mu <- rep(0,N)\n",
      "psi <- matrix(0,N,k)\n",
      "for(i in 2:N){\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-527.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9A88>\n",
      "25.5 MCMC Part II: Different Flavors 529\n",
      "\n",
      "### draw mu given rest\n",
      "b <- v°2ksum(psil[i-1,]/sigma™2)\n",
      "mu[i] <- rnorm(1,b,v)\n",
      "\n",
      "### draw psi given rest\n",
      "e <- (Z + mu[i]/sigma’2)/(1 + (1/sigma*2))\n",
      "d <- sqrt (i/(1 + (1/sigma™2)))\n",
      "psili,] <- rnorm(k,e,d)\n",
      "}\n",
      "list (mu=mu, psi=psi)\n",
      "\n",
      "}\n",
      "\n",
      "After running the chain, we can convert each y; back into p;\n",
      "by way of pj =e\" /(1+e%). The raw proportions are shown in\n",
      "Figure 25.6. Figure 25.5 shows “trace plots” of the Markov chain\n",
      "for p, and yt. (We should also look at the plots for pa, . .. , p20.)\n",
      "Figure 25.6 shows the posterior for js based on the simulated\n",
      "values. The second panel of Figure 25.6 shows the raw propor-\n",
      "tions and the Bayes estimates. Note that the Bayes estimates\n",
      "are “shrunk” together. The parameter T controls the amount of\n",
      "shrinkage. We set T =1 but in practice, we should treat T as an-\n",
      "other unknown parameter and let the data determine how much\n",
      "shrinkage is needed.\n",
      "\n",
      "So far we assumed that we know how to draw samples from\n",
      "the conditionals fyxjy(x]y) and fy|x(ylz). If we don’t know how,\n",
      "we can still use the Gibbs sampling algorithm by drawing each\n",
      "observation using a Metropolis-Hastings step. Let g be a pro-\n",
      "posal distribution for x and let g be a proposal distribution for\n",
      "y. When we do a Metropolis step for X we treat Y as fixed.\n",
      "Similarly, when we do a Metropolis step for Y we treat X as\n",
      "fixed. Here are the steps:\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-528.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "rr\n",
      "9\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-529.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n",
      "150 200\n",
      "\n",
      "100\n",
      "\n",
      "50\n",
      "\n",
      "25.5 MCMC Part II: Different Flavors 531\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      " ZALIMIWS\n",
      "\n",
      "raw estimates and Bayes estimates\n",
      "\n",
      "FIGURE 25.6. Top panel: posterior histogram of p.\n",
      "Lower panel: raw proptions and the Bayes posterior es-\n",
      "timates.\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-530.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9E48>\n",
      "532 25. Simulation Methods\n",
      "\n",
      " \n",
      "\n",
      "Metropolis within Gibbs:\n",
      "\n",
      "(1a) Draw a proposal Z ~ q(z|X,,)-\n",
      "\n",
      "(1b) Evaluate\n",
      "\n",
      "r= min { F2 Yn) g(Xn|Z) i}\n",
      "f(Xn,¥n) (Z]Xn)\" J”\n",
      "\n",
      "(1c) Set\n",
      "\n",
      "Yo= Z_— with probability r\n",
      "“ntl X,, with probability 1—r.\n",
      "\n",
      "(2a) Draw a proposal Z ~ q(z|Y,)-\n",
      "\n",
      "(2b) Evaluate\n",
      "\n",
      " \n",
      "\n",
      "min { F(Xns1s Z) U¥aIZ) |\n",
      "F(Xng1s Yn) (ZYa)? J 7\n",
      "\n",
      "(2c) Set\n",
      "\n",
      "Y, with probability 1—r.\n",
      "\n",
      " \n",
      "\n",
      "an { Z_ with probability r\n",
      "n+l =\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "Again, this generalizes to more than two dimensions.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-531.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "Chapter 8\n",
      "\n",
      "Fundamental Concepts in\n",
      "Inference\n",
      "\n",
      "Many inferential problems can be identified as being one of three types:\n",
      "estimation, confidence sets or hypothesis testing.\n",
      "\n",
      "Point estimation refers to providing a single “best guess” of some quan-\n",
      "tity of interest. The quantity of interest could be a parameter in a parametric\n",
      "model, a CDF F’, a probability density function f, a regression function f, or\n",
      "a prediction for a future value Y of some random variable.\n",
      "\n",
      "By convention, we denote a point estimate of 0 by @. Remember\n",
      "that 0 is a fixed, unknown quantity. The estimate @ depends on the\n",
      "data so it is a random variable.\n",
      "\n",
      "A confidence set C;,, is a set that contains a quantity of interest 0 with\n",
      "some prescribed probability 1— a. For example, we might want to construct\n",
      "an interval C,, = (a, 6) that traps an unknown parameter @ 95 per cent of the\n",
      "time.\n",
      "\n",
      "In hypothesis testing, we start with some default theory — called a null\n",
      "hypothesis — and we ask if the data provide sufficient evidence to reject the\n",
      "theory. If not we retain the null hypothesis.\n",
      "\n",
      "113\n",
      "\n",
      "The term “retaining\n",
      "the null hypothesis”\n",
      "is due to Chris Gen-\n",
      "ovese. Other termi-\n",
      "nology is “accepting\n",
      "the null” or “failing\n",
      "to reject the null.”\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-532.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "114 CHAPTER 8. FUNDAMENTAL CONCEPTS IN INFERENCE\n",
      "\n",
      "Example 8.1 (Coin Flipping.) Suppose X1,...,Xpn ~ Bernoulli(p) denote n in-\n",
      "dependent coin flips. A reasonable point estimate of p is p = n-* 7e_, Xi.\n",
      "A 1—a per cent confidence interval for p is Cn = (Pn — €n, Pn + €n) where\n",
      "e2 = {2n}—'log(2/a). To test the null hypothesis that the coin is fair, that\n",
      "is, p= 1/2, we can reject the null hypothesis when |Py — $|//Pn( — Pa)/n\n",
      "is larger than 2. The justification for these procedures will be the subject of\n",
      "the next few chapters.\n",
      "\n",
      "8.1 Point Estimation\n",
      "\n",
      "Let X),...,X, be n uD data point from some distribution F. A point\n",
      "estimator 0, of a parameter 0 is some function of X,,..., Xn:\n",
      "\n",
      "9n = g(X1,---,Xn).\n",
      "\n",
      "We define\n",
      "\n",
      "bias (0,,) = Ey(0,) — 0 (8.1)\n",
      "to be the bias of 6. We say that 6, is unbiased if E(6,) = 6. Unbiased-\n",
      "ness used to receive much attention but these days it is not considered very\n",
      "important; many of the estimators we will use are biased. A point estimator\n",
      "6, of a parameter @ is consistent if 0,74 6. Consistency is a reasonable\n",
      "requirement for estimators. The distribution of 0, is called the sampling\n",
      "distribution. The standard deviation of 6, is called the standard error,\n",
      "denoted by se:\n",
      "\n",
      "se = se (6,) = V(6n). (8.2)\n",
      "Often, it is not possible to compute the standard error but usually we can\n",
      "estimate the standard error. The estimated standard error is denoted by se.\n",
      "\n",
      "Example 8.2 Let X,,...,Xn ~ Bernoulli(p) and let Py = n-' 30; Xi. Then\n",
      "E(,) = n-! 0, E(X;) = p 80 Pn is unbiased. The standard error is se =\n",
      "\n",
      "JV(f.) = Vp — p)/n. The estimated standard error is sé = \\/p(1 — p)/n.\n",
      "|\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-533.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9F08>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.1. POINT ESTIMATION 115\n",
      "\n",
      "The quality of a point estimate is sometimes assessed by the mean\n",
      "squared error, or MSE, defined by\n",
      "\n",
      "MSE = E,(6,, — 0)?.\n",
      "\n",
      "Recall that E,(-) refers to expectation with respect to the distribution\n",
      "n\n",
      "S(e1,---52nj 0) =] Ses 8)\n",
      "i=1\n",
      "\n",
      "that generated the data. It does not mean we are averaging over a density\n",
      "for 0.\n",
      "\n",
      "Theorem 8.3 The MSE can be written as\n",
      "\n",
      " \n",
      "\n",
      "MSE = bias (0)? + Vo(On). (8.3)\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "PROOF. Let 8, = Ey(0,). Then\n",
      "\n",
      "Ey (0. — Dn + On — 8)?\n",
      "\n",
      "Ey(6, — On)” + 20n — 0)\"Ey (0, — On) + EG, — 8)?\n",
      "(Gn — 0)? + Eo(O, — On)?\n",
      "\n",
      "= bias? +V(,).\n",
      "\n",
      "E, (0, — 6)?\n",
      "\n",
      "Theorem 8.4 If bias + 0 and se — 0 as n— oo then 0, is consistent, that\n",
      "is, 0,40.\n",
      "\n",
      "Proor. If bias — 0 and se — 0 then, by Theorem 8.3, MSE — 0. It\n",
      "follows that 4,\" 6. (Recall definition 6.3.) The result follows from part (b)\n",
      "of Theorem 6.4. Hi\n",
      "\n",
      "Example 8.5 Returning to the coin flipping example, we have that E,(p,) = p\n",
      "\n",
      "so that bias = p— p= 0 and se = ,/p(1—p)/n > 0. Hence, Pr—p, that\n",
      "is, Pn is a consistent estimator. Wi\n",
      "\n",
      " \n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-534.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D88>\n",
      "116 CHAPTER 8. FUNDAMENTAL CONCEPTS IN INFERENCE\n",
      "\n",
      "Consider n data pairs (X1,Yi),---, (Xn, Yn) from a regression model\n",
      "Y= f*)+G\n",
      "\n",
      "where E(e;) = 0 and f is an unknown regresion function. An estimate of f\n",
      "is\n",
      "fn(a) = average of all Y; such that |X;— 2| <6\n",
      "\n",
      "for some 6 > 0. Notice that for every « we have an unknown parameter\n",
      "f(x) and an estimator f;,(z). The integrated mean squared error IMSE\n",
      "is defined by\n",
      "\n",
      "IMSE = [ MSEC) Pax\n",
      "[ (vias acca? + vale)? a\n",
      "[ries Fu(oyrac+ f va(a)yPae\n",
      "\n",
      "= integrated bias? + integrated variance. (8.4)\n",
      "\n",
      "Small values of 6 cause small bias but large variance. Large values of 6 cause\n",
      "large bias but small variance. This leads to the bias-variance tradeoff that\n",
      "we will discuss in detail when we cover nonparametric regression.\n",
      "In many cases, a point estimate has an asymptoptic Normal distribution,\n",
      "that is,\n",
      "6, ~ N(0,se”). (8.5)\n",
      "\n",
      "This is a fact we will often make use of.\n",
      "\n",
      "8.2 Confidence Sets\n",
      "\n",
      "A 1—a confidence interval for a parameter @ is an interval C,, = (a,b)\n",
      "where a = a(Xj,...,X,) and 6 = b(X,,...,Xn) are functions of the data\n",
      "such that\n",
      "\n",
      "Po(0EC,) >1—a, foralldeoO. (8.6)\n",
      "\n",
      "In words, (a,6) traps @ with probability 1 — a. We call 1 — a the coverage\n",
      "of the confidence interval. Commonly, people use 95 per cent confidence\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-535.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F92C8>\n",
      "8.2. CONFIDENCE SETS 117\n",
      "\n",
      "intervals which corresponds to choosing a = 0.05. Note: C, is random\n",
      "and @ is fixed! If 6 is a vector then we use a confidence set (such a sphere\n",
      "or an ellipse) instead of an interval.\n",
      "\n",
      "Warning! There is much confusion about how to interpret a confidence\n",
      "interval. A confidence interval is not a probability statement about @ since\n",
      "0 is a fixed quantity, not a random variable. Some texts interpret confidence\n",
      "intervals as follows: if I repeat the experiment over and over, the interval will\n",
      "contain the parameter 95 per cent of the time. This is correct but useless since\n",
      "we rarely repeat the same experiment over and over. A better interpretation\n",
      "is this:\n",
      "\n",
      "On day 1, you collect data and construct a 95 per cent confidence\n",
      "interval for a parameter 6;. On day 2, you collect new data and\n",
      "construct a 95 per cent confidence interval for an unrelated parameter\n",
      "62. On day 3, you collect new data and construct a 95 per cent\n",
      "confidence interval for an unrelated parameter 63. You continue this\n",
      "way constructing confidence intervals for a sequence of unrelated\n",
      "parameters 61, 02,... Then 95 per cent your intervals will trap the\n",
      "true parameter value. There is no need to introduce the idea of\n",
      "repeating the same experiment over and over.\n",
      "\n",
      "Example 8.6 Every day, newspapers report opinion polls. For example, they\n",
      "might say that “83 per cent of the population favor arming pilots with guns.”\n",
      "Usually, you will see a statement like “this poll is accurate to within 4 points\n",
      "95 per cent of the time.” They are saying that 83+ 4 is a 95 per cent\n",
      "confidence interval for the true but unknown proportion p of people who favor\n",
      "arming pilots with guns. If you form a confidence interval this way everyday\n",
      "for the rest of your life, 95 per cent of your intervals will contain the true\n",
      "parameter. This is true even though you are estimating a different quantity\n",
      "(a different poll question) every day.\n",
      "\n",
      "Later, we will discuss Bayesian methods in which we treat 0 as if it were\n",
      "ayvandom variable and we do make probability statements about 0. In par-\n",
      "ticular, we will make statements like “the probability that 6 is C,, given the\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-536.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9C48>\n",
      "118 CHAPTER 8. FUNDAMENTAL CONCEPTS IN INFERENCE\n",
      "\n",
      "data, is 95 per cent.” However, these Bayesian intervals refer to degree-of-\n",
      "belief probabilities. These Bayesian intervals will not, in general, trap the\n",
      "parameter 95 per cent of the time.\n",
      "\n",
      "Example 8.7 In the coin flipping setting, let C, = (Pn — €n, Pn + €n) where\n",
      "e = log(2/a)/(2n). From Hoeffding’s inequality (5.4) it follows that\n",
      "\n",
      "P(p€ Cn) >1—a\n",
      "for every p. Hence, C, is a1—a confidence interval. Ml\n",
      "\n",
      "As mentioned earlier, point estimators often have a limiting Normal dis-\n",
      "tribution, meaning that equation (8.5) holds, that is, 0, ~ N(0,%”). In this\n",
      "case we can construct (approximate) confidence intervals as follows.\n",
      "\n",
      "Theorem 8.8 (Normal-based Confidence Interval.) Suppose that 0, = N(0,86”).\n",
      "Let ® be the CDF of a standard Normal and let zq/. = ®-'(1 — (a/2)), that\n",
      "is, P(Z > Zaj2) = a/2 and P(—zap, < Z < %/2) =1—a where Z ~ N(0,1).\n",
      "Let\n",
      "Cn = (On = %a/2B, On + %aj2 8). (8.7)\n",
      "Then\n",
      "P9(8E C,) 2 1l-a. (8.8)\n",
      "\n",
      "Proor. Let Z, = (0, — 0)/S. By assumption Z, ~» Z where Z ~\n",
      "N(0, 1). Hence,\n",
      "\n",
      "Po9€C,) = Po (br — zona <0 <b, + zap 8)\n",
      "\n",
      "6, -0\n",
      "= PB (-sn < “3 <‘n]\n",
      ">\n",
      "\n",
      "P (—zaj2 < Z < Zay2)\n",
      "= l-a @\n",
      "\n",
      " \n",
      "\n",
      "For 95 per cent confidence intervals, a = 0.05 and zq/2 = 1.96 © 2 leading\n",
      "to the approximate 95 per cent confidence interval 0, + 2S€. We will discuss\n",
      "the construction of confidence intervals in more generality in the rest of the\n",
      "book.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-537.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9248>\n",
      "8.3. HYPOTHESIS TESTING 119\n",
      "\n",
      "Example 8.9 Let X1,...,Xn ~ Bernoulli(p) and let p, = n7' S77, Xi. Then\n",
      "V(Pn) = 0? Ye VX) = 0? Ve pC — p) = np — p) = pl -\n",
      "p)/n. Hence, se = ,/p(1—p)/n and s = \\/p,(1—p,)/n. By the Central\n",
      "Limit Theorem, p, ~ N(p, Se). Therefore, an approximate 1 — a confidence\n",
      "interval is\n",
      "\n",
      "on ms Blo\n",
      "\n",
      "P+ Zqpse = P+ Zq/2 ae\n",
      "Compare this with the confidence interval in the previous example. The\n",
      "Normal-based interval is shorter but it only has approximately (large sam-\n",
      "ple) correct coverage.\n",
      "\n",
      "8.3. Hypothesis Testing\n",
      "\n",
      "Your colleague has written an algorithm for simulating flips of a fair coin.\n",
      "‘You are suspicious of your colleague’s programming skills. Let X1,...,Xn\n",
      "denote n simulated coin flips and let p = P(X = 1). Let Ho denote the\n",
      "hypothesis that the simulator is correct and let H; denote the hypothesis\n",
      "that the simulator is not fair. Ho is called the null hypothesis and H, is\n",
      "called the alternative hypothesis. We can write the hypotheses as\n",
      "\n",
      "Hy: p=1/2 versus H,:p#1/2.\n",
      "\n",
      "If we get 100 simulated flips and all 100 are 1, we might reasonably take this\n",
      "as evidence against Hp. This is an example of hypothesis testing. Later\n",
      "we will see that common practice is to reject Hy when\n",
      "[Pn = 3\n",
      "a\n",
      "\n",
      "an\n",
      "\n",
      " \n",
      "\n",
      "> 2.\n",
      "\n",
      "We will discuss this and other hypothesis tests in detail in a later chapter.\n",
      "\n",
      "8.4 Technical Appendix\n",
      "\n",
      "Our definition of confidence interval requires that Ps(9 € C,) > 1—a\n",
      "for all 9 € ©. An pointwise asymptotic confidence interval requires that\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n",
      "out-538.jpg\n",
      "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1275x1650 at 0x243A80F9D48>\n",
      "120 CHAPTER 8. FUNDAMENTAL CONCEPTS IN INFERENCE\n",
      "\n",
      "lim inf, ,.0 P9(@ € Cn) > 1— a for all 0 € ©. An uniform asymptotic\n",
      "confidence interval requres that lim inf,_,.. infyeo Po(8 € Cr) > 1—a. The\n",
      "approximate Normal-based interval is a pointwise asymptotic confidence in-\n",
      "terval. In general, it might not be a uniform asymptotic confidence interval.\n",
      "\f",
      "\n",
      "-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import pytesseract\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Path is given for for 64 bit installer\n",
    "pytesseract.pytesseract.tesseract_cmd = \"C:/Program Files/Tesseract-OCR/tesseract.exe\"\n",
    "\n",
    "f = []\n",
    "t = []\n",
    "input_dir = r'C:/Users/jkl/Desktop/FreeL/yele/Images/'\n",
    "\n",
    "for root, dirs, filenames in os.walk(input_dir):\n",
    "    for filename in filenames:\n",
    "        try:\n",
    "            print(filename)\n",
    "            f.append(filename)\n",
    "            #print(f)\n",
    "            img = Image.open(input_dir + filename )\n",
    "            print(img)\n",
    "            text = pytesseract.image_to_string(img, lang = 'eng')\n",
    "            t.append(text)\n",
    "            print(text)\n",
    "            print('-='*20)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "\n",
    "df = pd.DataFrame(list(zip(f, t)),columns=['file_Name','Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_Name</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>out-001.jpg</td>\n",
       "      <td>All of Statistics: A Concise Course in\\nStatis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>out-002.jpg</td>\n",
       "      <td>14.\\n\\n15.\\n\\n16.\\n\\n17.\\n\\n18.\\n\\n19.\\n\\n20.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>out-003.jpg</td>\n",
       "      <td>Chapter 1\\n\\nIntroduction\\n\\nThe goal of this ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>out-004.jpg</td>\n",
       "      <td>12\\n\\nCHAPTER 1. INTRODUCTION\\n\\nral nets, boo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>out-005.jpg</td>\n",
       "      <td>13\\n\\nwould be a crime if they did not see any...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>out-534.jpg</td>\n",
       "      <td>116 CHAPTER 8. FUNDAMENTAL CONCEPTS IN INFEREN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>534</th>\n",
       "      <td>out-535.jpg</td>\n",
       "      <td>8.2. CONFIDENCE SETS 117\\n\\nintervals which co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>535</th>\n",
       "      <td>out-536.jpg</td>\n",
       "      <td>118 CHAPTER 8. FUNDAMENTAL CONCEPTS IN INFEREN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>out-537.jpg</td>\n",
       "      <td>8.3. HYPOTHESIS TESTING 119\\n\\nExample 8.9 Let...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>out-538.jpg</td>\n",
       "      <td>120 CHAPTER 8. FUNDAMENTAL CONCEPTS IN INFEREN...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>538 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       file_Name                                               Text\n",
       "0    out-001.jpg  All of Statistics: A Concise Course in\\nStatis...\n",
       "1    out-002.jpg  14.\\n\\n15.\\n\\n16.\\n\\n17.\\n\\n18.\\n\\n19.\\n\\n20.\\...\n",
       "2    out-003.jpg  Chapter 1\\n\\nIntroduction\\n\\nThe goal of this ...\n",
       "3    out-004.jpg  12\\n\\nCHAPTER 1. INTRODUCTION\\n\\nral nets, boo...\n",
       "4    out-005.jpg  13\\n\\nwould be a crime if they did not see any...\n",
       "..           ...                                                ...\n",
       "533  out-534.jpg  116 CHAPTER 8. FUNDAMENTAL CONCEPTS IN INFEREN...\n",
       "534  out-535.jpg  8.2. CONFIDENCE SETS 117\\n\\nintervals which co...\n",
       "535  out-536.jpg  118 CHAPTER 8. FUNDAMENTAL CONCEPTS IN INFEREN...\n",
       "536  out-537.jpg  8.3. HYPOTHESIS TESTING 119\\n\\nExample 8.9 Let...\n",
       "537  out-538.jpg  120 CHAPTER 8. FUNDAMENTAL CONCEPTS IN INFEREN...\n",
       "\n",
       "[538 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
